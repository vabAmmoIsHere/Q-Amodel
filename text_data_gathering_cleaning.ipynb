{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_data_gathering_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "12hFnNmz6fU4z-55YrDZ2PANuuolZFrKX",
      "authorship_tag": "ABX9TyMgGY1q+BZNgON6gDcSmVot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vabAmmoIsHere/Q-Amodel/blob/main/text_data_gathering_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YVuTUyMOdjx"
      },
      "source": [
        "GATHERING DATA from Data Science books\n",
        "\n",
        "\"https://www.learndatasci.com/free-data-science-books/\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HYXkQfGOaHo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "1249241e-6906-42ea-f618-56732a72f3eb"
      },
      "source": [
        "\n",
        "! pip install wget\n",
        "! pip install asyncio\n",
        "\n",
        "import wget\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=b3ae8723cb185ace57be88bb2d8b999312c2c22a8d647af68271a53fa46690ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting asyncio\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 3.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: asyncio\n",
            "Successfully installed asyncio-3.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iK-dO29Qm6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbc0cf8-aecb-404b-97db-8432a023e654"
      },
      "source": [
        "# creating fake header and using proxy\n",
        "! pip install fake-headers\n",
        "from fake_headers import Headers\n",
        "import random\n",
        "\n",
        "def headers():\n",
        "  os = random.choice(['win', 'mac', 'lin'])\n",
        "  browser = random.choice(['chrome', 'firefox', 'opera'])\n",
        "  return Headers(os=os, browser=browser, headers=True).generate()\n",
        "\n",
        "header = headers()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake-headers\n",
            "  Downloading fake_headers-1.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (from fake-headers) (1.0.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from fake-headers) (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->fake-headers) (4.6.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib->fake-headers) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->fake-headers) (0.5.1)\n",
            "Installing collected packages: fake-headers\n",
            "Successfully installed fake-headers-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYqJhq91P_hn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C50t8ts_RgYG"
      },
      "source": [
        "# crawling the web page to download the pdf books\n",
        "# I have changed the api key, create your own account crawlera/zyte and share the cert location (or SSLERROR)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "proxies = \"http://ea3b34d297754cff9f0c1e36bfc5d367:@proxy.crawlera.com:8011/\"  \n",
        "def get_pdf_recursion(url):\n",
        "  \n",
        "  count = 0\n",
        "  try:\n",
        "    response = requests.get(url=url, headers=header, proxies={\"http\": proxies, \"https\": proxies}, verify= \"/zyte-proxy-ca_NEW.crt\")\n",
        "    html = response.content\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    url_tags = soup.find_all(\"a\")\n",
        "    for tag in url_tags:\n",
        "      if \"pdf\" in tag.get(\"href\"):\n",
        "        try:\n",
        "          wget.download(tag.get(\"href\"))\n",
        "          count+=1\n",
        "          if count >=10:\n",
        "            break\n",
        "        except Exception as e:\n",
        "          e\n",
        "      elif (tag.get_text().strip() == \"View Free Book\" and \"pdf\" not in tag.get(\"href\")):\n",
        "        url = tag.get(\"href\")\n",
        "        try:\n",
        "          get_pdf_recursion(url)\n",
        "        except Exception as e:\n",
        "          e\n",
        "  except Exception as e:\n",
        "    e        \n",
        "  \n",
        "# executing the script\n",
        "url = \"https://www.learndatasci.com/free-data-science-books\"\n",
        "get_pdf_recursion(url)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxBL7PLWVw8R"
      },
      "source": [
        "################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTmCI53R0s33"
      },
      "source": [
        "Cleaning the data is the second phase in your data modeling and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2YoACxTcjFc"
      },
      "source": [
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixlhmih3cfOG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHvLfGsRczd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd29f383-14a6-4bf2-d351-675d766eda08"
      },
      "source": [
        "# read all the pdf files stored in the pdf directory\n",
        "\n",
        "import os\n",
        "! pip install tika\n",
        "pdfs = os.listdir(r'/content/pdf_files')\n",
        "print(pdfs)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "['KB_neural_data_mining.pdf', 'disruptive-possibilities.pdf', 'real-world-active-learning.pdf', 'ciml-v0_9-all.pdf', '240415.pdf', 'gsl_stats.pdf', 'SzeliskiBookDraft_20210930.pdf', '0904.3664.pdf', 'RLAlgsInMDPs.pdf', 'RW.pdf', 'artificial-intelligence-modern-approach.9780131038059.25368.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1H4b0VOwlqZ"
      },
      "source": [
        "# importing packages for data cleaning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stop"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uzDWGD9Db8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cc6969-aa60-4658-cf8a-4c47d444f217"
      },
      "source": [
        "# parsing pdf books\n",
        "from tika import parser\n",
        "\n",
        "data_list = []\n",
        "for pdf in pdfs:\n",
        "  if pdf.endswith(\".pdf\"):\n",
        "    path_with_file = os.path.join(r'/content/pdf_files/', pdf)\n",
        "    print(path_with_file)\n",
        "    with open(path_with_file, \"rb\") as f:\n",
        "      file_data = parser.from_file(f)\n",
        "      text = file_data['content']\n",
        "      data_list.append(text)\n",
        "\n",
        "# created dataframe object \n",
        "df = pd.DataFrame(data_list)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pdf_files/KB_neural_data_mining.pdf\n",
            "/content/pdf_files/disruptive-possibilities.pdf\n",
            "/content/pdf_files/real-world-active-learning.pdf\n",
            "/content/pdf_files/ciml-v0_9-all.pdf\n",
            "/content/pdf_files/240415.pdf\n",
            "/content/pdf_files/gsl_stats.pdf\n",
            "/content/pdf_files/SzeliskiBookDraft_20210930.pdf\n",
            "/content/pdf_files/0904.3664.pdf\n",
            "/content/pdf_files/RLAlgsInMDPs.pdf\n",
            "/content/pdf_files/RW.pdf\n",
            "/content/pdf_files/artificial-intelligence-modern-approach.9780131038059.25368.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5XyUApOYia"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2WmexB20gds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154e3440-8243-4992-c670-e8a2447af7d2"
      },
      "source": [
        "# Viewing data in data frame\n",
        "df[0].sample(2)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...\n",
              "5    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVsVkSNJ06o8"
      },
      "source": [
        "# few more stats that are important in nlp \n",
        "# word_count: \n",
        "df['word_count'] = df[0].apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgAJGYkn1-Tv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "27814b7c-fd0e-42b7-e05f-c4473fad469a"
      },
      "source": [
        "# let's view a randomly selected sample dataset and the word_count\n",
        "df.sample(10)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>34739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>118816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>315097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>76311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>39528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>26064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>6011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>447365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>164032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  word_count\n",
              "0   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...       34739\n",
              "9   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      118816\n",
              "4   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      315097\n",
              "3   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...       76311\n",
              "8   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...       39528\n",
              "1   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...       26064\n",
              "2   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...        6011\n",
              "10  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      447365\n",
              "6   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...           7\n",
              "5   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      164032"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WESXHUMG5Ehu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a114543-57f5-40c2-e2b0-8fd0aea2ca91"
      },
      "source": [
        "# maximum words in a cell (statistics chapter) and # minimum words in a cell \n",
        "max = df['word_count'].max()\n",
        "min = df['word_count'].min()\n",
        "print('maximun words in a chapter: ', max, '\\n\\nminimum words in a chapter: ', min)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximun words in a chapter:  447365 \n",
            "\n",
            "minimum words in a chapter:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RriJVpy5Q7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dbe8a7-e920-4c79-c93a-f8d7c4b54b7d"
      },
      "source": [
        "# char counts in each cell\n",
        "def char_count(text: str):\n",
        "  s = text.split()\n",
        "  new_text = ' '.join(s)\n",
        "  return len(new_text)\n",
        "\n",
        "print('no of characters: ', char_count('random writing to count characters in a text'))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of characters:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axzhLOcCKhUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "28de0796-f389-4cd3-fcd5-f7be8071537c"
      },
      "source": [
        "df['char_count'] = df[0].apply(lambda x: char_count(x))\n",
        "df.sample(2)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  word_count  char_count\n",
              "9  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      118816      691520\n",
              "5  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...      164032      956177"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzSTDc29Ba0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-YYzaD5SUyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "1190718b-8463-4177-c5c3-88f057d0377f"
      },
      "source": [
        "df['avg_word_length'] = df['char_count']/df['word_count']\n",
        "df.sample(4)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...  avg_word_length\n",
              "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...         5.394675\n",
              "6  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...         8.714286\n",
              "7  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...         5.214130\n",
              "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...         6.353186\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWqUCKe9S0T1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d2c4ad-ffd8-4e1d-ceb1-852a1190c80b"
      },
      "source": [
        "# stop words\n",
        "# calling stopword function from spacy(imported earlier)-stopwords identified by spacy\n",
        "print(stop) \n",
        "print(len(stop))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seems', 'each', 'off', 'its', 'before', 'this', 'serious', 'my', 'them', 'sometime', 'indeed', 'others', 'put', 'what', 'whereas', \"'ve\", 'have', 'above', 'give', 'ourselves', 'will', 'by', 'behind', 'any', 'rather', 'down', 'did', 'empty', 'too', 'afterwards', 'not', '‘ll', 'doing', 'whereafter', 'therein', 'hers', 'much', 'forty', 're', 'onto', 'nevertheless', 'back', 'ours', 'either', 'amount', 'while', 'until', 'side', 'hereafter', 'n’t', 'therefore', '’s', 'top', 'everywhere', 'you', 'five', 'every', 'keep', 'perhaps', 'may', 'somehow', 'himself', 'full', 'an', 'eleven', 'should', 'along', 'about', 'me', 'when', 'get', 'be', '’d', 'after', 'both', 'can', 'say', 'to', 'beyond', 'on', 'been', 'unless', 'per', 'one', 'due', 'itself', 'through', 'it', 'no', 'just', 'anyone', 'mine', 'became', 'really', 'n‘t', 'else', 'their', 'but', '’re', 'myself', 'besides', 'he', 'because', 'across', 'thus', 'seeming', 'formerly', 'two', 'someone', 'do', 'move', 'was', 'that', 'latterly', 'within', 'of', 'ten', 'up', 'already', 'everyone', 'fifteen', 'other', 'would', 'eight', 'might', 'among', 'become', 'together', 'namely', 'none', 'same', 'twenty', 'mostly', 'has', '’m', 'again', 'via', 'whence', '‘d', 'once', 'could', 'even', 'whenever', 'i', 'in', 'show', 'noone', 'all', 'latter', 'own', 'over', 'his', 'who', 'there', 'several', 'former', 'between', 'she', 'ever', 'whoever', 'various', 'another', 'made', 'our', 'thru', 'are', 'anyhow', \"n't\", 'nothing', 'yet', 'must', 'always', 'which', 'some', 'at', 'nobody', 'hundred', 'wherever', 'part', 'sixty', 'themselves', 'take', 'becoming', 'anything', 'many', 'never', 'used', 'everything', 'whereupon', 'further', 'wherein', 'yourselves', 'thereupon', 'three', 'well', 'they', 'then', 'against', 'neither', 'than', 'meanwhile', 'whither', 'third', 'whatever', 'alone', 'quite', '‘m', 'from', 'first', 'below', 'seem', 'if', 'is', 'throughout', 'sometimes', 'being', 'a', 'moreover', 'toward', 'herself', 'make', 'upon', 'how', 'go', 'since', 'using', 'less', 'beforehand', 'name', 'still', 'least', 'for', 'hence', 'thereafter', 'yourself', 'the', 'otherwise', 'why', 'as', 'thereby', 'cannot', 'last', 'only', 'more', 'around', 'seemed', 'without', 'am', 'also', 'please', 'something', 'we', 'anywhere', 'thence', 'nor', 'so', 'whole', 'call', 'beside', 'elsewhere', 'into', 'whether', 'yours', 'however', 'except', 'regarding', 'herein', 'does', 'her', 'bottom', 'four', 'very', \"'d\", 'had', 'though', '‘s', 'anyway', 'these', 'almost', 'ca', \"'s\", 'us', 'and', 'often', 'twelve', 'although', 'whereby', 'nine', '‘ve', 'out', 'with', 'nowhere', 'where', 'hereupon', 'hereby', 'towards', \"'re\", '’ve', 'whose', 'becomes', 'enough', 'were', 'or', 'those', \"'ll\", \"'m\", 'few', 'your', '‘re', 'whom', '’ll', 'such', 'during', 'see', 'fifty', 'him', 'next', 'now', 'amongst', 'here', 'somewhere', 'under', 'six', 'most', 'front', 'done'}\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGhSmSuiTp1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "65564d33-3810-42ad-c2ba-0e7274ac4862"
      },
      "source": [
        "# check for no. stopwords in our text\n",
        "df['stop_words_len'] = df[0].apply(lambda x: len([t for t in x.split() if t in stop]))\n",
        "df.sample(10)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>34739</td>\n",
              "      <td>213100</td>\n",
              "      <td>6.134316</td>\n",
              "      <td>5810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>26064</td>\n",
              "      <td>161348</td>\n",
              "      <td>6.190454</td>\n",
              "      <td>10598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>76311</td>\n",
              "      <td>426100</td>\n",
              "      <td>5.583730</td>\n",
              "      <td>29439</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...  stop_words_len\n",
              "7  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           12083\n",
              "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...            5810\n",
              "8  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           12329\n",
              "5  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           62575\n",
              "9  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           35875\n",
              "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           10598\n",
              "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...            2356\n",
              "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           86801\n",
              "6  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...               1\n",
              "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...           29439\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFR9I9-0l64x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "25c9541b-7678-48c7-d513-a58c160b0441"
      },
      "source": [
        "df['no_of_digits'] = df[0].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
        "df.sample(2)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "10  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...          7361\n",
              "2   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  ...            44\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cjYGdddox9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c168be9f-fdf6-4c89-f4e0-ac8d97631dbb"
      },
      "source": [
        "# remove special characters\n",
        "import re\n",
        "df[0] = df[0].apply(lambda x: re.sub(r'[^\\w ]+', \" \", x))\n",
        "df.sample(5)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms for Reinforcement Learning Draft o...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March 24  2009 Modeling with Data g...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox   SzeliskiBookDraft_20210930 pdf   Si...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction to Machine Learning 67577   Fall...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning and Machine Learn...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...  no_of_digits\n",
              "8   Algorithms for Reinforcement Learning Draft o...  ...           381\n",
              "5   gsl_stats March 24  2009 Modeling with Data g...  ...          4366\n",
              "6   Dropbox   SzeliskiBookDraft_20210930 pdf   Si...  ...             0\n",
              "7   Introduction to Machine Learning 67577   Fall...  ...           705\n",
              "4   My title Bayesian Reasoning and Machine Learn...  ...          9762\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajkBeh2sp53Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "106b1006-7ade-4ad7-ff5c-fde744ee5f94"
      },
      "source": [
        "# removing extra spaces\n",
        "df[0] = df[0].apply(lambda x: ' '.join(x.split()))\n",
        "df.sample(10)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction to Machine Learning 67577 Fall 20...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gaussian Processes for Machine Learning C E Ra...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "      <td>3962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Course in Machine Learning A Course in Machi...</td>\n",
              "      <td>76311</td>\n",
              "      <td>426100</td>\n",
              "      <td>5.583730</td>\n",
              "      <td>29439</td>\n",
              "      <td>1198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning and Machine Learni...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms for Reinforcement Learning Draft of...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>real world active learning Make Data Work stra...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March 24 2009 Modeling with Data gsl...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disruptive Possibilities http oreil ly 1T0KbBh...</td>\n",
              "      <td>26064</td>\n",
              "      <td>161348</td>\n",
              "      <td>6.190454</td>\n",
              "      <td>10598</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...             0\n",
              "7   Introduction to Machine Learning 67577 Fall 20...  ...           705\n",
              "9   Gaussian Processes for Machine Learning C E Ra...  ...          3962\n",
              "3   A Course in Machine Learning A Course in Machi...  ...          1198\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          7361\n",
              "4   My title Bayesian Reasoning and Machine Learni...  ...          9762\n",
              "8   Algorithms for Reinforcement Learning Draft of...  ...           381\n",
              "2   real world active learning Make Data Work stra...  ...            44\n",
              "5   gsl_stats March 24 2009 Modeling with Data gsl...  ...          4366\n",
              "1   Disruptive Possibilities http oreil ly 1T0KbBh...  ...           217\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmFwVxkNtpqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "0c475226-d932-4ff9-8355-e6722168bec5"
      },
      "source": [
        "! pip install bs4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "df[0] = df[0].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text().strip())\n",
        "df.sample(10)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning and Machine Learni...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction to Machine Learning 67577 Fall 20...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB Data Mining with Python sources KB Neural D...</td>\n",
              "      <td>34739</td>\n",
              "      <td>213100</td>\n",
              "      <td>6.134316</td>\n",
              "      <td>5810</td>\n",
              "      <td>4555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gaussian Processes for Machine Learning C E Ra...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "      <td>3962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms for Reinforcement Learning Draft of...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>real world active learning Make Data Work stra...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March 24 2009 Modeling with Data gsl...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disruptive Possibilities http oreil ly 1T0KbBh...</td>\n",
              "      <td>26064</td>\n",
              "      <td>161348</td>\n",
              "      <td>6.190454</td>\n",
              "      <td>10598</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "4   My title Bayesian Reasoning and Machine Learni...  ...          9762\n",
              "7   Introduction to Machine Learning 67577 Fall 20...  ...           705\n",
              "0   KB Data Mining with Python sources KB Neural D...  ...          4555\n",
              "9   Gaussian Processes for Machine Learning C E Ra...  ...          3962\n",
              "8   Algorithms for Reinforcement Learning Draft of...  ...           381\n",
              "2   real world active learning Make Data Work stra...  ...            44\n",
              "5   gsl_stats March 24 2009 Modeling with Data gsl...  ...          4366\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...             0\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          7361\n",
              "1   Disruptive Possibilities http oreil ly 1T0KbBh...  ...           217\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbh9CuLCg6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "d9191a7e-f881-4f39-b8be-79907c2f01e1"
      },
      "source": [
        "# remove accented characters\n",
        "import unicodedata\n",
        "\n",
        "def remove_accented_data(text: str):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "df[0] = df[0].apply(lambda x: remove_accented_data(x))\n",
        "df.sample(5)\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms for Reinforcement Learning Draft of...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>real world active learning Make Data Work stra...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gaussian Processes for Machine Learning C E Ra...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "      <td>3962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...             0\n",
              "8   Algorithms for Reinforcement Learning Draft of...  ...           381\n",
              "2   real world active learning Make Data Work stra...  ...            44\n",
              "9   Gaussian Processes for Machine Learning C E Ra...  ...          3962\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          7361\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il1eiYE-JLMO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "688d5282-cb9e-4fc8-a1bf-4856c99a7c1e"
      },
      "source": [
        "# removing stop words\n",
        "df[0] = df[0].apply(lambda x: ' '.join([t for t in x.split() if t not in stop]))\n",
        "df.sample(10)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Course Machine Learning A Course Machine Lea...</td>\n",
              "      <td>76311</td>\n",
              "      <td>426100</td>\n",
              "      <td>5.583730</td>\n",
              "      <td>29439</td>\n",
              "      <td>1198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March 24 2009 Modeling Data gsl_stat...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>real world active learning Make Data Work stra...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning Machine Learning D...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms Reinforcement Learning Draft lectur...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gaussian Processes Machine Learning C E Rasmus...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "      <td>3962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction Machine Learning 67577 Fall 2008 ...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB Data Mining Python sources KB Neural Data M...</td>\n",
              "      <td>34739</td>\n",
              "      <td>213100</td>\n",
              "      <td>6.134316</td>\n",
              "      <td>5810</td>\n",
              "      <td>4555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "3   A Course Machine Learning A Course Machine Lea...  ...          1198\n",
              "5   gsl_stats March 24 2009 Modeling Data gsl_stat...  ...          4366\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...             0\n",
              "2   real world active learning Make Data Work stra...  ...            44\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          7361\n",
              "4   My title Bayesian Reasoning Machine Learning D...  ...          9762\n",
              "8   Algorithms Reinforcement Learning Draft lectur...  ...           381\n",
              "9   Gaussian Processes Machine Learning C E Rasmus...  ...          3962\n",
              "7   Introduction Machine Learning 67577 Fall 2008 ...  ...           705\n",
              "0   KB Data Mining Python sources KB Neural Data M...  ...          4555\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "G7NPiIvyZiib",
        "outputId": "ffb19792-0dce-41c4-d37b-d476ec6d4575"
      },
      "source": [
        "# Removing digits\n",
        "\n",
        "df[0]=df[0].apply(lambda x: ' '.join([t for t in x.split() if t.isdigit() is False]))\n",
        "df.sample(10)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB Data Mining Python sources KB Neural Data M...</td>\n",
              "      <td>34739</td>\n",
              "      <td>213100</td>\n",
              "      <td>6.134316</td>\n",
              "      <td>5810</td>\n",
              "      <td>4555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms Reinforcement Learning Draft lectur...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>real world active learning Make Data Work stra...</td>\n",
              "      <td>6011</td>\n",
              "      <td>38189</td>\n",
              "      <td>6.353186</td>\n",
              "      <td>2356</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning Machine Learning D...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disruptive Possibilities http oreil ly 1T0KbBh...</td>\n",
              "      <td>26064</td>\n",
              "      <td>161348</td>\n",
              "      <td>6.190454</td>\n",
              "      <td>10598</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March Modeling Data gsl_stats March ...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Course Machine Learning A Course Machine Lea...</td>\n",
              "      <td>76311</td>\n",
              "      <td>426100</td>\n",
              "      <td>5.583730</td>\n",
              "      <td>29439</td>\n",
              "      <td>1198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction Machine Learning Fall Amnon Shash...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  no_of_digits\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          7361\n",
              "0   KB Data Mining Python sources KB Neural Data M...  ...          4555\n",
              "8   Algorithms Reinforcement Learning Draft lectur...  ...           381\n",
              "2   real world active learning Make Data Work stra...  ...            44\n",
              "4   My title Bayesian Reasoning Machine Learning D...  ...          9762\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...             0\n",
              "1   Disruptive Possibilities http oreil ly 1T0KbBh...  ...           217\n",
              "5   gsl_stats March Modeling Data gsl_stats March ...  ...          4366\n",
              "3   A Course Machine Learning A Course Machine Lea...  ...          1198\n",
              "7   Introduction Machine Learning Fall Amnon Shash...  ...           705\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtfem7iOK3jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d434e9e9-e728-48c4-e30b-50003086ffbb"
      },
      "source": [
        "! python -m spacy download \"en_core_web_lg\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=c27ed5b7eec3187efdf637712d5d3b072317086c6806ebb833b099354fb9c067\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3sk32av_/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhHUjnegNHU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3d5d70-aa0d-4a5f-fade-bbeb3a898f8d"
      },
      "source": [
        "! pip install symspellpy\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCgtO9HvumbR",
        "outputId": "930d515a-ccb6-46d3-a626-6741a4b70782"
      },
      "source": [
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import pkg_resources\n",
        "! pip install -U spacy\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _simplify_punctuation(text):\n",
        "    \"\"\"\n",
        "    This function simplifies doubled or more complex punctuation. The exception is '...'.\n",
        "    \"\"\"\n",
        "    corrected = str(text)\n",
        "    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n",
        "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
        "    return corrected\n",
        "\n",
        "def _normalize_whitespace(text):\n",
        "    \"\"\"\n",
        "    This function normalizes whitespaces, removing duplicates.\n",
        "    \"\"\"\n",
        "    corrected = str(text)\n",
        "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
        "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
        "    return corrected.strip(\" \")\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEfd5S88yGv",
        "outputId": "7e7cf503-14f0-4fc1-d335-6e3a682944a5"
      },
      "source": [
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def normalize_contractions(sentence_list):\n",
        "    contraction_list = json.loads(open('/content/english_contractions.json', 'r').read())\n",
        "    norm_sents = []\n",
        "    print(\"Normalizing contractions\")\n",
        "    for sentence in tqdm(sentence_list):\n",
        "        norm_sents.append(_normalize_contractions_text(sentence, contraction_list))\n",
        "    return norm_sents\n",
        "def _normalize_contractions_text(text, contractions):\n",
        "    \"\"\"\n",
        "    This function normalizes english contractions.\n",
        "    \"\"\"\n",
        "    new_token_list = []\n",
        "    token_list = text.split()\n",
        "    for word_pos in range(len(token_list)):\n",
        "        word = token_list[word_pos]\n",
        "        first_upper = False\n",
        "        if word[0].isupper():\n",
        "            first_upper = True\n",
        "        if word.lower() in contractions:\n",
        "            replacement = contractions[word.lower()]\n",
        "            if first_upper:\n",
        "                replacement = replacement[0].upper()+replacement[1:]\n",
        "            replacement_tokens = replacement.split()\n",
        "            if len(replacement_tokens)>1:\n",
        "                new_token_list.append(replacement_tokens[0])\n",
        "                new_token_list.append(replacement_tokens[1])\n",
        "            else:\n",
        "                new_token_list.append(replacement_tokens[0])\n",
        "        else:\n",
        "            new_token_list.append(word)\n",
        "    sentence = \" \".join(new_token_list).strip(\" \")\n",
        "    return sentence\n",
        "df[0]=df[0].apply(lambda x: ' '.join(normalize_contractions(sent_tokenize(x))))\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 75.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 114.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 504.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 42.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 19.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 92.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 81.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 25.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contractions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCJduGsi0jrU",
        "outputId": "1ae08b01-484d-473c-8b9f-5e52948c2ef4"
      },
      "source": [
        "! python -m pip install -U symspellpy"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSJ3n-_79b7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3103ad4-fc6b-47e0-c4c5-132afb8ed718"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "! python -m pip install symspellpy\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jdv8rGG23Xu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S-pSxniY0OP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f62d89-61f0-424e-9272-f47f8298814e"
      },
      "source": [
        "from itertools import islice\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell\n",
        "!curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\n",
        "!curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_bigramdictionary_en_243_342.txt\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"/content/frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"/content/frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1300k  100 1300k    0     0  4455k      0 --:--:-- --:--:-- --:--:-- 4455k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5016k  100 5016k    0     0  11.0M      0 --:--:-- --:--:-- --:--:-- 11.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smyp4fRBGdYZ",
        "outputId": "71257f07-3814-4b7c-9595-5c0dc22ac9fb"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "from symspellpy import SymSpell\n",
        "\n",
        "corpus = []\n",
        "for line in fetch_20newsgroups().data:\n",
        "    line = line.replace('\\n', ' ').replace('\\t', ' ').lower()\n",
        "    line = re.sub('[^a-z ]', ' ', line)\n",
        "    tokens = line.split(' ')\n",
        "    tokens = [token for token in tokens if len(token) > 0]\n",
        "    corpus.extend(tokens)\n",
        "corpus = Counter(corpus)\n",
        "\n",
        "corpus_dir = \"/content/\"\n",
        "corpus_file = \"frequency_dictionary_en_82_765.txt\"\n",
        "\n",
        "symspell = SymSpell()\n",
        "symspell.load_dictionary(corpus=corpus_dir+corpus_file, term_index=0, count_index=1)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B-v7JLU9oO0",
        "outputId": "d5727ab4-11ce-45a6-bad3-2deb8c7705a9"
      },
      "source": [
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "from nltk.tokenize import word_tokenize\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"/content/frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "def spell_suggestion(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  for token in tokens:\n",
        "    input_term = token  # misspelling of \"members\"\n",
        "# max edit distance per lookup\n",
        "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
        "    suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2)\n",
        "# display suggestion term, term frequency, and edit distance\n",
        "  suggs = []\n",
        "  for suggestion in suggestions:\n",
        "    suggs.append(suggestion)\n",
        "  return suggs\n",
        "for x in range(0, 11):\n",
        "  dSample = df[0][x]\n",
        "  correction = spell_suggestion(dSample)\n",
        "\n",
        "  print(f\"{x}\", correction)\n",
        "  print(f\"{x}\", dSample)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 []\n",
            "0 KB Data Mining Python sources KB Neural Data Mining Python sources Roberto Bello March Introduction The aim work present describe detail algorithms extract knowledge hidden inside data Python language allows read easily understand nature characteristics rules computing utilized opposed happens commercial applications available form running codes remain impossible modify The algorithms computing contained work minutely described documented available Python source format serve extract hidden knowledge data textual numerical kinds There examples usage underlining characteristics method execution providing comments obtained results The KB application consists programs computing KB_CAT extraction knowledge data cataloging records homogeneous groups KB_STA statistical analysis homogeneity groups groups order identify groups significant important variables characterize group KB_CLA instantaneous classification new records catalogued groups found program KB_CAT The programs written Python language easily understood commands instructions functions similar languages e g C C PHP Java Ruby programs comments explanations The KB application acquire hidden knowledge data result years study programming testing languages Clipper Fortran KB Neural Data Mining Python sources Roberto Bello Pag di Ruby C e C The cost book low considering importance included algorithms computing hard work programming subsequent repeated thorough testing input data files containing thousands records files arrived sectors interest medicine pharmacology food industry political polls sport performances market researches etc The author chosen use simple forms communication avoiding complicated mathematical formulas observing important concepts expressed easy simplistic way Albert Einstein said Do things easiest way possible simplifying The author hopes small contribution encouraging young people regain love maths hopes regain desire run programs computers avoid consume fingers surfing facebook downloading music films Internet In professional experience firstly computer fields industrial manager repeatedly realized programs mathematical contents statistics operations research considerably contributed economic results good management companies seen significant protagonist important projects Business Intelligence bad use statistics Statistics wrong people use mistakes They mistakes apply statistical aggregation instruments pieces information sources coming completely different objects situations First cut mix finally And finish expect pass judgement In way researcher political trends break opinions people interviewed mixing single answers joining crossing finally passing judgement certainties attributed virtual people interviewed created subjects exist real life certainly traceable individual people homogeneous groups people interviewed Similarly Business Intelligence makes available tools data analysis able cut data reassembling multidimensional structures peculiarities information starting positions destroyed So Business Intelligence mixes companies different sectors turnovers compatible different sizes belonging different markets etc abusing change time time variables data mining Which decisions subjects situations applied having destroyed global informative world original subjects situations To example file mammals men primates included obtain result mammals average legs Where I find mammal average legs To real statistics need conserve possible intact informative property starting data subject situation Techniques derived neural networks use analysis approach data respect informative properties starting data In fact ask user define variables cross allow occur absurd crossed values KB Neural Data Mining Python sources Roberto Bello Pag di Quite simply require maximum number groups algorithm create inserted The original informative contents destroyed subject s data processed relationship data subjects situations Retain information attributable subject create categories membership subjects situations subjects situations similar Other techniques able point significant variables aggregation aggregate values important group created Also indicate variables influential cataloging More sophisticated techniques process kind data set highlighting information file contain numbers characters related internal relations model follow data vice versa JB Benzecri Learning induction neural networks Induction important method learning living creatures One philosophers resort concept Aristotle attributed merit having discovered Socrates maintained induction fact process particular leads universal Top I Still according Aristotle senses induction rationality deduction gives guarantee truth intellectual intuition allows collect essence reality forming valid universal principles syllogistic reasoning draw coherent conclusions premises Learning life evolution linked In fact life evolution evolution learning necessary survival Learning capacity elaborate information critical intelligence Therefore critical elaboration information life Roberto Bello A simple example illustrate learns induction Let s imagine person seen containers glasses bottles jars cups vases boxes flagons jugs chalices tetra pack Without saying I real examples objects belong mentioned categories The person look smell touch weigh objects shown After having examined sufficient number objects person easily able objects categories containing objects similar favouring characteristics considered relevant When learning taken place I object shape glass different colour different material different weight obtaining cataloging object category glasses With help induction person training categories glasses handles beer mugs handles Learning allowed person recognize distinctive aspects object specific universal ignoring non relevant aspects The algorithms based neural networks particular referring map Kohonen SOM Self Organizing Map based principals illustrated example Such model neural networks demonstrates important way biological KB Neural Data Mining Python sources Roberto Bello Pag di mechanisms central nervous system studies demonstrated precise zones exist surface cranial cortex respond precise sensory muscular function Each neuron specializes responding precise stimulus continual interaction neighbouring neurons We zones reserved hearing sight muscular activity etc spacial demarcation different groups clear talk formation bubbles activity The neural networks model presented Kohonen imitates behaviour described The architecture simple network formed rectangular grate known Kohonen s layer neurons output level occupying precise position connected entry units The weight connections input output levels kept date thanks process learning connections neurons output level weights produce excitement surrounding neurons inhibition distant ones Diagram neural network The SOM networks applied practical problems able discover important properties autonomously input data especially useful process Data Mining problems cataloging The algorithms learning Kohonen network begin start phase synapse weights casual values space different neuron Subsequently weights presented network input values algorithm allows network self organize correct weights data input state equilibrium reached Kohonen s network known competitive network based principle competition neurons win remain active weight active units updated The winning unit possesses potential major activation unit active certain pattern input data vector KB Neural Data Mining Python sources Roberto Bello Pag di synapse weight similar inside pattern On basis idea possible find winning unit calculating euclidean distance input vector relevant vector synapse weight At point selected neuron corresponds minimum distance Once winning neuron determined carried automatic learning weight neuron neighbourhood based rule hebbian type In particular formula modification weights derives original rule Hebb considering increase weight infinity introduced factor forgetfulness pushing weights input vectors unit responds In way relative map characteristics input created neighbouring units respond precise stimulus admission thanks similarity synapse weights For aim necessary introduce concept function proximity determines area size r units active The dimension proximity lower number units layer Kohonen weights modified significantly higher capacity neurons differentiate acquire details increase complexity system learning According Kohonen size function proximity varied initially choosing cover units layer gradually reducing In way learning main features learning details specialized areas responding particular stimuli Representation gradual reduction proximity Once learning phase completed network able supply answers relation new input presented The property generalization derives fact neurons near selected modified The network self organize areas composed large set values input algorithm learns ensure input seen similar characteristics network able classify properly KB Neural Data Mining Python sources Roberto Bello Pag di Besides compared supervised algorithms self organized processes learning SOM result efficient incomplete incorrect input data characteristic makes neural network particularly suitable process Data Mining In fact Kohonen algorithms end phase non supervised training produces dimensional matrix classify new records groups similar characteristics While training phase require lot time run classifying new records groups similarities instantaneous making function especially useful processes real time reactions e g quality control productions continuous cycle automation industrial processes control systems monitoring messages Net etc The algorithms neural networks common aspect inability explain characteristics groups obtained It possible information contained training matrix resorting technical statistics provide information characteristics group helping researcher deepen analysis results gather better results research It possible determine overall view records training phase knowledge contents contrary data little connection suitable use research fact possible compute global index homogeneity groups Knowledge Index informing researcher suitability output files achieve expected goals KB Python KB The Python program language language freely downloaded Internet Python compatible Windows Linux Unix Mac OS X OS Amiga Smart phones Tablets Python distributed license Open Source use free charge commercial products The site Python language downloaded www python org download choosing compatible version computer Installing Python Windows involves choosing extended file msi download Internet To install Python Linux particular Linux Ubuntu use Software Manager Linux distribution automatically connects official site repository downloading necessary safe complete automatic installation Linux distributions usually contain Python language pre installed Whatever operating system installation Python programs command mode option opening file containing Python program example program py typing python program py DOS window execute Windows terminal window Linux KB Neural Data Mining Python sources Roberto Bello Pag di Details The KB application system extracts knowledge data based algorithms map Kohonen revised modified author KB elaborate table numeric data text tables line table destined description columns variables column table destined codes arbitrary identification record case In KB functions included aim normalizing numeric data comparing standard deviation maximum value variable column according user s choice transforming alphanumeric data numeric data conveniently returned equidistant inserting statistical functions able judge quality results cataloging group globally writing different output files records cases arranged group code according chosen category synthetic statistical information characteristics different groups relation statistical indexes reference entire populations records cases final training matrix having minimum error The neural networks known defect black boxes able catalog don t explain characteristics group columns variables important group cataloging homogeneous groups inside global sense input table contains information relation variables table purely group numbers letters inductive value Appendixes contain programs written Python KB_CAT cataloging KB_STA analysis results KB_CLA classification They converted file text form cut paste programs stored names kb_cat py kb_sta py kb_cla py The programs different kb_cat kb_sta kb_cla long extension py allow Python language recognize programs run Some test files reproduced results obtained shown DOS window Windows Terminal Window Linux results contained files text format Collecting arranging input data The use programs based algorithms Kohonen require data KB Neural Data Mining Python sources Roberto Bello Pag di prepared normalized To begin important carefully choose data analysed Information user intends extract knowledge contained tables following characteristics format text txt csv fields separated tabulation tab column destined identify line identification code record e g Client s code product production lot etc line contain descriptions columns separated tabulation tab values contained cells second column column second line line values columns lines separated tabulation tab fields containing exist column contain numerical data contain data text To convert tables text resort programs xls Excel OpenOfficeCalc ods able read input formats convert csv format choosing tabulation field tab space delimit text For quality results famous saying garbage garbage valid fundamental collect good quality data allows research described explained complete way possible You need decide size data input file following suggestions The neural networks weight variables inserted variable oscillates interval interval variations tend reduce importance second significant determining results classification To transformation techniques exist variables compatible making fall inside certain interval range The KB_CAT program apply different techniques normalization numeric values text data Numeric values normalized methods Normalization maximum new values column obtained dividing original values maximum value column way new values vary zero Standardization new values obtained subtracting original value mean column dividing result difference standard deviation column From columns containing strings characters extracted value containing different strings sorted counted determine attribution step numeric value The KB_CAT program foresee automatic transformation date time The date transformed user pseudo continue numeric variables assigning value remote date increasing unit subsequent date expressing days year thousandths according formula days year The year indicated variable The pair variables KB Neural Data Mining Python sources Roberto Bello Pag di preferably expressed ratio single value offer information clearer immediate way derived variables calculated starting input variables Let imagine variables present weight height person Considered separately little meaning better obtain coefficient body mass definitely good synthetic index obesity body weight expressed kilogrammes divided height meters squared Another important step preliminary elaboration data try simplify problem want resolve To useful reorganize space input values space grows essentially grows size data A technique reduce number variables improve ability learning neural networks principal component analysis try identify sub space m size significant possible respect input space n size The m final variables called principal components linear combinations n initial variables Other methods reduce size problem resolve elimination variables strongly linked useful achieve desired result In case important consider connection imply because effect relationship eliminating variables extreme care user It common reorganize file input data needs cataloged examining results processing runs indicate certain variables columns worthless elimination subsequent processing contribute improve cataloging having end noise useless variables columns In processing data relating clinical trials verified personal data gender nationality residence education etc giving cases contribution cataloging omitted improving quality new learning A important aspect consider related number records contained input file catalog Often better results obtained smaller files able generalize better produce training matrices predictive On contrary file containing large number records produce invalid training overfitting causing photographic effect classify new records identical phase cataloging As scientists Cern discovered s important properly analyse fraction data important interest process data TomHCAnderson In statistics talk overfitting excessive adaptation statistics model fits observed data sample excessive number parameters An absurd wrong model converges perfectly complex adapt quantity data available It impossible prove glance best number records contained file catalog depends number variables informative contents variables records present file The best suggestion carry distinct runs original file KB Neural Data Mining Python sources Roberto Bello Pag di files obtained lesser number records To obtain smaller sized file extract records original file random choice use small program KB_RND present appendix KB_RND KNOWLEDGE DISCOVERY IN DATA MINING RANDOM FILE SIZE REDUCE ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON InputFile cancer txt OutputFile cancer_rnd txt Out number cells Nun Records Input Elapsed time microseconds Indicate InputFile file want extract smaller sized output file OutputFile Indicate Out number cells number cells lines x columns output file Other times convenient remove initial input file records clearly contain values contradictory absurd missing reduce size file improve quality reducing noise General warnings KB programs It important files input output processing programs kb_cat kb_sta kb_cla open windows reading writing happens kb_cat kb_sta kb_cla error Processing programs interrupted pressing ctrl c keys KB_CAT Knowledge Data Mining cataloging homogeneous groups Generality aims functions KB_CAT programs use important It is purpose analyse kind textual file structured dimensional table containing numeric values text data KB_CAT controls table process contain errors format normalizes numeric values text data starts training phase searching minimum error decreases processing reaches minimum value alpha chosen user Once processing completed program write output file containing results programs KB_STA KB_CLA KB Neural Data Mining Python sources Roberto Bello Pag di Source KB_CAT attachment Test Input file copy paste save vessels txt fields separated tabulation Description Shape material height colour weight haft plug glass_1 cut_cone pewter pewter glass_2 cut_cone plastic white glass_3 cut_cone terracotta grey beer_jug cut_cone porcelain severals dessert_glass cut_cone glass transparent wine_glass cut_cone glass transparent jug cylinder terracotta white yes bottle_1 cylinder_cone glass green cork bottle_2 cylinder_cone glass transparent cork bottle_3 cylinder_cone glass opaque plastic bottle_4 cylinder_cone glass green metal magnum_bottle cylinder_cone glass green metal carboy ball_cone glass green cork ancient_bottle ball_cone glass green cork champagne_glass cut_cone crystal transparent cup_1 cut_cone ceramic white yes milk_cup cut_cone terracotta blue yes tea_cup cut_cone terracotta white yes cup_2 cut_cone glass transparent yes coffee_cup cut_cone ceramic white yes tetrapack1 parallelepiped mixed severals plastic tetrapack2 parallelepiped plastic severals plastic tetrapack3 parallelepiped millboard severals cleaning_1 parall_cone plastic white yes plastic cleaning_2 cylinder_cone plastic blue yes plastic KB Neural Data Mining Python sources Roberto Bello Pag di Description Shape material height colour weight haft plug tuna_can cylinder metal severals tuna_tube cylinder plastic severals plastic perfume parallelepiped glass transparent plastic cleaning_3 Cone plastic severals yes plastic visage_cream cylinder metal white cd parallelepiped plastic transparent trousse cylinder plastic silver yes watering_can Irregular plastic green yes umbrella_stand cylinder metal grey pot_1 cylinder metal grey yes pot_2 cut_cone metal grey yes yes toothpaste cylinder plastic severals plastic pyrex parallelepiped glass transparent glass plant_pot cut_cone terracotta brown pasta_case parallelepiped glass transparent metal How run Being positioned file containing kb_cat py input file process start KB_CAT t y p n g n commands window o f DOS Windows Terminal Linux command python kb_cat py python runs program python language kb_cat py The program start subsequently asking Input File vessels txt vessels txt file format txt containing table o f records cases catalog shown If want importance particular variable column duplicate value times additional variables columns want variable important times original weight create variables columns calling example shape1 shape2 values identical original variable Number Groups The value square root maximum number groups catalog KB Neural Data Mining Python sources Roberto Bello Pag di case training matrix cube form base maximum number training groups square value entered There useful rules fixing best number parameter Number Groups advisable initially try low values gradually carry processing higher values obtained groups containing records hand reduce value parameter Number Groups obtained groups containing records Sometimes researcher interested analysing groups numbers records rare singular characteristics case groups containing records welcome Normalization Max Std None m The value m M indicates request normalize numerical data dividing maximum value column variable The value s S indicates request normalize numerical data subtracting input value average variable column dividing result standard deviation variable column It advisable insert value N None presence variables different large difference minimum maximum value range Start Value alpha KB_CAT like algorithms neural networks runs cycles making loops consider input In loops alpha parameter plays important role initial value Start Value final value End Value considering value decreasing step Occasionally excessive length time processing noted having chosen large number groups file containing lot records distant start end values alpha small decreasing step alpha usually cases notice minimum error remains loops It advisable stop processing pressing keys ctrl e c repeat suitable parameter values End Value alpha The alpha parameter KB_CAT refine cataloging records different groups low alpha value involves longer cycle time computing possibility obtain lower final minimum error hypothetical greater chance fitting photo effect Decreasing step alpha Choose value step decreasing alpha applied loop Forced shut processing In case wanting shut processing running KB Neural Data Mining Python sources Roberto Bello Pag di need press keys ctrl c time Obviously files writing valid KB_CAT produce following output In window DOS Windows Terminal Linux KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON InputFile vessels txt Number Groups Normalization Max Std None m Start value alpha End value alpha Decreasing step alpha Record Columns Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha KB Neural Data Mining Python sources Roberto Bello Pag di Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha KB Neural Data Mining Python sources Roberto Bello Pag di Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha KB Neural Data Mining Python sources Roberto Bello Pag di Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Epoch min err min epoch alpha Epoch WITH MIN ERROR alpha Epoch min err min epoch alpha Min alpha reached KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON EPOCH WITH MIN ERROR starting alpha ending alpha Iterations Total Epochs Output File Catalog original vessels_M_g3_out txt Output File Catalog sort vessels_M_g3_outsrt txt Output File Summary sort vessels_M_g3_sort txt Output File Matrix Catal vessels_M_g3_catal txt Output File Means STD CV vessels_M_g3_medsd txt Output File CV Groups vessels_M_g3_cv txt Output File Training Grid vessels_M_g3_grid txt Output File Run Parameters vessels_M_g3_log txt Elapsed time seconds KIndex As processing minimum error decreases epoch epoch The processing completed epoch parameter value alpha reaches minimum value References output files listed Catalog original n p u t f l e cataloged NOT order groups original values NOT normalized Catalog sort input file cataloged IN ORDER groups original values NOT normalized Summary sort input file cataloged IN ORDER groups NORMALIZED values Matrix Catal files columns progressive number records group codes subgroup codes Means STD CV files column variable lines mean standard deviation coefficient variation CV Groups files coefficient variations groups KB Neural Data Mining Python sources Roberto Bello Pag di variables columns totals records classified groups Training Grid files containing values training matrix minimum error Run Parameters files containing references input files parameters computing output files KIndex Knowledge Index KB index measures knowledge contained cataloged groups KIndex reached maximum value group records constant values variables columns group different groups KIndex calculated means CV variables columns groups input files cataloging source program KB_CAT computing details In case examination Kindex value particularly high suggests run new processing increasing example number groups obtaining certain improvement Kindex File Output Catalog original vessels_M_g3_out txt It identical input file addition column input code group belongs The Output Catalog sort file interesting shows classified records group belong File Output Catalog sort vessels_M_g3_outsrt txt This identical previous file records cases order according code group belongs Group description shape material height colour weight haft plug G_00_00 ancient_bottle ball_cone glass Green cork G_00_00 bottle_1 cylinder_cone glass Green cork G_00_00 bottle_4 cylinder_cone glass Green metal G_00_00 carboy ball_cone glass Green cork G_00_00 magnum_bottle cylinder_cone glass Green metal G_00_00 plant_pot cut_cone terracotta Brown G_00_00 umbrella_stand cylinder metal Grey G_00_01 pot_1 cylinder metal Grey yes G_00_02 coffee_cup cut_cone ceramic White yes G_00_02 cup_1 cut_cone ceramic White yes G_00_02 cup_2 cut_cone glass transparent yes G_00_02 pot_2 cut_cone metal Grey yes yes G_01_00 beer_jug cut_cone porcelain severals G_01_00 bottle_2 cylinder_cone glass transparent cork G_01_00 bottle_3 cylinder_cone glass opaque plastic G_01_00 glass_1 cut_cone pewter pewter G_01_00 glass_3 cut_cone terracotta Grey G_01_00 tuna_can cylinder metal severals G_02_00 cd parallelepiped plastic transparent G_02_00 champagne_glass cut_cone crystal transparent KB Neural Data Mining Python sources Roberto Bello Pag di Group description shape material height colour weight haft plug G_02_00 dessert_glass cut_cone glass transparent G_02_00 glass_2 cut_cone plastic White G_02_00 pasta_case parallelepiped glass transparent metal G_02_00 perfume parallelepiped glass transparent plastic G_02_00 tetrapack1 parallelepiped mixed severals plastic G_02_00 tetrapack2 parallelepiped plastic severals plastic G_02_00 tetrapack3 parallelepiped millboard severals G_02_00 toothpaste cylinder plastic severals plastic G_02_00 trousse cylinder plastic silver yes G_02_00 tuna_tube cylinder plastic severals plastic G_02_00 visage_cream cylinder metal White G_02_00 wine_glass cut_cone glass transparent G_02_01 pyrex parallelepiped glass transparent glass G_02_02 cleaning_1 parall_cone plastic White yes plastic G_02_02 cleaning_2 cylinder_cone plastic Blue yes plastic G_02_02 cleaning_3 cone plastic severals yes plastic G_02_02 jug cylinder terracotta White yes G_02_02 milk_cup cut_cone terracotta Blue yes G_02_02 tea_cup cut_cone terracotta White yes G_02_02 watering_can irregular plastic Green yes On sight program KB_CAT able catalog records homogeneous groups content It important note vessels txt files formed records different For example group G_00_00 characterised objects primarily green colour haft group G_00_02 primarily formed objects cut_cone shape haft plug group G_02_00 characterised objects parallelepiped cylinder cut_cone shape haft group G_02_02 plastic terracotta objects haft If processed input file formed numerous records variables columns easy draw conclusions results cataloging visually examining files The KB_STA program dedicated resolving problem highlighted Output Means Std CV vessels_M_g3_medsd txt File containing Means Maximums Std CV normalized values population Low values CV coefficient variation indicate values variables columns dispersed shape material height colour weight haft plug Mean1 Mean2 Mean3 Mean4 Mean5 Mean6 Mean7 KB Neural Data Mining Python sources Roberto Bello Pag di shape material height colour weight haft plug Max1 Max2 Max3 Max4 Max5 Max6 Max7 Std1 Std2 Std3 Std4 Std5 Std6 Std7 CV_1 CV_2 CV_3 CV_4 CV_5 CV_6 CV_7 Output CV files vessels_M_g3_cv txt Groups shape material height colour weight haft plug Means N_recs G_00_00 G_00_01 G_00_02 G_01_00 G_02_00 G_02_01 G_02_02 Means Total The file contains information relevant measuring quality cataloging The value contained cell represents importance values variables columns group value close zero variable column important cataloging If value equal zero variable column group identical value example groups identical values variable haft The values cells penultimate column Means indicate groups internally homogeneous considering variables columns higher value close zero greater similarity record cases group consideration The groups G_00_02 G_01_00 homogeneous group G_00_00 important CV values variables weight plug It important compare values contained line column value contained lines Means Total referring records cataloging Output Training Grid vessels_M_g3_grid txt The file contains values dimensional training matrix minimum error matrix KB_CLA program classify new records KB Neural Data Mining Python sources Roberto Bello Pag di cases recognised classified according previously learnt program KB_CAT Group SubGroup Variable Column Values KB Neural Data Mining Python sources Roberto Bello Pag di Group SubGroup Variable Column Values Statistical analysis results cataloging The file contains results processing KB_CAT statistically analysed running program KB_STA parameters listed KB Neural Data Mining Python sources Roberto Bello Pag di KB_STA KNOWLEDGE DISCOVERY IN DATA MINING STATISTICAL PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON INPUT Catalogued Records File _outsrt txt vessels_M_g3_outsrt txt INPUT Groups CV File _cv txt vessels_M_g3_cv txt Group Consistency Variable Consistency Select groups containing records Select groups containing records Summary Detail report S D D Display Input Records Y N Y OUTPUT Report File vessels_M_g3_sta txt KB_STA Statistical Analysis vessels_M_g3_outsrt txt vessels_M_g3_cv txt Min Perc group Consistency Min Perc variable Consistency Min Number records Max Number records ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED G_00_00 Consistency Consistency Records Records shape Consistency Consistency G_00_00 ID record ancient_bottle Value ball_cone G_00_00 ID record bottle_1 Value cylinder_cone G_00_00 ID record bottle_4 Value cylinder_cone G_00_00 ID record carboy Value ball_cone G_00_00 ID record magnum_bottle Value cylinder_cone G_00_00 ID record plant_pot Value cut_cone G_00_00 ID record umbrella_stand Value cylinder Value cylinder_cone Frequency Percentage Value ball_cone Frequency Percentage Value cylinder Frequency Percentage Value cut_cone Frequency Percentage material Consistency Consistency G_00_00 ID record ancient_bottle Value glass G_00_00 ID record bottle_1 Value glass G_00_00 ID record bottle_4 Value glass G_00_00 ID record carboy Value glass G_00_00 ID record magnum_bottle Value glass G_00_00 ID record plant_pot Value terracotta G_00_00 ID record umbrella_stand Value metal Value glass Frequency Percentage Value terracotta Frequency Percentage Value metal Frequency Percentage height Consistency Consistency G_00_00 ID record ancient_bottle Value G_00_00 ID record bottle_1 Value G_00_00 ID record bottle_4 Value G_00_00 ID record carboy Value G_00_00 ID record magnum_bottle Value G_00_00 ID record plant_pot Value G_00_00 ID record umbrella_stand Value Min Max Step First Quartile end Frequency Second Quartile end Frequency Third Quartile end Frequency Fourth Quartile end Frequency KB Neural Data Mining Python sources Roberto Bello Pag di colour Consistency Consistency G_00_00 ID record ancient_bottle Value green G_00_00 ID record bottle_1 Value green G_00_00 ID record bottle_4 Value green G_00_00 ID record carboy Value green G_00_00 ID record magnum_bottle Value green G_00_00 ID record plant_pot Value brown G_00_00 ID record umbrella_stand Value grey Value green Frequency Percentage Value grey Frequency Percentage Value brown Frequency Percentage weight Consistency Consistency G_00_00 ID record ancient_bottle Value G_00_00 ID record bottle_1 Value G_00_00 ID record bottle_4 Value G_00_00 ID record carboy Value G_00_00 ID record magnum_bottle Value G_00_00 ID record plant_pot Value G_00_00 ID record umbrella_stand Value Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency G_00_00 ID record ancient_bottle Value G_00_00 ID record bottle_1 Value G_00_00 ID record bottle_4 Value G_00_00 ID record carboy Value G_00_00 ID record magnum_bottle Value G_00_00 ID record plant_pot Value G_00_00 ID record umbrella_stand Value Value Frequency Percentage plug Consistency Consistency G_00_00 ID record ancient_bottle Value cork G_00_00 ID record bottle_1 Value cork G_00_00 ID record bottle_4 Value metal G_00_00 ID record carboy Value cork G_00_00 ID record magnum_bottle Value metal G_00_00 ID record plant_pot Value G_00_00 ID record umbrella_stand Value Value cork Frequency Percentage Value Frequency Percentage Value metal Frequency Percentage G_00_02 Consistency Consistency Records Records shape Consistency Consistency G_00_02 ID record coffee_cup Value cut_cone G_00_02 ID record cup_1 Value cut_cone G_00_02 ID record cup_2 Value cut_cone G_00_02 ID record pot_2 Value cut_cone Value cut_cone Frequency Percentage material Consistency Consistency G_00_02 ID record coffee_cup Value ceramic G_00_02 ID record cup_1 Value ceramic G_00_02 ID record cup_2 Value glass G_00_02 ID record pot_2 Value metal Value ceramic Frequency Percentage Value metal Frequency Percentage Value glass Frequency Percentage height Consistency Consistency G_00_02 ID record coffee_cup Value KB Neural Data Mining Python sources Roberto Bello Pag di G_00_02 ID record cup_1 Value G_00_02 ID record cup_2 Value G_00_02 ID record pot_2 Value Min Max Step First Quartile end Frequency Second Quartile end Frequency Fourth Quartile end Frequency colour Consistency Consistency G_00_02 ID record coffee_cup Value white G_00_02 ID record cup_1 Value white G_00_02 ID record cup_2 Value transparent G_00_02 ID record pot_2 Value grey Value white Frequency Percentage Value transparent Frequency Percentage Value grey Frequency Percentage weight Consistency Consistency G_00_02 ID record coffee_cup Value G_00_02 ID record cup_1 Value G_00_02 ID record cup_2 Value G_00_02 ID record pot_2 Value Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency G_00_02 ID record coffee_cup Value yes G_00_02 ID record cup_1 Value yes G_00_02 ID record cup_2 Value yes G_00_02 ID record pot_2 Value yes Value yes Frequency Percentage plug Consistency Consistency G_00_02 ID record coffee_cup Value G_00_02 ID record cup_1 Value G_00_02 ID record cup_2 Value G_00_02 ID record pot_2 Value yes Value Frequency Percentage Value yes Frequency Percentage G_01_00 Consistency Consistency Records Records shape Consistency Consistency G_01_00 ID record beer_jug Value cut_cone G_01_00 ID record bottle_2 Value cylinder_cone G_01_00 ID record bottle_3 Value cylinder_cone G_01_00 ID record glass_1 Value cut_cone G_01_00 ID record glass_3 Value cut_cone G_01_00 ID record tuna_can Value cylinder Value cut_cone Frequency Percentage Value cylinder_cone Frequency Percentage Value cylinder Frequency Percentage material Consistency Consistency G_01_00 ID record beer_jug Value porcelain G_01_00 ID record bottle_2 Value glass G_01_00 ID record bottle_3 Value glass G_01_00 ID record glass_1 Value pewter G_01_00 ID record glass_3 Value terracotta G_01_00 ID record tuna_can Value metal Value glass Frequency Percentage Value terracotta Frequency Percentage Value porcelain Frequency Percentage Value pewter Frequency Percentage Value metal Frequency Percentage KB Neural Data Mining Python sources Roberto Bello Pag di height Consistency Consistency G_01_00 ID record beer_jug Value G_01_00 ID record bottle_2 Value G_01_00 ID record bottle_3 Value G_01_00 ID record glass_1 Value G_01_00 ID record glass_3 Value G_01_00 ID record tuna_can Value Min Max Step First Quartile end Frequency Second Quartile end Frequency Fourth Quartile end Frequency colour Consistency Consistency G_01_00 ID record beer_jug Value severals G_01_00 ID record bottle_2 Value transparent G_01_00 ID record bottle_3 Value opaque G_01_00 ID record glass_1 Value pewter G_01_00 ID record glass_3 Value grey G_01_00 ID record tuna_can Value severals Value severals Frequency Percentage Value transparent Frequency Percentage Value pewter Frequency Percentage Value opaque Frequency Percentage Value grey Frequency Percentage weight Consistency Consistency G_01_00 ID record beer_jug Value G_01_00 ID record bottle_2 Value G_01_00 ID record bottle_3 Value G_01_00 ID record glass_1 Value G_01_00 ID record glass_3 Value G_01_00 ID record tuna_can Value Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency G_01_00 ID record beer_jug Value G_01_00 ID record bottle_2 Value G_01_00 ID record bottle_3 Value G_01_00 ID record glass_1 Value G_01_00 ID record glass_3 Value G_01_00 ID record tuna_can Value Value Frequency Percentage plug Consistency Consistency G_01_00 ID record beer_jug Value G_01_00 ID record bottle_2 Value cork G_01_00 ID record bottle_3 Value plastic G_01_00 ID record glass_1 Value G_01_00 ID record glass_3 Value G_01_00 ID record tuna_can Value Value Frequency Percentage Value plastic Frequency Percentage Value cork Frequency Percentage G_02_00 Consistency Consistency Records Records shape Consistency Consistency G_02_00 ID record cd Value parallelepiped G_02_00 ID record champagne_glass Value cut_cone G_02_00 ID record dessert_glass Value cut_cone G_02_00 ID record glass_2 Value cut_cone G_02_00 ID record pasta_case Value parallelepiped G_02_00 ID record perfume Value parallelepiped KB Neural Data Mining Python sources Roberto Bello Pag di G_02_00 ID record tetrapack1 Value parallelepiped G_02_00 ID record tetrapack2 Value parallelepiped G_02_00 ID record tetrapack3 Value parallelepiped G_02_00 ID record toothpaste Value cylinder G_02_00 ID record trousse Value cylinder G_02_00 ID record tuna_tube Value cylinder G_02_00 ID record visage_cream Value cylinder G_02_00 ID record wine_glass Value cut_cone Value parallelepiped Frequency Percentage Value cylinder Frequency Percentage Value cut_cone Frequency Percentage material Consistency Consistency G_02_00 ID record cd Value plastic G_02_00 ID record champagne_glass Value crystal G_02_00 ID record dessert_glass Value glass G_02_00 ID record glass_2 Value plastic G_02_00 ID record pasta_case Value glass G_02_00 ID record perfume Value glass G_02_00 ID record tetrapack1 Value mixed G_02_00 ID record tetrapack2 Value plastic G_02_00 ID record tetrapack3 Value millboard G_02_00 ID record toothpaste Value plastic G_02_00 ID record trousse Value plastic G_02_00 ID record tuna_tube Value plastic G_02_00 ID record visage_cream Value metal G_02_00 ID record wine_glass Value glass Value plastic Frequency Percentage Value glass Frequency Percentage Value mixed Frequency Percentage Value millboard Frequency Percentage Value metal Frequency Percentage Value crystal Frequency Percentage height Consistency Consistency G_02_00 ID record cd Value G_02_00 ID record champagne_glass Value G_02_00 ID record dessert_glass Value G_02_00 ID record glass_2 Value G_02_00 ID record pasta_case Value G_02_00 ID record perfume Value G_02_00 ID record tetrapack1 Value G_02_00 ID record tetrapack2 Value G_02_00 ID record tetrapack3 Value G_02_00 ID record toothpaste Value G_02_00 ID record trousse Value G_02_00 ID record tuna_tube Value G_02_00 ID record visage_cream Value G_02_00 ID record wine_glass Value Min Max Step First Quartile end Frequency Second Quartile end Frequency Fourth Quartile end Frequency colour Consistency Consistency G_02_00 ID record cd Value transparent G_02_00 ID record champagne_glass Value transparent G_02_00 ID record dessert_glass Value transparent G_02_00 ID record glass_2 Value white G_02_00 ID record pasta_case Value transparent G_02_00 ID record perfume Value transparent G_02_00 ID record tetrapack1 Value severals G_02_00 ID record tetrapack2 Value severals KB Neural Data Mining Python sources Roberto Bello Pag di G_02_00 ID record tetrapack3 Value severals G_02_00 ID record toothpaste Value severals G_02_00 ID record trousse Value silver G_02_00 ID record tuna_tube Value severals G_02_00 ID record visage_cream Value white G_02_00 ID record wine_glass Value transparent Value transparent Frequency Percentage Value severals Frequency Percentage Value white Frequency Percentage Value silver Frequency Percentage weight Consistency Consistency G_02_00 ID record cd Value G_02_00 ID record champagne_glass Value G_02_00 ID record dessert_glass Value G_02_00 ID record glass_2 Value G_02_00 ID record pasta_case Value G_02_00 ID record perfume Value G_02_00 ID record tetrapack1 Value G_02_00 ID record tetrapack2 Value G_02_00 ID record tetrapack3 Value G_02_00 ID record toothpaste Value G_02_00 ID record trousse Value G_02_00 ID record tuna_tube Value G_02_00 ID record visage_cream Value G_02_00 ID record wine_glass Value Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency G_02_00 ID record cd Value G_02_00 ID record champagne_glass Value G_02_00 ID record dessert_glass Value G_02_00 ID record glass_2 Value G_02_00 ID record pasta_case Value G_02_00 ID record perfume Value G_02_00 ID record tetrapack1 Value G_02_00 ID record tetrapack2 Value G_02_00 ID record tetrapack3 Value G_02_00 ID record toothpaste Value G_02_00 ID record trousse Value G_02_00 ID record tuna_tube Value G_02_00 ID record visage_cream Value G_02_00 ID record wine_glass Value Value Frequency Percentage plug Consistency Consistency G_02_00 ID record cd Value G_02_00 ID record champagne_glass Value G_02_00 ID record dessert_glass Value G_02_00 ID record glass_2 Value G_02_00 ID record pasta_case Value metal G_02_00 ID record perfume Value plastic G_02_00 ID record tetrapack1 Value plastic G_02_00 ID record tetrapack2 Value plastic G_02_00 ID record tetrapack3 Value G_02_00 ID record toothpaste Value plastic G_02_00 ID record trousse Value yes G_02_00 ID record tuna_tube Value plastic G_02_00 ID record visage_cream Value G_02_00 ID record wine_glass Value Value Frequency Percentage KB Neural Data Mining Python sources Roberto Bello Pag di Value plastic Frequency Percentage Value yes Frequency Percentage Value metal Frequency Percentage G_02_02 Consistency Consistency Records Records shape Consistency Consistency G_02_02 ID record cleaning_1 Value parall_cone G_02_02 ID record cleaning_2 Value cylinder_cone G_02_02 ID record cleaning_3 Value cone G_02_02 ID record jug Value cylinder G_02_02 ID record milk_cup Value cut_cone G_02_02 ID record tea_cup Value cut_cone G_02_02 ID record watering_can Value irregular Value cut_cone Frequency Percentage Value parall_cone Frequency Percentage Value irregular Frequency Percentage Value cylinder_cone Frequency Percentage Value cylinder Frequency Percentage Value cone Frequency Percentage material Consistency Consistency G_02_02 ID record cleaning_1 Value plastic G_02_02 ID record cleaning_2 Value plastic G_02_02 ID record cleaning_3 Value plastic G_02_02 ID record jug Value terracotta G_02_02 ID record milk_cup Value terracotta G_02_02 ID record tea_cup Value terracotta G_02_02 ID record watering_can Value plastic Value plastic Frequency Percentage Value terracotta Frequency Percentage height Consistency Consistency G_02_02 ID record cleaning_1 Value G_02_02 ID record cleaning_2 Value G_02_02 ID record cleaning_3 Value G_02_02 ID record jug Value G_02_02 ID record milk_cup Value G_02_02 ID record tea_cup Value G_02_02 ID record watering_can Value Min Max Step First Quartile end Frequency Second Quartile end Frequency Fourth Quartile end Frequency colour Consistency Consistency G_02_02 ID record cleaning_1 Value white G_02_02 ID record cleaning_2 Value blue G_02_02 ID record cleaning_3 Value severals G_02_02 ID record jug Value white G_02_02 ID record milk_cup Value blue G_02_02 ID record tea_cup Value white G_02_02 ID record watering_can Value green Value white Frequency Percentage Value blue Frequency Percentage Value severals Frequency Percentage Value green Frequency Percentage weight Consistency Consistency G_02_02 ID record cleaning_1 Value G_02_02 ID record cleaning_2 Value G_02_02 ID record cleaning_3 Value G_02_02 ID record jug Value G_02_02 ID record milk_cup Value G_02_02 ID record tea_cup Value KB Neural Data Mining Python sources Roberto Bello Pag di G_02_02 ID record watering_can Value Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency G_02_02 ID record cleaning_1 Value yes G_02_02 ID record cleaning_2 Value yes G_02_02 ID record cleaning_3 Value yes G_02_02 ID record jug Value yes G_02_02 ID record milk_cup Value yes G_02_02 ID record tea_cup Value yes G_02_02 ID record watering_can Value yes Value yes Frequency Percentage plug Consistency Consistency G_02_02 ID record cleaning_1 Value plastic G_02_02 ID record cleaning_2 Value plastic G_02_02 ID record cleaning_3 Value plastic G_02_02 ID record jug Value G_02_02 ID record milk_cup Value G_02_02 ID record tea_cup Value G_02_02 ID record watering_can Value Value Frequency Percentage Value plastic Frequency Percentage Means Consistency Consistency Records Records shape Consistency Consistency material Consistency Consistency height Consistency Consistency colour Consistency Consistency weight Consistency Consistency haft Consistency Consistency plug Consistency Consistency Other input files KB_CAT animals txt The animals txt file formed records variables columns Fur Feather Eggs Milk Flying Aquatic Predatory Teeth Invertebrate Lungs Poisonous Flippers Legs Tail Domestic ANIMAL FUR FEATHER EGGS MILK FLYING AQUATIC PREDATORY TEETH INVERT LUNGS POIS FLIP LEGS TAIL DOM SKYLARK DUCK ANTELOPE BEE LOBSTER HERRING FIELD_MOUSE HAWK BUFFALO KANGAROO GOAT CARP CHUB CAVY DEER SWAN BOAR KB Neural Data Mining Python sources Roberto Bello Pag di ANIMAL FUR FEATHER EGGS MILK FLYING AQUATIC PREDATORY TEETH INVERT LUNGS POIS FLIP LEGS TAIL DOM LADYBIRD DOVE CROW HAMSTER DOLPHIN CODFISH ELEPHANT PHEASANT FALCON MOTH FLAMINGO SEAL GULL PRAWN CHEETAH GIRAFFE GORILLA CRAB SEAHORSE KIWI LION SEA_LION LEOPARD HARE SNAIL LYNX PIKE WOLF MONGOOSE CAT MOLLUSK FLY MIDGE OPOSSUM DUCKBILL BEAR SPARROW STURGEON PERCH SHARK PENGUIN PIRANHA POLYP CHICKEN PONY FLEA PUMA POLECAT FROG REINDEER TOAD SQUIRREL SCORPION KB Neural Data Mining Python sources Roberto Bello Pag di ANIMAL FUR FEATHER EGGS MILK FLYING AQUATIC PREDATORY TEETH INVERT LUNGS POIS FLIP LEGS TAIL DOM SEA_SNAKE SOLE STARFISH OSTRICH MOLE TORTOISE TERMITE TUNA TRITON VAMPIRE WORM WASP MINK CALF Processing animals txt file KB_CAT The animals txt file processed following parameters Input File animals txt Number Groups Normalization Max Std None M Start Value alpha End Value alpha Decreasing step alpha The processing ended Minimum Error time producing results files Output File Catalog original animals_M_g4_out txt Output File Catalog sort animals_M_g4_outsrt txt Output File Summary sort animals_M_g4_sort txt Output File Matrix Catal animals_M_g4_catal txt Output File Means STD CV animals_M_g4_medsd txt Output File CV Groups animals_M_g4_cv txt Output File Training Grid animals_M_g4_grid txt Output File Run Parameters animals_M_g4_log txt Output file Catalog sort ordered group animals txt Group ANIMAL FUR FEATH EGGS MILK FLYING AQUAT PRED TEETH VERT LUNGS POIS FLIP LEGS TAIL DOM G_00_00 ANTELOPE G_00_00 BUFFALO G_00_00 CALF G_00_00 CAT G_00_00 DEER G_00_00 ELEPHANT G_00_00 FLD_MOUSE G_00_00 GIRAFFE G_00_00 GOAT G_00_00 HAMSTER G_00_00 HARE G_00_00 KANGAROO KB Neural Data Mining Python sources Roberto Bello Pag di Group ANIMAL FUR FEATH EGGS MILK FLYING AQUAT PRED TEETH VERT LUNGS POIS FLIP LEGS TAIL DOM G_00_00 PONY G_00_00 REINDEER G_00_00 SQUIRREL G_00_00 VAMPIRE G_00_01 CAVY G_00_01 GORILLA G_00_02 BEE G_00_03 CRAB G_00_03 FLY G_00_03 LADYBIRD G_00_03 LOBSTER G_00_03 MIDGE G_00_03 MOLLUSK G_00_03 MOTH G_00_03 POLYP G_00_03 PRAWN G_00_03 STARFISH G_00_03 WASP G_01_00 BEAR G_01_00 BOAR G_01_00 CHEETAH G_01_00 LEOPARD G_01_00 LION G_01_00 LYNX G_01_00 MINK G_01_00 MOLE G_01_00 MONGOOSE G_01_00 OPOSSUM G_01_00 POLECAT G_01_00 PUMA G_01_00 WOLF G_01_02 SCORPION G_01_03 FLEA G_01_03 SNAIL G_01_03 TERMITE G_01_03 WORM G_02_00 DOLPHIN G_02_00 SEAL G_02_00 SEA_LION G_02_01 DUCKBILL G_02_02 TOAD G_02_02 TORTOISE G_03_00 CARP G_03_00 CHUB G_03_00 CODFISH G_03_00 HERRING G_03_00 PERCH G_03_00 PIKE G_03_00 PIRANHA G_03_00 SEAHORSE G_03_00 SEA_SNAKE G_03_00 SHARK G_03_00 SOLE KB Neural Data Mining Python sources Roberto Bello Pag di Group ANIMAL FUR FEATH EGGS MILK FLYING AQUAT PRED TEETH VERT LUNGS POIS FLIP LEGS TAIL DOM G_03_00 STURGEON G_03_00 TUNA G_03_01 FROG G_03_01 TRITON G_03_02 GULL G_03_02 KIWI G_03_02 PENGUIN G_03_03 CHICKEN G_03_03 CROW G_03_03 DOVE G_03_03 DUCK G_03_03 FALCON G_03_03 FLAMINGO G_03_03 HAWK G_03_03 OSTRICH G_03_03 PHEASANT G_03_03 SKYLARK G_03_03 SPARROW G_03_03 SWAN Output file CV ordered group animals txt Groups FUR FEATHER EGGS MILK FLYING AQUATIC PRED TEETH VERT LUNGS POIS FLIP LEGS TAIL DOMEST Mean N_recs G_00_00 G_00_01 G_00_02 G_00_03 G_01_00 G_01_02 G_01_03 G_02_00 G_02_01 G_02_02 G_03_00 G_03_01 G_03_02 G_03_03 Means Total In example importance difference CV file CV_Pop cataloging opposed means CV groups CV_Med The cataloging produced improvement Further confirmation derives presence zero values cells table values indicate variable column group value constantly repeated variable column constant value certainly important cataloging records group Verify validity manual cataloging Sometimes useful certify validity manual cataloging KB_CAT In example described variable column D1 contains input code KB Neural Data Mining Python sources Roberto Bello Pag di animal specie bird mammal insect fish variable column duplicated columns D2 e D3 The modification original file different consequences reinforces importance code animal specie respect variables columns new structure file allows KB_CAT process data similar way algorithms typical supervised networks different input variables columns variable column objective case D1 D2 D3 considered globally Input file KB_CAT animals_d txt ANIMAL FUR FEATHE R EGG S MILK XXX D1 D2 D3 SKYLARK bird bird Bird DUCK bird bird Bird ANTELOPE mammal mammal Mammal BEE insect insect Insect LOBSTER shellfish shellfish Shellfish HERRING fish fish Fish FIELD_MOUS E mammal mammal Mammal HAWK bird_of_prey bird_of_prey bird_of_pre y BUFFALO mammal mammal Mammal KANGAROO mammal mammal Mammal GOAT mammal mammal Mammal CARP fish fish Fish CHUB fish fish Fish CAVY mammal mammal Mammal DEER mammal mammal Mammal SWAN bird bird Bird BOAR mammal mammal Mammal LADYBIRD insect insect Insect DOVE bird bird Bird CROW bird bird Bird HAMSTER mammal mammal Mammal DOLPHIN mammal mammal Mammal CODFISH fish fish Fish ELEPHANT mammal mammal Mammal PHEASANT bird bird Bird FALCON bird_of_prey bird_of_prey bird_of_pre y MOTH insect insect Insect FLAMINGO bird bird Bird SEAL mammal mammal Mammal GULL bird bird Bird PRAWN shellfish shellfish Shellfish CHEETAH mammal mammal Mammal GIRAFFE mammal mammal Mammal GORILLA mammal mammal Mammal CRAB shellfish shellfish Shellfish SEAHORSE fish fish Fish KB Neural Data Mining Python sources Roberto Bello Pag di ANIMAL FUR FEATHE R EGG S MILK XXX D1 D2 D3 KIWI bird bird Bird LION mammal mammal Mammal SEA_LION mammal mammal Mammal LEOPARD mammal mammal Mammal HARE mammal mammal Mammal SNAIL invertebrate invertebrate Invertebrat e LYNX mammal mammal Mammal PIKE fish fish Fish WOLF mammal mammal Mammal MONGOOSE mammal mammal Mammal CAT mammal mammal Mammal MOLLUSK invertebrate invertebrate Invertebrat e FLY insect insect Insect MIDGE insect insect Insect OPOSSUM mammal mammal Mammal DUCKBILL mammal mammal Mammal BEAR mammal mammal Mammal SPARROW bird bird Bird STURGEON fish fish Fish PERCH fish fish Fish SHARK fish fish Fish PENGUIN bird bird Bird PIRANHA fish fish Fish POLYP invertebrate invertebrate Invertebrat e CHICKEN bird bird Bird PONY mammal mammal Mammal FLEA insect insect Insect PUMA mammal mammal Mammal POLECAT mammal mammal Mammal FROG amphibian amphibian Amphibian REINDEER mammal mammal Mammal TOAD amphibian amphibian Amphibian SQUIRREL mammal mammal Mammal SCORPION arachinida arachinida Arachinida SEA_SNAKE reptiles reptiles Reptiles SOLE fish fish Fish STARFISH echinoderm echinoderm Echinoder m OSTRICH bird bird Bird MOLE mammal mammal Mammal TORTOISE reptiles reptiles Reptiles TERMITE insect insect Insect TUNA fish fish Fish TRITON amphibian amphibian Amphibian VAMPIRE mammal mammal Mammal WORM invertebrate invertebrate Invertebrat e WASP insect insect Insect MINK mammal mammal Mammal KB Neural Data Mining Python sources Roberto Bello Pag di ANIMAL FUR FEATHE R EGG S MILK XXX D1 D2 D3 CALF mammal mammal Mammal The processing carried following parameters KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON Input File animals_d txt Number Groups Normalization Max Std None M Start Value alpha End Value alpha Decreasing step alpha OUTPUT Output File Catalog original animals_d_M_g4_out txt Output File Catalog sort animals_d_M_g4_outsrt txt Output File Summary sort animals_d_M_g4_sort txt Output File Matrix Catal animals_d_M_g4_catal txt Output File Means STD CV animals_d_M_g4_medsd txt Output File CV Groups animals_d_M_g4_cv txt Output File Training Grid animals_d_M_g4_grid txt Output File Run Parameters animals_d_M_g4_log txt Results obtained processing animals_d txt Output Catalog sort Group ANIMAL FUR FEATHER EGGS MILK XXX D1 D2 D3 G_00_00 BEE insect insect Insect G_00_00 CRAB shellfish shellfish Shellfish G_00_00 FLY insect insect Insect G_00_00 LADYBIRD insect insect Insect G_00_00 LOBSTER shellfish shellfish Shellfish G_00_00 MIDGE insect insect Insect G_00_00 MOLLUSK invertebrate invertebrate Invertebrate G_00_00 MOTH insect insect Insect G_00_00 POLYP invertebrate invertebrate Invertebrate G_00_00 PRAWN shellfish shellfish Shellfish G_00_00 SNAIL invertebrate invertebrate Invertebrate G_00_00 WASP insect insect Insect G_00_00 WORM invertebrate invertebrate Invertebrate G_00_01 FLEA insect insect Insect G_00_01 STARFISH echinoderm echinoderm Echinoderm G_00_01 TERMITE insect insect Insect G_00_03 CHICKEN bird bird bird G_00_03 CROW bird bird bird G_00_03 DOVE bird bird bird G_00_03 DUCK bird bird bird G_00_03 FALCON bird_of_prey bird_of_prey bird_of_prey G_00_03 FLAMINGO bird bird bird G_00_03 HAWK bird_of_prey bird_of_prey bird_of_prey G_00_03 OSTRICH bird bird bird G_00_03 PHEASANT bird bird bird G_00_03 SKYLARK bird bird bird G_00_03 SPARROW bird bird bird G_00_03 SWAN bird bird bird G_01_01 TORTOISE reptiles reptiles reptiles G_01_02 SCORPION arachinida arachinida arachinida G_01_02 TOAD amphibian amphibian amphibian G_01_03 GULL bird bird bird G_01_03 KIWI bird bird bird G_01_03 PENGUIN bird bird bird KB Neural Data Mining Python sources Roberto Bello Pag di Group ANIMAL FUR FEATHER EGGS MILK XXX D1 D2 D3 G_02_00 ANTELOPE mammal mammal mammal G_02_00 BUFFALO mammal mammal mammal G_02_00 DEER mammal mammal mammal G_02_00 ELEPHANT mammal mammal mammal G_02_00 FIELD_MOUSE mammal mammal mammal G_02_00 GIRAFFE mammal mammal mammal G_02_00 GORILLA mammal mammal mammal G_02_00 HARE mammal mammal mammal G_02_00 KANGAROO mammal mammal mammal G_02_00 SQUIRREL mammal mammal mammal G_02_00 VAMPIRE mammal mammal mammal G_02_02 DUCKBILL mammal mammal mammal G_02_03 FROG amphibian amphibian amphibian G_02_03 TRITON amphibian amphibian amphibian G_03_00 CALF mammal mammal mammal G_03_00 CAT mammal mammal mammal G_03_00 CAVY mammal mammal mammal G_03_00 GOAT mammal mammal mammal G_03_00 HAMSTER mammal mammal mammal G_03_00 PONY mammal mammal mammal G_03_00 REINDEER mammal mammal mammal G_03_01 BEAR mammal mammal mammal G_03_01 BOAR mammal mammal mammal G_03_01 CHEETAH mammal mammal mammal G_03_01 LEOPARD mammal mammal mammal G_03_01 LION mammal mammal mammal G_03_01 LYNX mammal mammal mammal G_03_01 MINK mammal mammal mammal G_03_01 MOLE mammal mammal mammal G_03_01 MONGOOSE mammal mammal mammal G_03_01 OPOSSUM mammal mammal mammal G_03_01 POLECAT mammal mammal mammal G_03_01 PUMA mammal mammal mammal G_03_01 WOLF mammal mammal mammal G_03_02 DOLPHIN mammal mammal mammal G_03_02 SEAL mammal mammal mammal G_03_02 SEA_LION mammal mammal mammal G_03_02 SEA_SNAKE reptiles reptiles reptiles G_03_03 CARP fish fish fish G_03_03 CHUB fish fish fish G_03_03 CODFISH fish fish fish G_03_03 HERRING fish fish fish G_03_03 PERCH fish fish fish G_03_03 PIKE fish fish fish G_03_03 PIRANHA fish fish fish G_03_03 SEAHORSE fish fish fish G_03_03 SHARK fish fish fish G_03_03 SOLE fish fish fish G_03_03 STURGEON fish fish fish G_03_03 TUNA fish fish fish The manual cataloging confirmed coloured records groups G_00_00 G_00_01 G_01_02 G_03_02 The questions researchers ask analogue cases complicated important real life companies organisations KB Neural Data Mining Python sources Roberto Bello Pag di errors occur gathering data errors exist inserting data mutations occur inside groups defects production working Is lack design manual classification Is necessary introduce variables columns Comparison results automatic cataloging iris txt recognized botanists The reliability algorithms neural networks appreciated coincidence automatic cataloging files iris txt records carried botanists KB_CAT cataloging groups following results Group RecId Sepal_Length Sepal_Width Petal_Length Petal_Width G_00_00 versicolor100 G_00_00 versicolor54 G_00_00 versicolor56 G_00_00 versicolor60 G_00_00 versicolor63 G_00_00 versicolor69 G_00_00 versicolor70 G_00_00 versicolor72 G_00_00 versicolor73 G_00_00 versicolor81 G_00_00 versicolor83 G_00_00 versicolor85 G_00_00 versicolor88 G_00_00 versicolor90 G_00_00 versicolor91 G_00_00 versicolor93 G_00_00 versicolor95 G_00_00 versicolor97 G_00_00 virginica107 G_00_00 virginica120 G_00_01 versicolor84 G_00_01 virginica102 G_00_01 virginica112 G_00_01 virginica114 G_00_01 virginica122 G_00_01 virginica124 G_00_01 virginica127 G_00_01 virginica128 G_00_01 virginica135 G_00_01 virginica139 G_00_01 virginica143 G_00_01 virginica147 G_00_01 virginica150 G_00_02 virginica101 G_00_02 virginica103 G_00_02 virginica105 G_00_02 virginica106 G_00_02 virginica108 KB Neural Data Mining Python sources Roberto Bello Pag di Group RecId Sepal_Length Sepal_Width Petal_Length Petal_Width G_00_02 virginica109 G_00_02 virginica110 G_00_02 virginica113 G_00_02 virginica115 G_00_02 virginica116 G_00_02 virginica118 G_00_02 virginica119 G_00_02 virginica121 G_00_02 virginica123 G_00_02 virginica125 G_00_02 virginica126 G_00_02 virginica129 G_00_02 virginica131 G_00_02 virginica132 G_00_02 virginica133 G_00_02 virginica136 G_00_02 virginica137 G_00_02 virginica140 G_00_02 virginica141 G_00_02 virginica142 G_00_02 virginica144 G_00_02 virginica145 G_00_02 virginica146 G_00_02 virginica148 G_00_02 virginica149 G_01_00 versicolor58 G_01_00 versicolor61 G_01_00 versicolor65 G_01_00 versicolor68 G_01_00 versicolor80 G_01_00 versicolor82 G_01_00 versicolor89 G_01_00 versicolor94 G_01_00 versicolor96 G_01_00 versicolor99 G_01_01 versicolor62 G_01_01 versicolor64 G_01_01 versicolor67 G_01_01 versicolor71 G_01_01 versicolor79 G_01_01 versicolor86 G_01_01 versicolor92 G_01_02 versicolor78 G_01_02 virginica104 G_01_02 virginica111 G_01_02 virginica117 G_01_02 virginica130 G_01_02 virginica138 G_02_00 setosa1 G_02_00 setosa10 G_02_00 setosa11 G_02_00 setosa12 G_02_00 setosa13 G_02_00 setosa14 G_02_00 setosa15 G_02_00 setosa16 G_02_00 setosa17 KB Neural Data Mining Python sources Roberto Bello Pag di Group RecId Sepal_Length Sepal_Width Petal_Length Petal_Width G_02_00 setosa18 G_02_00 setosa19 G_02_00 setosa2 G_02_00 setosa20 G_02_00 setosa21 G_02_00 setosa22 G_02_00 setosa23 G_02_00 setosa24 G_02_00 setosa25 G_02_00 setosa26 G_02_00 setosa27 G_02_00 setosa28 G_02_00 setosa29 G_02_00 setosa3 G_02_00 setosa30 G_02_00 setosa31 G_02_00 setosa32 G_02_00 setosa33 G_02_00 setosa34 G_02_00 setosa35 G_02_00 setosa36 G_02_00 setosa37 G_02_00 setosa38 G_02_00 setosa39 G_02_00 setosa4 G_02_00 setosa40 G_02_00 setosa41 G_02_00 setosa42 G_02_00 setosa43 G_02_00 setosa44 G_02_00 setosa45 G_02_00 setosa46 G_02_00 setosa47 G_02_00 setosa48 G_02_00 setosa49 G_02_00 setosa5 G_02_00 setosa50 G_02_00 setosa6 G_02_00 setosa7 G_02_00 setosa8 G_02_00 setosa9 G_02_02 versicolor51 G_02_02 versicolor52 G_02_02 versicolor53 G_02_02 versicolor55 G_02_02 versicolor57 G_02_02 versicolor59 G_02_02 versicolor66 G_02_02 versicolor74 G_02_02 versicolor75 G_02_02 versicolor76 G_02_02 versicolor77 G_02_02 versicolor87 G_02_02 versicolor98 G_02_02 virginica134 With exception records highlighted yellow automatic KB Neural Data Mining Python sources Roberto Bello Pag di cataloging confirmed botanists s opinion inserted column RecId reaching high value Knowledge Index Clinical trials hepatitis B virus In KB_CAT process data important research clinical trials subjects variables columns The research concerned hepatitis B virus characteristics typical carriers symptomless carriers low viral repetition possible evolution virus pathologies identification marker diagnosis treatment The problem identi f icat ion consists determining person characterist ics associate group carr iers virus The variables columns concern general t ies subjects age race weight height area birth residency omit ted resulted l t t le importance previous runs The weight height subjects misleading f adopted separately For reason t calculated body mass index BMI connects attr ibutes relat ionship BMI kg m In addition calculated index particularly significant correlates weight height gender age subject The real index weight takes account muscles body subjects fact percentage fat mass FAT There different formulas similar calculate index In case considered formula Deurenberg takes account age subject FAT BMI age gender gender men gender women Regarding variables related potus indicating number glasses wine beer spirits drunk daily long subjects drinking calculated alcohol units product values Consequently eliminated field indicating subject teetotaler The input file revision shown following table Generalities Age Gender BMI FAT Case Potus Total UA Total alcoholic Units Diagnosis Diagnosis Steatohepatitis Steatosis Therapy PreInterferone PreLamivudina PreAdefovir PrePegInterferone PreEntecavir PreTecnofovir Interferone Peginterferone Lamivudina Adefovir Tecnofovir KB Neural Data Mining Python sources Roberto Bello Pag di Laboratory trials AST ALT HBeAg AntiHBe AntiHBclgM HBVDNAqualitative HBVDNAquantitative GentipoHBV AntiDelta HIV AntiHCV HCVRNAQualitative GentipoHCV The significant characteristics numerous groups contained following table Group M FFAT Case UA Diagnosis Adefo vir AST ALTHBeAg AntiHBclgM 1_01 M Prevalent Chronic Hepatitis No high NegativeNegative 1_03 M Prevalent Chronic Hepatitis No medium NegativeNot researched 2_01 M Prevalent Carrier non repeating phase No normal NegativeNegative 2_08 F Prevalent Chronic Hepatitis carrier non repeating phase No normal NegativeNegative 3_01 M Prevalent HCC Cirrhosis No high NegativeNegative 3_04 M Incidental Chronic Hepatitis carrier non repeating phase No high NegativeNot researched 4_01 M Prevalent HCC Cirrhosis No high NegativePositive researched 8_01 M Prevalent Chronic Hepatitis No high PositiveNegative 8_08 M Prevalent Cirrhosis yes medium NegativeNegative From comprehensive analysis results obtained published detail following conclusions emerged women drink men consequently suffer hepatitis cirrhosis men age years diagnosis HCC persons steatosis steatohepatitis positive high level carriers virus non repetitive phase normal values AST ALT laboratory exams HIV positive exclusively men chronic hepatitis diagnosis cirrhosis present subjects age years carriers non repetitive phase percentage incident cases greater diagnoses subjects diagnosed HCC nearly prevalent cases highest values ALT AST regard subjects diagnosed cirrhosis HCC nearly women non repetit I have phase result teetotaler case men lowest values FAT concerns men diagnosed chronic hepatitis values units alcohol reduced Not good better mail Dear Roberto software winner As I anticipated I sent conclusions E P A hepatologist KB Neural Data Mining Python sources Roberto Bello Pag di publisher native Milan works Palermo owners database especially data carried detailed clinical statistical analysis He phoned morning asking doctor contributed analysing database writing report When I explained E Are doctors conclusions drawn software didn t want believe All I mean ALL conclusions correct coincide taken statistical analysis clinical observations I explained software process verified collaboration useful As case verifying conclusions software parallel clinical statistical analysis databases informative clinical publication released verify application field He pleased collaborate way start sending clinical databases analysed cases small KB tested A big pat congratulations KB_STA statistical analysis result cataloging Generalities aims functions The aim KB_STA help researchers analysis results processing KB_STA proven indispensable input file large size records number variables columns A purely visual exam records group difficult resulting lot hard work highlighting need subject groups costly external analysis complex questionable results KB_STA resolves problem black box typical algorithms neural networks KB_STA submit file CV groups statistical analysis evaluates degree homogeneity groups evaluates importance variables columns cataloging records groups groups records group variable column quartiles numeric frequency tables text values required shows group variable column original value input records KB Neural Data Mining Python sources Roberto Bello Pag di Source KB_STA attachment How use Having kb_sta py program input file process folder run KB_STA typing window python kb_sta py python ask kb_sta py run python language The program begins processing asking succession Catalogued Records File _outsrt txt vessels_M_g3_outsrt txt vessels_M_g3_outsrt txt file txt format containing table records cases cataloged arranged group_code sequence The file vessels_M_g3_outsrt txt results previous processing program KB_CAT Groups CV File _cv txt vessels_M_g3_cv txt vessels_M_g3_cv txt file txt format containing table CV groups The file vessels_M_g3_cv txt results previous processing KB_CAT program It important file previous come KB_CAT processing Report File output vessels_M_g3_report txt vessels_M_g3_report txt output file contain statistical analysis results obtained previous program cataloging It useful clarity report file beginnings previous exemplified case statistical analysis different parameters names change final example _r1 _r2 _r3 Display Input Records Y N n Group Consistency Parameter request display groups percentage homogeneity inside indicated advisable carry initial processing parameter set equal zero groups use different parameter relation results achieved Too high value parameter produce list Variable Consistency Parameter request display variables groups percentage homogeneity variable indicated advisable carry initial processing parameter set equal zero variables groups use different parameter relation results KB Neural Data Mining Python sources Roberto Bello Pag di obtained Too high value parameter produce list Select groups containing records The parameter request visualization groups composed x records The groups formed single record automatically homogeneous variables columns Select groups containing records The parameter request display groups composed number records x The parameter useful examining groups containing records Summary Detail report S D d If parameter value s S report contain values homogeneity consistency total number records percentage records cataloged group If parameter value d D report contain numerical values quartile text variable report contain frequency distribution different text values Display Input Records Y N n If parameter value n N input records belonging groups visualized contrary y Y visualized KB_STA running KB_STA KNOWLEDGE DISCOVERY IN DATA MINING STATISTICAL PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON Catalogued Records File _outsrt txt vessels_M_g3_outsrt txt Groups CV File _cv txt vessels_M_g3_cv txt Report File output vessels_M_g3_report txt Group Consistency Variable Consistency Select groups containing records Select groups containing records Summary Detail report S D d Display Input Records Y N n Elapsed time seconds Analysis results cataloging vessels txt Example group G_00_00 G_00_00 Consistency Consistency Records Records shape Consistency Consistency Value cylinder_cone Frequency Percentage Value ball_cone Frequency Percentage Value cylinder Frequency Percentage Value cut_cone Frequency Percentage material Consistency Consistency Value glass Frequency Percentage Value terracotta Frequency Percentage Value metal Frequency Percentage KB Neural Data Mining Python sources Roberto Bello Pag di height Consistency Consistency Mean Min Max Step First Quartile end Frequency Second Quartile end Frequency Third Quartile end Frequency Fourth Quartile end Frequency colour Consistency Consistency Value green Frequency Percentage Value grey Frequency Percentage Value brown Frequency Percentage weight Consistency Consistency Mean Min Max Step First Quartile end Frequency Fourth Quartile end Frequency haft Consistency Consistency Value Frequency Percentage plug Consistency Consistency Value cork Frequency Percentage Value Frequency Percentage Value metal Frequency Percentage The group G_00_00 composed predominantly glass containers height cm green color weight handle haft Analysis results political poll o f The analysed case takes consideration political poll carried 22nd 23rd November Prof Paolo Natale State University Milan Department Political Sciences After initial processing data gave evident results database updated eliminating fields evidently relevant grouping variables Fields size area residence judgement weight democracy politics eliminated The field province residence changed grouping provinces north centre south The new starting database contains records relating persons participated political poll answering questions follow gender men women coalition confidence regardless vote ES extreme left left SS CS center left center CC CD center right right DD ES extreme right answer profession believer yes religion expectations economic situation months judgment current state country s economy protection trust security perception prediction winner possible short term election opinion government s actions KB Neural Data Mining Python sources Roberto Bello Pag di opinion opposition s actions interest politics confidence coalition short term vote party voted party intend vote election PDL party age region qualifications attendance religious functions The processing program KB_CAT groups The results cataloging obtained KB_CAT processed KB_STA From analysis results obtained draw following observations supporters left believe exception group G_04_02 eventual winner center right respond future winners group G_02_04 supporters center left left defend government considering work average supporters centre left left average positive opinion opposition group G_04_04 s f o rmed o f apo l t l peop le agnos t c o r ex t reme l y rese rved peop l e w ho p fe r t t o exp ress op n ion t hey pens rs unemp l oyed ove r yea rs o f age exist past relationship profession economic condition trusted party category pensioners divided imagine victory centre right group G_02_03 prefer reply Group G_02_04 e G_04_04 age affect cataloging groups people want vote PDL groups people speculate winner center right G_01_04 G_02_03 G_04_01 A large observations expressed confirmation loss ideological values linked hard core belonging social class age group level education area residency etc important characteristics past political tendencies voters In idea liquid voters came use people longer supporter party alliance able evaluate results government opposition actions decide vote It obvious define detailed profiles voters formulate specific election programs based ideologies meaningless KB Neural Data Mining Python sources Roberto Bello Pag di KB_CLA Classification new records Generalities aims functions The KB_CAT program produces file containing training matrix example vessels_M_g3_grid txt immediately classify records similar records previously cataloged The use classification programs fly useful act quickly taking consideration knowledge acquired Classifications running real time found example banking insurance fields prevention illegal activity business mobile phones identify customers preparing competition quality control industrial processes products companies avoid cases insolvency clients Source KB_CLA attachment How run KB_CLA requires file new records cases classify structure similar content file previous KB_CAT processing For structure mean file new records cases number variables columns identical format data numerical textual For similar content means file new records cases contain records samples universe Acquired knowledge cataloging animals classify new containers Input files n_vessels txt Contents file n_vessels txt The records cases classify reported following table identified character N description description shape material height colour weight haft plug n_glass cut_cone terracotta transparent No n_bottle cylinder_cone glass brown No metal n_tea_cup cut_cone ceramic white Yes n_cup cut_cone glass transparent Yes n_coffee_cup cut_cone glass transparent Yes n_perfume cylinder glass transparent No plastic n_trousse cylinder plastic blue No yes n_plant_pot cut_cone terracotta brown No n_pasta_case cylinder glass transparent No metal KB Neural Data Mining Python sources Roberto Bello Pag di Number Groups Normalization Max Std None m File Training Grid vessels_M_g3_grid txt KB_CLA running KB_CLA KNOWLEDGE DISCOVERY IN DATA MINING CLASSIFY PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON InputFile n_vessels txt Number Groups Normalization Max Std None m File Training Grid vessels_M_g3_grid txt Output File Classify original n_vessels_CM_g3_out txt Output File Classify sort n_vessels_CM_g3_outsrt txt Output File Summary sort n_vessels_CM_g3_sort txt Output File Matrix Catal n_vessels_CM_g3_catal txt Output File Means STD CV n_vessels_CM_g3_medsd txt Output File CV Groups n_vessels_CM_g3_cv txt Output File Training Grid vessels_M_g3_grid txt Output File Run Parameters n_vessels_CM_g3_log txt Elapsed time seconds Analysis results classification n_vessels txt The new records recognisable letter N inserted previous table obtained KB_CAT Group description shape material height colour Weight haft plug G_00_00 ancient_bottle ball_cone glass green cork G_00_00 bottle_1 cylinder_cone glass green cork G_00_00 bottle_4 cylinder_cone glass green metal G_00_00 carboy ball_cone glass green cork G_00_00 magnum_bottle cylinder_cone glass green metal G_00_00 plant_pot cut_cone terracotta brown G_00_00 umbrella_stand cylinder metal grey G_00_00 n_bottle cylinder_cone glass brown metal G_00_00 n_glass cut_cone terracotta transparent G_00_00 n_pasta_case cylinder glass transparent metal G_00_00 n_plant_pot cut_cone terracotta brown G_00_01 pot_1 cylinder metal grey yes G_00_02 coffee_cup cut_cone ceramic white yes G_00_02 cup_1 cut_cone ceramic white yes G_00_02 cup_2 cut_cone glass transparent yes KB Neural Data Mining Python sources Roberto Bello Pag di Group description shape material height colour Weight haft plug G_00_02 pot_2 cut_cone metal grey yes yes G_00_02 n_coffee_cup cut_cone glass transparent yes G_00_02 n_tea_cup cut_cone ceramic white yes G_01_00 beer_jug cut_cone porcelain severals G_01_00 bottle_2 cylinder_cone glass transparent cork G_01_00 bottle_3 cylinder_cone glass opaque plastic G_01_00 glass_1 cut_cone pewter pewter G_01_00 glass_3 cut_cone terracotta grey G_01_00 tuna_can cylinder metal severals G_01_00 n_perfume cylinder glass transparent plastic G_01_00 n_trousse cylinder plastic blue yes G_01_02 n_cup cut_cone glass transparent yes G_02_00 cd parallelepiped plastic transparent G_02_00 champagne_glass cut_cone crystal transparent G_02_00 dessert_glass cut_cone glass transparent G_02_00 glass_2 cut_cone plastic white G_02_00 pasta_case parallelepiped glass transparent metal G_02_00 perfume parallelepiped glass transparent plastic G_02_00 tetrapack1 parallelepiped mixed severals plastic G_02_00 tetrapack2 parallelepiped plastic severals plastic G_02_00 tetrapack3 parallelepiped millboard severals G_02_00 toothpaste cylinder plastic severals plastic G_02_00 trousse cylinder plastic silver yes G_02_00 tuna_tube cylinder plastic severals plastic G_02_00 visage_cream cylinder metal white G_02_00 wine_glass cut_cone glass transparent G_02_01 pyrex parallelepiped glass transparent glass G_02_02 cleaning_1 parall_cone plastic white yes plastic G_02_02 cleaning_2 cylinder_cone plastic blue yes plastic G_02_02 cleaning_3 cone plastic severals yes plastic G_02_02 jug cylinder terracotta white yes G_02_02 milk_cup cut_cone terracotta blue yes G_02_02 tea_cup cut_cone terracotta white yes G_02_02 watering_can irregular plastic green yes The new records classified completely correct way records highlighted pink colour The n_glass record classified group G_00_00 variables colour e weight values present records group Political opinions Facebook January A sample political opinions present different groups discussion examined fb_casini fb_fini fb_bonino fb_di_pietro fb_corsera fb_fanpage fb_brambilla fb_storace fb_maroni fb_bersani fb_meloni fb_grillo fb_termometro_politico fb_fattoquotidiano For post taken consideration group discussion topic discussion e g gay marriage adoption interview TG3 Rai news moral drift political leader involved Bersani Casini Berlusconi etc attitude inferred judgment expressed author post point scale abusive opinion negative opinion indifferent opinion positive opinion laudatory judgment KB Neural Data Mining Python sources Roberto Bello Pag di The research aims explore possible relationships existing groups arguments politicians opinions expressed KB_CAT cataloged opinions groups contain significant number views Group Records groups fb_corsera fb_fanpage politicians Berlusconi Dell Utri Bersani topics candidated moral drift euro cheque Veronica abusive opinion negative opinion comment groups aligned especially fb_corsera group insults abound Group Records groups fb_maroni fb_storace fb_meloni politicians Maroni Storace Meloni topics diary positive laudatory opinions comment groups aligned Group Records groups fb_fanpage fb_grillo fb_fattoquotidiano politicians Grillo Ingroia topics positive opinion comment groups aligned Grillo Ingroia new attract people Group Records groups fb_brambilla fb_casini fb_bersani politicians Brambilla Casini Bersani topics positive laudatory opinions comment aligned groups animal rights mission pays Group Records groups fb_termometro_politico politicians Berlusconi Ingroia topics public services succession taxes offensive negative opinions comment aligned politicians topics hot Group Records groups fb_corsera fb_meloni fb_fanpage politicians Pannella Meloni Vendola topics alliances diary attack offensive negative opinions comment group fb_corsera aligned unpopular alliance Pannella KB Neural Data Mining Python sources Roberto Bello Pag di Storace Group Records groups fb_fanpage politician Monti topics no_marr_adop_gay monti_su_fb offensive negative opinions comment aligned disagreement no_marr_adop_gay Group Records groups fb_bonino fb_brambilla fb_casini politicians Bonino Brambilla Casini topics president_republic animalist_bill positive opinion comment aligned groups Group Records groups fb_casini fb_bersani politicians Casini Bersani topics lies interview TG3 national news interview TG5 national news negative opinion comment criticism aligned groups non shared opinions Group Records groups fb_dipietro fb_casini politicians Di Pietro Casini Grillo topics tv_adverts last_word positive laudatory opinion comment aligned groups Group Records groups fb_fanpage fb_corsera politicians Monti Grillo topics piper profile_fb monti_exorcist offensive opinion comment aligned groups Group Records groups fb_fanpage fb_grillo politicians Monti Grillo topics no_marr_adop_gay meeting_lecce positive laudatory opinion comment laudatory opinions topics Monti Grillo Group Records groups fb_bonino fb_casini politicians Bonino Casini topics regional pact_monti_bersani president_republic KB Neural Data Mining Python sources Roberto Bello Pag di positive opinion comment aligned groups Group Records groups fb_fanpage politician Bersani topics pact_monti_bersani offensive negative opinion comment aligned group Group Records groups fb_di_pietro politician Di Pietro topics rivoluzione_civile tv_advert offensive negative opinion comment aligned group disagreement rivoluzione_civile civil revolution Summary There close relationships typology groups politicians topics opinions In groups aligned positive laudatory opinions plentiful possible disagreements arise sympathizers share political positions opponents immediately marginalised obscene language rare syntactical grammatical forms proper In groups aligned prevails dissent consensus bad language norm syntactic grammar forms poor persons aware suffer criticism open discussions issues banned groups aligned Know4Business Cloud version Google App Engine Giulio Beltrami software engineer expert innovative architectures ICT social type transferred KB field Cloud Computing Google App Engine Know4Business Know4Business usable pay use mode obtainable Internet link http know4business appspot com Know4Business adds Google Apps powerful general purpose discovering hidden knowledge business data enabled known neural network self learning data mining algorithms Know4Business provides tools fulfill knowledge discovery SOURCE preparing checking normalizing cleaning filtering encoding input data KB Neural Data Mining Python sources Roberto Bello Pag di http know4business appspot com CATALOGUE discovering groups sample data way similar known structures STATISTICS evaluate success catalogue clustering CLASSIFIER generalizing known structures apply new data AGENCY return diagnosis suggestions user data management checking success classification plus interactive CONSOLE help data analyst run catalogue tools Report graph results tools Know4Business end end process knowledge discovery provides simple work flow useful feedback capabilities Classification operates forward catalogue sample data Statistics catalogue clustering suggest filtering rules cardinality dimensions sample data Agency influence source filtering rules data management depending circumstances enables kind data knowledge auto poiesis minimizing human intervention Know4Business Main advantages The ease use based simple HTML GUI use tools look results The clear implementation based object oriented paradigm authentic SaaS cloud computing architecture KB Neural Data Mining Python sources Roberto Bello Pag di APPENDIXES Appendix KB_CAT source coding utf KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON import os import random import copy import datetime def mean x mean n len x mean sum x n return mean def sd x standard deviattion n len x mean sum x n sd sum x mean x x n return sd class ndim 3D array flat array def __init__ self x y z d self dimensions x y z self numdimensions d self gridsize x y z KB Neural Data Mining Python sources Roberto Bello Pag di def getcellindex self location cindex cdrop self gridsize index xrange self numdimensions cdrop self dimensions index cindex cdrop location index return cindex def getlocation self cellindex res size reversed self dimensions res append cellindex size cellindex size return res use ndim class n ndim print n getcellindex print n getcellindex print n getcellindex print n getcellindex print n getlocation print n getlocation print n getlocation print n getlocation print print KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print input run parameters error True arch_input raw_input InputFile os path isfile arch_input print Oops File exist Try CTR C exit break True KB Neural Data Mining Python sources Roberto Bello Pag di try num_gruppi int raw_input Number Groups ValueError print Oops That valid number Try num_gruppi print Oops Number Groups low Try num_gruppi print Oops Number Groups big Try break True normaliz raw_input Normalization Max Std None normaliz normaliz upper normaliz normaliz normaliz M normaliz S normaliz N print Oops Input M S N Try break True try max_alpha float raw_input Start value alpha ValueError print Oops That valid number Try max_alpha print Oops Start value alpha big Try max_alpha print Oops Start value alpha low Try break True try min_alpha float raw_input End value alpha ValueError print Oops That valid number Try min_alpha print Oops alpha big Try min_alpha KB Neural Data Mining Python sources Roberto Bello Pag di print Oops alpha low Try break True try step_alpha float raw_input Decreasing step alpha ValueError print Oops That valid number Try step_alpha print Oops Decreasing step alpha big Try step_alpha print Oops Decreasing step alpha low Try break file_input arch_input gruppi_num num_gruppi tipo_norm normaliz alpha_min min_alpha alpha_max max_alpha alpha_step step_alpha outputs files file_input arch_input tipo_norm normaliz gruppi_num num_gruppi nome_input file_input split arch_output nome_input _ tipo_norm _g str gruppi_num _out txt arch_outsrt nome_input _ tipo_norm _g str gruppi_num _outsrt txt arch_sort nome_input _ tipo_norm _g str gruppi_num _sort txt arch_catal nome_input _ tipo_norm _g str gruppi_num _catal txt arch_medsd nome_input _ tipo_norm _g str gruppi_num _medsd txt arch_cv nome_input _ tipo_norm _g str gruppi_num _cv txt arch_grid nome_input _ tipo_norm _g str gruppi_num _grid txt arch_log nome_input _ tipo_norm _g str gruppi_num _log txt start time t0 datetime datetime read input file arr_r arr_orig KB Neural Data Mining Python sources Roberto Bello Pag di arr_c mtchx mtchy txt_col xnomi numbers variables columns record n_rows n_cols err_cols index line open file_input readlines linea line split index xnomi append linea n_cols len linea arr_r append linea len linea n_cols err_cols print Different numbers variables columns record str index cols str len linea index err_cols print File file_input contains errors Exit quit index index len arr_r linea arr_r index index_c index_c len linea linea index_c isdigit linea index_c float linea index_c index_c arr_r index linea index arr_orig copy deepcopy arr_r original input file testata_cat copy deepcopy xnomi original header row finding columns containing strings columns containing numbers testata xnomi testata_orig copy deepcopy xnomi n_cols len testata n_rows len arr_r ind_c err_type KB Neural Data Mining Python sources Roberto Bello Pag di ind_c len testata ind_r tipo_num tipo_txt ind_r len arr_r arr_c arr_r ind_r isinstance arr_c ind_c basestring tipo_txt tipo_num ind_r tipo_num tipo_txt print The columns variables testata ind_c contains strings numbers print arr_c err_type ind_c err_type print Oops The columns variables contains strings numbers Exit quit index_c index_c n_cols txt_col index index len arr_r arr_c arr_r index isinstance arr_c index_c str txt_col append arr_c index_c index set_txt_col set txt_col remove duplicates txt_col list set set_txt_col txt_col sort strings numbers len txt_col len txt_col passo1 len txt_col passo1 index index len arr_r arr_c arr_r index campo1 arr_c index_c indice1 txt_col index campo1 len txt_col values column KB Neural Data Mining Python sources Roberto Bello Pag di val_num1 float val_num1 float passo1 indice1 arr_c index_c val_num1 avoid zero values means prevent zero divide CV index index_c means max std xmeans xmaxs xmins aggiunto Roberto xsds xcv index_c index_c n_cols xmeans append xmaxs append xmins append aggiunto Roberto xsds append xcv append index_c means max index index n_rows arr_c arr_r index index_c index_c n_cols xmeans index_c arr_c index_c arr_c index_c xmaxs index_c xmaxs index_c arr_c index_c index_c index index_c index_c n_cols xmeans index_c xmeans index_c n_rows index_c std index index n_rows arr_c arr_r index index_c index_c n_cols xsds index_c arr_c index_c xmeans index_c KB Neural Data Mining Python sources Roberto Bello Pag di index_c index index_c index_c n_cols xsds index_c xsds index_c n_cols index_c Means Max Std CV output file medsd_file open arch_medsd w columns names medsd_file write s s Function t index_c index_c n_cols medsd_file write s s testata index_c t index_c medsd_file write s n means medsd_file write s s Mean t index_c index_c n_cols valore str xmeans index_c valore valore medsd_file write s s valore t index_c medsd_file write s n max medsd_file write s s Max t index_c index_c n_cols valore str xmaxs index_c valore valore medsd_file write s s valore t index_c medsd_file write s n std medsd_file write s s Std t index_c index_c n_cols valore str xsds index_c valore valore medsd_file write s s valore t KB Neural Data Mining Python sources Roberto Bello Pag di index_c medsd_file write s n CV medsd_file write s s CV t index_c med_cv_gen cv average columns variables index_c n_cols xmeans index_c media1 media1 xmeans index_c xcv index_c xsds index_c abs media1 valore str xcv index_c med_cv_gen xcv index_c valore valore medsd_file write s s valore t index_c med_cv_gen med_cv_gen n_cols str_med_cv_gen str med_cv_gen str_med_cv_gen str_med_cv_gen medsd_file write s n medsd_file close input standardization standardization max tipo_norm M index index n_rows arr_c arr_r index index_c index_c n_cols aggiornare anche kb_cla py xmaxs index_c xmaxs index_c arr_c index_c arr_c index_c xmaxs index_c index_c index standardization std tipo_norm S index index n_rows arr_c arr_r index KB Neural Data Mining Python sources Roberto Bello Pag di index_c index_c n_cols xsds index_c xsds index_c arr_c index_c arr_c index_c xmeans index_c xsds index_c arr_c index_c xmins index_c aggiunto Roberto xmins index_c arr_c index_c aggiunto Roberto index_c index aggiungo xmins eliminare valori negativi aggiunto da Roberto index index n_rows arr_c arr_r index index_c index_c n_cols arr_c index_c arr_c index_c xmins index_c print arr_c index_c index_c index fine aggiunta da Roberto start kohonen algorithm min max vectors vmaxs vmins index_c index_c n_cols vmaxs append vmins append index_c columns min max index index n_rows arr_c arr_r index index_c index_c n_cols arr_c index_c vmaxs index_c vmaxs index_c arr_c index_c arr_c index_c vmins index_c vmins index_c arr_c index_c index_c KB Neural Data Mining Python sources Roberto Bello Pag di index run parameters temp arrays n n_rows m n_cols nx gruppi_num ny gruppi_num ix integer random seed nsteps int nx ny number steps nepoks int nsteps n number epochs unit_calc int n m nx ny running units passo int n step visualization monitor rmax nx rmin passo passo grid training grid index index nx ny m grid append index n ndim nx ny m random seed ix initial value random seed obtain sequences new runs index index nx index_c index_c ny index_k index_k m ig n getcellindex index index_c index_k grid ig random random index_k index_c index gridp copy deepcopy grid initial previous grid current grid gridm copy deepcopy grid initial min grid current grid record epoch iter discrea current error discrep previous error nepoks nepoks min epochs KB Neural Data Mining Python sources Roberto Bello Pag di nepokx min_epok epoch min error min_err min error alpha float alpha_max initial value alpha parameter ir initial value ir parameter ir ne print print Record str n_rows Columns str n_cols main loop try ne nepoks ne passo print running message modulo division zero min_err_txt 5f min_err format integers decimals alpha_txt 5f alpha format integers decimals print Epoch str ne min err min_err_txt min epoch str min_epok alpha alpha_txt min_err nepokx min_err discrea discrep discrea discrea min_epok ne current epoch min min_err discrea copy current grid min grid gridm copy deepcopy grid min_err_txt 3f min_err format integers decimals alpha_txt 5f alpha format integer decimals print Epoch str ne WITH MIN ERROR min_err_txt alpha alpha_txt cheking current value alpha alpha alpha_min discrea discrep discrep copy current grid previous grid gridp copy deepcopy grid starting row ending row n_rows iter find best grid coefficient ihit jhit dhit igx KB Neural Data Mining Python sources Roberto Bello Pag di igy igx nx igy igy ny d neff k arr_c arr_r k m update sum squared deviation input value grid coefficient ig n getcellindex igx igy k d d arr_c k grid ig k d d float m d d m d dhit dhit d ihit int igx jhit int igy igy igx update iteration error discrep discrep dhit coordinates best grid coefficient ir max rmax float iter ir int ir new alpha value increase radius groups proximity alpha max alpha_max float ne alpha_step alpha_min update grid coefficients applying alpha parameter inn0 int ihit int ir inn9 int ihit int ir jnn0 int jhit int ir jnn9 int jhit int ir inn0 inn9 jnn0 int jhit int ir jnn0 jnn9 inn0 inn0 nx jnn0 jnn0 ny arr_c arr_r k k m ig n getcellindex inn0 jnn0 k grid ig alpha arr_c k grid ig k jnn0 inn0 KB Neural Data Mining Python sources Roberto Bello Pag di print print Min alpha reached print break ne KeyboardInterrupt print print KeyboardInterrupt Ctrl C print pass computing results grid grid min grid copy deepcopy gridm write min grid file arch_grid_file open arch_grid w ii ii nx j j ny k k m ig n getcellindex ii j k arch_grid_file write 6i s 6i s 6i s 7f s ii j k grid ig n k j ii arch_grid_file close catalog input min grid ii ii n_rows ihit jhit dhit numbers groups ir ir nx numbers groups jc jc ny numbers groups d neff KB Neural Data Mining Python sources Roberto Bello Pag di k k n_cols update sum squared deviation input value grid coefficient arr_c arr_r ii ig n getcellindex ir jc k d d arr_c k grid ig k d d m d dhit save coordinates best coefficient dhit d ihit ir jhit jc jc ir mtchx append ihit mtchy append jhit ii write arch_catal file arch_catal_file open arch_catal w ii ii n_rows arch_catal_file write 6i s 6i s 6i s ii mtchx ii mtchy ii n ii arch_catal_file close matrix statistics arr_cv CV array Groups Total arr_med means array Groups riga_cv CV row arr_cv arr_col group temporary array arr_grsg input data array normalized arr_grsg_c copy arr_grsg file sort input matrix sort group sequence ii ix ii n_rows ix gr1 str mtchx ii mtchx ii gr1 str mtchx ii sg1 str mtchy ii mtchy ii sg1 str mtchy ii KB Neural Data Mining Python sources Roberto Bello Pag di riga_norm arr_r ii I am riga_norm1 I am m riga_norm1 append str riga_norm I am I am riga_norm2 join riga_norm1 gr_sg_txt G_ gr1 _ sg1 str ix riga_norm2 arr_grsg append gr_sg_txt ii arr_grsg sort ii ii n_rows arr_grsg_c append arr_grsg ii ii setup arr_cv matrix num_gr gruppo0 ir ir n_rows grsg_key arr_grsg_c ir split grsg_key gruppo0 gruppo0 grsg_key num_gr ic riga1 riga1 append grsg_key ic m adding new columns row mean n records riga1 append ic arr_cv append riga1 cv row ir riga1 riga1 append Means adding new row cv mean ic ic m adding new column row mean n records riga1 append ic arr_cv append riga1 def found x ir ir len arr_cv linea_cv arr_cv ir key_cv linea_cv KB Neural Data Mining Python sources Roberto Bello Pag di key_cv x return ir ir ir irx len arr_grsg_c ic linea_cv arr_cv icx len linea_cv val_col ic icx ir gruppo val_col ir irx linea arr_grsg_c ir split linea gruppo gruppo gruppo linea val_col append float linea ic i_gruppo found gruppo linea_cv arr_cv i_gruppo media_v abs mean val_col media_v media_v std_v sd val_col cv_v std_v media_v linea_cv ic cv_v cv value linea_cv len linea_cv len val_col number records val_col val_col append float linea ic gruppo linea ir i_gruppo found gruppo linea_cv arr_cv i_gruppo media_v abs mean val_col media_v media_v std_v sd val_col cv_v std_v media_v linea_cv ic cv_v cv value linea_cv len linea_cv len val_col number records ic ir irx len arr_cv KB Neural Data Mining Python sources Roberto Bello Pag di linea_cv arr_cv icx len linea_cv ic num_rec1 ir irx rows mean media_riga ic num_col1 linea_cv arr_cv ir ic icx media_riga float linea_cv ic num_col1 ic linea_cv icx media_riga num_col1 num_rec1 linea_cv icx ir ir ic ic icx weighted mean columns media_col ir num_rec1 ir irx linea_cv arr_cv ir media_col media_col linea_cv ic linea_cv icx linea_cv icx number records num_rec1 num_rec1 linea_cv icx ir linea_cv arr_cv irx linea_cv ic media_col num_rec1 ic updating mean row linea_cv arr_cv irx linea_means linea_cv icx media_riga mean linea_means linea_cv icx media_riga Total mean linea_cv icx num_rec1 n records cv_media_gen_after str media_riga cv_media_gen_after cv_media_gen_after write cv file testata_cv testata testata_cv Groups KB Neural Data Mining Python sources Roberto Bello Pag di testata_cv append Mean testata_cv append N_recs arch_cv_file open arch_cv w ic ic icx arch_cv_file write s s testata_cv ic len testata_cv ic ic arch_cv_file write s n ir ir irx ic linea_cv arr_cv ir ic icx ic arch_cv_file write s s linea_cv ic icx arch_cv_file write 4f s linea_cv ic arch_cv_file write 6i s linea_cv ic ic arch_cv_file write s n ir ic media_xcv mean xcv icx ic icx print CV input catalogue ic arch_cv_file write s s CVinp ic icx arch_cv_file write 4f s xcv ic arch_cv_file write 4f s media_xcv arch_cv_file write 6i s linea_cv ic ic arch_cv_file write s n istruzioni aggiunte Roberto Bello know_index str float cv_media_gen_after float str_med_cv_gen know_index know_index arch_cv_file write s s s KIndex know_index n fine istruzioni aggiunte da Roberto Bello arch_cv_file close writing catalog file KB Neural Data Mining Python sources Roberto Bello Pag di testata_cat1 testata_cat1 append Group arch_output_file open arch_output w ic ic icx testata_cat1 append testata_cat ic ic ic ic len testata_cat1 arch_output_file write s s testata_cat1 ic len testata_cat1 ic ic arch_output_file write s n index index len arr_orig riga_orig arr_orig index ic ic len riga_orig isinstance riga_orig ic str riga_orig ic str riga_orig ic ic place gr sg gr1 str mtchx index mtchx index gr1 str mtchx index sg1 str mtchy index mtchy index sg1 str mtchy index arr_rig0 G_ gr1 _ sg1 arch_output_file write s arr_rig0 ic ic len riga_orig arch_output_file write s s riga_orig ic len riga_orig ic ic arch_output_file write s n index testata_cat1 testata_cat1 append Group testata_cat1 append RecNum arch_sort_file open arch_sort w ic ic icx testata_cat1 append testata_cat ic ic ic ic len testata_cat1 arch_sort_file write s s testata_cat1 ic len testata_cat1 ic KB Neural Data Mining Python sources Roberto Bello Pag di ic arch_sort_file write s n index index len arr_grsg_c riga_grsg arr_grsg_c index split ic ic len riga_grsg val_txt riga_grsg ic val_txt val_txt arch_sort_file write s s val_txt len val_txt ic index len arr_grsg_c arch_sort_file write s n index arch_sort_file close writing catalog sorted file arr_outsrt index index len arr_orig riga_sort place gr sg gr1 str mtchx index mtchx index gr1 str mtchx index sg1 str mtchy index mtchy index sg1 str mtchy index riga_sort append G_ gr1 _ sg1 ic riga_orig arr_orig index ic len riga_orig val_riga riga_orig ic riga_sort append val_riga ic arr_outsrt append riga_sort index line arr_outsrt line join line arr_outsrt sort testata_srt testata_srt append Group arch_outsrt_file open arch_outsrt w KB Neural Data Mining Python sources Roberto Bello Pag di ic ic icx testata_srt append testata_orig ic ic ic ic len testata_srt arch_outsrt_file write s s testata_srt ic len testata_srt ic ic arch_outsrt_file write s n index key_gruppo index len arr_outsrt riga_sort arr_outsrt index index_c index_c len riga_sort index_c riga_sort key_gruppo arch_outsrt_file write s n key_gruppo riga_sort valore riga_sort index_c arch_outsrt_file write s s valore len valore index_c index len arr_grsg_c arch_outsrt_file write s n index arch_outsrt_file close print print KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print arch_log_file open arch_log w arch_log_file write s s n arch_log_file write s s KB_CAT KNOWLEDGE DISCOVERY IN DATA MINING CATALOG PROGRAM n arch_log_file write s s ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED n arch_log_file write s s Language PYTHON n arch_log_file write s s n arch_log_file write s s s Input File file_input n arch_log_file write s s s Numer Groups str gruppi_num n KB Neural Data Mining Python sources Roberto Bello Pag di arch_log_file write s s s Normalization Max Std None tipo_norm n arch_log_file write s s s Start Value alpha str alpha_max n arch_log_file write s s s End Value alpha str alpha_min n arch_log_file write s s s Decreasing step alpha str alpha_step n arch_log_file write s OUTPUT n arch_log_file write s s s Output File Catalog original arch_output n arch_log_file write s s s Output File Catalog sort arch_outsrt n arch_log_file write s s s Output File Summary sort arch_sort n arch_log_file write s s s Output File Matrix Catal arch_catal n arch_log_file write s s s Output File Means STD CV arch_medsd n arch_log_file write s s s Output File CV Groups arch_cv n arch_log_file write s s s Output File Training Grid arch_grid n arch_log_file write s s s Output File Run Parameters arch_log n istruzioni aggiunte Roberto Bello know_index str float cv_media_gen_after float str_med_cv_gen know_index know_index arch_log_file write s s s KIndex know_index n fine istruzioni aggiunte da Roberto Bello min_err_txt 3f min_err format integer decimals alpha_txt 5f alpha format integer decimals alpha_min_txt 5f alpha_min format integer decimals print min_err print Oops No result Try new alpha parameters print print EPOCH str min_epok WITH MIN ERROR min_err_txt starting alpha alpha_min_txt ending alpha alpha_txt Iterations str iter Total Epochs str ne print print Output File Catalog original arch_output print Output File Catalog sort arch_outsrt print Output File Summary sort arch_sort print Output File Matrix Catal arch_catal print Output File Means STD CV arch_medsd print Output File CV Groups arch_cv print Output File Training Grid arch_grid print Output File Run Parameters arch_log KB Neural Data Mining Python sources Roberto Bello Pag di print CV Catalog str_med_cv_gen print CV Catalog cv_media_gen_after know_index str float cv_media_gen_after float str_med_cv_gen know_index know_index print Knowledge Index know_index print Elapsed time t1 datetime datetime elapsed_time t1 t0 print Elapsed time seconds str elapsed_time seconds print Appendix KB_STA source coding utf KB_STA KNOWLEDGE DISCOVERY IN DATA MINING STATISTICAL PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON import os import random import copy import datetime def fp_conversion value string containing number float try return float value ValueError return value def count s e frequencies count return len x x s x e print print KB_STA KNOWLEDGE DISCOVERY IN DATA MINING STATISTICAL PROGRAM print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print input output files run parameters error True file_input raw_input Cataloged Records File _outsrt txt KB Neural Data Mining Python sources Roberto Bello Pag di os path isfile file_input print Oops File exist Try CTR C exit break True file_gruppi raw_input Groups CV File _cv txt os path isfile file_gruppi print Oops File exist Try CTR C exit break True file_rappor raw_input Report File output os path isfile file_rappor print Oops File exist Try CTR C exit break True try omog_perc int raw_input Group Consistency ValueError print Oops That valid number Try omog_perc print Oops Group Consistency low Try omog_perc print Oops Group Consistency big Try break True try omog_vari int raw_input Variable Consistency ValueError print Oops That valid number Try omog_vari print Oops Variable Consistency low Try omog_vari print Oops Variable Consistency big Try break KB Neural Data Mining Python sources Roberto Bello Pag di True try rec_min int raw_input Select groups containing records ValueError print Oops That valid number Try rec_min print Oops Number records low Try break True try rec_max int raw_input Select groups containing records ValueError print Oops That valid number Try rec_max print Oops Number records low Try rec_max rec_min print Oops Number records str rec_min Try break True est_rapp raw_input Summary Detail report S D est_rapp est_rapp upper est_rapp est_rapp est_rapp S est_rapp D print Oops Input S D Try break inp_rapp N est_rapp D est_rapp d True inp_rapp raw_input Display Input Records Y N inp_rapp inp_rapp upper inp_rapp inp_rapp inp_rapp Y inp_rapp N print Oops Input Y N Try break start time KB Neural Data Mining Python sources Roberto Bello Pag di t0 datetime datetime initial setup arr_r input rows arr_c row list arr_c xnomi headings row len_var max string lenght variable numbers variables columns record n_rows n_cols err_cols index file_log file_input _log txt line open file_input readlines linea line split index xnomi append linea n_cols len linea arr_r append linea len linea n_cols err_cols print Different numbers variables columns record str index cols str len linea index err_cols print File file_input contains errors Exit quit index index len arr_r linea arr_r index index_c index_c len linea converting strings containing numbers float linea index_c fp_conversion linea index_c index_c arr_r index linea index testata xnomi n_cols len testata n_rows len arr_r index index len testata finding max string len report len_var append len testata index index index KB Neural Data Mining Python sources Roberto Bello Pag di index len arr_r linea arr_r index index_c index_c len linea isinstance linea index_c basestring text len_campo len linea index_c number len_campo len str linea index_c len_campo len_var index_c len_var index_c len_campo index_c index max_len max len_var arr_cv testata_cv index reading Groups CV file line open file_gruppi readlines linea line split index n_cols len linea testata_cv append linea arr_cv append linea len linea n_cols err_cols print Different numbers variables columns record str index cols str len linea index err_cols print File file_gruppi contains errors Exit quit line arr_cv index_c linea line index_c len linea converting strings containing numbers float linea index_c fp_conversion linea index_c index_c ind_fine len arr_cv value arr_cv ind_fine ind_fine arr_c arr_cv ind_fine row totals tot_omogen float arr_c consistency totals KB Neural Data Mining Python sources Roberto Bello Pag di tot_record float arr_c total records arch_rappor open file_rappor w index ind_fine len arr_cv ind_fine ind_fine row CV Totals testata_cv testata_cv testata_cv testata_cv removing columns arch_rappor write s s s s s KB_STA Statistical Analysis file_input file_gruppi n arch_rappor write s s s s s s s s s Min Perc group Consistency str omog_perc Min Perc variable Consistency str omog_vari nMin Number records str rec_min Max Number records str rec_max n arch_rappor write s ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED n index len arr_cv arr_c arr_cv index intero int arr_c totals records perc_omogen tot_omogen perc_omogen arr_c tot_omogen perc_omog int perc_omogen perc_omog perc_omog perc_rec intero tot_record perc_omog omog_perc rec_min intero rec_max intero arr_c Means arch_rappor write s n arch_rappor write s s 4f s 3s s 5s s 2f s arr_c Consistency arr_c Consistency str perc_omog Records str intero Records perc_rec n ind_c cod_gruppo arr_c ind_c len arr_c omogen_perc float arr_c ind_c arr_c ind_c arr_c omogen_perc arr_c ind_c arr_c CV group variabile divided CV group omogen_perc omogen_perc omogen_perc omogen_perc omog_vari est_rapp d est_rapp D KB Neural Data Mining Python sources Roberto Bello Pag di consistency value min parameter arch_rappor write s s s 4f s 2f s testata_cv ind_c max_len len testata_cv ind_c Consistency t float arr_c ind_c t Consistency t omogen_perc n computing variables frequencies quartiles variables frequencies ind_sort arr_temp variable array records included group ind_temp ind_sort len arr_r list variable values group linea arr_r ind_sort linea strip cod_gruppo arr_temp append linea ind_c est_rapp d est_rapp D inp_rapp y inp_rapp Y arch_rappor write s s s s s s linea tID record t str linea max_len len str linea Value linea ind_c n ind_temp ind_sort converting strings containing numbers float ind_temp tipo_num tipo_txt ind_temp len arr_temp texts numbers arr_temp ind_temp fp_conversion arr_temp ind_temp isinstance arr_temp ind_temp basestring tipo_txt tipo_num ind_temp tipo_num tipo_txt print The columns variable testata ind_c contains strings numbers Exit quit tipo_num variable text arr_temp sort computing frequencies key1 key_freq keys frequencies arr_t_index arr_t_index len arr_temp KB Neural Data Mining Python sources Roberto Bello Pag di arr_temp arr_t_index key1 kf_valore kf_valore append arr_temp count arr_temp arr_t_index kf_valore append arr_temp arr_t_index key_freq append kf_valore key1 arr_temp arr_t_index arr_t_index key_freq sort frequencies ascending sort key_freq reverse frequencies descending sort ris_out_index ris_out_index len key_freq est_rapp d est_rapp D kf_valore key_freq ris_out_index arch_rappor write s s s 7i s 2f s Value t kf_valore max_len len kf_valore Frequency t kf_valore tPercentage t kf_valore len arr_temp n ris_out_index tipo_txt variabile number computing means len arr_temp mean_arr sum arr_temp len arr_temp computing step quartiles arr_temp sort len arr_temp minimo arr_temp massimo arr_temp len arr_temp passo float massimo float minimo q1 minimo passo q2 q1 passo q3 q2 passo q4 q3 passo fr1 quartile fr2 second quartile fr3 quartile fr4 fourth quartile arr_index arr_index len arr_temp arr_temp arr_index q1 fr1 elif arr_temp arr_index q2 fr2 elif arr_temp arr_index q3 fr3 KB Neural Data Mining Python sources Roberto Bello Pag di fr4 arr_index records len arr_temp p1 fr1 records p2 fr2 records p3 fr3 records p4 fr4 records est_rapp d est_rapp D arch_rappor write s 2f s 2f s 2f s 2f s Mean t mean_arr Min t minimo tMax t massimo tStep t passo n p1 arch_rappor write s 2f s 2f s First Quartile end q1 Frequency t p1 n p2 arch_rappor write s 2f s 2f s Second Quartile end q2 Frequency t p2 n p3 arch_rappor write s 2f s 2f s Third Quartile end q3 Frequency t p3 n p4 arch_rappor write s 2f s 2f s Fourth Quartile end q4 Frequency t p4 n ind_c index arch_rappor close arch_log open file_log w arch_log write s s n arch_log write s s KB_STA KNOWLEDGE DISCOVERY IN DATA MINING STATISTICAL PROGRAM n arch_log write s s ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED n arch_log write s s Language PYTHON n arch_log write s s n arch_log write s s s INPUT Cataloged Records File _outsrt txt file_input n arch_log write s s s INPUT Groups CV File _cv txt file_gruppi n arch_log write s s s Group Consistency str omog_perc n arch_log write s s s Variable Consistency KB Neural Data Mining Python sources Roberto Bello Pag di str omog_vari n arch_log write s s s Select groups containing records str rec_min n arch_log write s s s Select groups containing records str rec_max n arch_log write s s s Summary Detail report S D est_rapp n arch_log write s s s Display Input Records Y N inp_rapp n arch_log write s s OUTPUT n arch_log write s s s Report File file_rappor n arch_log close Elapsed time t1 datetime datetime elapsed_time t1 t0 print Elapsed time seconds str elapsed_time seconds str elapsed_time microseconds print Appendix KB_CLA source coding utf KB_CLA KNOWLEDGE DISCOVERY IN DATA MINING CLASSIFY PROGRAM ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED Language PYTHON import os import random import copy import datetime def mean x mean n len x mean sum x n return mean def sd x standard deviattion n len x mean sum x n sd sum x mean x x n return sd class ndim 3D array flat array def __init__ self x y z d self dimensions x y z KB Neural Data Mining Python sources Roberto Bello Pag di self numdimensions d self gridsize x y z def getcellindex self location cindex cdrop self gridsize index xrange self numdimensions cdrop self dimensions index cindex cdrop location index return cindex def getlocation self cellindex res size reversed self dimensions res append cellindex size cellindex size return res use ndim class n ndim print n getcellindex print n getcellindex print n getcellindex print n getcellindex print n getlocation print n getlocation print n getlocation print n getlocation print print KB_CLA KNOWLEDGE DISCOVERY IN DATA MINING CLASSIFY PROGRAM print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print input run parameters error True arch_input raw_input InputFile os path isfile arch_input print Oops File exist Try CTR C exit break KB Neural Data Mining Python sources Roberto Bello Pag di True try num_gruppi int raw_input Number Groups ValueError print Oops That valid number Try num_gruppi print Oops Number Groups low Try num_gruppi print Oops Number Groups big Try break True normaliz raw_input Normalization Max Std None normaliz normaliz upper normaliz normaliz normaliz M normaliz S normaliz N print Oops Input M S N Try break True arch_grid raw_input File Training Grid os path isfile arch_grid print Oops File exist Try CTR C exit break file_input arch_input gruppi_num num_gruppi tipo_norm normaliz outputs files file_input arch_input tipo_norm normaliz gruppi_num num_gruppi nome_input file_input split arch_output nome_input _ C tipo_norm _g str gruppi_num _out txt arch_outsrt nome_input _ C tipo_norm _g str gruppi_num _outsrt txt arch_sort nome_input _ C tipo_norm _g str gruppi_num _sort txt arch_catal nome_input _ C tipo_norm _g str gruppi_num KB Neural Data Mining Python sources Roberto Bello Pag di _catal txt arch_medsd nome_input _ C tipo_norm _g str gruppi_num _medsd txt arch_cv nome_input _ C tipo_norm _g str gruppi_num _cv txt arch_log nome_input _ C tipo_norm _g str gruppi_num _log txt start time t0 datetime datetime read input file arr_r arr_orig arr_c mtchx mtchy txt_col xnomi numbers variables columns record n_rows n_cols err_cols index line open file_input readlines linea line split index xnomi append linea n_cols len linea arr_r append linea len linea n_cols err_cols print Different numbers variables columns record str index cols str len linea index err_cols print File file_input contains errors Exit quit index index len arr_r linea arr_r index index_c index_c len linea linea index_c isdigit linea index_c float linea index_c KB Neural Data Mining Python sources Roberto Bello Pag di index_c arr_r index linea index arr_orig copy deepcopy arr_r original input file testata_cat copy deepcopy xnomi original header row finding columns containing strings columns containing numbers testata xnomi testata_orig copy deepcopy xnomi n_cols len testata n_rows len arr_r ind_c err_type ind_c len testata ind_r tipo_num tipo_txt ind_r len arr_r arr_c arr_r ind_r isinstance arr_c ind_c basestring tipo_txt tipo_num ind_r tipo_num tipo_txt print The columns variables testata ind_c contains strings numbers err_type ind_c err_type print Oops The columns variables contains strings numbers Exit quit index_c index_c n_cols txt_col index index len arr_r arr_c arr_r index isinstance arr_c index_c str txt_col append arr_c index_c index set_txt_col set txt_col remove duplicates txt_col list set set_txt_col txt_col sort KB Neural Data Mining Python sources Roberto Bello Pag di strings numbers len txt_col len txt_col passo1 len txt_col passo1 index index len arr_r arr_c arr_r index campo1 arr_c index_c indice1 txt_col index campo1 len txt_col values column val_num1 float val_num1 float passo1 indice1 arr_c index_c val_num1 avoid zero values means prevent zero divide CV index index_c means max std xmeans xmaxs xmins aggiunto Roberto xsds xcv index_c index_c n_cols xmeans append xmaxs append xmins append aggiunto Roberto xsds append xcv append index_c means max index index n_rows arr_c arr_r index index_c index_c n_cols xmeans index_c arr_c index_c arr_c index_c xmaxs index_c xmaxs index_c arr_c index_c index_c index KB Neural Data Mining Python sources Roberto Bello Pag di index_c index_c n_cols xmeans index_c xmeans index_c n_rows index_c std index index n_rows arr_c arr_r index index_c index_c n_cols xsds index_c arr_c index_c xmeans index_c index_c index index_c index_c n_cols xsds index_c xsds index_c n_cols index_c Means Max Std CV output file medsd_file open arch_medsd w columns names index_c index_c n_cols medsd_file write s s s Col str index_c testata index_c t index_c medsd_file write s n means index_c index_c n_cols valore str xmeans index_c valore valore medsd_file write s s s Mean str index_c valore t index_c medsd_file write s n max index_c index_c n_cols valore str xmaxs index_c valore valore medsd_file write s s s Max str index_c valore t index_c KB Neural Data Mining Python sources Roberto Bello Pag di medsd_file write s n std index_c index_c n_cols valore str xsds index_c valore valore medsd_file write s s s Std str index_c valore t index_c medsd_file write s n CV index_c med_cv_gen cv average columns variables index_c n_cols xmeans index_c media1 media1 xmeans index_c xcv index_c xsds index_c abs media1 valore str xcv index_c med_cv_gen xcv index_c valore valore medsd_file write s s s CV_ str index_c valore t index_c med_cv_gen med_cv_gen n_cols str_med_cv_gen str med_cv_gen str_med_cv_gen str_med_cv_gen medsd_file write s n medsd_file close input standardization standardization max tipo_norm M index index n_rows arr_c arr_r index index_c index_c n_cols xmaxs index_c xmaxs index_c arr_c index_c arr_c index_c xmaxs index_c index_c index KB Neural Data Mining Python sources Roberto Bello Pag di standardization std tipo_norm S index index n_rows arr_c arr_r index index_c index_c n_cols xsds index_c xsds index_c arr_c index_c arr_c index_c xmeans index_c xsds index_c arr_c index_c xmins index_c aggiunto Roberto xmins index_c arr_c index_c aggiunto Roberto index_c index aggiungo xmins eliminare valori negativi aggiunto da Roberto index index n_rows arr_c arr_r index index_c index_c n_cols arr_c index_c arr_c index_c xmins index_c index_c index fine aggiunta da Roberto start kohonen algorithm n len arr_r m len arr_c nx gruppi_num ny gruppi_num rmax nx rmin grid training grid index index nx ny m grid append index n ndim nx ny m carico la Grid di addestramento da arch_grid line open arch_grid readlines linea line split index int linea index_c int linea index_k int linea KB Neural Data Mining Python sources Roberto Bello Pag di valore float linea ig n getcellindex index index_c index_k grid ig valore starting row ending row n_rows find best grid coefficient ihit jhit dhit igx igy igx nx igy igy ny d neff k arr_c arr_r k m update sum squared deviation input value grid coefficient ig n getcellindex igx igy k d d arr_c k grid ig k d d float m d d m d dhit dhit d ihit int igx jhit int igy igy igx computing results catalog input min grid ii ii n_rows ihit jhit dhit numbers groups ir ir nx numbers groups KB Neural Data Mining Python sources Roberto Bello Pag di jc jc ny numbers groups d neff k k n_cols update sum squared deviation input value grid coefficient arr_c arr_r ii ig n getcellindex ir jc k d d arr_c k grid ig k d d m d dhit save coordinates best coefficient dhit d ihit ir jhit jc jc ir mtchx append ihit mtchy append jhit ii write arch_catal file arch_catal_file open arch_catal w ii ii n_rows arch_catal_file write 6i s 6i s 6i s ii mtchx ii mtchy ii n ii arch_catal_file close matrix statistics arr_cv CV array Groups Total arr_med means array Groups riga_cv CV row arr_cv arr_col group temporary array arr_grsg input data array normalized arr_grsg_c copy arr_grsg file sort input matrix sort group sequence ii ix ii n_rows ix gr1 str mtchx ii mtchx ii KB Neural Data Mining Python sources Roberto Bello Pag di gr1 str mtchx ii sg1 str mtchy ii mtchy ii sg1 str mtchy ii riga_norm arr_r ii I am riga_norm1 I am m riga_norm1 append str riga_norm I am I am riga_norm2 join riga_norm1 gr_sg_txt G_ gr1 _ sg1 str ix riga_norm2 arr_grsg append gr_sg_txt ii arr_grsg sort ii ii n_rows arr_grsg_c append arr_grsg ii ii setup arr_cv matrix num_gr gruppo0 ir ir n_rows grsg_key arr_grsg_c ir split grsg_key gruppo0 gruppo0 grsg_key num_gr ic riga1 riga1 append grsg_key ic m adding new columns row mean n records riga1 append ic arr_cv append riga1 cv row ir riga1 riga1 append Means adding new row cv mean ic ic m adding new column row mean n records riga1 append ic arr_cv append riga1 def found x KB Neural Data Mining Python sources Roberto Bello Pag di ir ir len arr_cv linea_cv arr_cv ir key_cv linea_cv key_cv x return ir ir ir irx len arr_grsg_c ic linea_cv arr_cv icx len linea_cv val_col ic icx ir gruppo val_col ir irx linea arr_grsg_c ir split linea gruppo gruppo gruppo linea val_col append float linea ic i_gruppo found gruppo linea_cv arr_cv i_gruppo media_v abs mean val_col media_v media_v std_v sd val_col cv_v std_v media_v linea_cv ic cv_v cv value linea_cv len linea_cv len val_col number records val_col val_col append float linea ic gruppo linea ir i_gruppo found gruppo linea_cv arr_cv i_gruppo media_v abs mean val_col media_v media_v std_v sd val_col cv_v std_v media_v linea_cv ic cv_v cv value KB Neural Data Mining Python sources Roberto Bello Pag di linea_cv len linea_cv len val_col number records ic ir irx len arr_cv linea_cv arr_cv icx len linea_cv ic num_rec1 ir irx rows mean media_riga ic num_col1 linea_cv arr_cv ir ic icx media_riga float linea_cv ic num_col1 ic linea_cv icx media_riga num_col1 num_rec1 linea_cv icx ir ir ic ic icx weighted mean columns media_col ir num_rec1 ir irx linea_cv arr_cv ir media_col media_col linea_cv ic linea_cv icx linea_cv icx number records num_rec1 num_rec1 linea_cv icx ir linea_cv arr_cv irx linea_cv ic media_col num_rec1 ic updating mean row linea_cv arr_cv irx linea_means linea_cv icx media_riga mean linea_means linea_cv icx media_riga Total mean linea_cv icx num_rec1 n records cv_media_gen_after str media_riga cv_media_gen_after cv_media_gen_after KB Neural Data Mining Python sources Roberto Bello Pag di write cv file testata_cv testata testata_cv Groups testata_cv append Mean testata_cv append N_recs arch_cv_file open arch_cv w ic ic icx arch_cv_file write s s testata_cv ic len testata_cv ic ic arch_cv_file write s n ir ir irx ic linea_cv arr_cv ir ic icx ic arch_cv_file write s s linea_cv ic icx arch_cv_file write 4f s linea_cv ic arch_cv_file write 6i s linea_cv ic ic arch_cv_file write s n ir ic media_xcv mean xcv icx ic icx print CV input catalogue ic arch_cv_file write s s CVinp ic icx arch_cv_file write 4f s xcv ic arch_cv_file write 4f s media_xcv arch_cv_file write 6i s linea_cv ic ic arch_cv_file write s n istruzioni aggiunte Roberto Bello know_index str float cv_media_gen_after float str_med_cv_gen KB Neural Data Mining Python sources Roberto Bello Pag di know_index know_index arch_cv_file write s s s KIndex know_index n fine istruzioni aggiunte da Roberto Bello arch_cv_file close writing catalog file testata_cat1 testata_cat1 append Group arch_output_file open arch_output w ic ic icx testata_cat1 append testata_cat ic ic ic ic len testata_cat1 arch_output_file write s s testata_cat1 ic len testata_cat1 ic ic arch_output_file write s n index index len arr_orig riga_orig arr_orig index ic ic len riga_orig isinstance riga_orig ic str riga_orig ic str riga_orig ic ic place gr sg gr1 str mtchx index mtchx index gr1 str mtchx index sg1 str mtchy index mtchy index sg1 str mtchy index arr_rig0 G_ gr1 _ sg1 arch_output_file write s arr_rig0 ic ic len riga_orig arch_output_file write s s riga_orig ic len riga_orig ic ic arch_output_file write s n index testata_cat1 testata_cat1 append Group testata_cat1 append RecNum arch_sort_file open arch_sort w ic KB Neural Data Mining Python sources Roberto Bello Pag di ic icx testata_cat1 append testata_cat ic ic ic ic len testata_cat1 arch_sort_file write s s testata_cat1 ic len testata_cat1 ic ic arch_sort_file write s n index index len arr_grsg_c riga_grsg arr_grsg_c index split ic ic len riga_grsg val_txt riga_grsg ic val_txt val_txt arch_sort_file write s s val_txt len val_txt ic index len arr_grsg_c arch_sort_file write s n index arch_sort_file close writing catalog sorted file arr_outsrt index index len arr_orig riga_sort place gr sg gr1 str mtchx index mtchx index gr1 str mtchx index sg1 str mtchy index mtchy index sg1 str mtchy index riga_sort append G_ gr1 _ sg1 ic riga_orig arr_orig index ic len riga_orig val_riga riga_orig ic riga_sort append val_riga ic arr_outsrt append riga_sort index line arr_outsrt line join line KB Neural Data Mining Python sources Roberto Bello Pag di arr_outsrt sort testata_srt testata_srt append Group arch_outsrt_file open arch_outsrt w ic ic icx testata_srt append testata_orig ic ic ic ic len testata_srt arch_outsrt_file write s s testata_srt ic len testata_srt ic ic arch_outsrt_file write s n index key_gruppo index len arr_outsrt riga_sort arr_outsrt index index_c index_c len riga_sort index_c riga_sort key_gruppo arch_outsrt_file write s n key_gruppo riga_sort valore riga_sort index_c arch_outsrt_file write s s valore len valore index_c index len arr_grsg_c arch_outsrt_file write s n index arch_outsrt_file close print print KB_CLA KNOWLEDGE DISCOVERY IN DATA MINING CLASSIFY PROGRAM print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print arch_log_file open arch_log w arch_log_file write s s n arch_log_file write s s KB_CLA KNOWLEDGE DISCOVERY IN DATA MINING CLASSIFY PROGRAM n arch_log_file write s s ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED n arch_log_file write s s Language PYTHON KB Neural Data Mining Python sources Roberto Bello Pag di n arch_log_file write s s n arch_log_file write s s s Input File file_input n arch_log_file write s s s Numer Groups str gruppi_num n arch_log_file write s s s Normalization Max Std None tipo_norm n arch_log_file write s s s File Training Grid arch_grid n arch_log_file write s OUTPUT n arch_log_file write s s s Output File Classify original arch_output n arch_log_file write s s s Output File Classify sort arch_outsrt n arch_log_file write s s s Output File Summary sort arch_sort n arch_log_file write s s s Output File Matrix Catal arch_catal n arch_log_file write s s s Output File Means STD CV arch_medsd n arch_log_file write s s s Output File CV Groups arch_cv n arch_log_file write s s s Output File Training Grid arch_grid n arch_log_file write s s s Output File Run Parameters arch_log n istruzioni aggiunte Roberto Bello know_index str float cv_media_gen_after float str_med_cv_gen know_index know_index arch_log_file write s s s KIndex know_index n fine istruzioni aggiunte da Roberto Bello print print Output File Classify original arch_output print Output File Classify sort arch_outsrt print Output File Summary sort arch_sort print Output File Matrix Catal arch_catal print Output File Means STD CV arch_medsd print Output File CV Groups arch_cv print Output File Training Grid arch_grid print Output File Run Parameters arch_log print CV Catalog cv_media_gen_after know_index str float cv_media_gen_after float str_med_cv_gen know_index know_index print Knowledge Index know_index print Elapsed time t1 datetime datetime elapsed_time t1 t0 KB Neural Data Mining Python sources Roberto Bello Pag di print Elapsed time seconds str elapsed_time seconds str elapsed_time microseconds print Appendix KB_RND source coding utf import os import random import copy import datetime print print KB_RND KNOWLEDGE DISCOVERY IN DATA MINING RANDOM FILE SIZE REDUCE print ROBERTO BELLO COPYRIGHT MARCH ALL RIGHTS RESERVED print Language PYTHON print input run parameters error True arch_input raw_input InputFile os path isfile arch_input print Oops File exist Try CTR C exit break True arch_output raw_input OutputFile os path isfile arch_output print Oops File exist Try CTR C exit break True try num_cells_out int raw_input Out number cells ValueError print Oops That valid number Try num_cells_out print Oops Number Cells big Try break start time KB Neural Data Mining Python sources Roberto Bello Pag di t0 datetime datetime read input file arr_r arr_rnd arr_out xnomi index line open arch_input readlines linea line split index xnomi append linea arr_r append linea index rec_input index num_cols len linea num_records_out int num_cells_out num_cols print Nun Records Input str rec_input rec_input num_records_out num_records_out rec_input random values sequence ix integer random seed random seed ix initial value random seed obtain sequences new runs index index num_records_out val_rnd int random random rec_input doppio index_rnd item enumerate arr_rnd check duplicates item val_rnd doppio doppio arr_rnd append val_rnd index arr_out writing index arr_out append xnomi header index len arr_rnd KB Neural Data Mining Python sources Roberto Bello Pag di key1 arr_rnd index arr_out append arr_r key1 source random output index write arch_out_file arch_out_file open arch_output w index index len arr_out line arr_out index ncol ncol len line field line ncol strip ncol len line arch_out_file write s s field t arch_out_file write s s field n ncol index arch_out_file close Elapsed time t1 datetime datetime elapsed_time t1 t0 print Elapsed time microseconds str elapsed_time microseconds print KB Guarantee copyright The author guarantees work viruses malicious codes considering text pdf format python programs txt format contain malicious code easily verifiable simple reading test files txt format language processing programs python Open Source type universally recognised reliable safe As regards copyright author intend renounce legal rights algorithms method computing analysis contained KB programs Roberto Bello Graduate Economics Commerce specialization Operations Research Data Scientist expert Knowledge Mining Data Mining Open Source ICT Strategist ClubTI Milan www clubtimilano net Researcher AISF www accademiascienzeforensi Expert CTP ex CTU Technical Office Consultant Court Milan KB Neural Data Mining Python sources Roberto Bello Pag di http www accademiascienzeforensi http www clubtimilano net Author professional publications available www lulu com spotlight robertobb Founding associate AIPI Italian Professional Computer Association In past CIO Plasmon Wrangler Italy consultant important Italian food companies Linkedin linkedin com pub roberto bello 1a5 KB Neural Data Mining Python sources Roberto Bello Pag di Table contents KB Neural Data Mining Python sources Introduction Business Intelligence bad use statistics Learning induction neural networks KB Python KB Details Collecting arranging input data General warnings KB programs KB_CAT Knowledge Data Mining cataloging homogeneous groups Generality aims functions Source KB_CAT attachment Test Input file copy paste save vessels txt fields separated tabulation How run Input File vessels txt Number Groups Normalization Max Std None m Start Value alpha End Value alpha Decreasing step alpha Forced shut processing KB_CAT produce following output In window DOS Windows Terminal Linux File Output Catalog original vessels_M_g3_out txt File Output Catalog sort vessels_M_g3_outsrt txt Output Means Std CV vessels_M_g3_medsd txt Output CV files vessels_M_g3_cv txt Output Training Grid vessels_M_g3_grid txt Statistical analysis results cataloging Other input files KB_CAT animals txt Processing animals txt file KB_CAT Output file Catalog sort ordered group animals txt Verify validity manual cataloging Input file KB_CAT animals_d txt The processing carried following parameters Results obtained processing animals_d txt Output Catalog sort Comparison results automatic cataloging iris txt recognized botanists Clinical trials hepatitis B virus Not good better mail KB_STA statistical analysis result cataloging Generalities aims functions Source KB_STA attachment How use KB_STA running Analysis results cataloging vessels txt Analysis results political poll KB_CLA Classification new records Generalities aims functions Source KB_CLA attachment How run KB Neural Data Mining Python sources Roberto Bello Pag di Input files n_vessels txt Contents file n_vessels txt Number Groups Normalization Max Std None m File Training Grid vessels_M_g3_grid txt KB_CLA running Analysis results classification n_vessels txt Political opinions Facebook January Know4Business Cloud version Google App Engine APPENDIXES Appendix KB_CAT source Appendix KB_STA source Appendix KB_CLA source Appendix KB_RND source KB Guarantee copyright Roberto Bello KB Neural Data Mining Python sources Roberto Bello Pag di KB Neural Data Mining Python sources Introduction Business Intelligence bad use statistics Learning induction neural networks KB Python KB Details Collecting arranging input data General warnings KB programs KB_CAT Knowledge Data Mining cataloging homogeneous groups Generality aims functions Source KB_CAT attachment Test Input file copy paste save vessels txt fields separated tabulation How run Input File vessels txt Number Groups Normalization Max Std None m Start Value alpha End Value alpha Decreasing step alpha Forced shut processing KB_CAT produce following output In window DOS Windows Terminal Linux File Output Catalog original vessels_M_g3_out txt File Output Catalog sort vessels_M_g3_outsrt txt Output Means Std CV vessels_M_g3_medsd txt Output CV files vessels_M_g3_cv txt Output Training Grid vessels_M_g3_grid txt Statistical analysis results cataloging Other input files KB_CAT animals txt Processing animals txt file KB_CAT Output file Catalog sort ordered group animals txt Verify validity manual cataloging Input file KB_CAT animals_d txt The processing carried following parameters Results obtained processing animals_d txt Output Catalog sort Comparison results automatic cataloging iris txt recognized botanists Clinical trials hepatitis B virus Not good better mail KB_STA statistical analysis result cataloging Generalities aims functions Source KB_STA attachment How use KB_STA running Analysis results cataloging vessels txt Analysis results political poll KB_CLA Classification new records Generalities aims functions Source KB_CLA attachment How run Input files n_vessels txt Contents file n_vessels txt Number Groups Normalization Max Std None m File Training Grid vessels_M_g3_grid txt KB_CLA running Analysis results classification n_vessels txt Political opinions Facebook January Know4Business Cloud version Google App Engine APPENDIXES Appendix KB_CAT source Appendix KB_STA source Appendix KB_CLA source Appendix KB_RND source KB Guarantee copyright Roberto Bello\n",
            "1 []\n",
            "1 Disruptive Possibilities http oreil ly 1T0KbBh http oreil ly 1T0KbBh Jeffrey Needham Disruptive Possibilities How Big Data Changes Everything LSI Disruptive Possibilities How Big Data Changes Everything Jeffrey Needham Copyright Scale Abilities Inc All rights reserved Printed United States America Published O Reilly Media Inc Gravenstein Highway North Sebastopol CA O Reilly books purchased educational business sales promotional use Online editions available titles http safaribooksonline com For information contact corporate institutional sales department corporate oreilly com February First Edition Revision History First Edition First Release Second Release Third Release Fourth Release Fifth Release The O Reilly logo registered trademark O Reilly Media Inc Disruptive Possi bilities cover image related trade dress trademarks O Reilly Media Inc While publisher author s good faith efforts ensure information instructions contained work accurate publisher author s disclaim responsibility errors omissions including limitation responsibility damages resulting use reliance work Use information instructions contained work risk If code samples technology work contains describes sub ject open source licenses intellectual property rights responsibility ensure use thereof complies licenses rights http safaribooksonline com Table Contents The Wall Water And Then There Was One Commercial Supercomputing Comes Age A Stitch Time The Yellow Elephant Room Scaling Yahoovians Supercomputers Are Platforms Big Data Big Bang Endless Possibilities Big Data The Ultimate Computing Platform Introduction Platforms Come Fly Me Computing Platforms The End Era Back Future High Divers Fire Fighters I ll Take Silos Alex Engineering Big Data Platforms The Art Craft Platform Engineering Global Scale KISS Me Kate The Response Time Continuum Perpetual Prototyping Epic Fail Insurance Optimize Absolutely Everything Mind Gap iii Organizations The Other Platform From Myelin Metal Silos Industrial Grain Elevators Platforms Silos Panic Now Fear Loathing Risky Business Probability Outcome Quantitative Qualities The Soft Platform The Reservoir Data The Actual Internet Best Inbred Drowning Not Waving Spinning Rust A Spectrum Perishability Enclosed Water Towers The Big Data Water District The HDFS Reservoir Third Eye Blind Spectrum Analytic Visibility The Cost Flexibility Cloudy Chance Meatballs When Clouds Meet Big Data The Big Tease Scalability Motherboards Apple Pie Being Nothingness Parity Is Farmers Google Reverse Isolated Thunderstorms Private Cloud Myths The Network Is Backplane The Orientation Clouds My Other Data Center Is RV Park Converge Conventionally Haystacks Headgames The Demise Counting iv Table Contents Lose Haystack Mind Another Gap Believing Is Seeing Spatial Intelligence Associative Supercomputing Back Future Acknowledgments Table Contents v CHAPTER The Wall Water And Then There Was One And ecosystem platform com munity importantly force retrains vendors think customers Welcome tsunami Hadoop noSQL computing global scale represents Some enterprises businesses don t appear data far shoreline sirens faint Other organizations splashing surf deca des find watching tide recede rapidly The speed approaching wave unprecedented industry committed like decent youth movement innovation self destruction reinvention Welcome future computing Welcome big data Welcome end computing known years Big data type supercomputing commercial enterprises governments possible monitor pandemic happens anticipate bank robbery occur opti mize fast food supply chains predict voter behavior election day forecast volatility political uprisings hap pening The course economic history change criminals stand Hadoop clusters So seemingly diverse unrelated global activities big data ecosystem Like powerful technol ogy right hands propels limitless possibilities In wrong hands consequences unimaginably destruc tive The motivation big data immediate organizations If threatening organization gets tech orga nization trouble If Target gets Kohl s Chinese navy gets US navy criminal organizations banks powerful advantage The solutions require enterprises innovative lev els including technical financial organizational As 1950s cold war masters big data win arms race big data arms race 21st century Commercial Supercomputing Comes Age Trends computing industry mimic fashion wait long wear Many technologies found big data circulating industry decades clustering parallel processing distributed file systems Commercial supercomputing originated internet companies operating global scale needed process increasing numbers users data Yahoo 1990s Google Facebook However needed quickly eco nomically words affordably scale This commercial supercomputing known big data Big data bring disruptive changes organizations vendors reach far networks friends social network encompasses planet But changes come possibili ties Big data season s trendy hemline piece generations A Stitch Time Large scale computing systems new Weather forecasting nasty big data problem beginning time weather models ran single supercomputer fill gym nasium contained couple fast 1970s CPUs expensive memory Software 1970s primitive Chapter The Wall Water performance time clever hardware engineering By 1990s software improved point programs running monolithic supercomputers broken hun dred smaller programs working time workstations When programs finished running results stitched form week long weather simulation Even 1990s simulators days com pute seven days weather It didn t help farmers find rained week Today parallel simulations week long forecast complete hours As clairvoyant weather forecasting appears programs t predict future attempt simulate model behavior Actual humans forecasting art supercomputing craft Most weather forecasting agencies use variety simulators different strengths Simulators good predicting waterspout landfall Louisiana great predicting morning marine layer affect air operations San Francisco International Agency forecasters region pore output simulations differing sets initial conditions They look actual data weather stations look win dow meteorological equivalent Doppler radar Although lot data involved weather simulation considered Big Data computationally intense Computing problems science including meteorology engi neering known high performance computing HPC scientific supercomputing The electronic computers scientific computing calculating trajectories missiles cracking codes mathematical problems involving solution mil lions equations Scientific computing solves equations non scientific problems rendering animated films Big data commercial equivalent HPC called high performance commercial computing commercial A Stitch Time supercomputing Big data solve large computing problems equations discovering patterns In 1960s banks commercial computers automating accounts managing credit card business Today companies Amazon eBay large brick mortar retailers use commercial supercomputing solve business problems commercial supercomputing ana lyzing bounced checks discovering fraud managing Facebook friends The Yellow Elephant Room Hadoop commercial supercomputing software platform works scale affordable scale Hadoop exploits parlor trick parallelism established HPC world It developed house Yahoo solve specific problem quickly realized potential tackle comput ing problems Although Yahoo s fortunes changed huge ing contribution incubation Google Facebook big data Hadoop originally created affordably process Yahoo s flood clickstream data history links user clicks Since monetized prospective advertisers analysis click stream data tens thousands servers required massively scalable database economical build operate Chapter The Wall Water Yahoo found commercial solutions time incapable scaling unaffordable scale Yahoo build scratch DIY commercial supercomputing born Like Linux Hadoop open source software technology Just Linux spawned commodity HPC clusters clouds Hadoop created big data ecosystem new products old vendors new startups disruptive possibilities Because Hadoop portable available Linux The ability run open source product like Hadoop Microsoft operating system significant triumph open source community wild fantasy years ago Scaling Yahoovians Because Yahoo company operate scale understanding history key understanding big data s tory Jerry Yang Dave Filo started Yahoo project index web problem grew point conventional strategies simply growth content needed indexed Even Hadoop came Yahoo required computing plat form took time build web index large web grew Yahoo realized needed borrow parallelism trick HPC world achieve scala bility Yahoo s computing grid cluster infra structure Hadoop subsequently developed Just important technology Yahoo developed reorganized Engineering Operations teams support computing platforms magnitude Yahoo s experience operat ing massive computing plant spread multiple locations led reinvent IT department Complex platforms need initially developed deployed small teams Getting organization scale support platforms entirely different matter reinvention organization crucial getting hardware software infrastructure scale Scaling Yahoovians Like corporate departments HR Sales IT organiza tions traditionally achieved scalability centralizing skills There question having small team experts managing thou sand storage arrays cost effective paying salaries huge team Storage Admins don t working knowledge hundreds applications arrays Centralization trades generalist working knowledge cost effi ciency subject matter expertise Enterprises finally starting understand unintended consequences trade offs long ago produced silos inhibit ability implement big data Traditional IT organizations partition expertise responsibilities constrains collaboration groups Small errors misunderstandings tolerable small email servers small errors production supercomputers cost corporations lot time money Even error huge difference In world big data terabytes merely Tiny Data error tera bytes million megabytes Finding fixing errors scale countless hours Yahoo learned HPC community living large clusters years knows They learned small team working knowledge entire platform works best Silos data responsibility impediments scien tific supercomputing commercial supercomputing Internet scale computing plants work early practitioners learned key lesson supercomputers finely tuned platforms interdependent parts don t operate silos pro cessing Starting 1980s customers encouraged view computing platform stack autonomous layers functionality This paradigm easier understand increasingly com plex platforms layer metaphor began cognitively mask underlying complexity hindered prevented successful triage performance reliability problems Like F1 racing platform Boeing supercomputer plat forms understood single collection technologies efficiency manageability impaired Chapter The Wall Water Supercomputers Are Platforms In early history computer industry systems plat forms called mainframes typically came company supplied dedicated group engineers white shirts ties worked alongside customers ensure platform performed handed keys This method successful long enjoyed IBM customer fine line throat choke monopoly arrogance After IBM crossed line 1960s resulting industry offered choice better pricing industry silos Today companies dominate silo tend behave like monopoly long away As database server storage companies proliferated IT organizations mir rored alignment corresponding teams database server storage experts However order stand big data cluster successfully person touches works cluster physically organizationally close The collaborative teamwork required successful cluster deployments scale happens sub silo silo If company wants embrace big data gather magical place Big Data Meets The Cloud IT organization tear silos aware plat form Unfortunately organizations handle change let rapid change Chaos disruption constants industry accompanied possibility inno vation For enterprises willing acknowledge prepare wall water big data cleansing flood new ideas opportunities Big Data Big Bang As big data ecosystem evolves years inundate vendors customers number ways Supercomputers Are Platforms First disruption silo mentality IT organizations industry serves Big Story big data Second IT industry battered new technology big data products predate Hadoop laughably unaffordable scale Big data hardware software hundreds times faster existing enterprise scale products thousands times cheaper Third technology new disruptive big data resisted IT organizations corporate mandate requires obsess minimizing OPEX tolerate innovation forc ing IT big bad wolf big data Fourth IT organizations affected generation replaces invested careers Oracle Microsoft EMC The old adage gets fired buying historically IBM applies mature established technology imma ture disruptive technology Big data disruptive force industry seen introduction relational database Fifth big data requires data scientists programmers develop better understanding data flows underneath including introduction reintroduction computing platform makes possible This outside comfort zones similarly entrenched silos Professionals willing learn new ways collaborating working thinking prosper prosperity highly efficient small teams people highly efficient large groups servers Sixth finally civil liberties privacy compromised technology improvements affordable organization private public clandestine analyze patterns data behavior uses mobile phone Endless Possibilities Today big data isn t social networking machine generated web logs Agencies enterprises find answers questions afford ask big data help identify questions knew ask Chapter The Wall Water For time car manufacturers afford view global parts inventory spread hundreds plants collect petabytes data coming sensors vehicles Other companies able process analyze vast amounts data field collecting Prospecting oil involves seismic trucks field collecting terabytes data Previously data taken expensive datacenter transferred expensive supercomputers took lot expen sive time process Now Hadoop cluster spread fleet trucks sitting motel parking lot run job overnight provides guidance day s prospecting place In field farmers plant thousands environ mental sensors transmit data Hadoop cluster running barn watch crops grow Hadoop clusters affordable government agencies analyze data The WHO CDC able track regional global outbreaks like H1N1 SARS happen Although big data makes possible process huge data sets parallelism makes happen quickly Hadoop data sets don t qualify big data need pro cessed parallel Think tiny Hadoop cluster running artificial retina Whether wall data arrives form tsunami monsoon fog collected commonly accessible affordable reservoir possibilities real ized This reservoir drag drop enterprise data warehouse pyramid schema The data contained reservoir like fresh water found real reservoirs sustain future life business Disruptive opportunistic big data thrusting computer science away classic John von Neumann style computing finally stop looking piece hay millions hay stacks big data makes possible new form Endless Possibilities spatial supercomputing Long steps taken big data change course history Chapter The Wall Water CHAPTER Big Data The Ultimate Computing Platform Introduction Platforms A platform collection sub systems components operate like thing A Formula One racing vehicle drivers refer platform automobile equivalent super computer It aspect design fully optimized simply performance performance liter gas kilogram curb weight A litre engine creates 320HP instead 140HP efficient The engine higher horsepower better absolute formance performance means efficiency like HP KG miles gallon computing platforms jobs executed watt Performance measured ratio accomplished effort expended The descendant Honda s F1 technology found cars optimized technology derived racing program enabled Honda design powerful vehicles consumers A Honda Civic platform F1 The engine brakes steering suspension designed feels like driving car collection complex sub components Plat forms span rivers serve ads shoes reserve seats notable platform kind wings Come Fly Me The design development new commercial aircraft com plex costly tangled regulations making process justifia bly slow design flaws leave bodies scattered infield Platforms manufactured physical components require planning platforms manufactured software new set engines t downloaded week However modern aircraft designers understand value flexible software stuff First introduced military fighters fly wire technology refers flying electrical wire mechanical wire like bicycle brake cables In traditional aircraft stick pedals mechanically con nected control surfaces wings mechanical linkages controlled surfaces In fly wire aircraft controls cockpit inputs computer controls motorized actuators surfaces wings tail Fly wire software prevent fighter pilots flying unconsciousness Pilots bank turns steep black software detects conditions limits turns pilots conscious alive Similar features apply commercial aircraft sport sedans mak ing platforms safer efficient Unfortunately fly wire software easy change improve bugs design flaws result mess infield prefer avoid Computing Platforms In 1960s Bank America IBM built credit card processing systems Although early mainframes pro cessed fraction data compared eBay Ama zon engineering complex day Once credit cards popular processing systems built handle load importantly handle growth constant engineering Chapter Big Data The Ultimate Computing Platform These early platforms built mainframes peripheral equipment networks storage software single vendor IBM built massive database system project NASA Apollo program later evolved prod uct called IMS Because IBM developed solutions specific problems large customers faced resulting systems products They custom built highly integrated expensive platforms later evolve viable business IBM These solutions interconnected hardware soft ware components built single system usually small dedicated team specialists Small teams cross pollinated expertise expert storage networks databases acquired general working knowledge areas These solutions required development new hardware software technologies prolonged cross pollination expertise critical success project Team members close prox imity allowed body working knowledge emerge crit ical success platform Each team s job complete delivered finished integrated working platform customer fully functional solution business problem The End Era In 1970s IBM s monopoly curtailed start ups Amdahl DEC Oracle emerge begin provid ing IBM customers alternatives DEC built minicomputers provided superior price performance IBM mainframes compatibility Amdahl namesake Gene designed IBM provided compatible alternative cheaper IBM mainframe Companies develop sell products services thrive post monopoly world These pockets alternative value eventually led silos vendors silos expertise IT departments aligned vendors Like Amdahl Oracle directly benefited technol ogy IBM developed productized Larry Ellison s gen ius IBM s relational database technology place The End Era seminal VAX create enterprise software com panies post mainframe era When products silos niches sold customers putting system longer single supplier s responsibility customers job Today vendors imaginable silo net work switches storage switches storage arrays servers operating systems databases language compilers applications complication cost comes responsibility Big systems integrators like Accenture Wipro attempt fill gap operate constraints IT departments organizational silos established vendors Silos price paid post mainframe alternatives IBM Silos obfuscate true nature computing platforms single system interconnected hardware software Back Future Oracle profited post mainframe silo decades customers bought database technology ran Sun HP EMC hardware As applications complex con Chapter Big Data The Ultimate Computing Platform structing platforms silos difficult enter prises attempting use Oracle s clustering technology RAC found nearly impossible set Since failure result customers poor plat form engineering exposed bugs Oracle designed engineered platform combined components product engineering expertise successful experiences possible The resulting product Exadata originally designed data warehouse market found success mainstream Oracle RAC customers running applications like SAP Since Oracle hardware company initial release Exa data based HP hardware Exadata successful Oracle decided source hardware components partially motivated acquisition Sun By sourcing hardware software components Exadata Oracle resurrec ted stop shop mainframe model This stop shop model known throat choke On surface sounds appealing assumes throat choked Large customers Amgen Citibank AT T pur chase equipment services choke ven dor like things south However vast majority customers large manage databases support Oracle small demand good timely support Oracle stop shopping reduces customers leverage vendors Like Exadata big data supercomputers need constructed engineered platforms construction requires engineering approach hardware software components treated single system That s platform way way components sold silos vendors High Divers Fire Fighters Today architects responsible building new platforms found respective IT departments work subject matter experts particular silo Yet plat form architects like building architects extensive working knowledge entire platform including computer High Divers Fire Fighters science bits physical plant aspects business value entire platform Because component platform triaged repaired optimized platform architects versed carry conversation data center electricians network designers Linux Java programmers UI designers business owners controllers Platform architects able agile dive details deep end electrician climb dive pool accountants Too knowledge familiarity details area distort overall platform perspective Having ability selectively filter details required details come shapes sizes rela tive importance constantly shifts The cliche devil details accurate devil usually handful zillion details handful change daily Prioritizing important details ignoring irrele vant details important skills platform architect possess Designing systems platforms craft taught pick stumble accident necessity desperation This adventure rarely comes help encouragement cowork ers employers vendors It thankless learning process easily alienate colleagues groups appears platform architects trying everybody s job The truth platform architects trying job knows willing As result practitioners work IT organizations freelance rough edges things don t work scale recover Freelance platform architects typically hired triage problems customers wit s end Once fires narrow window opportunity educate customers platform I ll Take Silos Alex One surprises awaiting enterprises big data DIY supercomputing Whatever big data cluster stand comes Chapter Big Data The Ultimate Computing Platform factory applications data In order populate cluster data emancipated technical organizational silos Big data matters business value promises Data sci entists data wranglers need develop new methods ana lyze legacy data vast amounts new data flooding Both Development Operations responsible suc cess enterprise s big data initiative The walls busi ness data organizations platform exist global scale Similar nervous system big data cluster highly inter connected platform built collection commodity parts Neurons human brain building blocks nervous system simple parts The neurons jellyfish simple parts Just like far sum jellyfish parts brilliant personality nervous system s ultimate big data job big data cluster operates complex interconnected form computing intelligence human Watson Engineering Big Data Platforms Big data platforms operate process data scale leaves little room error Like Boeing big data clusters built speed scale efficiency Many enterprises venturing big data don t experience building operating super computers faced prospect Platform awareness increase chances success big data Engineering Big Data Platforms Legacy silos infrastructure organizational ven dor silos replaced platform centric perspective In past enterprises agencies satisfied purchasing SQL prompt building applications Today groups don t want read raw data science output need visualize t derive business value looking They need pictures numbers Unlike legacy infrastructure stack silos t exist data vis ualization stack Implemented successfully big data infrastructure delivers data right place analytics layer right time right cost If infrastructure aggregate richer set data tweets videos PDFs JPGs SQL analytics layer better chance delivering actionable intelligence business The Art Craft Platform Engineering Global Scale Platform engineering great hair raising adventure In order build platform built discover things business thought look lot lab work experiments need fail early sure platform deliver scale Many enterprise IT departments corresponding vendor silos continue impair platform awareness Many customers strug Chapter Big Data The Ultimate Computing Platform gle big data want apply enterprise grade practi ces global scale problems Disaster recovery DR good example silo perspec tive rarely produces strategy effectively efficiently recov ers platform Building silo centric DR plan forces precise coordination single silo organizationally com plex expensive Usually Storage group implements DR strategies storage Application Server team implements DR s limited application servers Although compa nies silo approach enterprise scale disaster recovery s rarely optimal At global scale doesn t work Thinking computing systems holistic organic inte grative way considered crazy worth bother espe cially systems built organizations operate successfully silos peak performance efficiency The silo approach achieves operational economies scale measured Measuring platform s efficiency hard building efficient platform place Although tenets platform engineering apply enterprise global scale computing difference enterprise scale mantras optional At global scale mandatory Platform engineering big data demands critical tenets avoid complexity prototype perpetually optimize thing KISS Me Kate There advantages avoiding complexity enterprise scale big data Keep It Simple Sunshine words live Even modest cluster racks nodes disks contains lot moving parts complex organism complexity con tributes major failure categories software bugs operator error Big data platforms designed scale continue work face failure Because law averages failures inevitably happen software provide ability scale KISS Me Kate node cluster continuously available face compo nent failures The software supports high availability HA pro viding redundancy service multiple pathways self correcting techniques reinstates data loss failures In traditional enterprise scale software HA capabilities val ued features HA new improved keeps things working But supercomputer clusters thousands interconnected components HA important scalability Historically HA software features designed address hard ware failures happens HA software fails Verify ing software robustness difficult exercise end case testing requires ruthless costly devotion negative testing And vendors ruthless environment use rarely end case identical customers environment No platforms alike built extremely repeat able specifications For reason big data pioneers gone great lengths minimize platform variance attempt avoid conditions trigger end case high complexity software failures nasty triage The Response Time Continuum High availability concerned avoiding outages When customers book airline reservation entire platform available responsive possible When parts platform available like reservation database copy database brought online customers continue book flights Database teams pride designing disaster recovery strategies continued availability possible rest platform isn t designed level availability customer experience suffers HA isn t keeping databases storage running s keeping running user s browser air conditioners data center Engineering HA avoiding long outages outage ones seconds When customers waiting platform return selection air line flights match search criteria seconds unavail Chapter Big Data The Ultimate Computing Platform ability reservation system forever especially reservation lost Availability responsiveness system response time continuum encompasses seconds minutes cus tomers wait reservation complete hours days system recover major dis aster The responsiveness degree unavailability determined expectations perception time Some online systems display messages don t away screen reser vation complete dials working working working manage users expectations responsiveness It OK customers wait minute longer ensure airline tickets booked correctly paid currency traders feel wait milliseconds unbearable Performance scalability recovery perceived separate topics platform design sub topics availability engineering Perpetual Prototyping Product development principles bit mixed bag backfire Although noble preserve design principles ensure product stability price paid sclerotic software product lifecycle Many enterprises find immense com petitive pressure add features computing plant tradi tional approaches product development produce better results old approach IT projects treated projects products Rapidly evolving requirements break project approach bureaucratic software product lifecycle In Perpetual Prototyping new world development testing migration begin blur single continuous activity In old software development model meetings dis cuss market requirements meetings discuss negotiate product requirements eventually appear release Finally actual development new features years By time features appeared old method competitors new approach released versions capable stable short run capable stable far faster old model A long product development cycle result expensive late uncompetitive product It lead company failure The traditional process simply responsive big data Because adding features quickly destabilize software system achieving equilibrium innovation stability impor tant To shift higher rate release provide features stable organizations work massive scale develop products approach called perpetual prototyping PP blurs discrete steps prototyping development testing release continuous loop The fea tures created delivered quickly released product big data platform highly functional prototype Companies PP style development pushed testing integration phases sections production environment Their production environments aka colos vast cost effective use small slice users spread entire set colos construct separate test colo This users guinea pigs model obviously negative effects users subjected untested code simulation extremely realistic steps taken sure new fea tures broken creates havoc The open source world strategies similar PP early versions code pushed users tech preview status In addition branches barely compile stable production versions tech preview operates like warning labels It work advertised doesn t shout Chapter Big Data The Ultimate Computing Platform This code production ready s limited exposure production environments Relative traditional methods open source development trades rapid evolution feature time market stability In early stages development product improves quickly qual ity going sideways traditional closed source model As products projects stabilize fewer features added devel opers lose interest work shiny bright thing rate change drops Even open source products eventually stop changing stable loss developers sign maturity Epic Fail Insurance Design doctrines enterprise grade platforms based estab lished principles sense critical enterprise scale com puting systems payroll To considered enterprise grade people think big data embrace enterprise scale doc trines But enterprise grade practices affordable designed use global scale Enterprise big data requires new doctrines combine robust ness enterprise grade practices affordability necessary handle hundreds millions users A good example sys tem redundancy strategy single point failure noSPOF Sys tem redundancy design principle applied platforms allow function presence abnormal operating conditions For example past Ethernet hardware interfaces unreliable needed protection redundancy As parts integrated servers reliability improved point software protecting failure reliable hardware designed protect At enterprise scale easy implement noSPOF policy cost complexity tolerable At global scale HA computing platforms span multiple datacenters requires affordable strategies At internet scale single points failure created equal applying principle potential points failure difficult Epic Fail Insurance implement complex manage expensive The categories system failure physical plant operator error software bugs In attempt reduce failures noSPOF policy overused introduces complexity ends reducing reliability At global scale negative effects greatly magnified Enterprise scale systems typically highly distributed susceptible critical pathways failure Dis tributed systems contain critical pathways fewer addition having parallel service pathways All systems contain critical pathways fail create form unavailability When trying figure points failure critical question happens fails important question happens fails The question assumes deterministic failure expressed Murphy s Law fail In reality doesn t fail There parts fail parts fail important assess probability failing The question ask outcome failure A critical pathway defined probability occurring severity outcome They pathway severity index PSI combination probability outcome reduc tion availability failure pathway Any pathway hardware software high PSI requires redundancy strategy The noSPOF strategy overused enterprise scale easier apply determine pathways high severity index The strategy deploying noSPOF compromises reliability complexity ends increasing PSI pathway Distributed systems pathways spread reduce risk critical pathway failures result complete platform failure Because platforms highly distributed effect dis tribution replaces high PSI pathways hundreds low PSI pathways And low PSI pathways require noSPOF Chapter Big Data The Ultimate Computing Platform At enterprise global scale rough estimates PSI help prioritize redundancy bets placed Locating aspects design gap recklessness risk aversion result optimized platform The noSPOF doctrine enterprise scale availability engineering The global scale version solved optimized economic architectural operational dimensions constrain platform Avoiding epic failure global scale mandates need stand distributed systems optimize availability keeping simple keeping easy keeps reliable Optimize Absolutely Everything Platforms operate high level performance com plexity cost solution space optimized intersection operations economics architecture These dimensions fundamental form platform design platform supercomputer cluster Honda Civic Platforms successful optimized dimensions optimized efficient Determining platform optimized difficult subjective designing platform place Enterprises want fully realize benefits big data find global scale expectations platforms staff vendors Big data platforms monster computers A single Hadoop cluster punch consists hundreds racks servers switches These racks include surrounding infrastructure bales data cluster Many enterprises set hundreds racks gear stand single supercomputing platform Getting Hadoop cluster running hard optimizing dimensions pool fish Optimization maximizing productivity making precious resources myelin metal baked sand Optimizing platform means spending money wisely spending appear value Any orga Optimize Absolutely Everything nization reduce costs spending money s optimization s spending money assuming quality remains constant laying employees affects remaining staff These kinds productivity improvements misconstrued optimization Cutting costs makes business profit able necessarily efficient strategically positioned Every platform contributes economic activity optimized forms activity value utility Bang buck way optimize bang comes It mean spending fewer bucks given bang squeezing bang single buck A simple optimization strategy data circuits running Texas Montana include improving software engineer ing route data buying cheaper set switches pro vide required capabilities renegotiating service level agreements carrier The strategy optimize operational example perpetual prototyping Mind Gap In order optimize aspects business accurate risk analysis required And order optimize aspects plat form accurate risk analysis required There continuum risk aversion recklessness Some businesses afford risk averse To mitigate risk corporations employ strategies require degree calculated risk Sometimes calculated accu rately numbers employees educated guesses In preparation big data enterprises agencies need opti mize computing systems heart lungs business Accurately analyzing risk failure platform including hardware software humans competitors key optimizing efficiency plat forms It odd consider competitive landscape group employees platforms behave like interconnec ted systems Chapter Big Data The Ultimate Computing Platform CHAPTER Organizations The Other Platform From Myelin Metal The world advanced big data platforms strange place Like Gilbert Sullivan musical drama farce mayhem act Once long curtain rises time stands magic works Platform engineering global scale art form delicate balance craft money personalities politics With commoditization IT craft little art Studies shown percent IT projects fail billions dollars wasted annually The end results simply inefficient frequently unusable Projects finish late budget missing require ments There immense pressure CIOs convert IT infrastruc ture commodity plumbing office buildings Deploying platforms scale required cloud com puting big data complex projects IT groups undertake Managing complex projects magnitude requires healthy IT culture ensure successful discovery insights business craves continuously deliver insights cost effective way Computing platforms deeply impact corporation serve mention end users vendors partners shareholders This mobius strip humanity technology lies heart model modern major enterprise A socially productive IT orga nization prerequisite success big data Humans organized hierarchies water cooler appeared In corporate organization hierarchies try bal ance specialization labor details specialists worry distilling minutiae leaders informed busi ness decisions confused overwhelmed Distilling minutiae relies preserving right detail abstracting rest Because details created equal goal abstraction prioritize right details mask ones because confusion fear cracker jack job impairing judgment When lot good decisions quickly course corrections mitigate bad decisions Since organizations people motivation emotions behavior combine understanding topics pro duce judgments rarely let efficiently Silos In large organizations expense having generalists organ ized platform IT departments set hierarchies specialization achieve economies scale Silos result hierarchies need organize people economically effec tive groups In IT silos groups specialists Chapter Organizations The Other Platform A group database administrators DBAs specialists scale economically group grow supporting tens hundreds databases DBAs specialists databases storage Storage Admins specialists spindles inexperienced tuning SQL queries However fixing poor platform performance requires actual collaborative work spe cialties merely attending meetings doesn t cut Smaller silos silos emerge large corporations example storage administration database administration typically collected Operations silo UI design application programming contained Development silo If s politically difficult DBAs communicate Storage Admins DBAs UI designers barely aware s existence Although enterprises like organize employees silos sub silos platform served platform fails scale recover accommodate new business silo potentially implicated All computing platforms span horizontally organizations physical plant way firewall Big data platforms span horizontally extreme technically financially politically The silo structure suited developing managing platforms global scale Though administrative economic value silos suppress cross functional awareness discourage generalists work ing knowledge platform fill important tech nical diplomatic role Some organizations Infrastructure Reference Architecture group populated indi viduals stewards platform Both technical non technical expertise represented group platform properly represented instead s staffed experienced technical experts deep expertise limited area platform frequently reports Opera tions little representation Development Marketing Finance If infrastructure group given authority behave unilater ally compromises diplomatic mission There fine Silos line diplomacy moral suasion unilateralism Done group serves platform business Done poorly group ends silo Other companies construct tiger teams forcing subject matter experts number different silos work temporar ily In contrast teams specialists 50s 60s needed develop working knowledge old mainframe systems given latitude time cross pollinate skills specialists area generalists Never time job specialists learning working generalists given time understand rest platform Not specialists comfortable adept cultivating breadth knowledge casting tiger teams extremely critical Tiger teams fail members miscast allowed forget silo work Industrial Grain Elevators If s hard IT departments tear silos imagine hard industry Silos partially arose ashes stop shop single vendor mainframe model Vendors specializ ing network storage products found easier sell network storage groups reinforced emerging silos specializa tion The products companies optimized specific demographics specialists evolved away platform awareness closer particular needs silo subject matter expertise Poor interoperability multiple vendors products best example force action time platform obfuscated Chapter Organizations The Other Platform Some vendors attempting revival stop approach increase growth business necessarily benefit platform customers big data But customers distant recent odd PC memories stop pleasant Throat choking harder looks closer inspection current stop attempts larger vendors t tear internal silos oddly vendors organizations They end operating competing businesses brand Vendors attempting stop shops prefer silo model especially franchise strength Vendors aspire use big data grow current portfolio products cer tainly don t want sacrifice existing revenue base For vendors zero sum game For far zero economic rules big data ecosys tem unlike economic rules current enterprise ecosys tem Like Kodak business margins based film instead memories traditional enterprise vendors need base big data offerings insight capture strand In past decade customers grown increasingly dependent advice vendors The codependency vendors IT Industrial Grain Elevators departments entrenched natural consequence silos It difficult IT groups informed consumers com moditization staff helped Drained advanced engineering talent IT outsourced expertise service companies vendors For example enterprises advice disaster recovery storage database vendors completely different answers Vendors try convince customers silo centric sol ution superior vendors don t custom ers best interests mind Like performance scalability disaster recovery tougher problems platform engineering Even benefit platform perspective DR requires delicate balance speed budget good set dice Attempting silos far painful silo centric solutions usually avoid implicated event actual disaster Most solutions consist piecemeal strategy cobbled competing vendors Once platform takes chin Platforms Silos The plethora intertwined software hardware platform stubbornly refuse operate like dozen independent silos Disaster recovery performance problems tough tri age modest enterprise platform degree complexity exponential node cluster Com mercial supercomputers transcend mechanics politics silos successful When performance problems triaged silos result like game schoolyard tag The group glaring symptoms gets tagged If s storage admins find fix storage problem prove wasn t fault The storage group rarely understands application code encouraged cross pollinate application developers Likewise developers far removed physical reality platform underneath incentive understand happens platform add ingly insignificant feature results extra disk reads Chapter Organizations The Other Platform Whenever goes seriously wrong computing plat form organization silos demands accountability There usually couple platform aware individuals lurking IT departments ones determine insignificant feature caused storage performance problem The good news silo s fault The bad news s s fault Advances Java hypervisors general reliance treating computing platform abstract terms reinforced notion longer necessary understand comput ers actually work Big data performance scalability knowing hardware software important Panic Now When working planned delivered time budget silos specialists economical sense organization When platforms fail underlying problem masked silos statements like perception reality start swirl water cooler If hear work start higher ground As groups scramble sure problem theirs combination fear ignorance starts set results impaired judgment panic Silos compete budgets time stats political credibil ity frequently leaves platform business undefended When organization important business compa nies worst enemy Fear Loathing Organizations size comprised humans varying tolerances fear risk In order good judg ments brains discern sort complex competing bits information Fear weird things human brain disrupting ability good judgments The impairment disruption lead dysfunctional behavior Panic Now But robots making decisions emotion considered psychological disorder Studies shown single decision emotional component matter insignificant Research subjects pathways fully disrupted find difficult choose cereal breakfast Decision making delicate balance signals emo tional brain amygdala thinking ventrome dial prefrontal cortex What good judgment big data Everything For organizations amped ego ambition combined intolerance error comes complexity scale big data means lot decisions quickly information And requires judgment Fear anger sides impairment coin The emotional primitive brain responsible fight flight Fear flight anger fight good impairing judg ment For example I m meeting I ve clashed previous project emotional perception opin ion distorted feelings I uncon sciously misinterpret saying body language tone voice choice words communicate infor mation Also I listen subtext plot downfall construct new conspiracy theory I listening Making good decisions isn t emotionally impaired fear anger It isn t knowing details prioritizing right Since details human brain learn find matter Finding requires old reptile new mammal brains dance fear anger definitely kill music A business relies staff sort prioritize details day Experience informed judgment required It s called business acumen educated guess When guess right hero guess wrong s OK need guess new information mistakes humans learn Big data new completely uncharted world problems encountered Right wrong guess Chapter Organizations The Other Platform faster learn faster Prepare lot mistakes learn lot Risky Business Small companies cultural acceptance risk gets diluted company grows Small companies appear reckless viewed risk averse end spectrum large com panies operate acceptance taking calculated risks reck Risk aversion safe better safe sorry sorry safe Every year surfers world attempt surf story waves coast Northern California Maverick s From viewpoint risk aversion surfers like lunatics Because reckless surfer dead surfer surfers effective risk techni cians Similarly rock climbers face El Capitan Yosemite National Park especially free climb variety considered lunatics In exchange determining risks involves invalua ble intuition years experience surfers climbers rewarded exhilarating experiences An organization s operating risk spectrum gap aver sion recklessness In business risk averse perception risk actual risk gap aversion recklessness contains competitors willing risk If don t believe large gap compla cent competition gap wide accommodate competitors opportunities new business Disruptive forces like big data widen gap accurately perceiving gap relies heavily old new brains Making better decisions requires better accurately assessing risk Probability Outcome Probability idea outcome experience Humans tend perceive risk based outcome probability Like mathematics probability based natural world func Risky Business tions empirical level probability idea come grounded experience Using classic case driving versus flying know s far riskier drive US Interstate catch shuttle Burbank doesn t wash human psyche If plane crashes subsequent outcome bad happening e death certain However probability killed car crash certain taking commuter flight You better chance surviving outcome risky Severity outcome bearing probability acci dent place brains work Good risk technicians fight instinct order things mere mor tals dream surf waves Maverick s surf capital burndown startup takes IBM Deterministic risk analysis example aversion In attempt protect business possible outcomes instead probable outcomes organizations assume worst They assume failures occur Deterministic analysis assumes possible failures happen probabilistic analysis assumes components likely fail ones actually fail Being better risk technician help optimize platform Quantitative Qualities One sure fire way accountants controllers mad ask quantify qualitative risk Turns happening spreadsheets s happening intuitive level time form thousands business decisions day An easy example qualitative risk analysis found making decision recruiting new employees The decision hire candidate subjective judgment involves brain making decisions qualitative information There mathematical intuition s unmen tionable word organizations Not isn t Chapter Organizations The Other Platform everyday decisions appears biased non linear random Good intuition far random allow quick decision making Having patience listen recognize good intuition makes possible people better decisions faster The Soft Platform Organizations don t kill projects people kill projects projects kill people All bad cliches organizations bad cliches Getting humans work oiled machine hardest soft platform hard understand hard preserve innovation hard change Changes organizational behavior happen glacial rate relative technology business conditions accompany trends like big data Humans simply t change patterns behavior fast technological advances It s cultural impedance mismatch The rate acceptance big data came quickly heels cloud computing craze necessarily slow erratic We figured clouds want Big data brings shift demographics professio nals open source programmers data scientists bring energy new approaches established industry Anthropology technology converging produce major shift consumes data enterprises customers agencies researchers studying humans behave organizations The Soft Platform CHAPTER The Reservoir Data The Actual Internet We wouldn t talking big data weren t explosion internet Several technologies drifting 1980s eventually converged boom possible Mainstream consumer culture experienced boom came Since 1990s internet taken evolutionary steps Running business computing plant global scale Yahoo Google Facebook attempted They encountered solved engineering organiza tional problems taking commercial supercomputing enterprise scale global scale But Yahoo demon strated making sustainably profitable business computing intensity different matter Traditional enterprises companies films 737s soap time experiencing global scale computing problems stuck decades old entrenched approach enterprise scale computing For remember hap pened 1990s point didn t happen skepticism Miracle Big Data justified Taken perspective early technologies example Java Apache involving billions users unproven boom going wishful thinking And lot wishful thinking going 1990s Many startup companies built prototypes early technologies like programming language Java easier quickly develop applications If startup s idea caught problem customers quickly overwhelmed designers inten tions Good problem Building platforms scale requires lot scaffolding tax startup wish customers building system handle millions customers expensive complex optimistic Silicon Valley start ups 1990s An application designed quickly demonstrate pet food purchased online demonstrating millions pet owners require annoying platform engineering bits work rarely came question seed stage later phases funding Startups good idea reason able application soon crushed inability scale Companies trying design killer app pressured con stantly tweak design attempt land millions customers quarter Core design requirements reverse weeks redesign whirlpool inescapable Very companies survived Amazon cited survivor original core architects built Amazon came Wal Mart built scale inventory management platforms planet Wal Mart impressive job changed rules supply chains inventory retail Startup companies acquire modest plat form engineering chops constrain instinct add thing survive despite having viable business plan thousand customers Best Inbred Platform engineering embodies mantra sum parts deficiencies par ticular technologies Components platform mature stable best breed myth corresponding current silo view enterprise engineering Chapter The Reservoir Data A best breed platform require components best breed platform assembled best breed technol ogy necessarily best breed Best breed concept introduced enterprise silo vendors s product brand strength given silo Best breed simply unaffordable global scale Building suc cessful big data platforms broader pedigree components pedigree dictated scalability affordability If architects data center noSQL database engines meet business requirements sophistication expense Oracle This doesn t mean MySQL replace Oracle surgically deploying DB2 table But platform needs handle hundreds millions users affordably secret sauce platform engineering aggregation best breed products Some enterprises living managing big data long time Healthcare companies trying archive patient histories built earliest databases Some records live legacy arrays hibernating tape reels In order discover insights legacy data accessible Moving data shiny new Hadoop cluster require solv ing platform engineering problems standing shiny new object look easy There s pent demand big data companies trying years vendors couldn t offer sol utions affordable scale And data lives single product suite products given vendor solve big data problems Even enterprises attempted stop shop approach decade ended isolated stranded sources data Customers data sources stranded Greenplum Netezza Exadata risk stranding new sources Cassandra Mongo Hadoop Like scientific supercomputing commercial supercomputing solved products single vendor Big data consists broad spectrum purpose built workloads traditional business Best Inbred intelligence products general purpose address diverse spectrum purpose built address nar row range workloads Big data requires strange new hybrid platform products software vendors fits designed heterogene ous product form fitted enterprise s peculiar mosh pit old new data makes lousy SKU compli cated story Vendors don t like complicated stories Drowning Not Waving By time read big data cliche rou tinely parodied YouTube For enterprises big data cruel expensive joke years ago The data warehousing prod ucts created 1990s outgrowths major RDBMS vendors got early glimpse tsunami This generation technology possible advances server technology Hardware companies like DEC HP IBM prodded startups like Pyramid Sequent SGI designed servers finally powerful execute queries terabyte data A small startup Teradata developed database kernels handle queries TB data Established database compa nies like Informix Oracle Sybase soon chasing Teradata Vendors spent years building kernels optimized transaction processing needed tool kernels order handle queries process thousand times data Some companies developed purpose built kernels handle spe cific class workloads point time big data started This early difficult clumsy expensive market called lot things years decision support OLAP data warehouse business intelligence BI 1990s important benchmark standards com mittee TPC defined benchmark help users qualify industry sol utions To extent benchmarks help customers purchas ing decisions artificial workloads defined generation tech nology capabilities As successful Teradata setting bar warehouse performance turned mom pop Chapter The Reservoir Data purpose built business like scientific supercomputing After years Teradata David Goliaths IBM Oracle EMC In commercial computing highly un sexy applications book keeping widget tracking money Yet Goliaths difficulty dominating big data high performance commercial computing reasons scien tific computing growth business purpose built complex engineering boutique revenues pregnant sales cycles Big data moving fast relative clumsy old industry standards bodies find difficult define general purpose benchmark purpose built world As soon possible extract transform load ETL warehouse quantities data enterprises started drowning Prior 1990s data sources abundant high cost storage meant stashing tape Tape technology lives cats The simple reason tape viable today economics Even disk storage approaches TB tape remains couple orders magnitude cheaper Big data starts live enterprises petabytes cluster afford load exa bytes In world tape alive sen sors exabytes originated producing zettabytes year Spinning Rust Hennessey Paterson shown processing technology tracked Moore s Law memory storage In early 2000s cost memory started fall line Moore s Law memory semiconductor storage technol ogy remained mechanical The technology disk drives today far removed disk drives 1980s The landmark IBM Winchester spinning platters rust oxidized particles flying mag Spinning Rust netic heads true today drives found Hadoop cluster The recent emergence flash storage technology Hadoop low cost alternative arrays expensive disks combine produce form disruption industry A flash based Hadoop cluster time able operate work ing set problems memory speeds However economics storing hundreds petabytes ensure forms spinning spooling rust required big data A Spectrum Perishability In old silo world enterprise data mission critical extremely valuable lost corrupted compromised Most enterprise vendors designed products extremely persistent cases databases coher ently persistent Today land flooded data expensive cherish bit necessary For time enterprises afford crunch absurd data analysis discovery insight The price admission able stage PB long crunch occur In cases TB keeping PB crunch impractical data tossed When petabytes exabytes exabytes zettabytes zettabytes yottabytes keeping tons data crunched option Data lives spectrum perishability spans seconds decades Data transient analysis complete hour shelf life insight expires data deleted room hour s data Perishability puts emphasis insight retention Histori cally enterprises chosen data long possi ble cheaply possible big data ideas policies duration cost retention revaluated Everything lives spectrum perishability data technology business Innovation drives rapid expiration Chapter The Reservoir Data Software vendors built warehouse products run dedicated hardware necessity ensure complex product work If vendor IBM products typically ran IBM hardware If vendor Oracle products typically ran hardware Oracle s hardware partners Dell HP IBM Prescribing packaging hardware platform increased odds successful deployment This trend engineered systems looks like platform aware evolu tion turns vendor franchise manage ment customer experience Plus increases likelihood stranding customers data Stranded captive data result vendors optimizing prod ucts margins markets This approach product devel opment tends stifle innovation If franchise remains strong enforced captives t escape vendors decent living But franchise exists today big data established players like IBM Oracle EMC Enterprise customers continue purchase new warehouse products promise solve data problems data failed platform new improved Improvements cost scale mean latest capa ble system ends data All old platforms good job snaring data technically politi cally difficult usually migrate new system Many enterprises large collection stranded data sources year s database expensive storage arrays vast repositories analog data X rays haven t cockroach storage mediums tape Enclosed Water Towers As tsunami data inundates enterprises feel existing water towers data clean safe contami nation Despite tanks built expensive relative millions gallons water come ashore wave hold little reveal The volume data Enclosed Water Towers coming enterprises fill Los Angeles County service reservoir minutes Because enterprises spent years constructing larger larger water tanks stranded captive data need start building reservoirs safely capture raw data including debris processed treated An Oracle database running EMC hardware capable water tank remains closed source analytic residents For enterprises reap benefits come able analyze aggregated data old new stop stranding data tanks start constructing open common reservoir data uncouples accessibility analy sis These new repositories function like fresh water reser voirs serve city size Los Angeles In California majority rain falls November March water demand constant Reservoirs efficient way store hundreds thousands acre feet water districts Chapter The Reservoir Data use plumbing pumps deliver water downstream cus tomers Like water tsunami wall earthen damn failed water extremely destructive force nature Too little water mammals like I end wrong end perishability scale At rest water usually considered hazard water influence gravity seismicity control because limitless destruction Data like water treated respect Mammals need fresh clean drinking water enterprises need safe clean data Since big data reservoir need efficiently accommodate hundreds exabytes worth bother building accessible robust reservoirs And critical sustainability enterprise The Big Data Water District The Tennessee Valley Authority largest public works projects history United States To enterprises building big data reservoir feel like project scale TVA A big data reservoir able hold water need collect accessible robust affordable construct present maintain future The file system contained Hadoop commer cial file systems meet criteria Hadoop consists major components file system HDFS parallel job sched uler When HDFS creates file spreads file available nodes makes copies job runs clus ter spare copies file ensure par allelism protection possible File systems closely associated databases operating systems A file system isn t usually thought distinct piece technology tightly integrated piece natural extension database operating system kernel For example Oracle s database kernel aspects file system built tables segments extents perform functions asso ciated traditional file system The Big Data Water District Veritas companies demonstrate file sys tem valuable stand product didn t embedded OS database Veritas longer wasn t functional autonomy file system bad idea Execution competitors egos sheer luck influence destiny commercial technologies The HDFS Reservoir The Hadoop Distributed File System complex feature rich kitchen sink file system things s eco nomical functional enormous scale Affordable At Scale Maybe s HDFS economically sustainable computational file sys tem existence Some file systems share like NFS Some file sys tems scale like Ceph Most file systems require user supply processing capabilities database pile scripts Hadoop comes scheduling capability enables diverse forms processing entire file system Sometimes processing analytics transcoding enables databases SQL queries Hadoop comes built processing capabilities Pig pile scripts Hive simple SQL database scheduled parallel cluster HDFS general pur pose transactional file system like NFS flexible purpose built hyper scalable analytics file system A big data reservoir possible traditional database products directly access HDFS provide canal enter prises channel old data sources new reservoir Big data reservoirs allow old new data coexist inter mingle For example DB2 currently supports table spaces tradi tional OS file systems supports HDFS directly provide customers built channel past future HDFS contains feature called federation time create reservoir reservoirs possible create planetary file systems act locally think globally Chapter The Reservoir Data Third Eye Blind The time engineering effort required navigate old data sour ces canal frequently exceed effort run Hadoop job small task Hadoop powerful pro gramming platform application platform Some customers surprised find Hadoop cluster comes factory This DIY buy dedicated hardware appliance Hadoop doesn t come applications business requires analyze data Doing involves having competent team engineers capable loading data writing applica tions process data These developers construct pro cessing workflow responsible generating insight The cast developers required includes data scientists workflow engineers data wranglers cluster engineers supercomputers fed clothed Third Eye Blind Clusters loaded ways existing stranded sources greenfield sources gaggle web server logs Moving old data existing stranded sources underfunded project astonishing complexity Like major canal project construction data canal legacy sour ces new reservoir complex platform project right Migrating large amounts data particularly annoying old systems need continue run unimpeded migration accessing systems delicate problem This migra tion platform hidden view architecturally serve masters moving data quickly carefully It difficult moving modest data example TB patient records old DB2 mainframe Hadoop cluster feel like moving passengers bus explodes slows Spectrum Analytic Visibility Chocolate vanilla analog versus digital big data comes flavors structured unstructured struc ture lives spectrum analytic visibility data Video data frequently cited unstructured data source seriously structured An MPEG transport stream contains sorts bits help set box find correct audio video streams Within streams structure set box disentangle audio video streams The degree structure required depends discov ered A set box aware layers structure bit stream analyst running big data job search criminals CCTV footage interested odd macro block doesn t care sound locks picture NoSQL databases popular affordability ease use operating global scale The organization data found family noSQL databases pejoratively described unstructured better way describe simply structured Chapter The Reservoir Data Viewed deep complex expensive relational data base simple structure completely unstructured structure required depends entirely looked fast found As far 1960s need access information way simple fast necessarily sequentially This method called index sequential access method Access method term today inside database engines describe data read tables An ISAM file single index key randomly access single records instead ponderous sequential scan A user supplied key values associated key returned user An ISAM like key value table constructed enterprise grade relational database simple table going expensive key value table limits size inability enterprise engine s ability construct The easiest way access hundreds terabytes data requires access methods simple implication scalable Simple scalable requires relatively simple method like key value pair The new generation fast cheap noSQL databases big data applications known key value pair data bases The structural antithesis noSQL class complex expensive uberSQL relational databases Big data workflow cleaning filtering analyzing patterns lead discovery Overly structured data definition editorialized refined transformed With ability aggregate raw data intermediate steps including mistakes final clean data big data reser voir exposes workflow The workflow ugly exposed scien tists business owners data wranglers greater potential discover things didn t know looking place This puts epiphany big data Historically process called extract transform load ETL But economics scale Hadoop changed order ELT raw data loaded scheduling power Map Reduce brought bear multiple transform pipelines Spectrum Analytic Visibility Hadoop users discovered ability clean processing pipelines justifies acquisition Hadoop clus ter In cases analysis conducted old legacy database silos tooling analysis pipelines tends difficult Hadoop redefines ETL ELTP P stands Park The raw data processed data archived data park single affordable reservoir The Cost Flexibility Initial attempts data discovery started relational databases data accumulated discovery worthwhile Most relational databases designed handle acre feet data designed proficient online transaction pro cessing OLTP Eventually data warehousing capabilities graf ted database engines grafting difficult early versions unsuccessful These early attempts analyzing big data impeded kernels optimized hundreds tables hundreds columns huge tables columns billions rows Eventually traditional database vendors developed effective meth ods handling queries huge tables resulted structure necessary A relational data model schema collection tables columns This model provided far flexibility approach replaced IBM s IMS hierarchi cal data model 1970s relational technology required users know ahead time columns went tables Common relational database design encouraged practice called normalization maximized flexibility case users needed add new tables new columns existing tables Normalization minimized duplication data tables disk space expensive This flexibility relational database quickly replaced hierarchical database technology de facto database point SQL queries frequently require tables joined The piece magic inside database kernel makes possi ble called SQL query parser optimizer cost based optimizer Chapter The Reservoir Data CBO SQL optimizers use algorithms determine cost retrieving data order select cost effective retrieval strategy Joining tables solve query quickly tortur ous exercise pretzel logic A CBO engineered highly normal ized OLTP schemas designed join complex tables thousands rows It designed big data schemas simpler tables billions rows OLTP based CBOs opti mize space time big data CBOs optimize time space Big data workloads consist broad spectrum purpose built workloads This spawned myriad new database products work scale purpose built possible build single general purpose database kernel CBO handle entire spectrum In attempts address big data general purpose ware house products customers end purchasing attempt result stranded water tank data By early 2000s vast quantities enterprise data stranded software hardware platforms designed big data Even software hardware components capable big data tooled CBO handle billion row tables expensive big data better described Big Bucks Large pools data need discovery need combined pools uberSQL repositories trapped slow complex expensive databases Building diversion canals stranded water towers big data reservoir difficult fully charged reservoir finally aggregates data single scalable repository single analytic view important legacy big data The Cost Flexibility CHAPTER Cloudy Chance Meatballs When Clouds Meet Big Data The Big Tease As scientific commercial supercomputing collide public private clouds ability design operate data centers computers poorly understood enterprises han dling million anythings The promise fully elastic cost effective computing plant seductive Yahoo Google Facebook solved problems terms Traditional enterprises agencies facing pressure compute global scale modest desire improve efficiency enterprise scale physical plant need identify requirements cloud computing big data Conventional clouds form platform engineering designed address specific financial organizational operational requirements Private clouds designed residents silos value requirements silo Clouds like platform designed meet variety requirements purely operational Everyone wants elastic platform cloud discussed Chapter designing platforms global scale comes trade offs elasticity come free Big data analytics clouds meet stringent performance scalability expectations require different form cloud The idea clouds meeting big data big data living clouds isn t simply marketing hype Because big data followed closely trend cloud computing customers vendors struggle understand differences enterprise centric perspectives On surface physical similarities technologies racks conventional cloud servers racks Hadoop servers constructed physical components But Hadoop transforms servers single supercomputer con ventional clouds host thousands private mailboxes Conventional clouds consist applications mailboxes Windows desktops web servers single instances applications longer saturate commodity servers Cloud technol ogy possible stack mailboxes commodity server achieve operational efficiency Since Hadoop easily saturates piece hardware hands Hadoop bad fit forced conventional cloud containing idle applications Although existing conventional clouds think big data work effortlessly s easy Hadoop clouds designed support supercomputers idle mailboxes A closer look reveals important differences conventional clouds big data significantly achieve scalability different ways different reasons Beyond mecha nisms scalability exploits desire big data clouds operates fully elastic supercomputing platform overlooks added complexity results convergence Complexity impedes scalability Scalability Stated simply basic concept scalability defines platform handles flood new customers friends miles new CCTV footage Achieving scalability requires deep standing platform attempting accomplish When river new data reaches flood stage Chapter Cloudy Chance Meatballs When Clouds Meet Big Data platform scales continue handle increased load quickly cost effectively Although concept scalability easy understand strate gies mechanisms achieve scalability given set workloads complex controversial involving philosophi cal arguments share things data resources time money know stuff humans rarely share How systems processes scale isn t merely propeller head com puter science topic scalability applies barbershops fast food res taurants vast swaths global economy Any business wants grow understands platforms need scale standing scale nasty bit The common notion scalability comes outside data center economies scale Building cars burgers googleplexes requires means production cost effectively meet growth demand In 20th century US economy affluent population consumers comprised market large company s product developed national following produced scale This tradition scalable production traces past Henry Ford past War Independence ancestral home industrial revolution Europe During minor tiff collec tion colonies British Army felt need exercise moral suasion Their field rifles pieces engineering nearly works art They high quality expensive build difficult maintain field The US army didn t engineering financial resources build rifles near quality knew build things simply cheaply sufficient quality tenets scalable manufacturing The catch simple quick cheap quality quality rifles aren t reliable fire dropped ditch quality means higher costs resulting fewer weapons The British museum quality rifles translate dead Americans quality wasted properly optimized This lesson things scale quality impedes scale little results lousy body count Scalability Scalability isn t things faster s enabling growth business able juggle chainsaws mar gins quality cost Pursuit scalability comes verse form disruptive possibilities Companies create great product service build franchise things scale business grows switch designing margins instead market share This great way drive profit growth franchise threatened Choosing margins markets reasonable strategy tertiary sectors mature subject continuous disruption However computing technology remain immature innovative disruptive time It hard imagine IT mature bridge building Golden Gate bridge built lunatics time Companies far margins path rarely switch market driven path eventually collapse organizational weight The collapse company fun involved like collapse old stars resultant supernova ejects talent market possible wave innovative businesses Motherboards Apple Pie Big data clusters achieve scalability based pipelining parallel ism assembly line principles found fast food kitchens The McDonald brothers applied assembly line burgers speed production reduce cost single burger Their assembly line consists burgers states parallel assembly In computer world called pipelining Pipelining helps efficiency breaking task making burgers series simpler steps Simple steps require skill incremental staff paid British rifle makers course construct perfect burger More burgers plus faster plus cost burger equals scalability In CPU design breaking circuitry pipeline executes instructions allows pipeline run higher speeds Whether kitchen CPU simple steps possible reduce effort required price increase production performance burgers instructions Chapter Cloudy Chance Meatballs When Clouds Meet Big Data Scalable systems require price performance optimized price performance trying let bacteria seep meat left counter Well s theory Once kitchen set efficiently series burgers adding burger assembly line grill staff operate double output kitchen It impor tant assembly line efficient possible s example thousands new restaurants Back inside CPU economic sense designers build multiple floating point functional units scien tific code run customers saturating CPU instructions Adding floating point grill program run twice fast customers gladly paid Scientific commercial supercomputing clusters parallelism little bit pipelining These clusters consist hundreds grills cooking data parallel produces extraordinary scalability Each cluster node performs exactly task complicated applying mustard ketchup cluster contains hundreds identi cally configured computers running exactly job private slices data Being Nothingness Big data cluster software like Hadoop breaks analysis sin gle massive dataset hundreds identical steps pipelining runs hundreds copies parallelism Unlike happens conventional cloud Hadoop trying cook hun dreds burgers s trying cook massive burger The ability mince problem charbroil genius lies commodity supercomputing System designers use term shared indicate pro cess hundreds nodes working away problem try ing bother nodes Shared nodes try hard share In practice little sharing propagate illusion single monolithic supercomputer On hand shared data architectures emphasize value gained having nodes common set data Soft Being Nothingness ware hardware mechanisms required ensure single view remains correct coherent mechanisms reduce iso lation The need ensure single shared view traded scala bility There class workloads benefit single shared view world big data massive scalability thing traded away shared When doubled size shared cluster scales perfectly operates twice fast job completes half time A shared cluster scales perfectly getting cluster faster doubled considered good scaling Several platform workload engineering optimization strategies increase efficiency cluster A cluster scales better remains smaller improves operational efficiency Hadoop demonstrated scale high node counts thousands nodes But node cluster generates throughput node cluster faster instead isn t important Hadoop s ability scale thousands nodes Very data processing platforms achieve let affordably Parity Is Farmers Hadoop commercial supercomputing accomplished scientific supercomputing 1990s Mon olithic HPC supercomputers 1960s Seymour Cray designed Control Data con sidered successful supercomputers He Chapter Cloudy Chance Meatballs When Clouds Meet Big Data worked short order cook previous lifetime design innovations resemble fast food kitchen What fast pipelining tasks CPU The steps broken easier cheaper design build After Cray finished pipelining CPU cir cuits turned parallelism adding extra burger grills designed specifically increase number mathematical floating point results calculated second After Cray left CDC took design step Cray noticing solving hundreds equations generated thousands identical instructions add floating numbers divide The difference instructions values operands instruction He decided build vector functional unit perform sets cal culations burger s Pipelining parallelism hallmarks HPC systems like Cray corporations afford Cray IBM mainframe Decades later microprocessors affordable capable figure lash hun dred shot creating poor man s Cray In late 1980s software technology developed break large monolithic jobs designed run Cray smaller jobs It wasn t long scheduling software skulking university campus dead night infesting network idle workstations If single workstation 50th speed Cray needed workstations break Two workstations times faster Cray price performance seriously good especially department probably paid workstations Starting early Beowulf clusters 1990s HPC clusters constructed piles identically configured commodity servers e pipeline simplicity Today node HPC cluster constructed nodes CPU cores zillions memory controllers problems dreamed 1990s Parity Is Farmers Google Reverse Hadoop software applies HPC cluster trick class global scale computing problems considered non scientific Hadoop evolution HPC clusters class non scientific workloads plaguing companies global scale non scientific datasets Financial services healthcare companies problems large HPC crowd Hadoop came way analyze big data relational databases Hadoop evolved directly commodity scientific supercomput ing clusters developed 1990s Hadoop consists parallel execution framework called Map Reduce Hadoop Distributed File System HDFS The file system scheduling capabilities Hadoop primarily designed operate tolerant unreliable commodity components A Hadoop cluster built laptops pulled dump ster work exactly enterprise grade work Yahoo couldn t initially afford build node clusters thing cheapest sub components rule assembly lines optimize cost effort steps pipeline Hadoop designed operate tera bytes data spread thousands flaky nodes thousands flaky drives Hadoop makes possible build large commercial supercomput ing platforms scale thousands nodes time scale affordably A Hadoop cluster couple orders magni tude hundreds times cheaper platforms built relational technology cases price performance orders magnitude thousands times better What happened big iron expensive HPC business early 1990s happen existing analytics data warehouse business The scale price performance Hadoop signifi cantly disrupting economics nature business analytics conducted Chapter Cloudy Chance Meatballs When Clouds Meet Big Data Isolated Thunderstorms Everybody understands value gained high resolution view consumers shopping habits possibility predicting crimes likely place This value realized ana lyzing massive piles data Hadoop find needle customer criminal large haystack quickly Hadoop breaks huge haystack hundreds small piles hay The act isolating haystacks smaller piles creates scalable shared environment hundreds piles searched simultaneously Superficially Hadoop related conventional cloud computing cloud implementations came companies attempting global scale computing The isolation benefits shared platform achieve scalability clouds scale However conventional clouds Hadoop achieve isolation different mechanisms The Hadoop isolation mechanism breaks single pile hay hundreds piles nodes works pri vate pile Hadoop creates synthesized isolation But cloud running mail application supports hundreds millions users started hundreds millions mailboxes little piles hay Conventional clouds natural isolation The isolation mecha nism results initial requirement email user iso lated privacy Although Hadoop infrastructure looks similar servers running mailboxes Hadoop clusters remain single monolithic supercomputer dis guised collection cheap commodity servers Isolated Thunderstorms Private Cloud Myths Many large enterprises 1990s hundreds thousands PCs needed care feeding meant installing patching making backups As workforce mobile laptops replaced desktops employees computers employers sensitive data difficult protect Early pri vate clouds set s My Documents folder data center better secured backed restored When personal computers began computing power servers running office accounting systems spare power went unused users spent days MS Office A single modern workstation early 2000s support users This bare iron workstation carved virtual PCs hypervisor like VMware s ESX Red Hat s KVM Each user got experience hav ing laptop gave company reduced costs Conventional clouds consist thousands virtual servers long server beating daylights good The problem running Hadoop clouds virtualized servers beats crap bare iron servers living Hadoop achieves impressive scalability shared isolation techniques Hadoop clusters hate share hard Chapter Cloudy Chance Meatballs When Clouds Meet Big Data ware Share data share hardware Hadoop shared While saving money worthwhile pursuit conventional cloud architecture complicated exercise scalable platform engi neering Most conventional clouds able support Hadoop rely individual application provide isolation Hadoop cloud means large pseudo monolithic supercomputer clusters lumbering Hadoop elastic like millions mailboxes It node supercomputer cluster thinks maybe wishes Cray Conventional clouds designed mailboxes elastic elastically accom modate supercomputing clusters single unit service span racks The Network Is Backplane Cloud computing evolved grid computing evolved client server computing IBM Selectric operating private clouds remains undiscovered country enterprises Grids scale plumbing seri ous adventures modern engineering Enterprises use clouds consolidate sprawling IT infrastructure reduce costs When groups attempt implement type cloud conven tional Hadoop private public likely use legacy practices affordable scale Private clouds implemented expensive tier legacy infrastructure practices It s Hadoop t run conventional clouds run cloud built dumpster laptops efficient When Hadoop clouds deployed legacy practices hardware performance scalability severely compro mised underpowered nodes connected underpowered external arrays users running guests physical servers contain Hadoop data nodes A single Hadoop node easily con sume physical servers endowed high disk I O throughput The Network Is Backplane Most conventional clouds share I O network resources based assumptions guests demanding Hadoop data nodes Running Hadoop conventional cloud melt array shared way SANs config ured connected clouds enterprises Conventional clouds designed built optimized web servers email app servers Windows desktops simply able withstand resource intensity Hadoop clusters Cloud computing enterprise network topologies designed manageability attachment Hadoop uses network topolo gies interconnect data processing nodes way throughput efficiency prioritized attachment man ageability creates topological conflicts Hadoop running conventional clouds If Hadoop imposed conventional cloud needs cloud designed run supercomputers conventional cloud remodeled repurposed reshaped Big data clouds designed big data The Orientation Clouds Everyone assumes Hadoop work conventional clouds easily works bare iron s true willing ignore high cost complexity manage triage Hadoop inability scale imposed conventional clouds Conventional cloud infrastructure architecturally eco nomically optimal running supercomputers like Hadoop And Hadoop proven self contained highly resilient cloud right Chapter Cloudy Chance Meatballs When Clouds Meet Big Data Because conventional clouds designed class applications throughput intensive consume resources These little clouds exploit resource profile end applications ecommerce web mail As generations hardware servers evolved powerful possible stack server instances web mail single physical chassis Commodity hardware outstripped combination bottle necks lethargic think times exist end appli cations Since applications deployed refreshed hardware benefit improved hardware technology stacking multi ple versions instances single hardware chassis better utilizes refreshed hardware The success VMware living proof applications exist large numbers Conventional clouds deploy manage applications OS image level clouds designed consolidate instances applications running idle servers These clouds support sev eral variants Windows Linux restart OS image application event failure Typi cally copies Windows running single hardware chassis If images remain lethargic world good The Orientation Clouds But applications start consume resources memory network disk bandwidth server instances start suffer Conventional cloud provisioning sys tems relocate resource intensive application images order minimize disruption When Hadoop cluster imposed conventional cloud platform assertions quickly fail The important assertion Hadoop processing nodes designed completely saturate hardware resources disposal Because Hadoop designed high throughput supercomputer processing nodes bottlenecks If Hadoop node OS images running chassis chassis built high performance plenty spindles memory channels images severely impac ted If Hadoop cluster deployed conventional cloud appears node cluster provisioned physical machines actual throughput node cluster A node Hadoop cluster collection standalone instances relocated restarted unified node data processing instance The assertion wreaks havoc running Hadoop con ventional clouds data elastic When Windows image fails private data needs relocated relatively small moved quickly But Hadoop data processing node typi cally reads hundreds terabytes private data course day Provisioning relocating Hadoop data processing node involves terabytes inelastic data Whatever cloud pick data inelastic terabytes speed infrastructure bottlenecks Even Hadoop t break laws physics HDFS bet ter job provisioning relocating data cloud optimized different set platform assertions Conventional clouds rely heavily shared storage S3 file system AWS traditional NFS pods required high performance s important shared storage accommodates provisioning relocation throughput scalability I O required little clouds Chapter Cloudy Chance Meatballs When Clouds Meet Big Data However Hadoop data processing nodes expect draw private data throughput shared storage shared storage pods Hadoop processing node required meet demand Hadoop strains econom ics conventional cloud infrastructure Hadoop far seam resource invasive s forced conventional cloud infrastructure Conventional clouds way provision schedule computing resources specific low throughput category applications conventional clouds deployed bare iron This type cloud converts thousands servers flexible pool computing resources allow broad heterogene ous collection Windows Linux servers operate effi ciently By definition conventional clouds heterogeneous general purpose clouds Hadoop deployed bare iron servers homogeneous purpose built supercomputing analytics cloud When viewed alternative deployment model cloud Hadoop superior topology expanding family Apache analytics applications A Hadoop analytics cloud differs commissions de commissions individual data processing nodes It provision copy Windows Linux provision relocate HBASE HIVE Accumulo Stinger MapReduce Storm Impala Spark Shark expanding ecosystem Apache analytics The art running cloud hardware infrastructure sure platform workload assertions hold don t understand implications trade offs Groups com pelled run Hadoop conventional clouds understand assertions differ heterogeneous conventional clouds homogeneous Hadoop analytics clouds discover types analytics applications work constraints conventional cloud My Other Data Center Is RV Park Companies attempting build conventional clouds Hadoop ana lytics clouds operate global scale discovering My Other Data Center Is RV Park need rethink thought knew design build run data centers The pioneering companies initially purchased servers ven dors like IBM HP Dell quickly realized value pricing servers driven features didn t need like graphics chips RAID cards The features useful regular customers completely irrelevant companies going sling servers run ning stitch Windows Hardware vendors building general purpose servers products appeal greatest number customers That s reasonable convention product design like scientific computing users commercial computing users require general purpose servers construct purpose built supercomputing platforms After learning buy build stripped servers companies quickly moved optimizing entire notion data center critical platform servers software It inevitable physical plant deconstructed optimized Since absolutely optimized global scale including computing plant data centers redesigned resized relocated Sometimes entire data centers deleted replaced container servers needs power water cooling network drop Today s data centers found stack intermodal containers like ocean freighters located lots abandoned RV parks Converge Conventionally Cloud computing evolved need handle millions free email users cheaply Big data evolved need solve emerg ing set supercomputing problems Grids clouds called future simply platforms high effi ciency scale Although clouds primarily operational aspects computing plant learning operate supercomputer like operating traditional IT platform Managing platforms Chapter Cloudy Chance Meatballs When Clouds Meet Big Data global scale extremely disruptive enterprise s notion means operate computers And time care feeding thousands supercomputers eventually lead change way computers operate Converge Conventionally CHAPTER Haystacks Headgames The Demise Counting So far I discussing big data differs previous methods computing provides benefits creates dis ruptions Even early stage safe predict big data multibillion dollar analytics BI business pos sibly subsume entire existing commercial ecosystem During process disrupted economics behavior understanding analyzes touches use model biochemistry personality disorders agencies know color underwear Big data going lay groundwork initiate set larger changes economics science computing But future contain elements past mainframes tape disks This chapter going trip future imagine post big data world look like The future require process zettabytes yottabytes data million node clusters In world individual haystacks thousands times size largest Hadoop clusters built dec ade We going discover end computing look like precisely end counting The electronic computers calculators steroids calculators When calculate pro grammed machinery fed data counting Early computers solved mathematical equations missile tra jectory solve equations simple math Solving equation way theoretical physicist human brains solve equations computers don t work like brains There attempts building computers mimic way brains solve equations engineering constraints practical build hyperactive calculator solves equations brute force ignorance Modern processors operate brute force e clock speed ignorance simple electronics add sub tract multiply divide clock tick On good day billion second If processors cores fed data fast hint t billion calculations second The software algebraic method possible runs crazy counting machines Lucky lot accomplished brute force igno rance Scientific computing clusters solve problems immense constructed thousands multi core processors The NOAA weather forecasting supercomputer good example despite immense computing power weather forecasters long supercomputer hundreds thousands times powerful Hadoop supercomputers follow architectural footsteps powerhouse modeling machines instead predicting hurricanes commercial supercomputers searching hay stacks data patterns They looking bought Prada shoes Facebook friend bought pair In order Facebook deliver targeted consumers advertisers Hadoop supercomputers break data friends information haystacks thousands piles analyze sin gle piece straw looking connections patterns shop ping habits social behavior Early experiences data pattern analysis began reveal connec tions started answer questions previously asked From electoral campaigns insurance providers restau rant chains discovered new questions ask big data Chapter Haystacks Headgames For computing platforms built brute force ignorance suddenly lot ignorant Traditional supercom puters calculators Hadoop supercomputers pat tern explorers Computers aren t supposed able predict social behavior Another aspect computing 21st century rapid relentless accumulation data big data leading wave tsunami As network connection speeds consumers increase data transforms basic text logs graphics eventually HD video 3D With percent human brain dedicated visual pro cessing web continue high resolution visual experience disrupting means broadcast enter tain fastest way transmit information human example s faster tell time analog watch read digits Text consumes little space small book fit old floppy disk I find visual information requires mil lions megabytes Processing visual data eventually teaching computers images starts hint computational prob lems ahead In short years Hadoop gone looking strings characters macro blocks video transitions tool discovery tool seeing understanding processing Lose Haystack Big data tools continue useful years usher era Super Sized Data Advances conventional computing technology extend life clusters Process ors early clusters expensive consumed lot power designed run software era In commercial supercomputing analysis limited fast data pulled processor quickly scanned patterns discarded room chunk Processors designed era overheated underemployed memory controllers Lose Haystack The generation extremely low power cheap processors optimized discovering calculating exabytes data possible build node clusters A cluster size push frontiers discovered world exabytes high resolution visual data needs seen superior scalability famous scientific clus ters Hadoop We need stop look ing needle haystack looking piece straw need stop counting The current approach computing based ideas John Von Neumann generally credited way computers work today In Hadoop cluster piece data inspected pieces remain small node ARM clus ters able extend shelf life current inspecting clus ters If piece hay tweet zip code pieces small If piece hay hours motion HD video com puting problem inspecting starts reach massive clusters When cluster data instead inspecting need create scalable strategies Mind Another Gap The human brain long recognized unique form supercomputer In crude sense billion node cluster consists high resolution analog logic combined digital FM transmission fabric nodes A neuron fires chemical signals accumulated synaptic gaps The neuron held state waiting molecule arrive fire When molecule neurotransmitter dumped synaptic gaps neuron finally fires Your billion node supercomputer neurons trigger resolution sensitive single molecule hun dreds trillions molecules brain Some neuro chemicals come things ingest RNA cells responsible creating amino acids neurotransmitters Chapter Haystacks Headgames Trying solve problems like people Alzheimer s reach Alzheimer s autoimmune disease son s brain chemistry attacks wiring neurons This disease destroys fabric destroys person Researchers believe neural cell sophisticated chemical computer right Many brain chemicals come RNA expression come cafe latte All genetic environmental chemical processes band charming self Combined sheer number neurons interconnec tion problem modeling human brain accuracy answer questions people develop schizophrenia fall love feel productive drinking coffee requires brain s biochemistry simulated However computing tech nology needs advance far largest conventional supercomputers construct Like weather forecasters neurologists computing power The exponential growth data scope problems need attention met exponential advancement science counting things In future problems modest million node Hadoop cluster tackle modeling human cortex won t stop counting things So steps computing The future involve parlor tricks found Hadoop clusters need steal tricks supercomputer noodle Believing Is Seeing My personal interest neuroscience started I born Sturge Weber syndrome rare developmental disorder Nobody knows causes SWS s good candidate sim ulate brain cluster built It is common symp tom port wine stain face The birthmark caused overabundance capillaries surface skin The instructions creating right number capillaries gets messed development If foul occurs Believing Is Seeing outer branches fifth cranial nerve result port wine stain If mistake occurs line closer cortex brain ends port wine stain sorts syndrome lethal Some newborns cortical version SWS seizures right birth sections cortex removed hemispherectomy order stop seizures It testament extraordinary elas ticity cortex relentless built survive patients survive operation recover thrive My SWS affected retina right eye The human eye complex internal network capillaries nourish photorecep tors These vessels sit retina flashes blood vessels doctor shines light My port wine stain wasn t cortex face retina My right eye wasn t healthy I eventually lost sight I Adolescence tough time peo ple I relearn simple things like hit baseball walk crowded hallway I later understand neuroscience cognitive processes taking place turned I spent wasted youth rewiring cortex Simple things like walking hallway supercom puting problem solve As kid I good baseball player fell hitting slump I monocular As I slowly relearned hit point easier I stop ped seeing eye started seeing muscles body position Coaches spatial intelligence encourage players develop skill sports including hitting pitching My brain spatially intelligent walk hallway In order locate pitch strike zone inch precise repeated location human body space required In order kind precision pitchers usually born natural spatial intelligence I crude version intelligence near spatial genius Mariano Rivera The change brain greater reliance right hemisphere responsible spatial processing This Chapter Haystacks Headgames hemisphere lot spatial work fed eye s worth data The brain stereo device eye half data went missing chunk lost going stereo mono vision In order I imagine model 3D world Over time brain learned synthesize dimensional world limited dimensional data I born left handed I left eyed I right brained seeing thinking A good example brains spatially model world occurs I drive night rain I able fully exploit spatial trick pull I battle low light data deep shadows contrast flare reflections visual noise wet road surfa ces All obstacles result little roadway telemetry com ing windshield I t possibly goes I imagine safely driving freeway look like look aberra tions visual field use visual information correct internal model I spatial supercomputer survive humans stereo vision spatial supercomputers I need extra plugins For cliche I ll believe I I ll I believe Unlike supercomputers human brains examine piece hay developed strategies like associative memory habituated learning retina single reduce data orders magnitude Brains look things interesting The future computing going stop sifting haystacks means saying goodbye Dr von Neumann Spatial Intelligence One drawbacks studying brain computer centric perspective pretending brain works like computer deceptively convenient Although neuroscience recent excellent progress fairly young science To fair neurologists decipher billion node supercomputer billion node supercomputers Spatial Intelligence clusters exactly version operating system shortage bugs The generation elec tronic supercomputers need adapt strategies central nervous system perfected live QA millions years The CNS easily distracted lazy probably fan big data ruthless ignoring data longer interesting It interrupt driven gets quickly bored static data An experiment closed eye gentle pressure open eyeball This temporarily stops involun tary jitter motion eye positioning muscles causes vis ual field slowly fade black The eye muscles constantly need attention photore ceptors world fades away The neural technique ignoring data called habituation Data changing longer novel Because CNS designed easily distracted balance habituation novelty disrupted disor ders like ADD result The eye million photoreceptors million cables running visual cortex reduc tion data jittery retinas receive push upstream immense Retinas spatially encode infor mation like width hallway navigated Seeing retinas makes encoding extremely precise precision case calculated cortex If I m tired sick tipsy I start seeing mistakes By performing processing healthy retinas visual cor tex free concentrate looking familiar crow ded hallway The human brain recognize familiar faces speeds exceeding threshold human perception The CNS processes encodes information source need bring data brain There lot computing happening periphery differentiates brain architecture conventional centralized counting architectures comput ers The human brain ridiculously powerful spatial supercomputer driven crafty lazy easily distracted nervous system Recent FMRI studies humans playing listening music reveals large swaths brain lighted like Christmas tree Chapter Haystacks Headgames don t know music important humans know brain music happy supercomputer Put high performance athletes FMRI watch happy computers work Both big data eventually emerging form spatial supercomputing simultaneously George Orwell s worst nightmare Oliver Sacks dream come true Associative Supercomputing The ability recognize mother s face crowd blink eye depends heavily mother s face remem bered Associative memory brain critical speed facial recognition engine I t drive freeway rain night I t imagine safe version look like I compare crude incomplete image coming retina That pattern stored complex spatial memory consisting road signs landmarks This imagi nation strategy successful I t remember stretch road The brain good retrieving stretch road Spatial memories come pieces potentially useful information A stretch road associated mem ories short ramps habit producing unexpec ted data form accelerating vehicles Unexpected situations need minimized quickly learning new situation impaired operating conditions produces errors perception disruptive possibility fender bender Associative memory indexing easy brain diffi cult current computing technology Spatial supercomputers difficult build early versions rely heavily massive classic supercomputers New approaches hardware software design required implement associative computer recognize faces milliseconds I suspect billion node cluster probably won t small cantaloupe Back Future A Hadoop cluster doesn t crunching big data Like HPC clusters Hadoop works busts problem Associative Supercomputing thousand pieces works parallel With big data sets pretty way fly small datasets things like Hadoop equally effective getting work parallel time Our retinas like tiny Hadoop clusters If Hadoop cluster tiny board hundreds processors TB mem ory installed pair sunglasses Brute force linear parallelism remain useful strategy figuring haystacks worth searching threshold computing As hard I tried dispense advances counting years old form computing won t away quickly quietly The new form computing counting hybrid classic techniques combined crafty ways processing information similar ways brains work Big data going data going discovery insight derived having improved workflows It intuition results brain visu ally integrating information quickly workflow unconscious Whatever future computing certainly starts change Chapter Haystacks Headgames Acknowledgments I appreciate authoritative feedback constructive criticism friends colleagues I began writing book million years ago Ed Gasiorowski Bruce Nel son Doug Rady Rikin Shah I express gratitude Douglas Paris White translated non linear thoughts opinions coherent form diminishing voice Thanks Cynthia Georgia suffering writing process Finally book dedicated memory friend men tor David L Willingham Acknowledgments About Author Jeffrey Needham founder CEO Fabrix Analytix Inc Silicon Valley consulting firm Informed decades experi ence hardware software engineering fre quent writer speaker topics database performance scalability platform engineering Today Jeffrey focuses dis tributed analytic platforms involve taking big data data center going downrange field film sets bring economics processing power Hadoop users produce data consume information Clients partners appreciate innovative strategies evaluating implementing big data initiatives He lives family Santa Cruz mountains Cover Strata Copyright Table Contents Chapter The Wall Water And Then There Was One Commercial Supercomputing Comes Age A Stitch Time The Yellow Elephant Room Scaling Yahoovians Supercomputers Are Platforms Big Data Big Bang Endless Possibilities Chapter Big Data The Ultimate Computing Platform Introduction Platforms Come Fly Me Computing Platforms The End Era Back Future High Divers Fire Fighters I ll Take Silos Alex Engineering Big Data Platforms The Art Craft Platform Engineering Global Scale KISS Me Kate The Response Time Continuum Perpetual Prototyping Epic Fail Insurance Optimize Absolutely Everything Mind Gap Chapter Organizations The Other Platform From Myelin Metal Silos Industrial Grain Elevators Platforms Silos Panic Now Fear Loathing Risky Business Probability Outcome Quantitative Qualities The Soft Platform Chapter The Reservoir Data The Actual Internet Best Inbred Drowning Not Waving Spinning Rust A Spectrum Perishability Enclosed Water Towers The Big Data Water District The HDFS Reservoir Third Eye Blind Spectrum Analytic Visibility The Cost Flexibility Chapter Cloudy Chance Meatballs When Clouds Meet Big Data The Big Tease Scalability Motherboards Apple Pie Being Nothingness Parity Is Farmers Google Reverse Isolated Thunderstorms Private Cloud Myths The Network Is Backplane The Orientation Clouds My Other Data Center Is RV Park Converge Conventionally Chapter Haystacks Headgames The Demise Counting Lose Haystack Mind Another Gap Believing Is Seeing Spatial Intelligence Associative Supercomputing Back Future Acknowledgments About Author\n",
            "2 []\n",
            "2 real world active learning Make Data Work strataconf com Presented O Reilly Cloudera Strata Hadoop World cutting edge data science new business fundamentals intersect merge n Learn business applications data technologies n Develop new skills trainings depth tutorials n Connect international community thousands work data Job http strataconf com cmp pd data confreg lp na_free_report_ad Ted Cuzzillo Real World Active Learning Applications Strategies Human loop Machine Learning LSI Real World Active Learning Ted Cuzzillo Copyright O Reilly Media Inc All rights reserved Printed United States America Published O Reilly Media Inc Gravenstein Highway North Sebastopol CA O Reilly books purchased educational business sales promotional use Online editions available titles http safaribooksonline com For information contact corporate institutional sales department corporate oreilly com Editor Shannon Cutt Production Editor Melanie Yarbrough Copyeditor Amanda Kersey Interior Designer David Futato Cover Designer Karen Montgomery Illustrator Rebecca Demarest Cover Photo Credit Jamie McCaffrey February First Edition Revision History First Edition First Release The O Reilly logo registered trademark O Reilly Media Inc Real World Active Learning cover image related trade dress trademarks O Reilly Media Inc While publisher author good faith efforts ensure information instructions contained work accurate publisher author disclaim responsibility errors omissions including limi tation responsibility damages resulting use reliance work Use information instructions contained work risk If code samples technology work contains describes subject open source licenses intellectual property rights responsi bility ensure use thereof complies licenses rights http safaribooksonline com Table Contents O Reilly Strata Conferenc iii Introduction When Active Learning Works Best Gold Standard Data A Best Practice Method Assessing Labels Managing Crowd Expert level Contributors Machines Humans Work Best Together vii Real World Active Learning Introduction The online world blossomed machine driven riches We don t send letters email We don t look restaurant guide book look OpenTable When computer makes possible goes wrong search solution online We thrive multitude signals available But s signal s noise inaccurate inappropriate simply unhelpful information gets way For example receiving email fend spam scouting new employment receive automated job referrals wildly inap propriate matches filters catch porn confuse medical photos We filter noise point trouble s worth machines algorithms things easier To filter spam mail example machine algorithm set known good known bad emails examples algorithm educated guesses filtering mail Even solid examples algorithms fail block impor tant emails filter useful content because variety problems As ll explore report point algorithms fail precisely s opportunity insert human judgment actively improve algorithm s perfor mance In recent article Wired The Huge Unseen Operation Behind Accuracy Google Maps caught glimpse http www wired com google maps ground truth http www wired com google maps ground truth massive active learning operation management Goo gle Maps During visit Google reporter Greg Miller got scenes look Ground Truth team refines Goo gle Maps machine learning algorithms manual labor The algorithms collect data satellite aerial Google s Street View images extracting data like street numbers speed limits points interest Yet Google algorithms certain point humans need step manually check cor rect data Google takes advantage help citizens different crowdsourcing input Google s Map Maker program contribute data road locations Street View cars t drive Active learning relatively new strategy gives machines guiding hand nudging accuracy algorithms tolerable range perfection In crowdsourcing closely related trend possible Internet humans crowd con tributors labelers workers turkers Amazon Mechanical Turk feedback label content labels fed algorithm short time algorithm improves point results useable Active learning strategy hard deploy hard perfect For practical applications tips turned experts field bring knowledge ve gained projects active learning When Active Learning Works Best The concept active learning simple involves feedback loop human machine eventually tunes machine model The model begins set labeled data uses judge incoming data Human contributors label select sample machine s output work plowed model Humans continue label data model achieves suf ficient accuracy Active learning works best cases s plenty cheap unla beled data tweets news articles images While s abundance content classified cost labeling expensive deciding label key consider ations The trick label data greatest Real World Active Learning https developers google com events io sessions impact model s training data feed classifier appropriate accurately labeled data Real World Example The Spam Filter Imagine spam filter initial work filtering email relies solely machine learning By machine learning achieve accuracy Accuracy improves user corrects machine s output relabeling messages spam vice versa Those relabeled messages feed classifier s train ing data finer tuning future email While method let user label random selection output case email takes lot time lacks effi ciency A effective system use classifier estimates certainty verdict e g spam spam presents user uncertain items When user labels uncertain items labels far effective training classifier randomly selected ones Gradually classifier learns accurately determines spam periodic testing continues improve time Real World Example Matching Business Listings GoDaddy A complex example active learning found GoDaddy Locu team s Get Found service provides businesses central platform managing online presence content including address business hours menus services Because online data riddled inconsistencies e g Joe s Pizza listed C Street Cambridge St lis ted Joe s Italian Get Found provides easy way businesses implement consistent presence web While inconsis tencies Joe s Pizza listed Joe s Italian easily stump algorithm human labeler knows glance listings represent restaurant Adam Marcus direc tor data Locu team notes wide range businesses including restaurants flower shops yoga studios garages rely products Get Found type business listing ser vice To identify listings describing business Locu team allows algorithms automatically match simple cases like Joe s Pizza Joe s Pizzas reaches humans When Active Learning Works Best CrowdFlower challenging cases like Joe s Pizza Joe s Italian This active learning loop humans fill details retrain algorithms perform better future Real World Example Ranking Top Search Results Yahoo Another real world example active learning involves ranking online search results Several years ago Yahoo Lukas Biewald CEO crowdsourcing service provider CrowdFlower wanted improve Yahoo s ranking search results This project involved identifying search results millions Biewald s team realized simplest strategy wasn t necessarily best labeling uniform sample millions results include pages relevant team chose use results training data Even bad outcomes picks misleading sample based algorithms work For instance based results classifier assume machine generated page energy savings repeated thousand times relevant page mentions necessarily case So classifier know results belonged The classifier seen search results deep web included test data So Biewald team addressed labeling feeding uncertain cases model repeti tion process model significantly improved results Where Active Learning Works Best Is crowdsourcing worth trouble expense An experiment referenced Biewald talk active learning Strata Conference February bears dramatic result The task label articles based content identifying cov ered baseball hockey Figure shows efficiency classifi ers classifier represented dotted line worked randomly selected labels generated active learning achieves accuracy The classifier represented solid line worked labels generated active learning achieved accuracy half Real World Active Learning labels Biewald points rise efficiency shown Fig ure rising end showing s demand labels Figure Comparing accuracy selection methods dotted line represents randomly selected data generated active learning solid line represents data generated active learning Settles Basic Principles Labeling Data Imagine new email classifier pass small batch data found email classified spam valid Figure shows red dots representing spam green dots representing valid email The diagonal line represents division spam indicates border verdict In figure dots close center line indicate instances machine certain judgment When Active Learning Works Best Figure The colored dots near center line represent judgments machine certain At point key consideration dots case spam email labeled order maxi mum impact classifier According Lukas Biewald CrowdFlower basic principles labeling data Bias uncertainty Labels effect classifier applied instances machine uncertain For example spam email classifier confidently toss email Viagra subject line s confident longtime correspondent uses word The machine s certain judgments likely based content model knows little In instan ces email close known good sample somewhat close known bad sample machine certain instances abundance training data verdict clear You ll biggest impact labeling data gives classifier confidence Real World Active Learning labeling data merely affirms knows Bias ensemble disagreement A popular strategy active learning use multiple methods classification Using multiple methods opportunity improve classi fier allows learn instances results different methods disagree For example spam classi fier label email words Nigerian prince spam data second classifier indicate Nigerian prince actually long term email correspondent helps classifier judge correctly message valid email Bias labels likely influence classi fier Classifiers generally uncertain label data random unusual items appear It helps label items likely influence classifier label data s similar labeled data For instance Biewald s team Yahoo set improve search engine s ranking results algorithm showed odd results It confused included web pages completely irrelevant The team showed classifier labeled data types irrelevant pages confusing produced dramatically better results Bias denser regions training data The selection training data corrected areas data vol ume greatest This challenge brought previously mentioned principles usually result bias outliers For example labeling data algo rithm uncertain skews training sparse data s problem useful training occurs data density highest Beyond Basics For greater accuracy slightly advanced strategies applied When Active Learning Works Best Active cleaning Look training data largest error The data point far norm Figure far influential data model If data correctly labeled teach model outliers If s incorrectly labeled common occurrence taken Active cleaning hand curation The attention given data goes model better classifier work Look edge cases label hand clas sifier training data Figure The red triangle represents data correct labeling good effect accuracy classification model Lukas Biewald Gold Standard Data A Best Practice Method Assessing Labels Patrick Philips crowdsourcing expert data scientist Euclid Analytics describes best practice method active learning mulating gold standard data Before crowd contributors sees data job Philips spends hours scoring small subset data hand He adds gold standard data extracted contributor labeled data strong agreement labels contributors Creating managing set gold standard data e g exam Real World Active Learning ples class data provides standard judging labels come contributors ways First s filter worker s contributions automat ically compared gold standard data measure standing ability trustworthiness job Second gold standard data allows ongoing monitoring provides means train retrain workers offer corrections improve performance Third gold standard data allows score worker s accuracy automatically exclude work falls certain percentage accuracy addition provides opportunity discover problems data Elements Gold Standard Data Gold standard data standard data application measured The Benefits Setting gold standard data gives overview data helps decide labels need It helps avoid designing unhelpful bad labels result severe mislabeling problems later Your early work sub set gold standard data save time money later Tips Start small subset data examples class Use gold standard data measure performance contributor know retrain workers When contributor s score falls accuracy exclude work retrain Continually review gold standard data ensure s accurate useful possible maintains purpose Managing Crowd The crowd solves problem definite usefulness active learning flip Humans wrong answers explains Adam Marcus GoDaddy Mislabeled items result boredom resentment Managing Crowd Also questions unintentionally misleading contributors raced little attention questions answers Whatever because wrong answers badly skew training data hours correct One simple solution explains Marcus ask questions times Redundant questioning establishes confidence labeling An item s labeled certain way differ ent contributors far likely correctly classified s labeled contributor A complex beneficial solution creation worker hierarchies A hierarchy allows simple redundant label ing sends items low certainty ladder trusted workers Hierarchies rely long term relationships contribu tors To enable hierarchies organizations recruit com panies oDesk Elance online marketplaces plentiful supply customer rated candidates As workers known trusted given work asked review workers They receive recognition bonuses interesting tasks man age projects We reviewers running jobs way interesting work hired says Marcus These incentives contributors clear sense upward mobility Contributors work falters hand given work The weaker performance scrutiny receive work volume incrementally reduced The hierarchy system improves training At GoDaddy s Locu team new worker recruited oDesk agency week training practice work cleaning classifier s output trusted worker review new worker According Marcus weeks new recruit s work improves A Challenge Overconfident Contributors Redundant questioning gold standard data methods helping address common problem identified crowdsourcing expert Patrick Philips overconfidence contributors Confi dence bias phenomenon known psychologists sys tematic overconfidence individuals ability complete objective tasks accurately Real World Active Learning In experiment Philips described blog post Confidence Bias Evidence Crowdsourcing individuals asked answer set standardized verbal math related ques tions identify confident answer The difference individual s average confidence actual performance estimate confidence bias Of people answered questions overestimated abilities Philips found confidence bias rises person s level edu cation age number questions answer accurately In experiment US contributors accurate slightly biased average Individuals India average accuracy higher confidence In looking gender Philips found women accurate biased men More Tips Managing Contributors Pick problems carefully Think problem trying solve structure way makes easy meaningful feedback Pick solvable problem team try Accord ing Philips If t team s probably solvable problem If find seemingly unsolvable problem consider parallel problem solved easily Make sure task clearly defined Whatever want labelers test team If team trouble labelers certainly gives chance sure task clearly defined Use objective labels reasonable people agree In Phi lips previous role LinkedIn company set classify content newsfeed having crowdsource workers label content handful descriptors exciting exhilarating insightful interesting The team sent articles labeling task easy labeled data came It mess says Philips No agreed internally labels meant sending task couldn t agree results accurate came Managing Crowd http www crowdflower com blog confidence bias evidence crowdsourcing Results improved team switched objec tive labels tier selection accounted overall quality based coherence spelling grammar second selection indicated general content nonfiction fic tion op ed When Skip Crowd You discover need crowd answer right In project LinkedIn Philips team wanted refer people job postings appropriate level experience The team hoped crowdsource workers classify members categories individual contributor man ager executive manager Though seemingly straightforward task proved difficult For starters job titles vary wildly companies title size company impacts role example vice president Google belong seniority category vice president startup Other indicative data salary wasn t available team tried proxies They looked number years gradu ation useful Other proxies included endorsements network seniority immediate connec tions maps profiles members viewed Eventually team LinkedIn found solution based data LinkedIn members write recommendations explicitly indicate relationship person peer manager direct report With help millions LinkedIn recommenda tions team developed system rank employees seniority company Crowdsourcing great tool s challenges says Philips Definitely look data need Expert level Contributors In cases active learning requires expert level knowledge educated judgment recruitment management labelers complex This occurs task graduates Real World Active Learning simple accurate assessments spam human face tasks expert crowd perform In addition finding expert crowd challenge find expert labelers labels wrong ran dom non expert know Only specialists distinguish example American tree sparrow white crowned sparrow Panos Ipeirotis leading researcher crowdsourcing associate professor New York University recalled instance asked contributors Apollo astronaut Neil Armstrong s wife The choices included Apollo Gemini Laika None I know Only options likely human Laika actually dog sent space Soviet Union answer chosen aspiring experts In cases contribu tors choosing answer plausible says Ipeirotis want convey information don t want admit don t know Ipeirotis found retrospect replacing I know Skip proved better choice What complicates matter plausible wrong answer t easily detected machine algorithm If people plausible sounding answer example algorithm confident based inaccurate data resulting bad classification s reinforced workers collective agreement In short best labelers admit don t know answer How Find Experts For tasks require expert knowledge usual crowdsource marketplaces offer little support challenge usually supply contributors specialized knowledge Ornithologists historians fluent speakers languages Swahili Sicilian Southern Tujia example recruited differently One promising method expert recruitment Quizz described research paper Quizz Targeted Crowdsourcing Billion Potential Users Panagiotis G Ipeirotis Evgeniy Gabrilo Expert level Contributors https www quizz list http www ipeirotis com wp content uploads fp267 ipeirotis pdf http www ipeirotis com wp content uploads fp267 ipeirotis pdf vich The authors found best way find subject matter experts lure demonstrating knowledge Ipeirotis Gabrilovich began experiment quizzes placed ads popular websites Each quiz challenged passersby question example question What symptom morgellons Those medical knowl edge know morgellons involves delusions having things crawling skin Each quiz question offered plausible choices shown Figure offered answer learned instantly correct Figure A sample Quizz question In background algorithm created Ipeirotis Gabri lovich kept score judged expertise respondent Participants judged sufficiently knowledgeable invited Quizz continued measure total contribution quality results In addition scoring participants Quizz algorithm advertising targeting capabilities score websites ads Real World Active Learning appeared Sites produced qualified candidates dropped way continually optimize results The algorithm recorded origin sites gave good answers began recruiting sites heavily For example recruiting algorithm quickly learned consumer oriented medi cal websites Mayo Clinic Healthline produced qualified labelers medical knowledge ads medical websites professional audience manage attract con tributors sufficient willingness participate Participants clicked ad answered quiz questions constituted conversion tallied algorithm At time Ipeirotis Gabrilovich wrote paper Quizz application began conversion rate time rose conversion rate simply giving feedback advertising targeting algorithm Managing Expert level Contributors A key consideration managing experts contributor According Ipeirotis trick balance types questions type calibration estimates con tributor s knowledge type collection collects knowledge Balancing types questions allows sustain stream collected knowledge long possible explore person s potential The optimal balance types questions calibration ver sus collection depends contributor s recent behav ior depends expected behavior based users For example user shows signs dropping likely steered proven survival mix con tributors motivated mainly contribution good informa tion survival mix let us questions likely answer correctly followed prompt acknowledgement work Payment factor consider managing expert level con tributors In research Ipeirotis Gabrilovich found paid workers cost produced poorer quality data knowledgeable unpaid Ipeirotis Gabrilovich describe experiment selection contributors paid piecemeal rates bonuses Expert level Contributors based scores group dropped lower rate selec tion unpaid workers However paid workers stay ing submitting lower quality answers unpaid Interestingly offering payment linked high quality answers payment simply sustained workers presuma bly cases unpaid workers lacking satisfaction offer ing high quality answers given Recruiting Expert level Contributors Recruiting contributors expert knowledge different recruiting everyday crowdsource workers For tips turned researchers Panagiotis G Ipeirotis Evgeniy Gabrilovich The Essential Strategies The best approach recruiting experts encourage lend expertise The best contributors unpaid Unlike everyday crowd source workers Ipeirotis Gabrilovich found expert contributors produce information volume higher quality weren t paid The best motivator contribution good information By means recruiting experts track success monitor quality quantity recruitments modify efforts accordingly A Real World Example Expert Stylists Machine Learning Expert contributors identify birds medical symptoms In application customers actually trust experts trust Stitch Fix online sonal styling shopping service women relies expert contributors machine learning present customers styles based personal data The process Stitch Fix begins basic model estimate customers like based stated preferences style budget Then model evolves based information actual purchases Notably customer s model disrupted For Real World Active Learning example model find customer gave size actually purchases items size clothes describes bohemian chic style actually people preppy buy clothes reveal higher budget gave Handling types disruptions matching stated actual preferences biggest challenges Stitch Fix says Chief Algorithms Analytics Officer Eric Colson In addition lack industry standard clothing sizes adds problem example size store size Custom ers bad data aspirational size e anticipates weight loss true size They misunderstand industry terms confusing size fit example Aside customer based data second set data describes item clothing fine grain detail Stitch Fix s expert merchandis ers evaluate new piece clothing encode attributes subjective objective structured data color fit style material pattern silhouette brand price trendiness These attributes compared customer profile machine produces recommendations based model But time comes recommend merchandise cus tomer machine t possibly final This Stitch Fix stylists step Stitch Fix hands final selection rec ommendations roughly human stylists serves set customers Stylists assess unstructured data images videos merchandise available customer comments e g I need clothes big meeting work They reach outside machine s recommen dations use judgment final selections pieces customer Before shipment goes stylist scrutinizes piece look explain selections customer According Colson occasional smart risk built algorithm Stitch Fix deliberately injects randomness add value stay completely safe narrow range customer preferences truncate possibilities On 10th 11th shipment says Colson s need start mixing A school teacher dresses conservatively week example probably conservative clothing What s going Expert level Contributors create meaningful relationship asks Colson It journey Stylists work Internet access paid hourly report intangible benefits satis faction happy customers Machines Humans Work Best Together Futurists dreamed machines guided unseen autopilot Little visionaries know autopilot help crowd Active learning machines hand hand humans success far hints huge potential If duo choose clothing thwart email spammers classify subtly different images Real World Active Learning About Author Ted Cuzzillo covered technology journalist industry analyst years Topics included telecommunications computer networking environmental technology Most recently regular contributor business intelligence media include TDWI BI This Week Information Management Smart Data Collective popular blog Data doodle Current interests include practice storytelling col laboration technology supports Cover O Reilly Strata Conference Table Contents Real World Active Learning Introduction When Active Learning Works Best Real World Example The Spam Filter Real World Example Matching Business Listings GoDaddy Real World Example Ranking Top Search Results Yahoo Where Active Learning Works Best Basic Principles Labeling Data Beyond Basics Gold Standard Data A Best Practice Method Assessing Labels Managing Crowd A Challenge Overconfident Contributors More Tips Managing Contributors When Skip Crowd Expert level Contributors How Find Experts Managing Expert level Contributors A Real World Example Expert Stylists Machine Learning Machines Humans Work Best Together\n",
            "3 []\n",
            "3 A Course Machine Learning A Course Machine Learning Hal Daume III Copyright Hal Daume III Published TODO http hal3 courseml TODO First printing September http hal3 courseml For students teachers Often TABLE OF CONTENTS About Book Decision Trees Geometry Nearest Neighbors The Perceptron Practical Issues Beyond Binary Classification Linear Models Probabilistic Modeling Neural Networks Kernel Methods Learning Theory Ensemble Methods Efficient Learning Unsupervised Learning Expectation Maximization Semi Supervised Learning Graphical Models Online Learning Structured Learning Tasks Bayesian Learning Code Datasets Notation Bibliography Index ABOUT THIS BOOK Machine learning broad fascinating field It called attractive fields work in1 It applications incredibly wide variety application areas medicine advertising military pedestrian It is importance likely grow areas turn way dealing massive amounts data available How Use Book Why Another Textbook The purpose book provide gentle pedagogically orga nized introduction field This contrast existing ma chine learning texts tend organize things topically pedagogically exception Mitchell s book2 unfortu Mitchell nately getting outdated This makes sense researchers field sense learners A second goal book provide view machine learning focuses ideas models math It possible advisable avoid math But math aid understanding hinder Finally book attempts minimal dependencies fairly easily pick choose chapters read When dependencies exist listed start chapter list dependencies end chapter The audience book knows differential calcu lus discrete math program reasonably A little bit linear algebra probability hurt An undergraduate fourth fifth semester fully capable understand ing material However suitable year graduate students slightly faster pace Organization Auxilary Material There associated web page http hal3 courseml contains online copy book associated code data It contains errate For instructors ability solutions manual This book suitable single semester undergraduate course graduate course semester course supple mented readings decided instructor Here suggested course plans courses year long course obtained simply covering entire book Acknowledgements http hal3 courseml DECISION TREES Dependencies None At basic level machine learning predicting fu ture based past For instance wish predict user Alice like movie hasn t seen based ratings movies seen This means making informed guesses unobserved property object based observed properties object The question ll ask mean learn In order develop learning machines know learning actually means determine success failure You ll question answered limited learning setting progressively loosened adapted rest book For concreteness focus simple model learning called decision tree todo VIGNETTE ALICE DECIDES WHICH CLASSES TO TAKE What Does Mean Learn Alice begun taking course machine learning She knows end course expected learned topic A common way gauging learned teacher Bob exam She learning exam But makes reasonable exam If Bob spends entire semester talking machine learning gives Alice exam History Pottery Alice s performance exam representative learning On hand exam asks questions Bob answered exactly lec tures bad test Alice s learning especially s open notes exam What desired Alice observes specific Learning Objectives Explain difference memorization generalization Define inductive bias recog nize role inductive bias learning Take concrete task cast learning problem formal tion input space features output space generating distribution loss function Illustrate regularization trades underfitting overfit ting Evaluate use test data cheating The words printed concepts You experiences Carl Frederick decision trees examples course answer new related questions exam This tests Alice ability generalize Generalization central concept machine learning As running concrete example book use course recommendation system undergraduate computer science students We collection students collection courses Each student taken evaluated subset courses The evaluation simply score terrible awesome The job recommender system predict particular student Alice like particular course Algorithms Given historical data course ratings e past trying predict unseen ratings e future Now unfair system We ask Alice likely enjoy History Pottery course This unfair system idea History Pottery prior experience course On hand ask Alice like Artificial Intelligence took year rated awesome We expect system predict like isn t demonstrating system learned s simply recalling past experience In case expecting system generalize experience unfair In case expecting generalize This general set predicting future based past core machine learning The objects algorithm predictions examples In recommender sys tem setting example particular Student Course pair Alice Algorithms The desired prediction rating Alice Algorithms Figure The general supervised ap proach machine learning learning algorithm reads training data computes learned function f This function automatically label future text examples To concrete Figure shows general framework induction We given training data algorithm ex pected learn This training data examples Alice observes machine learning course historical ratings data recommender system Based training data learning algorithm induces function f map new example cor responding prediction For example function guess f Alice Machine Learning high training data said Alice liked Artificial Intelligence We want algorithm able lots predictions refer collection examples evaluate algorithm test set The test set closely guarded secret final exam learning algorithm tested If algorithm gets peek ahead time s going cheat better Why bad learning algo rithm gets peek test data course machine learning The goal inductive machine learning training data use induce function f This function f evalu ated test data The machine learning algorithm succeeded performance test data high Some Canonical Learning Problems There large number typical inductive learning problems The primary difference type thing trying predict Here examples Regression trying predict real value For instance predict value stock tomorrow given past performance Or predict Alice s score machine learning final exam based homework scores Binary Classification trying predict simple yes response For instance predict Alice enjoy course Or predict user review newest Apple product positive negative product Multiclass Classification trying example num ber classes For instance predict news story entertainment sports politics religion etc Or predict CS course Systems Theory AI Other Ranking trying set objects order relevance For stance predicting order web pages response user query Or predict Alice s ranked preferences courses hasn t taken For types canon ical machine learning problems come concrete examples The reason convenient break machine learning prob lems type object trying predict measuring error Recall goal build system good predictions This begs question mean prediction good The different types learning problems differ define goodness For instance regres sion predicting stock price better The hold multi class classification There accidentally predicting entertainment instead sports better worse predicting politics The Decision Tree Model Learning The decision tree classic natural model learning It closely related fundamental computer science notion di vide conquer Although decision trees applied decision trees learning problems begin simplest case binary clas sification Suppose goal predict unknown user enjoy unknown course You simply answer yes In order guess allowed ask binary ques tions user course consideration For example You Is course consideration Systems Me Yes You Has student taken Systems courses Me Yes You Has student liked previous Systems courses Me No You I predict student like course The goal learning figure questions ask order ask answer predict asked questions Figure A decision tree course recommender system text dialog drawn The decision tree called write set ques tions guesses tree format Figure In figure questions written internal tree nodes rectangles guesses written leaves ovals Each non terminal node children left child specifies swer question right child specifies yes In order learn I training data This data consists set user course examples paired correct answer examples given user enjoy given course From construct questions For concreteness small data set Table Appendix book This training data consists course rating examples course ratings answers questions ask pair We interpret ratings liked ratings hated In follows refer questions ask features responses questions feature values The rating called label An example set feature values And training data set examples paired labels There lot logically possible trees build small number features number millions It computationally infeasible consider try choose best Instead build decision tree greedily We begin asking If I ask question question I ask Figure A histogram labels entire data set b e examples data set value features You want find feature useful helping guess student enjoy course A useful way think A colleague related story getting year old nephew guess number His nephew s questions Is bigger YES Is YES Does NO Is NO It took questions sufficient At nephew hadn t figured divide conquer http blog computationalcomplexity org getting year old interested html http blog computationalcomplexity org getting year old interested html http blog computationalcomplexity org getting year old interested html http blog computationalcomplexity org getting year old interested html http blog computationalcomplexity org getting year old interested html http blog computationalcomplexity org getting year old interested html course machine learning look histogram labels feature This shown features Figure Each histogram shows frequency like hate labels possible value associated feature From figure asking feature useful value s hard guess label similarly answer yes On hand asking second feature useful value pretty confident student hate course answer yes pretty confident student like course More formally consider feature turn You consider feature Is System s course This feature possible value yes Some training examples answer let s NO set Some training examples answer yes let s YES set For set NO YES build histogram labels This second histogram Figure Now suppose ask question random example observe value Further suppose immediately guess label example You guess like s preva lent label NO set actually s label NO set Alternatively recieve answer yes guess hate prevalent YES set So single feature know guess Now ask I guess train ing data I In particular ex amples I classify correctly In NO set guessed like classify correctly In YES set guessed hate classify correctly So overall classify correctly Thus ll score Is System s course question How training examples classify correctly features Figure You repeat computation available features compute scores When choose feature consider want choose highest score But let us choose feature ask This feature goes root decision tree How choose subsequent features This notion divide conquer comes You ve decided feature Is Systems course You partition data parts NO YES The NO subset data value feature YES half rest This divide step decision trees Algorithm DecisionTreeTrain data remaining features guess frequent answer data default answer data labels data unambiguous return Leaf guess base case need split remaining features return Leaf guess base case split need query features f remaining features NO subset data f YES subset data f yes score f majority vote answers NO majority vote answers YES accuracy queried f end f feature maximal score f NO subset data f YES subset data f yes left DecisionTreeTrain NO remaining features f right DecisionTreeTrain YES remaining features f return Node f left right end Algorithm DecisionTreeTest tree test point tree form Leaf guess return guess tree form Node f left right f yes test point return DecisionTreeTest left test point return DecisionTreeTest right test point end end The conquer step recurse run routine choosing feature highest score NO set left half tree separately YES set right half tree At point useless query additional fea tures For instance know Systems course know hate So immediately predict hate asking additional questions Similarly point queried available feature whittled single answer In cases need create leaf node guess prevalent answer current piece training data looking Putting arrive algorithm shown Al gorithm This function DecisionTreeTrain takes argu There nuanced algorithms building decision trees discussed later chapters book They primarily differ compute score funciton course machine learning ments data set unused features It base cases data unambiguous remaining features In case returns Leaf node containing likely guess point Otherwise loops remaining fea tures find highest score It partitions data NO YES split based best feature It constructs left right subtrees recursing In recursive uses partitions data removes selected feature consideration Is Algorithm guaranteed terminate The corresponding prediction algorithm shown Algorithm This function recurses decision tree following edges specified feature values test point When reaches leaf returns guess associated leaf TODO define outlier Formalizing Learning Problem As ve seen issues ac count formalizing notion learning The performance learning algorithm measured unseen test data The way measure performance depend problem trying solve There strong relationship data algorithm sees training time data sees test time In order accomplish let s assume gives loss function arguments The job tell bad system s prediction comparison truth In particu lar y truth y system s prediction y y measure error For canonical tasks discussed use following loss functions Regression squared loss y y y y absolute loss y y y y Binary Classification zero loss y y y y This notation means loss zero prediction correct Multiclass Classification zero loss Why bad idea use zero loss measure perfor mance regression problem Note loss function decide based goals learning decision trees Now defined loss function need consider data training test comes The model use probabilistic model learning Namely prob ability distribution D input output pairs This called data generating distribution If write x input user course pair y output rating D distri bution x y pairs A useful way think D gives high probability reasonable x y pairs low probability unreasonable x y pairs A x y pair unreasonable ways First x unusual input For example x related Intro Java course highly probable x related Geometric Solid Modeling course probable Second y unusual rating paired x For instance Alice AI times remembering took course time Perhaps semesters slightly lower score un likely x Alice AI paired y It important remember making assump tions distribution D looks like For instance assuming looks like Gaussian common distri bution We assuming know D In fact know priori data generating distribution learning problem significantly easier Perhaps hardest thing machine learning don t know D random sample This random sample training data Our learning problem defined quantities Consider following prediction task Given paragraph written course predict paragraph positive negative review course This sentiment analysis prob lem What reasonable loss function How define data generating distribution The loss function captures notion important learn The data generating distribution D defines sort data expect We given access training data random sample input output pairs drawn D Based training data need induce function f maps new inputs x corresponding prediction y The key property f obey measured future examples drawn D Formally s expected loss e D repsect small possible e E x y D y f x x y D x y y f x course machine learning The difficulty minimizing expected loss Eq don t know D All access training data sampled Suppose denote training data set D The training data consists N input output pairs x1 y1 x2 y2 xN yN Given learned function f compute training error e e N N n yn f xn That training error simply average error train ing data Verify calculation write training error E x y D y f x thinking D distribution places probability N example D probabiliy Of course drive e zero simply memorizing train ing data But Alice find memorizing past exams generalize new exam This fundamental difficulty machine learning thing access training error e But thing care minimizing expected error e In order expected error learned function needs generalize training data future data seen So putting formal definition induction machine learning Given loss function ii sample D unknown distribution D compute function f low expected error e D respect Inductive Bias What We Know Before Data Arrives decision trees In book write things like E x y D y f x expected loss Here expectation means average In words saying drew bunch x y pairs indepen dently random D average loss More formally aver age y f x random draws More formally D discrete probability distribution expectation expanded E x y D y f x x y D D x y y f x This exactly weighted average loss x y pairs D weighted probability D x y distribution D In particular D finite discrete distribution instance defined finite data set x1 y1 xN yN puts equal weight example case equal weight means proba bility N E x y D y f x x y D D x y y f x definition expectation N n D xn yn yn f xn D discrete finite N n N yn f xn definition D N N n yn f xn rearranging terms Which exactly average loss dataset In case distribution continuous need replace discrete sum continuous integral space E x y D y f x D x y y f x dxdy This exactly continuous space discrete space The important thing remember equivalent ways think expections The expectation function g weighted average value g weights given underlying probability distribution The expectation function g best guess value g draw single item underlying probability distribution MATH REVIEW EXPECTATED VALUES Figure course machine learning Figure dt bird bird training images Figure dt birdtest bird test images In Figure ll find training data binary classification problem The labels A B exam ples label Below Figure test data These images left unlabeled Go quickly based training data label images Really read I ll wait Most likely produced labelings ABBAAB ABBABA Which solutions right The answer tell based training data If example people come ABBAAB prediction come ABBABA prediction Why Presumably group believes relevant distinction bird non bird second group believes relevant distinc tion fly fly This preference distinction bird non bird fly fly bias different human learners In con text machine learning called inductive bias absense data narrow relevant concept type solutions likely prefer Two thirds people inductive bias favor bird non bird inductive bias favor fly fly It possible correct classification test data BABAAA This corresponds bias background focus Somehow come classification rule Throughout book learn approaches machine learning The decision tree model approach These approaches differ primarily sort inductive bias exhibit Consider variant decision tree learning algorithm In variant allow trees grow pre defined maximum depth d That queried d fea tures query best guess point This variant called shallow decision tree The key question What inductive bias shallow decision trees Roughly bias decisions look ing small number features For instance shallow decision tree good learning function like students like AI courses It bad learning function like student liked odd number past courses like This parity function requires inspect feature prediction The inductive bias decision tree sorts things want learn predict like example like second example decision trees Not Everything Learnable Although machine learning works astonishingly cases important mind magical There reasons machine learning algorithm fail learning task There noise training data Noise occur feature level label level Some features corre spond measurements taken sensors For instance robot use laser range finder compute distance wall However sensor fail return incorrect value In sentiment classification problem typo review course These lead noise feature level There noise label level A student write scathingly negative review course accidentally click wrong button course rating The features available learning simply insufficient For example medical context wish diagnose patient cancer You able collect large data patient gene expressions X rays family histories etc But knowing information exactly impossible judge sure pa tient cancer As contrived example try classify course reviews positive negative But erred downloading data gotten char acters review If rest features able But limited feature set s Some examples single correct answer You building system safe web search removes offen sive web pages search results To build system collect set web pages ask people classify offen sive However person considers offensive completely reasonable person It common consider form label noise Nevertheless designer learning system control problem helpful isolate source difficulty Finally learning fail inductive bias learn ing algorithm far away concept learned In bird non bird data think gotten training examples able tell intended bird non bird classification fly fly classification However I ve talked come background focus classification Even course machine learning training points unusual distinction hard figure In case inductive bias learner simply misaligned target classification learn Note inductive bias source error fundamentally dif ferent sources error In inductive bias case particular learning algorithm cope data Maybe switched different learning algorithm able learn For instance Neptunians evolved care greatly backgrounds focus easy classification learn For sources error issue particular learning algorithm The error fundamental learning problem Underfitting Overfitting As problems useful think extreme cases learning algorithms In particular extreme cases decision trees In extreme tree ask questions We simply immediately prediction In extreme tree That possible question asked branch In tree leaves associated training data For simply choose arbitrarily yes Consider course recommendation data Table Sup pose build decision tree data Such decision tree prediction regardless input allowed ask questions input Since likes hates training data versus decision tree simply predict likes The training error e On hand build decision tree Since row data unique guarantee leaf decision tree examples assigned leaves example rest For leaves corresponding training points decision tree correct prediction Given training error e Of course goal build model gets error training data This easy Our goal model future unseen data How expect models future data The tree likely better worse future data We expect decision trees continue error Life complicated decision tree Certainly given test example identical training examples right thing assuming noise But error This means test point happens identical training points error In practice probably optimistic maybe examples match training example yielding error Convince proof simulation case imbalanced data stance data average positive negative pre dictor guesses randomly positive negative error So case tree ve achieved error case tree ve achieved error This promising One hope better In fact notice simply queried single feature data able low training error wouldn t forced guess randomly Which feature s training error This example illustrates key concepts underfitting overfitting Underfitting opportunity learn didn t A student hasn t studied coming exam underfit exam consequently This tree Overfitting pay attention idiosyncracies training data aren t able generalize Often means model fitting noise supposed fit A student memorizes answers past exam questions understand ing overfit training data Like tree student exam A model overfit underfit expected best future Separation Training Test Data Suppose graduating job working company provides personalized recommendations pottery You implement new algorithms based learned machine learning class learned power generaliza tion All need convince boss good job deserve raise How convince boss fancy learning algo rithms working Based ve talked underfitting overfitting tell boss training error Noise notwithstanding easy training error zero simple database query grep prefer Your boss fall The easiest approach set aside available data course machine learning test data use evaluate performance learning algorithm For instance pottery recommendation service work collected examples pottery ratings You select training data set aside final test data You run learning algorithms training points Only apply learned model test points report test error points boss The hope process test points indicative likely future This analogous estimating support presidential candidate asking small random sample people opinions Statistics specifically concentration bounds Central limit theorem famous example tells sam ple large good representative The split magic s simply fairly established Occasionally people use split instead especially lot data If data dis posal split preferable split The cardinal rule machine learning touch test data Ever If s clear Never touch test data If thing learn book let Do look test data Even Even tiny peek Once test data Yes algorithm hasn t seen But And likely better learner learning algorithm Consciously decisions based seen Once look test data model s performance longer indicative s performance future unseen data This simply future data unseen test data longer Models Parameters Hyperparameters The general approach machine learning captures ex isting learning algorithms modeling approach The idea come formal model data For instance model classification decision student course pair decision tree The choice tree represent model choice We arithmetic circuit polynomial function The model tells sort things learn tells inductive bias For models associated parameters These things use data decide Parameters decision decision trees tree include specific questions asked order asked classification decisions leaves The job decision tree learning algorithm DecisionTreeTrain data figure good set parameters Many learning algorithms additional knobs adjust In cases knobs tuning inductive bias algorithm In case decision tree obvious knob tune maximum depth decision tree That modify DecisionTreeTrain function stops recursing reaches pre defined maximum depth By playing depth knob adjust underfitting tree depth overfitting tree depth Go DecisionTree Train algorithm modify takes maximum depth pa rameter This require adding lines code modifying Such knob called hyperparameter It called parameter controls parameters model The exact definition hyperparameter hard pin s things easier identify define However key identifiers hyperparameters main reason because consternation naively adjusted training data In DecisionTreeTrain machine learning learn ing algorithm essentially trying adjust parameters model minimize training error This suggests idea choosing hyperparameters choose minimize train ing error What wrong suggestion Suppose treat maximum depth hyperparameter tried tune training data To maybe simply build collection decision trees tree0 tree1 tree2 tree100 treed tree maximum depth d We computed training error trees chose ideal maximum depth minimizes training error Which pick The answer pick d Or general pick d large possible Why Because choosing bigger d hurt training data By making d larger simply encouraging overfitting But evaluating training data fitting actually looks like good idea An alternative idea tune maximum depth test data This promising test data peformance want optimize tuning knob test data like good idea That won t accidentally reward overfitting Of course breaks cardinal rule test data touch test data So idea immediately table However test data wasn t magic We simply took examples called training data called course machine learning test data So instead let s following Let s original data points select training data From remainder development data3 remaining Some people validation data held data test data The job development data allow tune hyperparameters The general approach follows Split data training data development data test data For possible setting hyperparameters Train model setting hyperparameters training data b Compute model s error rate development data From collection models choose achieved lowest error rate development data Evaluate model test data estimate future test perfor mance In step choose model trained train ing data best development data Or choose hyperparameter settings best retrain model union training development data Is options obviously better worse Chapter Summary Outlook At point able use decision trees machine learning Someone data You ll split training development test portions Using training development data ll find good value maximum depth trades underfitting overfitting You ll run resulting decision tree model test data estimate likely future You think I read rest book Aside fact machine learning awesome fun field learn s lot left cover In chapters ll learn models different inductive biases decision trees You ll useful way thinking learning geometric view data This guide follows After ll learn solve problems complicated simple binary classification Machine learning people like binary classification lot s simplest non trivial problems work After things diverge ll learn ways think learning formal optimization problem ways speed learning ways learn labeled data little labeled data sorts fun topics decision trees But focus view machine learning ve seen You select model associated induc tive biases You use data find parameters model work training data You use development data avoid fitting overfitting And use test data ll look touch right estimate future model performance Then conquer world Exercises Exercise TODO GEOMETRY AND NEAREST NEIGHBORS Dependencies Chapter You think prediction tasks mapping inputs course reviews outputs course ratings As learned previ ous chapter decomposing input collection features e g words occur review forms useful abstraction learn ing Therefore inputs lists feature values This suggests geometric view data dimen sion feature In view examples points high dimensional space Once think data set collection points high dimen sional space start performing geometric operations data For instance suppose need predict Alice like Algorithms Perhaps try find student similar Alice terms favorite courses Say student Jeremy If Jeremy liked Algorithms guess Alice This example nearest neighbor model learn ing By inspecting model ll completely different set answers key learning questions discovered Chapter From Data Feature Vectors An example collection feature values example instance data Table Appendix To person features meaning One feature count times reviewer wrote excellent course review Another count number exclamation points A tell text underlined review To machine features meaning Only feature values vary examples mean thing machine From perspective think example represented feature vector consisting dimension feature dimenion simply real value Consider review said excellent times excla Learning Objectives Describe data set points high dimensional space Explain curse dimensionality Compute distances points high dimensional space Implement K nearest neighbor model learning Draw decision boundaries Implement K means algorithm clustering Our brains evolved rain find berries getting killed Our brains evolve help grasp large numbers look things thousand dimensions Ronald Graham geometry nearest neighbors mation point underlined text This represented feature vector An identical review happened underlined text feature vector Note imposed convention binary features yes features corresponding feature values respectively This arbitrary choice We wanted But convenient helps interpret feature values When discuss practical issues Chapter reasons good choice Figure A figure showing projections data dimension ways text Top horizontal axis corresponds feature TODO vertical axis corresponds second feature TODO Middle horizonal second feature vertical Bottom horizonal vertical Figure shows data Table views These views constructed considering features time different pairs In cases plusses denote positive examples minuses denote negative examples In cases points fall unique points figures Match example ids Ta ble points Figure The mapping feature values vectors straighforward case real valued features trivial binary features mapped zero It clear categorical features For example goal identify object image tomato blueberry cucumber cockroach want know color Red Blue Green Black One option map Red value Blue value Green value Black value The problem mapping turns unordered set set colors ordered set set In necessarily bad thing But use features measure examples based distances By map ping essentially saying Red Blue similar distance Red Black distance This probably want A solution turn categorical feature dif ferent values Red Blue Green Black binary features IsItRed IsItBlue IsItGreen IsItBlack In gen eral start categorical feature takes V values map V binary indicator features The computer scientist saying actually map log2 V binary features Is good idea With able data set map example feature vector following mapping Real valued features copied directly Binary features false true Categorical features V possible values mapped V binary indicator features course machine learning After mapping think single example vec tor high dimensional feature space If D fea tures expanding categorical features feature vector D components We denote feature vectors x x1 x2 xD xd denotes value dth fea ture x Since vectors real valued components D dimensions belong space RD For D feature vectors points plane like Figure For D dimensional space For D hard visualize You resist temptation think D time things confusing Unfortunately sorts problems encounter ma chine learning D considered low dimensional D medium dimensional D high dimensional Can think problems haps ones mentioned book low dimensional That medium dimensional That high dimensional K Nearest Neighbors The biggest advantage thinking examples vectors high dimensional space allows apply geometric concepts machine learning For instance basic things vector space compute distances In dimensional space distance given In general D dimensional space Euclidean distance vectors b given Eq Figure geometric intuition dimensions d b D d ad bd Figure A figure showing Euclidean distance dimensions Verify d Eq gives result previous computation Figure knn classifyit A figure showing easy NN classification problem test point positive Now access distances examples start thinking means learn Consider Fig ure We collection training data consisting positive examples negative examples There test point marked question mark Your job guess correct label point Most likely decided label test point positive One reason thought believe label example similar label nearby points This example new form inductive bias The nearest neighbor classifier build insight In com parison decision trees algorithm ridiculously simple At training time simply store entire training set At test time test example x To predict label find training ex ample x similar x In particular find training geometry nearest neighbors Algorithm KNN Predict D K x S n N S S d xn x n store distance training example n end S sort S lowest distance objects y k K dist n Sk n kth closest data point y y yn vote according label nth training point end return sign y return y y example x minimizes d x x Since x training example corresponding label y We predict label x y Figure A figure showing easy NN classification problem test point positive NN actually negative point s noisy Despite simplicity nearest neighbor classifier incred ibly effective Some frustratingly effective However particularly prone overfitting label noise Consider data Figure You probably want label test point positive Unfortunately s nearest neighbor happens negative Since nearest neighbor algorithm looks single nearest neighbor consider preponderance evidence point probably actually positive example It un necessary error A solution problem consider single nearest neighbor making classification decision We con sider K nearest neighbors let vote correct class test point If consider nearest neighbors test point Figure positive negative Through voting positive win Why good idea use odd number K The algorithm K nearest neighbor classification given Algorithm Note actually training phase K nearest neighbors In algorithm introduced new conventions The training data denoted D We assume N training examples These examples pairs x1 y1 x2 y2 xN yN Warning confuse xn nth training example xd dth feature example x We use denote list append list Our prediction x called y course machine learning The step algorithm compute distances test point training points lines The data points sorted according distance We apply clever trick summing class labels K nearest neighbors lines sign sum prediction Why sign sum com puted lines majority vote associated training examples The big question course choose K As ve seen K run risk overfitting On hand K large instance K N KNN Predict predict majority class Clearly underfitting So K hyperparameter KNN algorithm allows trade overfitting small value K underfitting large value K Why t simply pick value K best training data In words treat like hy perparameter parameter One aspect inductive bias ve seen KNN assumes nearby points label Another aspect different decision trees features equally important Recall decision trees key question features useful classification The learning algorithm decision tree hinged finding small set good features This thrown away KNN classifiers feature This means data relevant features lots irrelevant features KNN likely poorly Figure A figure ski snow board width mm height cm Figure Classification data ski vs snowboard 2d A related issue KNN feature scale Suppose trying classify object ski snowboard Figure We given features data width height As standard skiing width measured millime ters height measured centimeters Since features actually plot entire training set Figure ski positive class Based data guess KNN classifier Figure Classification data ski vs snowboard 2d width rescaled mm Suppose measurement width com puted millimeters instead centimeters This yields data shown Figure Since width values tiny compar ison height values KNN classifier effectively ignore width values classify purely based height The pre dicted class displayed test point changed feature scaling We discuss feature scaling Chapter For important mind KNN power decide features important geometry nearest neighbors Decision Boundaries The standard way ve thinking learning algo rithms query model Based training data learn I query example guess s label Figure decision boundary 1nn An alternative passive way think learned model ask sort test examples classify positive sort classify negative In Figure set training data The background image colored blue regions classified positive query issued colored red regions classified negative This coloring based nearest neighbor classifier In Figure solid line separating positive regions negative regions This line called decision boundary classifier It line positive land negative land Figure decision boundary knn k Decision boundaries useful ways visualize complex ity learned model Intuitively learned model decision boundary jagged like coastline Norway complex prone overfitting A learned model decision boundary simple like bounary Arizona Utah potentially underfit In Figure deci sion boundaries KNN models K As boundaries simpler simpler K gets bigger Figure decision tree ski vs snowboard Now know decision boundaries natural ask decision boundaries decision trees look like In order answer question bit formal build decision tree real valued features Remember algorithm learned previous chapter implicitly assumed binary feature values The idea allow decision tree ask questions form value feature greater That real valued features decision tree nodes param eterized feature threshold feature An example decision tree classifying skis versus snowboards shown Fig ure Figure decision boundary dt previous figure Now decision tree handle feature vectors talk decision boundaries By example decision boundary decision tree Figure shown Figure In figure space split half according query axis Then depending half space look split axis simply classified Figure good visualization decision boundaries decision trees general Their decision boundaries axis aligned course machine learning cuts The cuts axis aligned nodes query single feature time In case decision tree shallow decision boundary relatively simple What sort data yield simple decision boundary decision tree complex decision boundary nearest neighbor What way K Means Clustering Up point learned supervised learn ing particular binary classification As example use geometric intuitions data going temporarily consider unsupervised learning problem In unsupervised learn ing data consists examples xn contain corre sponding labels Your job sense data provided correct labels The particular notion making sense talk clustering task Figure simple clustering data clusters UL UR BC Consider data shown Figure Since unsupervised learning access labels data points simply drawn black dots Your job split data set clusters That label data point A B C way want For data set s pretty clear You prob ably labeled upper left set points A upper right set points B set points C Or permuted labels But chances clusters The K means clustering algorithm particularly simple effective approach producing clusters data like Fig ure The idea represent cluster s cluster center Given cluster centers simply assign point nearest center Similarly know assignment points clusters compute centers This introduces chicken egg problem If knew clusters compute centers If knew centers compute clusters But don t know Figure iterations k means running previous data set The general computer science answer chicken egg problems iteration We start guess cluster centers Based guess assign data point closest center Given new assignments recompute cluster centers We repeat process clusters stop moving The erations K means algorithm shown Figure In example clusters converge quickly Algorithm spells K means clustering algorithm de tail The cluster centers initialized randomly In line data point xn compared cluster center k It assigned cluster k k center smallest distance That argmin step The variable zn stores assignment value K example n In lines cluster centers computed First Xk geometry nearest neighbors Algorithm K Means D K k K k random location randomly initialize mean kth cluster end repeat n N zn argmink k xn assign example n closest center end k K Xk xn zn k points assigned cluster k k mean Xk estimate mean cluster k end s stop changing return z return cluster assignments define vector addition scalar addition subtraction scalar multiplication norms define mean MATH REVIEW VECTOR ARITHMETIC NORMS AND MEANS Figure stores examples assigned cluster k The center cluster k k computed mean points assigned This process repeats means converge An obvious question algorithm converge A second question long converge The question actually easy answer Yes And practice usually converges quickly usually fewer iterations In Chapter actually prove converges The question long takes converge actually interesting question Even K means algorithm dates mid 1950s best known convergence rates terrible long time Here ter rible means exponential number data points This sad situation empirically knew converged quickly New algorithm analysis techniques called smoothed analysis invented fast convergence K means algorithms These techniques scope book author suffice K means fast practice provably fast theory It important note K means guaranteed converge guaranteed converge quickly guaranteed converge right answer The key problem unsupervised learning way knowing right answer Convergence bad solution usually poor initialization For example poor initialization data set yields convergence like seen Figure As algorithm course machine learning converged It converged satisfac tory What difference un supervised supervised learning means know right answer supervised learning unsupervised learning Warning High Dimensions Scary Visualizing dimensional space incredibly difficult humans After huge amounts training people reported visualize dimensional space heads But impossible If want try intu itive sense dimensions looks like I highly recommend short book Flatland A Romance Many Dimensions Edwin Abbott Abbott You read online gutenberg org ebooks In addition hard visualize addi tional problems high dimensions refered curse dimensionality One computational mathematical Figure 2d knn overlaid grid cell test point highlighted From computational perspective consider following prob lem For K nearest neighbors speed prediction slow large data set At look train ing example time want prediction To speed things want create indexing data structure You break plane grid like shown Figure Now test point comes quickly identify grid cell lies Now instead considering training points limit training points grid cell neighboring cells This potentially lead huge computa tional savings In dimensions procedure effective If want break space grid cells clearly grid cells dimensions assuming range features simplicity In dimensions ll need grid cells In dimensions ll need By time low dimensional data dimensions ll need grid cells s trillion times US national debt January So dimensions gridding technique useful trillion training examples For medium dimensional data approximately dimesions number grid cells followed numbers decimal point For comparison number atoms universe approximately followed zeros So atom yielded googul training examples d far fewer examples grid cells For high dimensional data approximately di mensions followed zeros Far big number comprehend Suffice moderately high dimensions computation involved problems enormous How analysis relate number data points need fill decision tree D features What importance shallow trees In addition computational difficulties working high gutenberg org ebooks geometry nearest neighbors dimensions large number strange mathematical oc curances In particular intuitions ve built working dimensions carry high dimensions We consider effects countless The high dimensional spheres look like porcupines like balls The second distances This result related Mark Reid heard Marcus Hutter points high dimensions approximately Figure 2d spheres spheres Let s start dimensions Figure We ll start green spheres radius touching exactly green spheres Remember dimensions sphere circle We ll place red sphere middle touches green spheres We easily compute radius small sphere The pythagorean theorem says r solving r r Thus calculation blue sphere lies entirely cube cube square contains grey spheres Yes obvious picture going Figure 3d spheres spheres Now experiment dimensions shown Figure Again use pythagorean theorem compute radius blue sphere Now r r This entirely enclosed cube width holds grey spheres At point difficult produce figures ll apply imagination In dimensions green spheres called hyperspheres radius They inside cube called hypercube width The blue hypersphere radius r Continuing dimensions blue hypersphere embedded green hyperspheres radius r In general D dimensional space 2D green hyper spheres radius Each green hypersphere touch exactly n hyperspheres The blue hyperspheres middle touch radius r D Think moment As number dimensions grows radius blue hypersphere grows bound For example dimensions radius blue hypersphere But radius blue hypersphere squeezing green hypersphere touching edges hypercube In dimensional space radius approxi mately pokes outside cube Figure porcupine versus ball This high dimensional spheres look like por cupines balls Figure The moral story machine learning perspective intuitions space carry high dimensions For example course machine learning think looks like round cluster dimensions look round high dimensions Figure uniform random points dimensions The second strange fact consider dis tances points high dimensions We start considering random points dimension That generate fake data set consisting random points zero We dimensions dimensions See Figure data distributed uniformly unit hypercube different dimensions Now pick points random compute dis tance Repeat process pairs points average results For data shown Figure average distance points dimension di mensions dimensions The fact increase dimension increases surprising The furthest points dimensional hypercube line furthest dimensional hypercube square opposite corners furthest d hypercube In general furthest points D dimensional hypercube D You actually compute values analytically Write UniD uniform distribution D dimensions The quantity interested computing avgDist D Ea UniD Eb UniD b We actually compute closed form Exercise bit calculus refresher arrive avgDist D D Because know maximum distance points grows like D says ratio average distance maximum distance converges What interesting variance distribu tion distances You D dimensions variance constant independent D This means look variance divided max distance variance behaves like 18D means effective variance continues shrink D grows Sergey Brin Near neighbor search large metric spaces In Conference Very Large Databases VLDB distance sqrt dimensionality p ai rs o f p oi nt s th d ta nc e dimensionality versus uniform point distances dims dims dims dims dims Figure histogram distances D When I saw proved result I skeptical I imagine So I implemented In Figure results This presents histogram distances random points D dimensions D As distances begin concentrate D medium dimension problems You terrified bit information KNN gets distances And ve seen moderately high di mensions distances equal So isn t case geometry nearest neighbors KNN simply work Figure knn mnist histogram distances multiple D mnist The answer The reason data uniformly distributed unit hypercube We looking real world data sets The image data set hand written digits zero Section Although data originally dimensions pixels pixels artifically reduce dimensionality data In Figure histogram average distances points data number dimensions As histograms distances con centrated single value This good news means hope learning algorithms work Nevertheless moral high dimensions weird Extensions KNN There fundamental problems KNN classifiers First neighbors better Second test time formance scales badly number training examples increases Third treats dimension independently We address issue solved makes great thought question Figure data set 5nn test point closest negatives far positives Regarding neighborliness consider Figure Using K near est neighbors test point classified positive However actually believe classified negative negative neighbors closer positive neighbors Figure previous e ball There ways addressing issue The e ball solution Instead connecting data point fixed number K nearest neighbors simply connect neigh bors fall ball radius e Then majority class points e ball wins In case tie guess report majority class Figure shows e ball test point happens yield proper classifica tion When e ball nearest neighbors KNN hyper parameter changes K e You need set way KNN One issue e balls e ball test point How handle An alternative e ball solution weighted nearest neighbors The idea consider K nearest neighbors test point uneven votes Closer points vote points When classifying point x usual strat egy training point xn vote decays exponentially distance x xn Mathematically vote neigh course machine learning bor n gets exp x xn Thus nearby points vote close far away points vote close The overall prediction positive sum votes positive neighbors outweighs sum votes negative neighbors Could combine e ball idea weighted voting idea Does sense idea trump The second issue KNN scaling To predict label single test point need find K nearest neighbors test point training data With standard implementation O ND K log K time4 For large data sets The ND term comes computing distances test point training points The K log K term comes finding K smallest values list distances median finding algorithm Of course ND dominates K log K practice impractical Figure knn collapse figures points collapsed mean good results dire results A attempt speed computation represent class representative A natural choice representative mean We collapse positive examples mean negative examples mean We run nearest neighbor check test point closer mean positive points mean negative points Figure shows example probably work example probably work poorly The problem collapsing class mean aggressive Figure knn collapse2 data previous bad case collapsed L cluster test point classified based means nn A aggressive approach use K means algo rithm clustering You cluster positive examples L clusters L avoid variable overloading cluster negative examples L separate clusters This shown Figure L Instead storing entire data set store means L positive clusters means L negative clusters At test time run K nearest neighbors algorithm means training set This leads faster runtime O LD K log K probably dominated LD Clustering classes intro duced way making things faster Will things worse help Exercises Exercise TODO THE PERCEPTRON Dependencies Chapter Chapter So far ve seen types learning models decision trees small number features decisions nearest neighbor algorithms features equally Neither extremes desirable In problems want use features use In chapter ll discuss perceptron algorithm learn ing weights features As ll learning weights features amounts learning hyperplane classifier basically di vision space halves straight line half positive half negative In sense perceptron seen explicitly finding good linear decision boundary Bio inspired Learning Figure picture neuron Folk biology tells brains bunch little units called neurons send electrical signals The rate firing tells activated neuron A single neuron like shown Figure incoming neurons These incoming neurons firing different rates e dif ferent activations Based incoming neurons firing strong neural connections main neu ron decide strongly wants fire And brain Learning brain happens neurons becom ming connected neurons strengths connections adapting time Figure figure showing feature vector weight vector products sum The real biological world complicated However goal isn t build brain simply inspired work We going think learning algorithm single neuron It receives input D neurons input feature The strength inputs fea ture values This shown schematically Figure Each incom ing connection weight neuron simply sums weighted inputs Based sum decides fire Learning Objectives Describe biological motivation perceptron Classify learning algorithms based error driven Implement perceptron algorithm binary classification Draw perceptron weight vectors corresponding decision boundaries dimensions Contrast decision boundaries decision trees nearest neighbor algorithms perceptrons Compute margin given weight vector given data set course machine learning Firing interpreted positive example firing interpreted negative example In particular weighted sum positive fires doesn t fire This shown diagramatically Figure Mathematically input vector x x1 x2 xD arrives The neuron stores D weights w1 w2 wD The neuron computes sum D d wdxd determine s activation If activiation posi tive e predicts example positive example Otherwise predicts negative example The weights neuron fairly easy interpret Suppose feature instance System s class gets zero weight Then activation regardless value feature So features zero weight ignored Features positive weights indicative positive examples because activation increase Features negative weights indicative negative examples because activiation decrease What happen encoded binary features like Sys tem s class yes standard yes It convenient non zero threshold In words want predict positive value The way convenient achieve introduce bias term neuron activation increased fixed value b Thus compute D d wdxd b If wanted activation thresh old instead value b This complete neural model learning The model pa rameterized D weights w1 w2 wD single scalar bias value b Error Driven Updating The Perceptron Algorithm todo VIGNETTE THE HISTORY OF THE PERCEPTRON The perceptron classic learning algorithm neural model learning Like K nearest neighbors frustrating algorithms incredibly simple works amazingly types problems perceptron Algorithm PerceptronTrain D MaxIter wd d D initialize weights b initialize bias iter MaxIter x y D Dd wd xd b compute activation example ya wd wd yxd d D update weights b b y update bias end end end return w0 w1 wD b Algorithm PerceptronTest w0 w1 wD b x Dd wd x d b compute activation test example return sign The algorithm actually different decision tree algorithm KNN algorithm First online This means instead considering entire data set time looks example It processes example goes Second error driven This means long doesn t bother updating parameters The algorithm maintains guess good parameters weights bias runs It processes example time For given example makes prediction It checks prediction correct recall training data access true labels If prediction correct Only prediction incorrect change parameters changes way better example time It goes example Once hits example training set loops specified number iterations The training algorithm perceptron shown Algo rithm corresponding prediction algorithm shown Algorithm There trick training algorithm probably silly useful later It line check want update We want update current prediction sign incorrect The trick multiply true label y activation compare zero Since label y need realize ya positive y sign In words product ya positive current prediction correct It important check ya ya Why course machine learning The particular form update perceptron simple The weight wd increased yxd bias increased y The goal update adjust parameters bet ter current example In words saw example twice row better job second time To particular update achieves consider fol lowing scenario We current set parameters w1 wD b We observe example x y For simplicity suppose posi tive example y We compute activation error Namely We update weights bias Let s new weights w w D b Suppose observe exam ple need compute new activation We proceed little algebra D d w dxd b D d wd xd xd b D d wdxd b D d xdxd D d x2d So difference old activation new activa tion d x d But x d s squared So value Thus new activation old activation plus Since positive example suc cessfully moved activation proper direction Though note s guarantee correctly classify point second fourth time This analysis hold case pos itive examples y It hold negative examples Work Figure training test error early stopping The hyperparameter perceptron algorithm MaxIter number passes training data If passes training data algorithm likely overfit This like studying long exam confusing On hand going data time lead underfitting This shown experimentally Figure The x axis shows number passes data y axis shows training error test error As sweet spot test performance begins degrade overfitting One aspect perceptron algorithm left underspecified line says loop training examples The natural implementation loop constant order The actually bad idea perceptron Consider perceptron algorithm data set consisted positive examples followed negative examples After seeing positive examples maybe likely decide example positive stop learning It examples hit batch negative examples Then maybe examples start predicting negative By end pass data learned handful examples case Figure training test error permuting versus permuting So thing need avoid presenting examples fixed order This easily accomplished permuting order examples beginning cycling data set permuted order iteration However turns actually better permute examples iteration Figure shows effect permuting convergence speed In practice permuting iteration tends yield savings number iterations In theory actually prove s expected twice fast If permuting data iteration saves time cases want permute data iteration Geometric Intrepretation A question asking decision boundary perceptron look like You actually answer question mathematically For perceptron decision bound ary precisely sign activation changes In words set points x achieve zero ac tivation The points clearly positive negative For simplicity ll consider case bias term equivalently bias zero Formally decision boundary B B x d wdxd We apply linear algebra Recall d wdxd dot product vector w w1 w2 wD vector x We write w x Two vectors zero dot product perpendicular Thus think weights vector w decision boundary simply plane perpendicular w course machine learning dot products definition perpendicular normalization projections think basis vectors projections quadratic rule vectors dot products unit vectors maximized point direction b blah blah blah MATH REVIEW DOT PRODUCTS Figure Figure picture data points hyperplane weight vector This shown pictorially Figure Here weight vector shown s perpendicular plane This plane forms decision boundary positive points negative points The vector points direction positive examples away negative examples One thing notice scale weight vector irrele vant perspective classification Suppose weight vector w replace 2w All activations doubled But sign change This makes complete sense geometri cally matters plane test point falls far plane For reason common work normalized weight vectors w length e w If I arbitrary non zero weight vector w I compute weight vector w points direction norm Figure picture projections weight vector TODO points dimensional axis zero marked The geometric intuition help realize dot products compute projections That value w x distance x origin projected vector w This shown Figure In figure data points projected w Below think dimensional version data data point placed according projection w This distance w exactly activiation example bias From start thinking role bias term Previously threshold zero Any example negative projection w classified negative exam ple positive projection positive The bias simply moves threshold Now projection computed b added overall activation The projection plus b compared zero Thus geometric perspective role bias shift decision boundary away origin direction w It shifted exactly b units So b positive boundary shifted away w b negative boundary shifted w This shown Figure This makes intuitive sense positive bias means examples classified positive By moving decision boundary negative direction space yields perceptron positive classification The decision boundary perceptron magical thing In D dimensional space D dimensional hyperplane In dimensions d hyperplane simply line In di mensions d hyperplane like sheet paper This hyperplane divides space half In rest book ll refer weight vector hyperplane defines interchangeably Figure perceptron picture update bias The perceptron update considered geometrically For simplicity consider unbiased case Consider situ ation Figure Here current guess hyper plane positive training example comes currently mis classified The weights updated w w yx This yields new weight vector shown Figure In case weight vector changed training example correctly classified Interpreting Perceptron Weights TODO Perceptron Convergence Linear Separability You intuitive feeling perceptron works moves decision boundary direction training exam ples A question asking percep tron converge If converge And long It easy construct data sets perceptron algorithm converge In fact consider uninteresting learn ing problem features You data set consisting positive example negative example Since fea tures thing perceptron algorithm adjust bias Given data run perceptron bajillion iterations settle As long bias non negative negative example because decrease As long non positive positive example because increase Ad infinitum Yes contrived example Figure separable data What mean perceptron converge It means entire pass training data making updates In words correctly classified training example Geometrically means found hyperplane correctly segregates data positive nega tive examples like shown Figure Figure inseparable data In case data linearly separable This means course machine learning exists hyperplane puts positive examples negative examples If training linearly separable like shown Figure perceptron hope converging It possibly classify point correctly The somewhat surprising thing perceptron algorithm data linearly separable converge weight vector separates data And data inseparable converge This great news It means perceptron converges remotely possible converge The second question long converge By long mean updates As case learning theory able answer form converge updates This asking The sort answer hope form converge updates What expect perceptron con verge quickly easy learning problems hard learning problems This certainly fits intuition The question define easy hard meaningful way One way def inition notion margin If I data set hyperplane separates like shown Figure margin distance hyperplane nearest point Intuitively problems large margins easy s lots wiggle room find separating hyperplane problems small margins hard specific tuned weight vector Formally given data set D weight vector w bias b margin w b D defined margin D w b min x y D y w x b w separates D In words margin defined w b actually separate data In case separates data find point minimum activation activation multiplied label So long margin positive Geometrically makes sense Eq yeild For historical reason unknown author mar gins denoted Greek letter gamma One talks margin data set The margin data set largest attainable margin data Formally margin D sup w b margin D w b In words compute margin data set try possi ble w b pair For pair compute margin We perceptron largest overall margin data If data You read sup max like difference technical difference case handled linearly separable value sup value margin There famous theorem Rosenblatt2 shows Rosenblatt number errors perceptron algorithm makes bounded More formally Theorem Perceptron Convergence Theorem Suppose perceptron algorithm run linearly separable data set D margin Assume x x D Then algorithm converge updates todo comment norm w norm x picture maximum margins The proof theorem elementary sense use fancy tricks s algebra The idea proof follows If data linearly separable margin exists weight vector w achieves margin Obviously don t know w know exists The perceptron algorithm trying find weight vector w points roughly direction w For large roughly rough For small roughly precise Every time perceptron makes update angle w w changes What prove angle actually decreases We steps First dot product w w increases lot Second norm w increase Since dot product increasing w isn t getting long angle shrinking The rest algebra Proof Theorem The margin realized set parameters x Suppose train perceptron data Denote w initial weight vector w weight vector update w k weight vector kth update We essentially ignoring data points perceptron doesn t update First w w k grows quicky function k Second w k grow quickly First suppose kth update happens example x y We trying w k aligned w Because updated know example misclassified yw k x After update w k w k yx We little computa tion w w k w w k yx definition w k w w k yw x vector algebra w w k w margin course machine learning Thus time w k updated projection w incrases Therefore w w k k Next need increase w occurs w k getting closer w s getting ex ceptionally long To compute norm w k w k w k yx definition w k w k y2 x 2yw k x quadratic rule vectors w k assumption x Thus squared norm w k increases date Therefore w k k Now things learned By conclusion know w w k k But second con clusion k w k Finally w unit vector know w k w w k Putting k w k w w k k Taking left right terms k k Dividing sides k k k This means ve updates Perhaps don t want assume x norm If norm R achieve simi lar bound Modify perceptron convergence proof handle case It important mind proof shows It shows I perceptron data linearly separable margin perceptron converge solution separates data And converge quickly large It solution fact separates data In particular proof makes use maximum margin separator But perceptron guaranteed find maximum margin separator The data separable margin perceptron find separating hyperplane margin Later Chapter algorithms explicitly try find maximum margin solution Why perceptron conver gence bound contradict earlier claim poorly ordered data points e g positives fol lowed negatives because perceptron astronom ically long time learn Improved Generalization Voting Averaging In beginning chapter comment ceptron works amazingly This half truth The vanilla perceptron perceptron algorithm amazingly In order competitive learning algorithms need modify bit better generalization The key issue vanilla perceptron counts later points counts earlier points To consider data set examples Suppose examples perceptron learned good classifier It s good goes exam ples making updates It reaches 000th example makes error It updates For know update 000th example completely ruines weight vector data What like weight vectors survive long time weight vectors overthrown quickly One way achieve voting As perceptron learns remembers long hyperplane survives At test time hyperplane encountered training votes class test example If particular hyperplane survived examples gets vote If survived example gets vote In particular let w b w b K K weight vectors encountered training c c K survival times weight vectors A weight vector gets immediately updated gets c survives round gets c Then prediction test point y sign K k c k sign w k x b k This algorithm known voted perceptron works practice nice theory showing guaranteed generalize better vanilla perceptron Unfortunately completely impractical If updates perceptron learning voted perceptron requires store weight vectors counts This requires absurd storage makes prediction times slower vanilla perceptron The training algorithm voted perceptron vanilla perceptron In particular line Algorithm ac tivation training example computed based current weight vector based voted prediction Why A practical alternative averaged perceptron The idea similar maintain collection weight vectors survival times However test time predict according average weight vector voting In particular predic tion y sign K k c k w k x b k The difference voted prediction Eq course machine learning Algorithm AveragedPerceptronTrain D MaxIter w b initialize weights bias u initialize cached weights bias c initialize example counter iter MaxIter x y D y w x b w w y x update weights b b y update bias u u y c x update cached weights y c update cached bias end c c increment counter regardless update end end return w 1c u b c return averaged weights bias averaged prediction Eq presense interior sign operator With little bit algebra rewrite test time prediction y sign K k c k w k x K k c k b k The advantage averaged perceptron simply maintain running sum averaged weight vector blue term averaged bias red term Test time prediction efficient vanilla perceptron The training algorithm averaged perceptron shown Algorithm Some notation changed original perceptron vector operations written vector opera tions activation computation folded error check ing It probably immediately apparent Algorithm computation unfolding precisely calculation averaged weights bias The natural implementation track averaged weight vector u At end example increase u u w similarly bias However implementation require updated aver aged vector example examples incorrectly classified Since hope eventually ceptron learns good job hope updates example So ideally like update averaged weight vector actual weight vector changes The slightly clever computation Algorithm achieves By writing computation averaged weights Eq telescoping sum derive computation Algorithm Figure perc avgperc train test performance vanilla versus averaged perceptron early stopping The averaged perceptron better perceptron ceptron sense generalizes better test data However free having early stopping It eventually overfit Figure shows performance vanilla perceptron averaged perceptron data set training test performance As averaged perceptron generalize better But begin overfit eventually Limitations Perceptron Figure picture xor problem Although perceptron useful fundamentally limited way decision trees KNN It is limitation decision boundaries linear The classic way showing limitation XOR problem XOR exclusive The XOR problem shown graphically Figure It consists data points corner unit square The labels points diagonals You try able find linear decision boundary perfectly separates data points One question ask XOR like problems exist real world Unfortunately perceptron answer yes Consider sentiment classification problem features simply given word contained review course These features excellent terrible The excellent feature indicative positive reviews terrible feature indicative negative reviews But presence feature categorization flips One way address problem adding feature combina tions We add additional features excellent terrible indicate conjunction base features By assigning weights follows achieve desired effect wexecellent wterrible wnot wexecllent wterrible In particular case addressed problem However start D features want add pairs ll blow D2 O D features feature mapping And s guarantee pairs features We need triples features D3 O D features These additional features drastically increase computation result stronger propensity overfitting Suppose took XOR problem added new fea ture x3 x1 x2 logical existing features Write feature weights bias achieve perfect classification data In fact XOR problem significant basically killed research classifiers linear decision boundaries decade course machine learning Later book alternative approaches taking key ideas perceptron generating classifiers non linear decision boundaries One approach combine multi ple perceptrons single framework neural networks approach Chapter The second approach find computa tionally efficient ways feature mapping computationally statistically efficient way kernels approach Chap ter Exercises Exercise TODO PRACTICAL ISSUES TODO examples feature Dependencies Chap ter Chapter Chapter At point seen qualitatively different models learning decision trees nearest neighbors perceptrons You learned clustering K means algorithm You shortly learn complex models variants things know However attempting understand complex models learning important firm grasp use machine learning practice This chapter abstract learning problem concrete implementation You examples best practices justifications practices In ways going abstract problem concrete learn ing task art science However art huge impact practical performance learning systems In cases moving complicated learning algorithm gain percent improvement Going better representa tion gain order magnitude improvement To end discuss high level ideas help develop better artistic sensibility The Importance Good Features Machine learning magical You data manages classify data For actually outperform human But like problems world significant garbage garbage aspect machine learning If data trash learning algorithm unlikely able overcome Consider problem object recognition images If start pixel image easy feature representation image dimensional vector dimension corresponds red green blue component pixel image So feature red pixel feature green pixel This Learning Objectives Translate problem de scription concrete learning problem Perform basic feature engineering image text data Explain use cross validation tune hyperparameters esti mate future performance Compare contrast differ ences evaluation metrics Explain feature combinations important learning models Explain relationship learning techniques seen far Apply debugging techniques learning algorithms In theory difference theory practice But practice Jan L A van de Snepscheut course machine learning pixel representation images Figure prac imagepix object recognition pixels One thing mind pixel representation throws away locality information image Learning algorithms don t care features care feature values So I permute features effect learning algorithm long I apply permutation training test examples Figure shows images who has pixels randomly permuted case pixels permuted colors All objects things ve seen plenty examples identify Should expect machine able Figure prac imagepatch object recognition patches Figure prac imageshape object recognition shapes An alternative representation images patch represen tation unit interest small rectangular block image single pixel Again permuting patches effect classifier Figure shows images patch representation Can identify A final representation shape representation Here throw color pixel infor mation simply provide bounding polygon Figure shows images representation Is iden tify If find answers end chap ter Figure prac bow BOW repr positive negative review In context text categorization instance sentiment recognition task standard representation bag words representation Here feature unique word appears document For feature happy feature value number times word happy appears document The bag words BOW representation throws away position information Figure shows BOW representation docu ments positive negative Can tell Irrelevant Redundant Features One big difference learning models robust addition noisy irrelevant features Intuitively irrelevant feature completely uncorrelated prediction task A feature f expectation depend label E f Y E f irrelevant For instance presence word largely irrelevant predicting course review positive negative Is possible feature f expectation depend label useful prediction A secondary issue algorithms deal redun dant features Two features redundant highly cor related regardless correlated task For example having bright red pixel image position probably highly redundant having bright red pixel practical issues position Both useful e g identifying fire hy drants images structured features likely co occur frequently When thinking robustness irrelevant redundant fea tures usually worthwhile thinking case great features bad feature The interesting case bad features outnumber good features outnumber large degree For instance number good features like log D set D total features The question robust algorithms case You think s crazy irrelevant features cases ve seen far bag words bag pixels reasonable examples How words entire English vocabulary roughly words actually useful predicting positive negative course reviews For shallow decision trees model explicitly selects features highly correlated label In particular limiting depth decision tree hope model able throw away irrelevant features Redundant features certainly thrown select feature second feature looks useless The possible issue irrelevant features irrelevant happen correlate class label training data chance As thought experiment suppose N training ex amples exactly half positive examples half negative examples Suppose s binary feature f completely uncorrelated label This feature chance ap pearing example regardless label In principle deci sion tree select feature But chance especially N small feature look correlated label This anal ogous flipping coins simultaneously N times Even coins independent s entirely possible observe sequence like H H T T H H H H makes look entirely correlated The hope N grows likely In fact explicitly compute likely happen To let s fix sequence N labels We flip coin N times consider likely exactly matches label This easy probability 5N Now confused exactly matched label probability So chance looks perfectly correlated 5N 5N 5N Thankfully shrinks small e g data points This makes happy The problem don t ir relevant feature D log D irrelevant features If ran domly pick irrelevant feature values prob ability perfectly correlating 5N But independent coins chance correlates perfectly 5N 5N In general K irrelevant features course machine learning random independent coins chance perfectly correlates 5N K This suggests sizeable number K irrelevant features d better K training examples Unfortunately situation actually worse In analysis considered case perfect correlation We consider case partial correlation yield higher probabilities This left Exercise want practice probabilistic analysis Suffice decision trees confused Figure prac addirel data high dimensional warning interpolated In case K nearest neighbors situation dire Since KNN weighs feature feature introduction irrelevant features completely mess KNN prediction In fact saw high dimensional space randomly distributed points look approximately distance apart If add lots lots randomly distributed features data set distances converge This shown experimentally Figure start digit categorization data con tinually add irrelevant uniformly distributed features generate histogram distances Eventually distances converge In case perceptron hope learn assign zero weight irrelevant features For instance consider binary feature randomly zero independent label If perceptron makes updates positive examples negative examples reasonable chance feature weight zero At small What happens perceptron truly redundant features e literally copy Figure prac noisy dt knn perc increasing amounts noise To better practical sense sensitive algorithms irrelevant features Figure shows test performance algorithms increasing number compltely noisy features In cases hyperparameters tuned validation data TODO Feature Pruning Normalization In text categorization problems words simply appear Perhaps word groovy appears exactly train This typically positive indicator US 1970s ing document positive Is worth keeping word feature It s dangerous endeavor s hard tell training example correlated positive class noise You hope learning algorithm smart figure Or remove That means learning algorithm won t figure b ve reduced number dimensions things efficient scary Figure prac pruning effect pruning text data practical issues data mean variance moments expectations etc MATH REVIEW DATA STATISTICS MEANS AND VARIANCES Figure This idea feature pruning useful applied applications It easiest case binary features If binary feature appears small number K times training data fair looking test data simply remove consideration You want remove features appear K documents instance word appears pretty English document written Typical choices K depending size data On text data set documents cutoff probably reasonable On text data set size web cut probably reasonable3 Figure shows effect According Google following words appear times web moudlings agag gagctg setgravity rogov prosomeric spunlaid piyushtwok telelesson nes mysl brighnasa For comparison word appears billion times pruning sentiment analysis task In beginning pruning hurt helps eventually prune away interesting words performance suffers Figure prac variance effect pruning vision In case real valued features question extend idea occur real values A reasonable def inition look features low variance In fact binary features ones appear appear low variance Figure shows result pruning low variance features digit recognition task Again pruning hurt helps eventually thrown useful features Earlier discussed problem scale features e g millimeters versus centimeters Does impact variance based feature pruning Once pruned away irrelevant features useful normalize data consistent way There basic types normalization feature normalization exam ple normalization In feature normalization feature adjust way examples In example normalization example adjusted individually Figure prac transform picture centering scaling variance scaling absolute value The goal types normalization easier learning algorithm learn In feature normalization standard things Centering moving entire data set centered origin Scaling rescaling feature following holds Each feature variance training data b Each feature maximum absolute value train ing data course machine learning These transformations shown geometrically Figure The goal centering sure features arbitrarily large The goal scaling sure features roughly scale avoid issue centimeters versus millimeters For models know KNN DT Perceptron sensitive center ing Which sensitive scaling These computations fairly straightforward Here xn d refers dth feature example n Since rare apply scaling previously applying centering expressions scaling assume data centered Centering xn d xn d d Variance Scaling xn d xn d d Absolute Scaling xn d xn d rd d N n xn d d N n xn d d rd maxn xn d In practice dynamic range features subset probably worth effort centering scaling It s effort centering scaling calculations apply test data However features orders magnitude larger helpful Remember know best difference scale actually significant problem rescaling throw away useful informa tion One thing wary centering binary data In cases binary data sparse given example features For instance vocabulary words given document probably contains From storage computation perspective useful However centering data longer sparse pay dearly outrageously slow implementations Figure prac exnorm example example normalization In example normalization view examples time The standard normalization ensure length example vector example lies unit hypersphere This simple transformation Example Normalization xn xn xn This transformation depicted Figure The main advantage example normalization makes comparisons straightforward data sets If I hand practical issues data sets differ norm feature vectors e scaled version difficult compare learned models Example normalization makes straightfor ward Moreover saw perceptron convergence proof mathematically easier assume normalized data Combinatorial Feature Explosion You learned Chapter linear models like perceptron solve XOR problem You learned performing combinatorial feature explosion But came computational expense gigantic feature vectors Of algorithms ve seen far perceptron gain feature combination And decision tree gain In fact decision tree construction essentially building meta features Or building meta features constructed purely logical ands Figure prac dttoperc turning DT set meta features This observation leads heuristic constructing meta features perceptrons decision trees The idea train decision tree training data From decision tree extract meta features looking feature combinations branches You add feature combinations meta features feature set perceptron Figure shows small decision tree set meta features extract There hyperparameter length paths extract tree case paths length extracted For bigger trees data benefit longer paths Figure prac log performance text categ word counts versus log word counts In addition combinatorial transformations logarithmic transformation useful practice It like strange thing useful doesn t fundamentally change data However learning algorithms operate linear operations features perceptron KNN log transform way product like operations The question following feels applicable data time feature increases I m equally likely predict positive label time feature doubles I m equally like predict positive label In case stick linear features second case switch log transform This important transformation text data presence word excellent good indicator positive review seeing excellent twice better indicator difference seeing excellent times seeing times isn t big deal A log transform achieves course machine learning Experimentally difference test performance word count data log word count data Figure Here transformation actually xd log2 xd ensure zeros remain zero sparsity retained Evaluating Model Performance So far focus classifiers achieve high accuracy In cases want For instance trying predict patient cancer better err saying cancer don t die Similarly letting little spam slip better accidentally blocking email boss There major types binary classification problems One X versus Y For instance positive versus negative sentiment Another X versus X For instance spam versus non spam The argument lots types non spam Or context web search relevant document versus irrelevant document This subtle subjective decision But X versus X problems feel X spotting true distinction X Y Can spot spam spot relevant documents For spotting problems X versus X ap propriate success metrics accuracy A popular information retrieval precision recall metric Precision asks question X s found actually X s Recall asks X s find Formally precision recall defined A colleague analogy US court system s saying Do promise tell truth truth In case truth means high recall truth means high precision P I S R I T S number Xs system found T number Xs data I number correct Xs system found Here S mnemonic System T mnemonic Truth I mnemonic Intersection It generally accepted definitions Thus system found preci sion perfect find recall perfect Figure prac spam bunch emails spam nospam sorted model predicion perfect Once compute precision recall able produce precision recall curves Suppose attempting practical issues identify spam You run learning algorithm predictions test set But instead taking yes answer allow algorithm produce confidence For instance perceptron use distance hyperplane confidence measure You sort test emails according ranking You spam like emails spam like emails like Figure How confidence decision tree KNN Figure prac prcurve precision recall curve Once sorted list choose aggressively want spam filter setting threshold list One hope set threshold high likely high precision low recall If set thresh old low ll high recall low precision By consider ing possible place threshold trace curve precision recall values like Figure This allows ask question fixed precision sort recall I Obviously closer curve upper right corner better And comparing learning algorithms A B A dominates B A s precision recall curve higher B s Table Table f measures varying precision recall values Precision recall curves nice allow visualize ways use system However like single number informs quality solution A popular way combining precision recall single number taking harmonic mean This known balanced f measure f score F P R P R The reason want use harmonic mean arithmetic mean favors sys tems achieve roughly equal precision recall In extreme case P R F P R But imbalanced case instance P R overall f measure modest Table shows f measures function precision recall important balanced values In cases believe precision impor tant recall This idea leads weighted f measure parameterized weight beta F P R P R For reduces standard f measure For focuses entirely recall focuses entirely preci sion The interpretation weight F measures perfor course machine learning mance user cares times precision recall One thing mind precision recall f measure depend crucially class considered thing wish find In particular binary data set flip means positive negative example end completely difference precision recall values It case precision flipped task equal recall original task vice versa Consequently f measure For tasks people sure want occasionally report sets precision recall f measure numbers vary based class considered thing spot There standard metrics different com munities For instance medical community fond sensi tivity specificity metric A sensitive classifier finds looking high recall In fact sensitivity exactly recall A specific classifier good job finding things doesn t want find Specificity precision negation task hand You compute curves sensitivity specificity like precision recall The typical plot referred ceiver operating characteristic ROC curve plots sensitivity specificity Given ROC curve compute area curve AUC metric provides mean ingful single number system s performance Unlike f measures tend low require agreement AUC scores tend high great systems This ran dom chance AUC best possible AUC The main message evaluation metrics choose whichever makes sense In cases sense In case commonly field There reason outlier because Cross Validation In Chapter learned development data held data set hyperparameters The main disadvantage develop ment data approach throw training data estimating hyperparameters An alternative idea cross validation In cross validation break training data equally sized partitions You train learning algorithm test remaining practical issues Algorithm CrossValidate LearningAlgorithm Data K e store lowest error encountered far unknown store hyperparameter setting yielded hyperparameter settings err track K error estimates k K train xn yn Data n mod K k test xn yn Data n mod K k test Kth example model Run LearningAlgorithm train err err error model test add current error list errors end avgErr mean set err avgErr e e avgErr remember settings best far end end You times time holding different partition development You average performance parts estimate model perform future You repeat process possible choice hyperparameters estimate performs best The general K fold cross validation technique shown Algorithm K preceeding discussion In fact development data approach seen approxi mation cross validation K loops line Algorithm executed Typical choices K N By far com mon K fold cross validation Sometimes efficiency reasons And subtle statistical rea sons rare In case K N known leave cross validation abbreviated LOO cross val idation After running cross validation choices You select K trained models final model predictions train new model data hyperparameters selected cross validation If time probably better options It LOO cross validation prohibitively expensive run This true learning algorithms K nearest neighbors For KNN leave actually natural We loop training point ask example correctly classified different possible values K This requires computation computing K nearest neighbors highest value K This popular effective approach KNN classification spelled course machine learning Algorithm KNN Train LOO D errk k N errk stores kNN n N Sm xn xm m m n compute distances points S sort S lowest distance objects y current label prediction k N dist m Sk y y ym let kth closest point vote y ym errk errk error kNN end end end return argmink errk return K achieved lowest error Algorithm Overall main advantage cross validation develop ment data robustness The main advantage development data speed One warning mind goal cross valida tion development data estimate future This question statistics holds test data looks like training data That drawn distribution In practical cases entirely true For example person identification try classify pixel image based contains person If training images pixels total 1m training examples The classification pixel image highly dependent classification neighboring pixel image So pixels happens fall training data development cross validation data model unreasonably In case important cross validate use development data images pixels The goes text problems want classify things word level handed collection documents The important thing mind images documents drawn independently data distribution pixels words drawn dependently practical issues Hypothesis Testing Statistical Significance story VIGNETTE THE LADY DRINKING TEA Suppose ve presented machine learning solution boss achieves error cross validation Your nemesis Gabe gives solution boss achieves error cross vali dation How impressed boss It depends If improvement measured examples impressed It mean Gabe got exactly example right In fact probably got right wrong If impressed measured examples impressive This fundamental questions statistics You scientific hypothesis form Gabe s algorithm better You wish test hypothesis true You testing null hypothesis Gabe s algo rithm better You ve collected data 1m data points measure strength hypothesis You want ensure difference performance algorithms statistically significant e probably random luck A common question statisticians ask drug treatment better placebo competitor s drug There ways hypothesis testing Like evaluation metrics number folds cross validation discipline specific Here discuss popular tests paired t test bootstrapping These tests statistical tests underlying assumptions instance sumptions distribution observations strengths instance small large samples In cases goal hypoth esis testing compute p value probability observed difference performance chance The standard way reporting results like chance difference chance The value arbitrary occasionally people use weaker test stronger tests The t test example parametric test It applicable null hypothesis states difference responses mean zero unknown variance The t test actually assumes data distributed according Gaussian distribution course machine learning probably true binary responses Fortunately large samples binary seamples approximated Gaussian distribution So long sample sufficiently large t test reasonable regression classification problems t significance Table Table significance values t test Suppose evaluate algorithm N examples On example compute algorithm correct prediction Let a1 aN denote error algorithm example Let b1 bN denote error second algorithm You compute b means b respecitively Finally center data b b b The t statistic defined t b N N n n b n After computing t value compare list values computing confidence intervals Assuming lot data N compare t value Table determine significance level difference What mean means b apart How affect t value What happens variance increases One disadvantage t test easily applied evaluation metrics like f score This f score com puted entire test set decompose set individual errors This means t test applied Fortunately cross validation gives way problem When K fold cross validation able compute K error metrics data For example run fold cross validation compute f score fold Perhaps f scores This gives average f score folds The standard deviation set f scores N n ai You assume distribution scores approximately Gaussian If true approximately proba bility mass lies range lies range lies range So comparing algorithm average f score certain superior performance chance Had run fold cross validation able tighter confidence intervals WARNING A confidence mean There chance I better All means I reran ex practical issues Algorithm BootstrapEvaluate y y NumFolds scores k NumFolds truth list values want predict pred list values actually predicted n N m uniform random value N sample test point truth truth ym add truth pred pred y m add prediction end scores scores f score truth pred evaluate end return mean scores stddev scores periment times experiments I win These different statements If people know statistics mad One disadvantage cross validation computationally expensive More folds typically leads better estimates new fold requires training new classifier This time consuming The technique bootstrapping closely related idea jack knifing address problem Suppose didn t want run cross validation All single held test set data points You run classifier predictions data points You like able compute metric like f score test set confidence intervals The idea bootstrapping set random draw distribution We like multiple random draws distribution evaluate We simulate multiple draws repeatedly subsampling examples replacement To perform single bootstrap sample random points test set random points This sampling replacement example sampled ll end original test set This gives bootstrapped sample On sample compute f score metric want You times fold bootstrap For bootstrapped sam ple different f score The mean standard deviation set f scores estimate confidence interval algorithm The bootstrap resampling procedure sketched Algorithm This takes arguments true labels y predicted labels y number folds run It returns mean standard deviation compute confidence interval course machine learning Debugging Learning Algorithms Learning algorithms notoriously hard debug experienced implemented models presented far The main issue learning algorithm doesn t learn s unclear s bug learning problem hard s noise Moreover bugs lead learning algorithms performing better especially hard catch bit disappointing catch Obviously reference implementation tempt match output Otherwise things try help debug The power learning algorithm overfit If overfit training data s definitely wrong The second feed tiny hand specified dimensional data set know plot output The easiest way try learning algorithm overfit add new feature You feature CheatingIsFun feature6 The feature value associated feature Note cheating actually fun shouldn t positive example zero negative example In words feature perfect indicator class example If add CheatingIsFun feature algorithm near training error noisy features confusing You remove lot features feature value CheatingIsFun algorithm looks If algorithm overfit likely bug Remember remove CheatingIsFun feature final implementation A second thing try hand craft data set know algorithm work This useful ve man aged model overfit simply noticed generalize For instance run KNN XOR data Or run perceptron easily linearly separable data instance positive points line x2 x1 neg ative points line x2 x1 Or decision tree nice axis aligned data When debugging hand crafted data remember know models considering For instance know perceptron converge linearly separable data try linearly separable data set You know decision trees data relevant features practical issues label easy combination features y x1 x2 x3 You know KNN work data sets classes separated try data sets The important thing mind lot goes getting good test set performance First model right data So crafting data helpful Second model fit training data try overfit Third model generalize sure tune hyperparameters Figure prac imageanswers object recognition answers TODO answers image questions Exercises Exercise TODO BEYOND BINARY CLASSIFICATION Dependencies In preceeding chapters learned simple form prediction predicting bits In real world need predict complex objects You need categorize document categories sports en tertainment news politics etc You need rank web pages ads based relevance query You need simultaneously classify collection objects web pages important information links These problems com monly encountered fundamentally complex binary classification In chapter learn use know binary classification solve complicated problems You s relatively easy think binary classifier black box reuse solving complex problems This useful abstraction allows reuse knowledge having build new learning models algorithms scratch Learning Imbalanced Data Your boss tells build classifier identify fraudulent transactions credit card histories Fortunately transactions legitimate data positive stance The imbalanced data problem refers fact large number real world problems number positive exam ples dwarfed number negative examples vice versa This actually misnomer data imbalanced distribution data drawn And distribution imbalanced data Imbalanced data problem machine learning algo rithms smart good For learning algo rithms data negative posi tive simply learn predict negative Why Because Learning Objectives Represent complex prediction prob lems formal learning setting Be able artifically balance imbalanced data Understand positive neg ative aspects reductions multiclass classification binary classification Recognize difference regression ordinal regression Implement stacking method collective classification binary classification trying minimize error achieve error If teacher told study exam true false questions true unlikely study long Really problem data way defined learning problem That care accuracy care If want learning algorithm reasonable job tell want Most likely want optimize accuracy optimize measure like f score AUC You want algorithm positive predictions simply prefer good We shortly discuss heuristics dealing problem subsampling weighting In subsampling throw negative examples left bal anced data set positive negative This scare bit throwing data like bad idea makes learning efficient In weighting instead throw ing positive examples given lower weight If assign importance weight positive ex amples weight associated positive examples negative examples Before formally defining heuristics need mech anism formally defining supervised learning problems We proceed example binary classification canonical learning problem Given An input space X An unknown distribution D X Compute A function f minimizing E x y D f x y TASK BINARY CLASSIFICATION As binary classification examples ve seen input space RD There distri bution produces labeled examples input space You access distribution obtain samples Your goal find classifier minimizes error distribu tion A small modification definition gives weighted classifi cation problem believe positive class times course machine learning Algorithm SubsampleMap Dweighted true x y Dweighted draw example weighted distribution u uniform random variable y u return x y end end important negative class Given An input space X An unknown distribution D X Compute A function f minimizing E x y D y f x y TASK WEIGHTED BINARY CLASSIFICATION The objects given weighted binary classification iden tical standard binary classification The difference cost misprediction y cost misprediction y In follows assume If simply swap labels use The question ask suppose I good algorithm solving BINARY CLASSIFICATION problem Can I turn good algorithm solving WEIGHTED BINARY CLASSIFICATION problem In order need define transformation maps concrete weighted problem concrete unweighted problem This transformation needs happen training time test time need transformation Algorithm sketches training time sub sampling transformation Algo rithm sketches test time transformation case trivial All training algorithm retaining positive ex amples fraction negative examples The algorithm explicitly turning distribution weighted examples dif ferent distribution binary examples A vanilla binary classifier trained induced distribution Aside fact algorithm throws lot data especially large reasonable thing In fact reductions perspective optimal algorithm You prove following result binary classification Theorem Subsampling Optimality Suppose binary classifier trained Algorithm achieves binary error rate e Then error rate weighted predictor equal e This theorem states binary classifier induced distribution learned predictor original distribution Thus successfully converted weighted learning problem plain classification problem The fact error rate weighted predictor exactly times unweighted predictor unavoidable error metric evaluated times bigger Why unreasonable expect able achieve instance error e sublinear The proof theorem straightforward prove It simply involves algebra expected values Proof Theorem Let Dw original distribution let Db induced distribution Let f binary classifier trained data Db achieves binary error rate eb distribution We compute expected error ew f weighted problem ew E x y Dw y f x y x X y Dw x y y f x y x X Dw x f x Dw x f x x X Db x f x Db x f x E x y Db f x y eb And We implicitly assumed X discrete In case continuous data need replace sums x integrals x result holds Instead subsampling low cost class alternatively oversample high cost class The easiest case teger Now positive point include copies induced distribution Whenever negative point include single copy How handle non integral instance This oversampling algorithm achieves exactly theoretical result subsampling algorithm The main advantage sampling algorithm throw data The main advantage subsampling algorithm computa tionally efficient Modify proof optimality subsampling algorithm applies oversampling algorithm course machine learning You asking intuitively oversampling algo rithm like better idea subsampling algorithm don t care computational efficiency But ory tells What going Of course theory isn t wrong It s assumptions effectively dif ferent cases Both theorems state error e binary problem automatically error e weighted problem But possible error e binary problem Since oversampling al gorithm produces data points subsampling algorithm concievable lower binary error sampling subsampling The primary drawback oversampling computational ineffi ciency However learning algorithms straightforward include weighted copies data points cost The idea store unique data points maintain counter saying times replicated This easy percep tron takes work easy decision trees KNN For example decision trees recall Algorithm changes ensure line computes fre quent weighted answer change lines compute weighted errors Why hard change ceptron Hint fact perceptron online How modify KNN account weights Multiclass Classification Multiclass classification natural extension binary classification The goal assign discrete label examples instance document entertainment sports finance world news The difference K classes choose Given An input space X number classes K An unknown distribution D X K Compute A function f minimizing E x y D f x y TASK MULTICLASS CLASSIFICATION Note identical binary classification presence K classes In K K In fact set K exactly recover binary classification The game play gives binary classi fier use solve multiclass classification prob binary classification Algorithm OneVersusAllTrain Dmulticlass BinaryTrain K Dbin relabel Dmulticlass class positive negative fi BinaryTrain Dbin end return f1 fK Algorithm OneVersusAllTest f1 fK x score initialize K scores zero K y f x scorei scorei y end return argmaxk scorek lem A common approach versus technique called OVA versus rest To perform OVA train K binary classifiers f1 fK Each classifier sees training data Classifier fi receives examples labeled class positives examples negatives At test time whichever classifier predicts positive wins ties broken randomly Suppose N data points K classes evenly divided How long train OVA classifier base binary classifier takes O N time train What base classifier takes O N2 time The training test algorithms OVA sketched Algo rithms In testing procedure prediction ith classifier added overall score class Thus predic tion positive class gets vote prdiction negative implicitly gets vote In fact learning algorithm output confidence discussed Section better confidence y simple Why confidence help OVA natural easy implement natural It works practice long good job choosing good binary classification algorithm tuning hyperparameters It is weakness somewhat brittle Intuitively particularly robust errors underlying classifiers If classifier makes mistake eis possible entire prediction erroneous In fact entirely possible K classifiers predicts positive actually worst case scenario theoretical perspective This explicit OVA error bound Theorem OVA Error Bound Suppose average binary error K binary classifiers e Then error rate OVA multiclass predictor K e Proof Theorem The key question erroneous predictions binary classifiers lead multiclass errors We break false negatives predicting truth false positives course machine learning predicting truth When false negative occurs testing procedure chooses randomly available options labels This gives K K probability multiclass error Since binary error necessary happen efficiency error mode K K K K Multiple false positives occur simultaneously Suppose m false positives If simultaneously false negative error In order happen m errors efficiency M In case simulta neous false negative error probability m m This requires m errors leading efficiency m The worse case false negative case gives efficiency K K Since K opportunities err multiply K bound K e The constants relatively unimportant aspect matters scales linearly K That number classes grows expected error To develop alternative approaches useful way think turning multiclass classification problems binary classification problems think like tournaments football soccer aka football cricket tennis appeals You K teams entering tournament unfortunately sport playing allows compete time You want set way pairing teams having compete figure team best In learning teams classes trying figure class best The sporting analogy breaks bit OVA K games played team play simultane ously teams One natural approach team compete ev ery team The team wins majority matches declared winner This versus AVA approach called pairs The natural way think training K2 classifiers Say fij j k classifier pits class class j This classifier receives class examples positive class j examples negative When test point arrives run fij classifiers Every time fij predicts positive class gets point class j gets point After running K2 classifiers class votes wins Suppose N data points K classes evenly divided How long train AVA classifier base binary classifier takes O N time train What base classifier takes O N2 time How com pare OVA The training test algorithms AVA sketched Algo rithms In theory AVA mapping complicated weighted binary case The result stated proof omitted Theorem AVA Error Bound Suppose average binary error binary classification Algorithm AllVersusAllTrain Dmulticlass BinaryTrain fij j K K Dpos x Dmulticlass labeled j K Dneg x Dmulticlass labeled j Dbin x x Dpos x x Dneg fij BinaryTrain Dbin end end return fijs Algorithm AllVersusAllTest f j x score initialize K scores zero K j K y f j x scorei scorei y scorej scorej y end end return argmaxk scorek K2 binary classifiers e Then error rate AVA multiclass predictor K e The bound AVA K e bound OVA K e Does mean OVA necessarily better AVA Why Figure data set OVA terribly linear classifiers Consider data Figure assume percep tron base classifier How OVA data What AVA At point wondering s possible bet ter linear K Fortunately answer yes The solution like computer science divide conquer The idea construct binary tree classifiers The leaves tree correspond K labels Since log2 K decisions root leaf log2 K chances error Figure example classification tree K An example classification tree K classes shown Figure At root distinguish classes classes This means train binary clas sifier positive examples data points multiclass label negative examples data points multiclass label Based decision classifier walk appropriate path tree When K powwr tree This classification tree algorithm achieves following bound Theorem Tree Error Bound Suppose average binary classifiers error e Then error rate tree classifier dlog2 Ke e Proof Theorem A multiclass error classifier course machine learning path root correct leaf makes error Each probability e making error path consists dlog2 Ke binary decisions One think mind tree classifiers control tree defined In OVA AVA classification problems created In tree classifiers thing matters root half classes considered positive half considered negative You want split classes way classification decision easy possible You use happen know classification problem try separate classes reasonable way Can better dlog2 Ke e It turns answer yes algorithms relatively complicated You actu ally 2e idea error correcting tournaments Moreover prove lower bound states best possible e This means error correcting tourna ments factor worse optimal Ranking You start new web search company called Goohooing Like search engines user inputs query set documents trieved Your goal rank resulting documents based rel evance query The ranking problem collection items sort according notion preference One trickiest parts ranking learning properly define loss function Toward end section general loss function let s consider special cases Continuing web search example given collection queries For query given collection documents desired ranking documents In follow ing ll assume N queries query M documents In practice M probably vary query ease ll consider simplified case The goal train binary classifier predict preference function Given query q documents di dj classifier predict di preferred dj respect query q As previous examples things care train classifier predicts preferences turn predicted preferences ranking Unlike previous examples second step somewhat complicated binary classification Algorithm NaiveRankTrain RankingData BinaryTrain D n N j M j prefered j query n D D xnij j prefered query n D D xnij end end end return BinaryTrain D Algorithm NaiveRankTest f x score initialize M scores zero j M j y f x ij predicted ranking j scorei scorei y scorej scorej y end return argsort score return queries sorted score ranking case This need predict entire ranking large number documents assimilating preference function overall permutation For notationally simplicity let xnij denote features associated comparing document document j query n Training fairly straightforward For n pair j create binary classification example based features xnij This example positive preferred j true ranking It neg ative j preferred In cases true ranking express preference objects case exclude j j pair training Now tempted evaluate classification perfor mance binary classifier The problem approach s impossible tell looking output j pair good overall ranking This intermediate step turning pairwise predictions coherent ranking What need measure ranking based predicted preferences compares true ordering Algorithms naive algorithms training testing ranking function These algorithms actually work case bipartite ranking problems A bipartite ranking problem trying predict binary response instance course machine learning document relevant evaluated according metric like AUC This essentially goal bipartite problems ensure relevant documents ahead irrelevant documents There notion relevant document relevant For non bipartite ranking problems better First preferences training time nuanced relevant incorporate preferences training time Effectively want higher weight binary prob lems different terms perference Sec ond producing list scores calling arbi trary sorting algorithm actually use preference function sorting function inside implementation quicksort We formalize problem Define ranking function maps objects ranking documents desired position list M If u v u preferred v e appears earlier ranked document list Given data ob served rankings goal learn predict rankings new objects We define M set ranking functions M objects We wish express fact making mistake pairs worse making mistake This encoded cost function omega j cost ac cidentally putting position j gone position To valid cost function valid symmet ric monotonic satisfy triangle inequality Namely j j j k j k j k j j k k With definitions properly define ranking problem Given An input space X An unknown distribution D X M Compute A function f X M minimizing E x D u v u v v u u v f x TASK RANKING In definition complex aspect loss function This loss sums pairs objects u v If true ranking binary classification Algorithm RankTrain Drank BinaryTrain Dbin x Drank u v y sign v u y u prefered v w u v w cost misclassification Dbin Dbin y w xuv end end return BinaryTrain Dbin prefers u v predicted ranking prefers v u incur cost u v Depending problem care set standard options If j j achieve Kemeny distance measure simply counts number pairwise misordered items In applications care getting K predictions correct For instance web search algorithm display K results user In case define j min j K j In case errors K elements penalized Swap ping items irrelevant K Finally bipartite ranking case express area curve AUC metric j M2 M M M M j M j M M Here M total number objects ranked M number actually good Hence M M number actually bad bipartite problem You penalized rank good item position greater M rank bad item position equal M In order solve problem follow recipe similar naive approach sketched earlier At training time biggest change weight training example bad mess This change depicted Algorithm binary classiciation data weights w provided saying important given example These weights derived cost function At test time instead predicting scores sorting list essentially run quicksort algorith f comparison course machine learning Algorithm RankTest f x obj obj contains elements return obj p randomly chosen object obj pick pivot left elements smaller p right elements larger p u obj p y f xup probability u precedes p uniform random variable y left left u right right u end end left RankTest f x left sort earlier elements right RankTest f x right sort later elements return left p right end function At step Algorithm pivot p chosen Every object u compared p f If f thinks u better sorted left sorted right There major difference algorithmand quicksort compari son function allowed probabilistic If f outputs probabilities instance predicts u probability better p puts left probability right probability The pseudocode written way f predicts algorithm works This algorithm better naive algorithm ways First makes O M log2 M calls f expectation O M2 calls naive case Second achieves better error bound shown Theorem Rank Error Bound Suppose average binary error f e Then ranking algorithm achieves test error 2e general case e bipartite case Collective Classification Figure example face finding image pixel mask You writing new software digital camera face identification However instead simply finding bounding box faces image predict face pixel level So input image pixels low resolution camera output set binary predictions pixel You given large collection binary classification training examples An example input output pair shown Figure Your attempt train binary classifier predict pixel j face You feed features classifier RGB values pixel j pixels window arround For instance pixels region k j l k l Figure bad pixel mask previous image You run classifier notice predicts weird things like Figure You realize predicting pixel independently bad idea If pixel j face significantly increases chances pixel j face And similarly pixels This collective classifi cation problem trying predict multiple correlated objects time Similar problems come time Cast following collec tive classification problems web page categorization labeling words sentence noun verb adjec tive etc finding genes DNA sequences predicting stock market The general way formulate problems undi rected graph prediction problems Our input takes form graph vertices input output pairs edges represent correlations putputs Note edges need express correlations inputs simply encoded nodes For example face identi fication case pixel correspond vertex graph For vertex corresponds pixel input set features want pixel including features neighboring pixels There edges vertex instance vertices If predicting K classes vertex given graph vertices labeled pairs x k X K We write G X K denote set graphs A graph set denoted G V E vertices V edges E Our goal function f takes input graph G X predicts label K vertices Formulate example problems graph prediction prob lems Given An input space X number classes K An unknown distribution D G X K Compute A function f G X G K minimizing E V E D v V y v yv yv label associated vertex v G y v label predicted f G TASK COLLECTIVE CLASSIFICATION In collective classification like able use course machine learning labels neighboring vertices help predict label given vertex For instance want add features predict given vertex based labels neighbor At training time easy true labels neighbor However test time difficult predicting labels neighbor This presents chicken egg problem You trying predict collection labels But prediction label depends prediction labels If remember general lution problem iteration begin guesses try improve guesses time Alternatively fact graph scream dynamic programming Rest assured skip forward Chapter lots detail Figure charicature stack ing works This idea stacking solving collective classification Figure You train classifiers The classifier predicts value pixel independently like Figure This doesn t use graph structure In second level repeat classification However use outputs level initial guesses labels In general Kth level stack use inputs pixel values predictions K previous levels stack This means training K binary classifiers based different feature sets The prediction technique stacking sketched Algorithm This takes list K classifiers corresponding level stack input graph G The variable Y k v stores prediction classifier k vertex v graph You predict node vertex layer stack neighboring information For rest layers add features node based predictions lower levels stack neighboring nodes N u denotes neighbors u The training procedure follows similar scheme sketched Al gorithm It largely follows schematic prediction algorithm training fed After classifier k level trained predict labels node graph These labels later levels stack features One thing aware MulticlassTrain con ceivably overfit training data For example possible layer actually achieve error case reason iterate But test time probably error misleading There ways address issue The use cross validation training use predictions obtained cross validation predictions StackTest This typically safe somewhat expensive The alternative simply regularize training algorithm In particular instead trying find hyperparameters binary classification Algorithm StackTrain Dcc K MulticlassTrain Dmc generated multiclass data Y k n v k K n N v Gn initialize predictions levels k K n N v Gn x y features label node v x x Y l n u u N u l k add features neighboring nodes lower levels stack Dmc Dmc y x add multiclass data end end fk MulticlassTrain Dbin train kth level classifier n N Y k n v StackTest f1 fk Gn predict kth level classifier end end return f1 fK return classifiers Algorithm StackTest f1 fK G Y k v k K v G initialize predictions levels k K v G x features node v x x Y l u u N u l k add features neighboring nodes lower levels stack Y k v fk x predict according kth level end end return Y K v v G return predictions node layer best development data performance try find hyperparameters training performance approximately equal devel opment performance This ensure predictions kth layer indicative algorithm actually test time TODO finish discussion Exercises Exercise TODO LINEAR MODELS Dependencies In Chapter learned perceptron algorithm linear classification This model linear classifier algorithm perceptron update rule In section separate consider general ways optimizing linear models This lead aspects optimization aka mathematical programming far At end chapter pointers literature optimization interested The basic idea perceptron run particular algorithm linear separator found You ask better al gorithms finding linear separator We follow idea formulate learning problem explicit optimization prob lem find linear separator complicated We finding optimal separator actually computationally prohibitive need relax optimality requirement This lead convex objective combines loss func tion training data regularizer complicated learned model This learning framework known Tikhonov regularization structural risk mini mization The Optimization Framework Linear Models You seen perceptron way finding weight vector w bias b good job separating positive train ing examples negative training examples The perceptron model algorithm Here interested separating issues We focus linear models like perceptron But think generic ways finding good parameters models The goal perceptron find separating hyperplane training data set For simplicity ignore issue overfitting Not data sets linearly sepa Learning Objectives Define plot surrogate loss functions squared loss logistic loss exponential loss hinge loss Compare contrast optimiza tion loss surrogate loss functions Solve optimization problem squared loss quadratic regularizer closed form Implement debug gradient descent subgradient descent The essence mathematics simple things compli cated complicated things simple Stanley Gudder linear models rable In case training data isn t linearly separable want find hyperplane makes fewest errors training data We write formal mathematics optimization problem follows min w b n yn w xn b In expression optimizing variables w b The objective function thing trying minimize In case objective function simply error rate loss linear classifier parameterized w b In expression indicator function true zero You remember yw x trick perceptron discus sion If convince right thing We know perceptron algorithm guaranteed find parameters model data linearly separable In words optimum Eq zero perceptron efficiently find parameters model The notion efficiency depends margin data perceptron You ask happens data linearly separable Is efficient algorithm finding optimal setting parameters Unfortunately answer There polynomial time algorithm solving Eq P NP In words problem NP hard Sadly proof complicated scope book relies reduction variant satisfiability The key idea turn satisfiability problem optimization problem clause satisfied exactly hyperplane correctly separates data You come okay I don t need exact solution I m willing solution makes errors Unfortunately situation bad Zero loss NP hard appproximately minimize In words efficient algorithm finding solution s small constant worse optimal The best known constant time However getting disillusioned enter prise remember s entire chapter framework going remember optimizing Eq isn t want In particular says minimal training error It says test error like In order try find solution generalize test data need ensure overfit data To introduce regularizer parameters model For vague regularizer looks like simply arbitrary function R w b course machine learning This leads following regularized objective min w b n yn w xn b R w b In Eq trying optimize trade lution gives low training error term solution simple second term You think maximum depth hyperparameter decision tree form regularization trees Here R form regularization hyperplanes In formulation hyperparameter optimization Assuming R right thing value s lead fitting What value s lead underfitting The key remaining questions given formalism How adjust optimization problem efficient algorithms solving What good regularizers R w b hyperplanes Assuming adjust optimization problem appropriately algorithms exist efficiently solving regularized opti mization problem We address questions sections Convex Surrogate Loss Functions You ask optimizing zero loss hard Intuitively reason small changes w b large impact value objective function For instance positive training example w x b adjusting b wards decrease error rate But adjusting upwards effect This makes difficult figure good ways adjust parameters Figure plot zero versus margin To clearly useful look plots relate margin loss Such plot zero loss shown Figure In plot horizontal axis measure margin data point vertical axis measures loss associated margin For zero loss story simple If positive margin e y w x b loss zero Otherwise loss By thinking plot changes parameters change margin little bit enormous effect overall loss Figure plot zero versus margin S version You decide reasonable way address problem replace non smooth zero loss smooth approxima tion With bit effort probably concoct S shaped function like shown Figure The benefit S function smooth potentially easier optimize The difficulty convex Figure plot convex non convex functions chords linear models If remember calculus convex function looks like happy face On hand concave function looks like sad face _ easy mnemonic hide concave function There equivalent definitions concave function The s second derivative non negative The second geometric defition chord function lies This shown Figure There convex function non convex function chords drawn In case convex function chords lie function In case non convex function parts chord lie function Convex functions nice easy minimize Intu itively drop ball convex function tually minimum This true non convex functions For example drop ball left end S function Figure This leads idea convex surrogate loss functions Since zero loss hard optimize want optimize instead Since convex functions easy optimize want approximate zero loss convex function This approxi mating function called surrogate loss The surrogate losses construct upper bounds true loss function guarantees minimize surrogate loss pushing real loss Figure surrogate loss fns There common surrogate loss function properties hinge loss logistic loss exponential loss squared loss These shown Figure defined These defined terms true label y predicted value y w x b Zero y y yy Hinge hin y y max yy Logistic log y y log log exp yy Exponential exp y y exp yy Squared sqr y y y y In definition logistic loss 1log term sim ply ensure log y This ensures like surrogate loss functions logistic loss upper bounds zero loss In practice people typically omit constant affect optimization There big differences loss functions The difference upset erroneous predictions In course machine learning case hinge loss logistic loss growth function y goes negative linear For squared loss exponential loss super linear This means exponential loss examples little wrong example wrong The difference deal confident correct predictions Once yy hinge loss care logistic exponential think better On hand squared loss thinks s bad predict positive example predict positive example Weight Regularization In learning objective Eq term correspond zero loss training data plus regularizer goal ensure learned function didn t crazy Or formally ensure function overfit If place zero loss surrogate loss obtain following objective min w b n yn w xn b R w b The question R w b look like From discussion surrogate loss function like ensure R convex Otherwise point optimization difficult Beyond common desire components weight vector e wds small close zero This form inductive bias Why small values wd good Or precisely small values wd correspond simple functions Suppose example x label We believe ex amples x nearby x label For example I obtain x taking x changing component small value e leaving rest think classification If difference tween y y exactly ew1 So w1 reasonably small unlikely effect classification decision On hand w1 large large effect Another way saying thing look derivative predictions function w1 The derivative w x b respect w1 w x b w1 d wdxd b w1 x1 Interpreting derivative rate change rate change prediction function proportional linear models individual weights So want function change slowly want ensure weights stay small One way accomplish simply use norm weight vector Namely R norm w b w d w d This function convex smooth makes easy minimize In prac tice s easier use squared norm R sqr w b w d w2d removes ugly square root term remains convex An alternative sum squared weights use sum absolute weights R abs w b d wd Both norms convex Why regularize bias term b In addition small weights good argue zero weights better If weight wd goes zero means feature d classification decision If large number irrelevant features want weights zero possible This suggests alternative regularizer R cnt w b d xd Why want use R cnt regularizer This line thinking leads general concept p norms Technically called p ell p norms notation clashes use loss This family norms general flavor We write w p denote p norm w w p d wd p p You check norm exactly corresponds usual Eu clidean norm norm corresponds absolute regularizer described You actually identify R cnt regularizer p norm Which value p gives Hint limit Figure loss norms2d level sets p norms When p norms regularize weight vectors interest ing aspect trade multiple features To behavior p norms dimensions plot contour level set Figure shows contours p norms dimensions Each line denotes dimensional vectors norm assignes total value By changing value p interpolate square called max norm circle norm diamond norm pointy star shaped thing p norm The max norm corresponds limp Why called max norm In general smaller values p prefer sparser vectors You noticing contours small p norms stretch axes It reason small p norms tend yield weight vectors zero entries aka sparse weight vec tors Unfortunately p norm non convex As guess means norm popular choice sparsity seeking applications course machine learning sure closed squared error MATH REVIEW GRADIENTS Figure Algorithm GradientDescent F K z initialize variable optimizing k K g k zF z k compute gradient current location z k z k k g k step gradient end return z K Optimization Gradient Descent Envision following problem You taking new hobby blindfolded mountain climbing Someone blindfolds drops mountain Your goal peak mountain quickly possible All feel mountain standing steps How mountain Perhaps feel find direc tion feels upward step direction If repeatedly hope moun tain Actually friend promises drop purely concave mountains eventually peak The idea gradient based methods optimization exactly Suppose trying find maximum function f x The optimizer maintains current estimate parameter interest x At step measures gradient function trying optimize This measurement occurs current location x Call gradient g It takes step direction gradient size step controlled parameter eta The complete step x x g This basic idea gradient ascent The opposite gradient ascent gradient descent All learning problems framed minimization problems trying reach ditch hill There fore descent primary approach use One major conditions gradient ascent able find true global min imum objective function convexity Without convexity lost The gradient descent algorithm sketched Algorithm The function takes arguments function F minimized number iterations K run sequence learning rates linear models K This address case want start mountain climbing taking large steps small steps close peak The real work need apply gradient descent method able compute derivatives For concreteness suppose choose exponential loss loss function norm regularizer Then regularized objective function L w b n exp yn w xn b w The strange thing objective replaced The reason change gradients cleaner We compute derivatives respect b L b b n exp yn w xn b b w n b exp yn w xn b n b yn w xn b exp yn w xn b n yn exp yn w xn b Before proceeding worth thinking says From practical perspective optimization operate updating b b L b Consider positive examples examples yn We hope examples current prediction w xn b large possible As value tends term exp goes zero Thus points contribute step However current prediction small exp term positive non zero This means bias term b increased exactly want Moreover points classified derivative goes zero This considered case posi tive examples What happens negative examples Now easy case let s gradient respect w wL w n exp yn w xn b w w n w yn w xn b exp yn w xn b w n ynxn exp yn w xn b w Now repeat previous exercise The update form w w wL For classified points ones tend yn gradient near zero For poorly classified points course machine learning gradient points direction ynxn update form w w cynxn c constant This like perceptron update Note c large poorly classified points small relatively classified points By looking gradient related regularizer update says w w w w This effect shrinking weights zero This exactly expect regulaizer Figure good bad step sizes The success gradient descent hinges appropriate choices step size Figure shows happen gradient descent poorly chosen step sizes If step size big accidentally step optimum end oscillating If step size small way long optimum For chosen step size gradient descent approach optimal value fast rate The notion convergence objective value converges true minimum Theorem Gradient Descent Convergence Under suitable condi tions1 appropriately chosen constant step size e Specifically function opti mized needs strongly convex This true problems pro vided For rate bad O k convergence rate gradient descent O k More specifi cally letting z global minimum F F z k F z z z k A naive reading theorem choose huge values It obvi ous right What missing The proof theorem bit complicated makes heavy use linear algebra The key set learning rate L L maximum curvature function optimized The curvature simply size second derivative Functions high curvature gradients change quickly means need small steps avoid overstepping optimum This convergence result suggests simple approach decid ing stop optimizing wait objective function stops changing An alternative wait parameters stop changing A final example percep tron early stopping Every iteration check performance current model held data stop optimizing performance plateaus From Gradients Subgradients As good exercise try deriving gradient descent update rules different loss functions different regularizers ve learned However notice hinge loss norm regularizer differentiable In linear models particular norm differentiable wd hinge loss differentiable yy The solution use subgradient optimization One way think subgradients think essen tially need ignore fact forgot function wasn t differentiable try apply gradient descent To concrete consider hinge function f z max z This function differentiable z differentiable z differentiable z You derive differentiation parts z f z z z z z z z z z z z z Figure hinge loss sub Thus derivative zero z z matching intuition Figure At non differentiable point z use subderivative generalization derivatives non differentiable functions Intuitively think derivative f z tangent line Namely line touches f z f convex functions The subderivative denoted f set lines At differentiable positions set consists actual derivative At non differentiable positions contains slopes define lines lie function contact operating point This shown pictorally Figure example subderivatives shown hinge loss function In particular case hinge loss value valid subderivative z In fact subderivative closed set form b b derived looking limits left right This gives way computing derivative like things non differentiable functions Take hinge loss example For given example n subgradient hinge loss computed w max yn w xn b w yn w xn b yn w xn b w0 yn w xn b wyn w xn b yn w xn b ynxn course machine learning Algorithm HingeRegularizedGD D MaxIter w b initialize weights bias iter MaxIter g g initialize gradient weights bias x y D y w x b g g y x update weight gradient g g y update bias derivative end end g g w add regularization term w w g update weights b b g update bias end return w b MATH REVIEW MATRIX MULTIPLICATION AND INVERSION Figure If plug subgradient form Algorithm obtain Algorithm This subgradient descent regularized hinge loss norm regularizer Closed form Optimization Squared Loss Although gradient descent good generic optimization algorithm cases better An example case norm regularizer squared error loss function For actually obtain closed form solution optimal weights How obtain need rewrite optimization problem terms matrix operations For simplicity consider unbiased version extension Exercise This precisely linear regression setting You think training data large matrix X size N D Xn d value dth feature nth example You think labels column tall vector Y dimension N Finally think weights column vector w size D Thus matrix vector product Xw dimension N In particular Xw n d Xn dwd This means particular actually predictions model Instead calling Y The squared error linear models says minimize n Y n Yn written vector form minimization Y Y Verify squared error actually written vector norm This expanded visually x1 x1 x1 D x2 x2 x2 D xN xN xN D X w1 w2 wD w d x1 dwd d x2 dwd d xN dwd Y y1 y2 yN Y So compactly optimization problem written min w L w Xw Y w If recall calculus minimize function setting derivative zero We start weights w gradients wL w X Xw Y w X Xw X Y w X X I w X Y We equate zero solve yielding X X I w X Y X X ID w X Y w X X ID 1X Y Thus optimal solution weights computed matrix multiplications matrix inversion As sanity check sure dimensions match The matrix X X dimension D D inverse term The inverse D D X D N product D N Multiplying N vector Y yields D vector precisely want weights For keen linear algebra worried matrix invert invertible Is actually problem Note gives exact solution modulo numerical innacu racies computing matrix inverses In contrast gradient descent progressively better solutions eventually converge optimum rate k This means want answer s accuracy e need order thousand steps The question getting exact solution efficient To run gradient descent step O ND time relatively small constant You run K iterations course machine learning yielding overall runtime O KND On hand closed form solution requires constructing X X takes O D2N time The inversion O D3 time standard matrix inver sion routines The final multiplications O ND time Thus overall runtime order O D3 D2N In standard cases true time N D domi nated O D2N Thus overall question need run D iterations gradient descent If matrix inversion roughly faster Otherwise gradient descent roughly faster For low medium dimensional problems D probably faster closed form solution matrix inversion For high dimensional problems D probably faster gradient descent For things middle s hard sure Support Vector Machines At beginning chapter looked convex surrogate loss functions asked come They derived different underlying principles essentially correspond different inductive biases Figure picture data points hyperplanes RGB G best Let s start thinking original goal linear classifiers find hyperplane separates positive training examples negative ones Figure shows data po tential hyperplanes red green blue Which like best Most likely chose green hyperplane And likely chose furthest away closest training points In words large margin The desire hyperplanes large margins perfect example inductive bias The data tell hyperplanes best choose source information Following line thinking leads support vector ma chine SVM This simply way setting optimization problem attempts find separating hyperplane large margin possible It written constrained optimization problem min w b w b subj yn w xn b n In optimization trying find parameters maximize margin denoted e minimize reciprocal margin linear models subject constraint training examples correctly classi fied Figure hyperplane margins sides The odd thing optimization problem quire classification point greater simply greater zero However problem doesn t fundamen tally change replace positive constant Exercise As shown Figure constant interpreted visually ensuring non trivial margin positive points negative points The difficulty optimization problem Eq happens data linearly separable In case set parameters w b simultaneously satisfy constraints In optimization terms feasible region The feasible region simply set parame ters satify constraints For reason refered hard margin SVM enforcing margin hard con straint The question modify optimization problem handle inseparable data Figure bad point slack The key idea use slack parameters The intuition slack parameters following Suppose find set param eters w b good job data points The points perfectly classifed achieve large margin But s pesky data point left proper margin noisy See Figure You want able pretend point hyperplane proper You pay little bit long aren t moving lot points good idea In picture point denoted xi By introducing slack parameter training example penalizing having use slack create objective function like following soft margin SVM min w b w b large margin C n n small slack subj yn w xn b n n n n The goal objective function ensure points correctly classified constraint But point n correctly classified set slack n greater zero correct direction However non zero slacks pay objective function proportional slack The hyperparameter C controls overfitting course machine learning versus underfitting The second constraint simply says negative slack What values C lead fitting What values lead underfitting One major advantage soft margin SVM original hard margin SVM feasible region That going solution regardless training data linearly separable Suppose I data set Without looking data construct feasible solution soft margin SVM What value objective solution It s thing write optimization problem It s thing try solve There large number ways optimize SVMs essentially popular learning model Here talk simple way More complex methods discussed later book bit background To progress need able measure size margin Suppose gives parameters w b optimize hard margin SVM We wish measure size margin The observation hyperplane lie exactly halfway nearest positive point nearest negative point If margin bigger simply sliding way adjusting bias b Figure copy figure p5 cs544 svm tutorial By observation positive example lies exactly unit hyperplane Call x w x b Similarly negative example x lies exactly margin w x b These points x x way measure size margin As shown Figure measure size margin looking difference lengths projections x x hyperplane Since projection requires normalized vector measure distances d w w x b d w w x b We compute margin algebra d d w w x b w w x b w w x w w x w w w linear models This remarkable conclusion size margin inversely proportional norm weight vector Thus maximizing margin equivalent minimizing w This serves addi tional justification norm regularizer having small weights means having large margins However goal wasn t justify regularizer un derstand hinge loss So let soft margin SVM plug new knowledge margins min w b w large margin C n n small slack subj yn w xn b n n n n Now let s play thought experiment Suppose handed solution optimization problem consisted weights w bias b forgot slacks Could recover slacks information In fact answer yes For simplicity let s consider positive examples Suppose look positive example xn You need figure slack n There cases Either w xn b If s large want set n Why It zero second constraint Moreover set greater zero pay unnecessarily objective So case n Next suppose w xn b big In order satisfy constraint ll need set n But objective ll want set larger necessary ll set n exactly Following argument positive negative points gives solutions w b automatically compute optimal variables n yn w xn b yn w xn b In words optimal value slack variable exactly hinge loss corresponding example Thus write SVM objective unconstrained optimization problem min w b w large margin C n hin yn w xn b small slack Multiplying objective C obtain exactly reg ularized objective Eq hinge loss loss function norm regularizer course machine learning TODO justify term dimensional projections Exercises Exercise TODO PROBABILISTIC MODELING Dependencies Many models algorithms learned far relatively disconnected There alternative view machine learning unites generalizes learned This probabilistic modeling framework explicitly think learning problem statistical inference In chapter learn flavors probabilistic models generative conditional You ap proaches supervised unsupervised seen cast probabilistic models Through new view able develop learning algorithms inductive biases closer designer believe Moreover chap ters follow heavy use probabilistic modeling approach open doors learning problems Classification Density Estimation Our underlying assumption majority book learning problems characterized unknown probability distribution D input output pairs x y X Y Suppose told D In particular gave Python function computeD took inputs x y returned probability x y pair D If access func tion classification simple We define Bayes optimal classifier classifier test input x simply returns y maximizes computeD x y formally f BO x arg max y Y D x y This classifier optimal specific sense possible classifiers achieves smallest zero error Theorem Bayes Optimal Classifier The Bayes Optimal Classifier f BO achieves minimal zero error deterministic classifier Learning Objectives Define generative story naive Bayes classifier Derive relative frequency lution constrained optimization problem Compare contrast generative conditional discriminative learning Explain generative models likely fail Derive logistic loss regularizer probabilistic perspective course machine learning chain rule marginalization Bayes rule MATH REVIEW RULES OF PROBABILITY Figure This theorem assumes comparing deterministic classifiers You actually prove stronger result f BO opti mal randomized classifiers proof bit messier However intuition given x f BO chooses label highest probability minimizing probability makes error Proof Theorem Consider classifier g claims better f Then x g x f x Fix x Now probability f makes error particular x D x f BO x probability g makes error x D x g x But f BO chosen way maximize D x f BO x greater D x g x Thus probability f errs particular x smaller probability g errs This applies x f x g x f achieves smaller zero error g The Bayes error rate Bayes optimal error rate error rate Bayes optimal classifier It best error rate hope achieve classification problem zero loss The home message gave access data distribution forming optimal classifier trivial Unfortunately gave distribution analysis suggests good way build classifier try estimate D In words try learn distribution D hope similar D use distribution classification Just preceding chapters try form estimate D based finite training set The direct way attempt construct probability distribution select family parametric distribu tions For instance Gaussian Normal distribution parametric s parameters mean covariance The job learning infer parameters best far observed train ing data concerned inductive bias bring A key assumption need training data access drawn independently D In particular draw examples x1 y1 D x2 y2 D nth draw xn yn drawn D depend probabilistic modeling previous n samples This assumption usually false usually sufficiently close true useful Together assumption training data drawn distribution D leads d assumption independently identically distributed assumption This key assumption al machine learning Statistical Estimation Suppose need model coin possibly biased think modeling label binary classification problem observe data HHTH H means flip came heads T means came tails You assume flips came coin flip independent data d Further choose believe coin fixed probability coming heads coming tails Thus parameter model simply scalar Describe case assumptions making coin flip false The basic computation perform maximum like lihood estimation select paramter maximizes probability data parameter In order need compute probability data p D p HHTH definition D p H p H p T p H data independent Thus want parameter maximizes probability data derivative respect set equal zero solve Thus maximum likelihood probably selected intuition You solve problem generally follows If H heads T tails probability data sequence H T You try derivative respect follow recipe products things difficult A course machine learning friendly solution work log likelihood log proba bility instead The log likelihood data sequence H log T log Differentiating respect H T To solve obtain H T H T Thus H H T H H T finally yeilding H H T simply fraction observed data came heads In case maximum likelihood estimate relative frequency observing heads How know solution H H T actually maximum Now suppose instead flipping coin rolling K sided die instance pick label multiclass classification problem You model saying parameters K specifying respectively probabilities given comes role Since probabilities k zero sum ks Given data set consists x1 rolls x2 rolls probability data k xk k yielding log probability k xk log k If pick particular parameter deriva tive respect x3 want equate zero This leads This obviously wrong From mathematical formulation s correct fact setting ks maximize k xk k non negative xks The problem need constrain s sum In particular constraint k k forgot enforce A convenient way enforce con straints technique Lagrange multipliers To problem consistent standard minimization problems convenient minimize negative log probabilities instead maxi mizing log probabilities Thus constrainted optimization problem min k xk log k subj k k The Lagrange multiplier approach involves adding new variable problem called Lagrange variable corresponding constraint use constraint objective The result case max min k xk log k k k Turning constrained optimization problem s corresponding Lagrangian straightforward The mystical aspect works In case idea follows Think adversary probabilistic modeling trying maximize function trying minimize If pick parameters actually satisfy constraint green term Eq goes zero matter adversary On hand constraint slightly unsatisfied tend blow objective So order non infinite objective value optimizer find values satisfy constraint If solve inner optimization Eq differentiating respect x1 yielding x1 In general solution k xk Remembering goal enforce sums constraint set k xk verify solution Thus optimal k xk k xk completely corresponds intuition Naive Bayes Models Now consider binary classification problem You looking parameterized probability distribution describe training data To concrete task predict movie review positive negative label based words features appear review Thus probability single data point written p y x p y x1 x2 xD The challenge working probability distribution like Eq s distribution lot variables You try sim plify applying chain rule probabilities p x1 x2 xD y p y p x1 y p x2 y x1 p x3 y x1 x2 p xD y x1 x2 xD p y d p xd y x1 xd At point equality exact probability distribution However difficult craft probability distribution 10000th feature given previous Even difficult accurately estimate At point assumptions A classic assumption called naive Bayes sumption features independent conditioned label In movie review example saying know s positive review probability word excellent appears independent amazing appeared Note imply words independent course machine learning don t know label certainly Formally assumption states Assumption p xd y xd p xd y d d Under assumption simplify Eq p y x p y d p xd y naive Bayes assumption At point start parameterizing p Suppose labels binary features binary In case model label biased coin probability heads e g positive review given Then label imagine having biased coin feature So D features ll 2D total coins label label feature combination In movie review example expect percent movie reviews positive high probability words like excellent amazing good high probability words like terrible boring hate You rewrite probability single example follows eventually leading log probability entire data set p y x p y d p xd y naive Bayes assumption y y d xd y d y d xd model assumptions Solving identical solving biased coin case relative frequency positive labels data doesn t depend x For parameters repeat exercise 2D coins independently This yields N n yn d n yn xn d n yn d n yn xn d n yn In case features binary need choose dif ferent model p xd y The model chose Bernouilli distribution effectively distribution independent probabilistic modeling remove people discrete bernoulli binomial multinomial gaussian distributions MATH REVIEW COMMON PROBABILITY DISTRIBUTIONS Figure coin flips For types data distributions appropriate The die example corresponds discrete distribution If data continuous choose use Gaussian distribution aka Normal distribution The choice dis tribution form inductive bias inject knowledge problem learning algorithm Prediction Consider predictions naive Bayes model Bernoulli features Eq You better understand model con sidering decision boundary In case probabilistic models decision boundary set inputs likelihood y precisely Or words set inputs x p y x p y x In order thing notice p y x p y x p x In ratio p x terms cancel leaving p y x p y x Instead computing ratio easier compute log likelihood ratio LLR log p y x log p y x computed LLR log d xd d d xd log d xd d d xd model assumptions log log d xd log d log d d xd log d log d logs rearrange d xd log d d d xd log d d log simplify log terms d xd log d d log d d d log d d log group x terms x w b course machine learning wd log d d d d b d log d d log The result algebra naive Bayes model precisely form linear model Thus like perceptron models ve previous studied decision boundary linear TODO MBR Generative Stories A useful way develop probabilistic models tell generative story This fictional story explains believe training data came existence To things interesting con sider multiclass classification problem continuous features modeled independent Gaussians Since label values K use discrete distribution die roll model opposed Bernoilli distribution For example n N Choose label yn Disc b For feature d D Choose feature value xn d Nor yn d yn d This generative story directly translated likelihood function replacing s products p D example n yn choose label d 22yn d exp 22yn d xn d yn d choose feature value feature You logs arrive log likelihood log p D n log yn d log 2yn d 22yn d xn d yn d const To optimize need add sums constraint This leads previous solution ks propor tional number examples label k In case s probabilistic modeling derivative respect k obtain log p D k k n d 22yn d xn d yn d ignore irrelevant terms k n yn k 22k d xn k ignore irrelevant terms n yn k 2k d xn k derivative Setting equal zero solving yields k n yn k xn n yn k Namely sample mean ith feature data points fall class k A similar analysis 2k yields log p D 2k 2k y yn k log 2k 22k xn k ignore irrelevant terms y yn k 22k 2k xn k derivative 24k y yn k xn k 2k simplify You set equal zero solve yielding 2k n yn k xn k n yn k Which sample variance feature class k What estimate decided given class k features equal variance What assumed feature equal variance class Under circumstances good idea assumptions Conditional Models In foregoing examples task formulated attempting model joint distribution x y pairs This wasteful prediction time care p y x model directly Starting case regression actually somewhat simpler starting classification case Suppose believe course machine learning relationship real value y vector x linear That expect y w x b hold parameters w b Of course data exactly obey s fine think deviations y w x b noise To form probabilistic model assume distribution noise convenient choice zero mean Gaussian noise This leads following generative story For example n N Compute tn w xn b b Choose noise en Nor c Return yn tn en In story variable tn stands target It noiseless variable observe Similarly en error noise example n The value actually observe yn tn en See Figure Figure pictorial view targets versus labels A basic property Gaussian distribution additivity Namely Nor b c b Nor c Given generative story derive shorter gener ative story For example n N Choose yn Nor w xn b Reading log likelihood dataset generative story obtain log p D n log w xn b yn model assumptions n w xn b yn const remove constants This precisely linear regression model encountered Section To minimizing negative log probability need solve regression coefficients w b In case binary classification Gaussian noise model sense Switching Bernoulli model de scribes binary outcomes makes sense The remaining difficulty parameter Bernoulli value zero probability heads model produce values A classic approach produce real valued target transform target value zero probabilistic modeling maps maps A function logistic function1 defined plotted Figure Also called sigmoid function s S shape Figure sketch logistic function Logistic function z exp z exp z exp z The logistic function nice properties verify z z z z2 z Using logistic function write generative story binary classification For example n N Compute tn w xn b b Compute zn Ber tn c Return yn 2zn The log likelihood model log p D n yn log w xn b yn log w xn b model properties n log yn w xn b join terms n log exp yn w xn b definition n log yn w xn b definition log As log likelihood precisely negative scaled version logistic loss Chapter This model logistic regression model logisitic loss originally derived TODO conditional versus joint Regularization Priors In foregoing discussion parameters model selected according maximum likelihood criteria find parameters maximize p D The trouble approach easy simple coin flipping example If flip coin twice comes heads times maximum likelihood estimate course machine learning bias coin come heads This true flipped If course flipped million times come heads time find reasonable solution This clearly undesirable behavior especially data expen sive machine learning setting One solution seek parameters balance tradeoff likelihood data prior belief values parameters likely Taking case logistic regression priori believe small values w likely large values choose represent Gaussian prior component w The maximum posteriori principle method incoporat ing data prior beliefs obtain balanced parameter estimate In abstract terms consider probabilistic model data D parameterized parameters If think pa rameters random variable write model p D maximum likelihood amounts choosing maximize p D However instead maximize probability parameters given data Namely maximize p D This term known posterior distribution computed Bayes rule p D posterior prior p likelihood p D p D evidence p D dp p D This reads posterior equal prior times likelihood di vided evidence The evidence scary looking term The evidence called marginal likelihood integral note perspective seeking parameters maximize posterior evidence constant depend ignored Returning logistic regression example Gaussian priors weights log posterior looks like log p D n log yn w xn b d w2d const model definition n log yn w xn b w reduces regularized logistic function squared norm regularizer weights A norm regularizer probabilistic modeling obtained Laplace prior w Gaussian prior w Exercises Exercise TODO NEURAL NETWORKS Dependencies The learning models learned decision trees nearest neighbor models created complex non linear decision boundaries We moved perceptron classic linear model At point non linear learning models learned linear learning far This chapter presents extension perceptron learning non linear decision boundaries taking biological inspiration neu rons In perceptron thought input data point e g image directly connected output e g label This called single layer network layer weights Now instead directly connecting inputs outputs insert layer hidden nodes moving single layer network multi layer network But introducing non linearity inner layers non linear decision boundaires In fact networks able express function want linear functions The trade flex ibility increased complexity parameter tuning model design Bio inspired Multi Layer Networks One major weaknesses linear models like perceptron regularized linear models previous chapter linear Namely unable learn arbitrary decision bound aries In contrast decision trees KNN learn arbitrarily complicated decision boundaries Figure picture layer network inputs hidden units One approach chain collection perceptrons build complex neural networks An example layer network shown Figure Here inputs features fed hidden units These hidden units fed single output unit Each edge figure corresponds different weight Even looks like layers called layer network don t count Learning Objectives Explain biological inspiration multi layer neural networks Construct layer network solve XOR problem Implement propogation algorithm training multi layer networks Explain trade depth breadth network structure Contrast neural networks ra dial basis functions k nearest neighbor learning neural networks inputs real layer That s layers trained weights Prediction neural network straightforward generaliza tion prediction perceptron First compute activations nodes hidden unit based inputs input weights Then compute activations output unit given hidden unit activations second layer weights The major difference computation ceptron computation hidden units compute non linear function inputs This usually called activation function link function More formally wi d weights edge connecting input d hidden unit activation hidden unit computed hi f wi x Where f link function wi refers vector weights feeding node One example link function sign function That incoming signal negative activation Otherwise activation This potentially useful activiation function guessed problem non differentiable Figure picture sign versus tanh EXPLAIN BIAS A popular link function hyperbolic tangent function tanh A comparison sign function tanh function Figure As reasonable approximation sign function convenient differentiable It s derivative tanh2 x Because looks like S Greek character S Sigma functions usually called sigmoid functions Assuming tanh link function overall prediction layer network computed Algorithm This function takes matrix weights W cor responding layer weights vector weights v corre sponding second layer You write entire computation line y vi tanh wi x v tanh Wx Where second line short hand assuming tanh vector input product vector output Is necessary use link function What happen identify function link course machine learning Algorithm TwoLayerNetworkPredict W v x number hidden units hi tanh wi x compute activation hidden unit end return v h compute output unit y x0 x1 x2 Table Small XOR data set The claim layer neural networks expressive single layer networks e perceptrons To construct small layer network solving XOR prob lem For simplicity suppose data set consists data points given Table The classification rule y x1 x2 features You solve problem layer network hidden units The key idea hidden unit compute function x1 x2 The second hidden unit compute function x1 x2 The output combine single prediction mimics XOR Once hidden unit activate second need set output weights respectively Verify output weights actually XOR To achieve behavior start setting bias weights real features You check right thing link function sign function Of course s s tanh To tanh mimic sign need dot product large small You accomplish set ting bias weights Now activation unit slightly x slightly examples This shows create function How create function At point ve seen layer networks aka percep trons represent linear function linear functions You ve seen layer networks represent non linear functions like XOR A natural question additional representational power moving layers The answer partially provided following Theorem originally George Cybenko particular type link function ex tended later Kurt Hornik arbitrary link functions Theorem Two Layer Networks Universal Function Approxima tors Let F continuous function bounded subset D dimensional space Then exists layer neural network F finite number hidden units approximate F arbitrarily Namely x domain F F x F x e Or colloquial terms layer networks approximate neural networks function This remarkable theorem Practically says function F error tolerance parameter e I construct layer network computes F In sense says going layer layers completely changes representational capacity model When working layer networks key question hidden units I If data D dimensional K hidden units total number parameters D K The bias second second layer weights Following heuristic examples parameter trying estimate suggests method choosing number hid den units roughly bND c In words tons tons examples safely lots hidden units If examples probably restrict number hidden units network The number units form inductive bias form regularization In view number hidden units controls complex function Lots hidden units complicated function Figure shows training test error neural networks trained different numbers hidden units As number increases training performance continues better But point test performance gets worse network overfit data The Back propagation Algorithm The propagation algorithm classic approach training neural networks Although originally seen way based know chapter summarize propagation propagation gradient descent chain rule More specifically set exactly You going optimize weights network minimize ob jective function The difference predictor longer linear e y w x b non linear e v tanh Wx The question gradient descent compli cated objective For ignore idea regularization This reasons The know deal regular ization ve learned applies The second historically neural networks regularized Instead course machine learning people early stopping method controlling overfit ting Presently s obvious better solution valid options To completely explicit focus optimizing squared error Again historic reasons You easily replace squared error loss function choice Our overall objective min W v n yn vi f wi xn Here f link function like tanh The easy case differentiate respect v weights output unit Without math able guess looks like The way think vs perspective linear model attempting minimize squared error The funny thing inputs activa tions h examples x So gradient respect v linear case To things notationally convenient let en denote error nth example e blue term let hn denote vector hidden unit activations example Then v n enhn This exactly like linear case One way interpreting output weights change prediction better This easy question answer easily measure changes affect output The complicated aspect deal weights corre sponding layer The reason difficult weights layer aren t necessarily trying produce specific values They simply trying produce acti vations fed output layer So change want depends crucially output layer interprets Thankfully chain rule calculus saves Ignoring sum data points compute L W y vi f wi x L wi L fi fi wi L fi y vi f wi x vi evi fi wi f wi x x neural networks Algorithm TwoLayerNetworkTrain D K MaxIter W D K matrix small random values initialize input layer weights v K vector small random values initialize output layer weights iter MaxIter G D K matrix zeros initialize input layer gradient g K vector zeros initialize output layer gradient x y D K ai wi x hi tanh ai compute activation hidden unit end y v h compute output unit e y y compute error g g eh update gradient output layer K Gi Gi evi tanh2 ai x update gradient input layer end end W W G update input layer weights v v g update output layer weights end return W v Putting gradient respect wi wi evi f wi x x Intuitively sense If overall error predictor e small want small steps If vi small hidden unit means output particularly sensitive activation ith hidden unit Thus gradient small If vi flips sign gradient wi flip signs The propagation comes fact propagate gradients backward network starting end The complete instantiation gradient descent layer network K hidden units sketched Algorithm Note exactly gradient descent algorithm different computation gradients input layer moderately complicated What happen algo rithm wanted optimize exponential loss instead squared error What wanted add weight regularization As bit practical advice implementing propagation algorithm bit tricky Sign errors abound A useful trick W fixed work training v Then v fixed work training W Then If like matrix calculus derive algorithm starting Eq course machine learning Initialization Convergence Neural Networks Based know linear models tempted initialize weights neural network zero You noticed Algorithm s initialized small random values The question The answer initialization W v lead uninteresting solutions In words initialize model way eventually stuck bad local optimum To realize example x activation hi hidden units zero W This means iteration gradient output weights v zero stay Furthermore gradient w1 d dth feature ith unit exactly gradient w2 d feature second unit This means weight matrix gradient step change exactly way hidden unit Thinking example iterations values hidden units exactly means weights feeding hidden units exactly Eventually model converge converge solution advantage having access hidden units This shows neural networks sensitive initialization In particular function optimize non convex meaning plentiful local optima One trivial local optimum described preceding paragraph In sense neural networks local optima Suppose layer network hidden units s optimized You weights w1 inputs hidden unit weights w2 puts second hidden unit weights v1 v2 hidden units output If I network w1 w2 swapped v1 v2 swapped network computes exactly thing markedly different weight structure This phenomena known symmetric modes mode referring optima meaning symmetries weight space It thing lots modes symmetric finding good finding Unfortunately additional local optima global optima Figure convergence randomly initialized networks Random initialization weights network way address problems By initializing network small random weights uniform network unlikely fall trivial symmetric local optimum Moreover training collection networks different random neural networks initialization obtain better solutions initialization In words train networks different random seeds pick best held data Figure shows prototypical test set performance networks different random initialization plus eleventh plot trivial symmetric network initialized zeros One typical complaints neural networks finicky In particular large number knobs tune The number layers The number hidden units layer The gradient descent learning rate The initialization The stopping iteration weight regularization The minor early stopping easy regularization method require effort tune somewhat significant Even layer networks having choose number hidden units learning rate initialization right bit work Clearly automated nonetheless takes time Another difficulty neural networks weights difficult interpret You ve seen linear networks interpret high weights indicative positive examples low weights indicative negative examples In multilayer networks difficult try understand different hidden units Beyond Two Layers Figure multi layer network The definition neural networks propagation algo rithm generalized layers arbitrary directed acyclic graph In practice common use layered net work like shown Figure strong rea son aka inductive bias different However view directed graph sheds different sort insight propagation algorithm Figure DAG network Suppose network structure stored directed acyclic graph like Figure We index nodes graph u v The activation applying non linearity node au non linearity hu The graph single sink output node y activation ay non linearity performed course machine learning Algorithm ForwardPropagation x input nodes u hu corresponding feature x end nodes v network parent s computed av u par v w u v hu hv tanh av end return ay Algorithm BackPropagation x y run ForwardPropagation x compute activations ey y ay compute overall network error nodes v network error ev computed u par v gu v evhu compute gradient edge eu eu evwu v tanh2 au compute error parent node end end return gradients ge output unit The graph D inputs e nodes parent activations hu given input example An edge u v parent child e input hidden unit hidden unit sink Each edge weight wu v We par u set parents u There relevant algorithms forward propagation propagation Forward propagation tells compute activation sink y given inputs Back propagation computes derivatives edge weights given input Figure picture forward prop The key aspect forward propagation algorithm iter atively compute activations going deeper deeper DAG Once activations parents node u com puted compute activation node u This spelled Algorithm This explained pictorially Figure Figure picture prop Back propagation Algorithm opposite com putes gradients network The key idea compute error node network The error output unit true error For input unit error gradi ent coming children e higher network These errors computed backwards network propagation gradients This explained pictorially Figure Given propagation algorithm directly run gradi ent descent subroutine computing gradients neural networks Breadth versus Depth At point ve seen train layer networks train arbitrary networks You ve seen theorem says layer networks universal function approximators This begs question layer networks great care deeper networks To understand answer borrow ideas CS theory idea circuit complexity The goal functions good idea use deep network In words functions require huge number hidden units force network shallow small number units allow deep The example ll use parity function ironically generalization XOR problem The function defined binary inputs parity x d xd mod number 1s x odd number 1s x Figure nnet paritydeep deep function computing parity It easy define circuit depth O log2 D O D gates computing parity function Each gate XOR ar ranged complete binary tree shown Figure If want disallow XOR gate fix allowing depth doubled replacing XOR AND OR NOT combination like beginning chapter This shows allowed deep construct circuit computes parity number hidden units linear dimensionality So shallow circuits The answer It s famous result circuit complexity parity requires exponentially gates compute constant depth The formal theorem Theorem Parity Function Complexity Any circuit depth K log2 D computes parity function D input bits contain Oe D gates This famous result shows constant depth circuits powerful deep circuits Although neural net work isn t exactly circuit generally believed result holds neural networks At gives strong indication depth important considera tion neural networks What neural networks makes theorem circuits apply di rectly One way thinking issue breadth versus depth number parameters need estimated By course machine learning heuristic need roughly examples parameter deep model potentially require exponentially fewer examples train shallow model This flips question deep potentially better doesn t use deep networks There answers First makes architecture selection problem significant Namely use layer network hyperparameter choose hidden units middle layer When choose deep network need choose layers width layers This somewhat daunting A second issue training deep models propagation In general propagation works way model sizes gradients shrink You work mathematically intuition simpler If beginning deep network changing single weight unlikely significant effect output units getting This directly implies derivatives small This turn means propagation essentially moves far initialization run deep networks While small derivatives training difficult good reasons reasons Finding good ways train deep networks active research area There general strategies The attempt ini tialize weights better layer wise initialization strategy This unlabeled data After initializa tion propagation run tweak weights classification problem care A second approach use complex optimization procedure gradient descent You learn procedures later book Basis Functions At point ve seen neural networks mimic linear functions b learn complex functions A rea sonable question mimic KNN classifier efficiently e hidden units A natural way train neural network mimic KNN classifier replace sigmoid link function radial basis function RBF In sigmoid network e network sigmoid links hidden units computed hi tanh wi x In RBF network hidden units computed hi exp wi x neural networks Figure nnet rbfpicture D picture RBF bumps Figure nnet unitsymbols picture nnet sigmoid rbf units In words hidden units behave like little Gaussian bumps centered locations specified vectors wi A dimensional example shown Figure The parameter specifies width Gaussian bump If large data points close wi non zero activations To distinguish sigmoid networks RBF networks hidden units typically drawn sigmoids Gaussian bumps Figure Training RBF networks involves finding good values Gas sian widths centers Gaussian bumps wi con nections Gaussian bumps output unit v This propagation The gradient terms v main unchanged derivates variables differ Exercise One big questions RBF networks Gaussian bumps centered One course apply propagation attempt find centers Another option spec ify ahead time For instance potential approach RBF unit data point centered data point If carefully choose s vs obtain looks nearly identical distance weighted KNN This added advantage futher use propagation learn good Gaussian widths voting factors v nearest neighbor algorithm Consider RBF network hidden unit training point centered point What bad thing happen use propagation estimate s v data careful How careful Exercises Exercise TODO KERNEL METHODS Dependencies Linear models great easy understand easy optimize They suffer learn simple decision boundaries Neural networks learn com plex decision boundaries lose nice convexity properties linear models One way getting linear model behave non linearly transform input For instance adding feature pairs addi tional inputs Learning linear model representation convex computationally prohibitive low dimen sional spaces You ask instead explicitly expanding fea ture space possible stay original data representation feature blow implicitly Surprisingly answer yes family techniques makes possible known kernel approaches From Feature Combinations Kernels In Section learned method increasing expressive power linear models explode feature space For instance quadratic feature explosion map feature vector x x1 x2 x3 xD expanded version denoted x x 2x1 2x2 2x3 2xD x21 x1x2 x1x3 x1xD x2x1 x x2x3 x2xD x3x1 x3x2 x x2xD xDx1 xDx2 xDx3 x D Note repetitions hopefully learning algorithms deal redundant features particular 2x1 terms collapsing repetitions Learning Objectives Explain kernels generalize feature combinations basis functions Contrast dot products kernel products Implement kernelized perceptron Derive kernelized version regularized squares regression Implement kernelized version perceptron Derive dual formulation support vector machine kernel methods You train classifier expanded feature space There primary concerns The computa tional learning algorithm scales linearly number fea tures ve squared computation need perform ve squared memory ll need The second statistical heuristic examples feature need quadratically training examples order avoid overfitting This chapter dealing computational issue It turn Chapter deal statistical issue hope regularization sufficient attenuate overfitting The key insight kernel based learning rewrite linear models way doesn t require ex plicitly compute x To start think purely computational trick enables use power quadratic feature mapping actually having compute store mapped vectors Later s actually bit deeper Most algorithms discuss involve product form w x performing feature mapping The goal rewrite algorithms depend dot products tween examples x z depend x z To understand helpful consider quadratic expansion dot product vectors You x z x1z1 x2z2 xDzD x21z x1xDz1zD xDx1zDz1 xDx2zDz2 x2Dz D d xdzd d e xdxezdze 2x z x z x z Thus compute x z exactly time compute x z plus time takes perform addition multiply nanoseconds circa processor The rest practical challenge rewrite algorithms depend dot products examples explicit weight vectors Kernelized Perceptron Consider original perceptron algorithm Chapter peated Algorithm linear algebra notation fea ture expansion notation x In algorithm places course machine learning Algorithm PerceptronTrain D MaxIter w b initialize weights bias iter MaxIter x y D w x b compute activation example ya w w y x update weights b b y update bias end end end return w b reminder U ui set vectors RD span U set vectors writ linear combinations uis span U aiui a1 R aI R null space U s left RD span U TODO pictures MATH REVIEW SPANS AND NULL SPACES Figure x explicitly The computing activation line second updating weights line The goal remove explicit dependence algorithm weight vector To observe point algorithm weight vector w written linear combination expanded training data In particular point w n n xn parameters Initially w choosing yields If update occurs nth training example resolution weight vector simply yn xn equivalent setting n yn If second update occurs mth training example need update m m ym This true multiple passes data This observation leads following representer theorem states weight vector perceptron lies span training data Theorem Perceptron Representer Theorem During run perceptron algorithm weight vector w span assumed non training data x1 xN Proof Theorem By induction Base case span non set contains zero vector initial weight vec tor Inductive case suppose theorem true kth update suppose kth update happens example n By inductive hypothesis write w xi kernel methods Algorithm KernelizedPerceptronTrain D MaxIter b initialize coefficients bias iter MaxIter xn yn D m m xm xn b compute activation example yna n n yn update coefficients b b y update bias end end end return b update The new weight vector xi yn xn yn n xi span training data Now know write w n n xn additionall compute activations line w x b n n xn x b definition w n n xn x b dot products linear This depends dot products data points explicitly requires weight vector You rewrite entire perceptron algorithm refers explicitly weights depends pairwise dot products examples This shown Algorithm The advantage kernelized algorithm form feature expansions like quadratic feature expansion introduction free For example exactly cost quadratic features use cubic feature map computed x z x z corresponds way inter actions variables And general polynomial degree p computational complexity Kernelized K means For complete change pace consider K means algorithm Section This algorithm clustering notion training labels Instead want partition data coher ent clusters For data RD involves randomly initializing K course machine learning cluster means K The algorithm alternates following steps convergence x replaced x eventual goal For example n set cluster label zn arg mink xn k For cluster k update k 1Nk n zn k xn Nk number n zn k The question perform steps ex plicitly computing xn The representer theorem straight forward perceptron The mean set data definition span data choose ais equal N Thus long initialize means span data guaranteed means span data Given know write mean expansion data k n k n xn parame ters k n N K parameters Given expansion order execute step need compute norms This follows zn arg min k xn k definition zn arg min k xn m k m xm definition k arg min k xn m k m xm xn m k m xm expand quadratic term arg min k m m k m k m xm xm m k m xm xn const linearity constant This computation replace assignments step K means The mean updates direct step k Nk n zn k xn k n Nk zn k What Makes Kernel A kernel form generalized dot product You think simply shorthand x z commonly written K x z Or clear context simply K x z kernel methods This refered kernel product x z mapping In view ve seen preceding sections rewrite perceptron algorithm K means algorithm depend kernel products data points actual datapoints This pow erful notion enabled development large number non linear algorithms essentially free applying called kernel trick ve seen twice This raises interesting question If rewritten algorithms depend data function K X X R stick function K algorithms K forbidden In sense use K real question types functions K algorithms retain properties expect like convergence optimality etc One way answer question K valid kernel corresponds inner product vectors That K valid exists function K x z x z This direct definition clear K satisfies algorithms expected derived You ve seen general class polynomial kernels form K poly d x z x z d d hyperparameter kernel These kernels correspond polynomial feature expansions There alternative characterization valid kernel function mathematical It states K X X R kernel K positive semi definite shorthand psd This property called Mercer s condition In context means functions f square integrable e f x 2dx zero function following property holds f x K x z f z dxdz This likely like came Unfortunately connection scope book covered external sources For simply given equivalent requirement For inclined appendix book gives proof requires bit knowledge function spaces understand The question alternative characterization useful It useful gives alternative way construct kernel course machine learning functions For instance easily prove following difficult definition kernels inner prod ucts feature mappings Theorem Kernel Addition If K1 K2 kernels K defined K x z K1 x z K2 x z kernel Proof Theorem You need verify positive semi definite property K You follows f x K x z f z dxdz f x K1 x z K2 x z f z dxdz definition K f x K1 x z f z dxdz f x K2 x z f z dxdz distributive rule K1 K2 psd More generally positive linear combination kernels kernel Specifically K1 KM kernels M K x z m mKm x z kernel You use property following Gaus sian kernel called RBF kernel psd K RBF x z exp x z Here hyperparameter controls width Gaussian like bumps To gain intuition RBF kernel consider prediction looks like perceptron f x n nK xn x b n n exp xn z In computation training example getting vote label test point x The vote nth training example gets proportional negative exponential dis tance test point This like RBF neural network Gaussian bump training example variance ns act weights connecting RBF bumps output Showing kernel positive definite bit exercise analysis particularly integration parts difficult Again proof provided appendix kernel methods So far seen bsaic classes kernels polynomial kernels K x vz x z d includes linear kernel K x z x z RBF kernels K x z exp x z The direct connection feature expansion RBF networks You know combine kernels new kernels addition In fact product kernels kernel As far library kernels goes Polynomial RBF far popular A commonly techni cally invalid kernel hyperbolic tangent kernel mimics behavior layer neural network It defined K tanh tanh x z Warning psd A final example common nonetheless interesting subsets kernel Suppose D features binary values Let A D subset features let fA x d A xd conjunction features A Let x feature vector As 2D features vector You compute kernel associated feature mapping K subs x z d xdzd Verifying relationship kernel subsets feature mapping left exercise closely resembles ex pansion quadratic kernel Support Vector Machines Kernelization predated support vector machines SVMs def initely model popularized idea Recall definition soft margin SVM Chapter particular opti mization problem attempts balance large margin small w small loss small ns n slack nth training example This problem repeated min w b w C n n subj yn w xn b n n n n Previously optimized explicitly computing slack variables n given solution decision boundary w b However expert Lagrange multipliers course machine learning optimize constrained problems The overall goal going rewrite SVM optimization problem way longer ex plicitly depends weights w depends examples xn kernel products There 2N constraints optimization slack constraint requirement slacks non negative Unlike time constraints inequalities require slightly different solution First rewrite inequalities read add cor responding Lagrange multipliers The main difference Lagrange multipliers constrained non negative sign augmented objective function matters The second set constraints proper form set rewritten yn w xn b n You ready construct Lagrangian multipliers n set constraints n second set L w b w C n n n nn n n yn w xn b n The new optimization problem min w b max max L w b The intuition exactly If able find solution satisfies constraints e g purple term prop erly non negative ns hurt solution On hand purple term negative corresponding n breaking solution You solve problem taking gradients This bit te dious important step realize fits Since goal remove dependence w step gradient respect w set equal zero solve w terms variables wL w n nynxn w n nynxn At point immediately recognize similarity kernelized perceptron optimal weight vector takes exactly form algorithms You new expression w plug expression L removing w consideration To avoid subscript overloading replace n expression kernel methods w m This yields L b m mymxm C n n n nn n n yn m mymxm xn b n At point s convenient rewrite terms sure un derstand following comes L b n m nmynymxn xm n C n n n m nmynymxn xm n n ynb n n m nmynymxn xm n C n n b n nyn n n n Things starting look good ve successfully removed de pendence w written terms dot prod ucts input vectors This difficult problem solve need continue attempt remove remaining variables b The derivative respect b L b n nyn This doesn t allow substitute b w mean fourth term b n nyn goes zero optimum The original variables n derivatives case look like L n C n n C n n Again doesn t allow substitute mean rewrite second term n C n n n nn This cancels final term However need careful remember When optimize n n constrained non negative What means dropping optimization need ensure n C corresponding need negative course machine learning allowed You finally wind following xn xm replaced K xn xm L n n n m nmynymK xn xm If comfortable matrix notation compact form Let denote N dimensional vector 1s let y denote vector labels let G N N matrix Gn m ynymK xn xm following form L G The resulting optimization problem maximize L function subject constraint ns non negative C constraint added removing variables Thus problem min L n m nmynymK xn xm n n subj n C n One way solve problem gradient descent The complication making sure s satisfy constraints In case use projected gradient algorithm gradient update adjust parameters satisfy constraints projecting feasible region In case projection trivial gradient step n simply set n C set C Understanding Support Vector Machines The prior discussion involved bit math derive repre sentation support vector machine terms Lagrange variables This mapping actually sufficiently standard thing The original problem variables w b called primal variables Lagrange variables called dual variables The optimization problem results removing primal variables called dual problem A succinct way saying ve found converting SVM dual possible kernelize To understand SVMs step peek dual formula tion Eq The objective terms depends data second depends dual variables The thing notice second term s want kernel methods large possible The constraint ensures ex ceed C means general tendency s grow close C possible To understand dual optimization problem useful think kernel measure similarity data points This analogy clear case RBF kernels case linear kernels examples unit norm dot product measure similarity Since write prediction function f x sign n nynK xn x natural think n importance training example n n means test time Consider data points label yn ym This means ynym objective function term looks like nmK xn xm Since goal term small things happen K small nm small If K small doesn t affect setting corresponding s But K large strongly encourages n m zero So data points similar label corresponding s small This makes intuitive sense data points basically x y sense need Suppose data points different labels ynym Again K xn xm small happens But large corresponding s encouraged large possible In words similar examples dif ferent labels strongly encouraged corresponding s large C An alternative way understanding SVM dual problem geometrically Remember point introducing variable n ensure nth training example correctly classified modulo slack More formally goal n ensure yn w xn b n Suppose constraint satisfied There important result optimization theory called Karush Kuhn Tucker conditions KKT conditions short states optimum product Lagrange multiplier constraint value constraint equal zero In case says optimum n yn w xn b n In order true means follow ing true n yn w xn b n course machine learning A reasonable question ask circumstances n non zero From KKT conditions discern n non zero constraint holds exactly yn w xn b n When constraint hold ex actly It holds exactly points precisely margin hyperplane In words training examples n lie precisely unit away maximum margin decision boundary Or moved corre sponding slack These points called support vectors support decision boundary In general number sup port vectors far smaller number training examples naturally end solution uses subset training data From discussion know points wind support vectors exactly confusable sense examples nearby different la bels This completely line previous discussion If decision boundary pass confusable points end set support vectors Exercises Exercise TODO LEARNING THEORY Dependencies By expert building learning algorithms You probably understand work intuitively And stand generalize However basic questions want know answer Is learning possible How training examples I need good job learning Is test performance going worse training performance The key idea underlies answer simple functions generalize The amazing thing actually prove strong results address questions In chapter learn important results learning theory attempt answer questions The goal chapter theory theory s sake way better understand learning models work use theory build better algorithms As concrete example norm regularization prov ably leads better generalization performance justifying common practice The Role Theory In contrast quote start chapter practitioner friend said I happily percent perfor mance algorithm I understand Both perspectives completely valid actually contradictory The second statement presupposing theory helps understand hopefully ll find case chapter Theory serve roles It justify help understand common practice works This theory view It serve suggest new algorithms approaches turn work practice This theory view Often turns mix Practitioners discover works surpris ingly Theorists figure works prove And process better find new algo Learning Objectives Explain inductive bias necessary Define PAC model explain P A necessary Explain relationship complexity measures regulariz ers Identify role complexity generalization Formalize relationship margins complexity For ought posited reason given self evident known experience proved authority Sacred Scripture William Occam c course machine learning rithms directly exploit property theory Theory help understand s possible s possible One things ll general ma chine learning work Of course work means need think harder means learning algo rithms work By understanding s possible focus energy things Probably biggest practical success story theoretical machine learning theory boosting won t actually chapter You ll wait Chapter Boosting simple style algorithm came theoretical machine learn ing proven incredibly successful practice So de facto algorithms run gives new data set In fact Yoav Freund Rob Schapire won ACM s Paris Kanellakis Award boosting algorithm AdaBoost This award given theoretical accomplishments significant demonstrable effect practice computing In Corinna Cortes Vladimir Vapnik won support vector machines Induction Impossible One nice thing theory forces precise trying You ve seen formal definition binary classification Chapter But let s step analyze means learn binary classification From algorithmic perspective natural question ultimate learning algorithm Aawesome solves Binary Classification problem In words wasting time learning KNN Perceptron decision trees Aawesome What ultimate learning algorithm You like data set D produce function f No matter D looks like function f perfect classification future examples drawn distribution produced D A little bit introspection demonstrate impos sible For instance label noise distribution As simple example let X e dimensional binary distribution Define data distribution D D D D In words data points distrubtion x y learning theory don t No matter function learning algorithm produces s way better error data It s clear algorithm pro duces deterministic function better error What produces stochastic aka randomized function Given hopeless algorithm Aawesome achieves error rate zero The best hope error rate large Unfortunately simply weakening requirement error rate learning possible The second source difficulty comes fact access data distribution sampling In particular trying learn distribution like data points drawn distribution You know eventually data points sample representative distribution happen immediately For instance fair coin come heads probability s completely plausible sequence coin flips tails tails So second thing hope Aawesome work In particular happen lousy sample data D need allow Aawesome completely unreasonable Thus hope Aawesome perfectly time We hope pretty time Nor hope perfectly time The best best reasonably hope Aawesome pretty time Probably Approximately Correct Learning Probably Approximately Correct PAC learning formalism inductive learning based realization best hope algorithm good job e approximately correct time e probably appoximately correct Leslie Valiant invented notion PAC learning In received Turing Award highest honor computing work learning theory computational complexity parallel systems Consider hypothetical learning algorithm You run dif ferent binary classification data sets For comes functions f1 f2 f10 For reason run f4 test point crashes computer For learned functions performance test data error If situtation guaranteed happen hypothetical learning algorithm PAC learning algorithm It satisfies probably failed cases s approximate achieved low non zero error remainder cases This leads formal definition e PAC learning algo rithm In definition e plays role measuring accuracy course machine learning previous example e plays role measuring failure previous Definitions An algorithm A e PAC learning algorithm distributions D given samples D probability returns bad function bad function test error rate e D There notions efficiency matter PAC learning The usual notion computational complexity You prefer algorithm runs quickly takes forever The second notion sample complexity number examples required algorithm achieve goals Note goal measure complexity bound scarse resource algorithm uses In computational case resource CPU cycles In sample case resource labeled examples Definition An algorithm A efficient e PAC learning al gorithm e PAC learning algorithm runtime polynomial 1e In words suppose want algorithm achieve error rate The runtime required exponential factor PAC Learning Conjunctions To better sense PAC learning start completely irrelevant uninteresting example The purpose example help understand PAC learning works The setting learning conjunctions Your data points binary vectors instance x Someone guarantees boolean conjunction defines true labeling data For instance x1 x2 x5 allowed In formal terms true underlying classification function concept So saying concept trying learn conjunction In case boolean function assign negative label example Since know concept trying learn con junction makes sense represent function conjunction For historical reasons function learn called hypothesis denoted h However keeping notation book continue denote f Formally set follows There distribution DX binary data points vectors x x1 x2 xD There fixed learning theory concept conjunction c trying learn There noise example x true label simply y c x y x1 x2 x3 x4 Table Data set learning con junctions What reasonable algorithm case Suppose observe example Table From example know true formula include term x1 If exam ple negative By reason ing include x2 By analogous reasoning include term x3 term x4 This suggests algorithm Algorithm colloquially Throw Out Bad Terms algorithm In algorith begin function includes possible 2D terms Note function initially classify negative You process example sequence On negative example On positive example throw terms f contradict given positive example Verify Algorithm main tains invariant errs classifying examples negative errs way If run algorithm data Table sequence f s cycle f x x1 x1 x2 x2 x3 x3 x4 x4 f x x1 x2 x3 x4 f x x1 x3 x4 f x x1 x3 x4 The thing notice algorithm processing example guaranteed classify example correctly This observation requires noise data The second thing notice s computationally ef ficient Given data set N examples D dimensions takes O ND time process data This linear size data set However order efficient e PAC learning algorithm need able bound sample complexity algorithm Sure know run time linear number example N But examples N need order guarantee achieves error rate e cases Perhaps N gigantic like D e probably guarantee small error The goal prove number samples N required probably achieve small error big The general proof technique essentially flavor PAC learning proof First define bad thing In case bad thing term x8 thrown wasn t Then bad things happen Then notice bad thing happened course machine learning Algorithm BinaryConjunctionTrain D f x1 x1 x2 x2 xD xD initialize function positive examples x D d D xd f f term xd f f term xd end end end return f seen positive training examples x8 So example x8 low probability seen So bad things common Theorem With probability Algorithm requires N examples achieve error rate e Proof Theorem Let c concept trying learn let D distribution generates data A learned function f mistake contains term t c There initially 2D terms f c We want ensure probability f makes error e It sufficient ensure For term t e g x5 t negates example x t x Call term t bad appear c b probability e 2D appearing respect unknown distribution D data points First bad terms left f f error rate e We know f contains 2D terms begins 2D terms throws The algorithm begins 2D terms variable negated variable Note f type error positive examples negative negative example positive Let c true concept true boolean formula term bad appear c A specific bad term e g x5 because f err positive examples contain corresponding bad value e g x5 TODO finish What ve shown theorem true underly ing concept boolean conjunction noise Throw Out Bad Terms algorithm needs N examples order learning theory learn boolean conjunction likely achieve er ror e That sample complexity Throw Out Bad Terms Moreover algorithm s runtime linear N efficient PAC learning algorithm Occam s Razor Simple Solutions Generalize The previous example boolean conjunctions warm exercise understand PAC style proofs concrete setting In section generalize argument larger range learning problems We assume noise makes analysis simpler Don t worry noise added eventually William Occam c c English friar philosopher famous later known Oc cam s razor popularized Bertrand Russell The principle ba sically states assume need Or verbosely explain phenomenon assuming hypothetical entity ground assuming e opt explanation terms fewest possible number causes factors variables What Occam actually wrote quote began chapter In machine learning context reasonable paraphrase simple solutions generalize In words features looking If able explain predictions use The Occam s razor theorem states good idea theo retically It essentially states learning unknown concept able fit training data perfectly don t need resort huge class possible functions learned function generalize It s amazing theo rem partly simplicity proof In ways proof actually easier proof boolean conjunctions follows basic argument In order state theorem explicitly need able think hypothesis class This set possible hypotheses algorithm searches find best In case boolean conjunctions example hypothesis class H set boolean formulae D variables In case perceptron hypothesis class set possible linear classifiers The hypothesis class boolean conjunctions finite hypothesis class linear classifiers infinite For Occam s razor work finite hypothesis classes course machine learning Theorem Occam s Bound Suppose A algorithm learns function f finite hypothesis class H Suppose learned function gets zero error training data Then sample com plexity f log H TODO COMMENTS Proof Theorem TODO This theorem applies directly Throw Out Bad Terms algo rithm hypothesis class finite b learned func tion achieves zero error training data To apply Oc cam s Bound need compute size hypothesis class H boolean conjunctions You compute noticing total 2D possible terms formula H Moreover term formula So 22D 4D possible formulae H 4D Applying Occam s Bound sample complexity algorithm N Of course Occam s Bound general capture learning algorithms In particular capture decision trees In noise setting decision tree fit train ing data perfectly The remaining difficulty compute size hypothesis class decision tree learner Figure thy dt picture decision tree For simplicity s sake suppose decision tree algorithm learns complete trees e branch root leaf length D So number split points tree e places feature queried 2D See Figure Each split point needs assigned feature D choices This gives D2D trees The thing 2D leaves tree possible values depending leaf classified 2D 2D possibilities Putting togeter gives total number trees H D2D 12D D22D D4D Applying Occam s Bound TODO examples learn decision tree Complexity Infinite Hypothesis Spaces Occam s Bound fantastic result learning finite hypothesis spaces Unfortunately completely useless H This proof works N training examples throw bad hypotheses small number left But H throwing finite number step infinite number remaining This means want establish sample complexity results infinite hypothesis spaces need new way measuring learning theory size complexity A prototypical way measure complexity hypothesis class number different things As silly example consider boolean conjunctions Your input vector binary features However instead representing hypothesis boolean conjunction choose represent conjunction inequalities That instead writing x1 x2 x5 write x1 x2 x5 In representation feature need choose inequality threshold Since thresholds arbitrary real values infinitely possibilities H 2D However immediately recognize binary features difference x2 x2 number infinitely possibilities In words infinitely hypotheses finitely behaviors Figure thy vcex figure examples The Vapnik Chernovenkis dimension VC dimension classic measure complexity infinite hypothesis classes based intuition3 The VC dimension classification oriented Yes Vapnik credited creation support vector machine tion complexity The idea look finite set unlabeled ex amples Figure The question matter points labeled able find hypothesis correctly classifies The idea add points able represent arbitrary labeling harder harder For instance regardless points labeled find linear classifier agrees classification However points exists labeling find perfect classifier The VC dimension maximum number points find classifier What labeling What s You think VC dimension game adversary To play game choose K unlabeled points want Then adversary looks K points assigns binary labels wants You find hypothesis classifier agrees labeling You win find hypothesis wins The VC dimension hypothesis class maximum number points K win game This leads following formal definition interpret exists adversary s Definitions For data drawn space X VC dimension hypothesis space H X maximal K exists set X X size X K binary labelings X exists function f H matches labeling course machine learning In general easier VC dimension value harder value For example following example Figure image points plus little argumentation VC dimension linear classifiers dimension To VC dimension exactly suffices find set points win game adversary This difficult In proof VC dimension simply need provide example points work small number possible labelings data To need argue matter set point pick win game VC margins small norms Learning Noise Agnostic Learning Error versus Regret Despite fact s way better error distribution nice learn thing For instance predictor guesses y x like right thing Based observation maybe rephrase goal learning find function distribution allows In words data hope error On distribution hope X error X best This notion best sufficiently important Bayes error rate This error rate best possible classifier called Bayes optimal classifier If knew underlying distribution D actually write exact Bayes optimal classifier explicitly This learning unin teresting case know D It simply form f Bayes x D x D x The Bayes optimal error rate error rate hypothetical classifier achieves eBayes E x y D y f Bayes x learning theory Exercises Exercise TODO ENSEMBLE METHODS Dependencies Groups people better decisions individuals especially group members come biases The true machine learning Ensemble methods learning models achieve performance combining opinions multiple learners In away simpler learners achieve great performance Moreover ensembles inherantly parallel efficient training test time access multiple processors In chapter learn ways combining base learners ensembles One shocking results learning model slightly better chance turn arbitrarily good learning model technique known boosting You learn ensembles decrease variance predictors perform regularization Voting Multiple Classifiers All learning algorithms seen far deterministic If train decision tree multiple times data set tree In order effect voting multiple classifiers need differ There primary ways variability You change learning algorithm change data set Building emsemble training different classifiers straightforward approach As single model learning given data set classification Instead learning single classifier e g decision tree data set learn multiple different classifiers For instance train decision tree perceptron KNN multiple neural networks different architectures Call classifiers f1 fM At test time predic tion voting On test example x compute y f1 x Learning Objectives Implement bagging explain reduces variance predictor Explain difference weak learner strong learner Derive AdaBoost algorithm Understand relationship boosting decision stumps linear classification ensemble methods y M fM x If 1s list y1 yM predict predict The main advantage ensembles different classifiers unlikely classifiers mistake In fact long error minority classifiers achieve optimal classification Unfortunately inductive biases different learning algorithms highly correlated This means different algorithms prone similar types errors In particular ensembles tend reduce variance classifiers So classification algorithm tends sensitive small changes training data ensembles likely useful Which classifiers ve learned far high variance Note voting scheme naturally extends multiclass clas sification However sense contexts regres sion ranking collective classification This rarely exact output predicted twice different regression models ranking models collective classification mod els For regression simple solution mean median prediction different models For ranking collective clas sification different approaches required Instead training different types classifiers data set train single type classifier e g decision tree multiple data sets The question multiple data sets come given training time One option fragment original data set For instance break pieces build decision trees pieces individually Unfortunately means decision tree trained small entire data set likely perform poorly Figure picture sampling replacement A better solution use bootstrap resampling This tech nique statistics literature based following observa tion The data set given D sample drawn d unknown distribution D If draw new data set D random sampling D replacement1 D sample D To sample replacement imagine putting items D hat To draw single sample pick element random hat write Figure shows process bootstrap resampling objects Applying idea ensemble methods yields technique known bagging You start single data set D contains N train ing examples From single data set create M boot strapped training sets D D M Each bootstrapped sets contains N training examples drawn randomly D replacement You train decision tree model seperately data sets obtain classifiers f1 fM As use classifiers vote new test points Note bootstrapped data sets similar However similar For example N large number course machine learning examples present particular bootstrapped sample relatively large The probability training example selected N The probability selected N N As N tends e Already N correct decimal points So original training examples represented given bootstrapped set Figure graph depicting overfitting regularization versus bagging Since bagging tends reduce variance provides alternative approach regularization That learned clas sifiers f1 fM individually overfit likely overfit different things Through voting able overcome sig nificant portion overfitting Figure shows effect comparing regularization hyperparameters regularization bagging Boosting Weak Learners Boosting process taking crummy learning algorithm tech nically called weak learner turning great learning algorithm technically strong learner Of ideas origi nated theoretical machine learning community boosting greatest practical impact The idea boosting reminiscent like thought learned file compression If I compress file compress compress eventually I ll end final s byte size To formal let s define strong learning algorithm L follows When given desired error rate e failure probability access labeled examples distribution D high probability L learns classifier f error e This precisely definition PAC learning learned Chapter Building strong learning algorithm difficult We instead possible build weak learning algorithmW achieve error rate arbitrary user defined parameter e arbitrary strictly fine Boosting framework algorithm It s frame work taking weak learning algorithmW turning strong learning algorithm The particular boosting algorithm dis cussed AdaBoost short adaptive boosting algorithm AdaBoost famous practical boosting algorithms runs polynomial time require define large number hyperparameters It gets benefit automatically adapts data ensemble methods Algorithm AdaBoost W D K d 1N N N Initialize uniform importance example k K f k W D d k Train kth classifier weighted data y n f k xn n Make predictions training data e k n d k n yn y n Compute weighted training error k log e k e k Compute adaptive parameter d k n 1Z d k n exp k yn y n n Re weight examples normalize end return f x sgn k k f k x Return weighted voted classifier The intuition AdaBoost like studying exam past exam You past exam grade The questions got right pay attention Those got wrong study Then exam repeat process You continually weight importance questions routinely answer correctly weight importance ques tions routinely answer incorrectly After going exam multiple times hope mastered The precise AdaBoost training algorithm shown Algorithm The basic functioning algorithm maintain weight dis tribution d data points A weak learner f k trained weighted data Note implicitly assume weak learner accept weighted training data relatively mild assumption nearly true The weighted error rate f k de termine adaptive parameter controls important f k As long weak learner achieve error greater zero As error drops zero grows bound What happens weak learn ing assumption violated e equal What worse What mean practice After adaptive parameter computed weight distibution updated iteration As desired examples cor rectly classified yny n weight decreased multiplicatively Examples incorrectly classified yny n weight increased multiplicatively The Z term nom ralization constant ensure sum d e d interpreted distribution The final classifier returned Ad aBoost weighted vote individual classifiers weights given adaptive parameters To better understand defined suppose weak learner simply returns constant function returns weighted majority class So total weight positive exam ples exceeds negative examples f x x f x x To problem moderately interesting suppose original training set positive ex course machine learning amples negative examples In case f x It s weighted error rate e gets negative example wrong Computing log Before normaliza tion new weight positive correct example exp log The weight negative incorrect example exp log We compute Z Therefore normalization weight distribution single positive example weight negative example However positive examples negative exam ples cumulative weight positive examples cumulative weight negative examples Thus single boosting iteration data precisely evenly weighted This guarantees iteration weak learner interesting majority voting achieve error rate required This example uses concrete num bers result holds matter data distribution looks like examples Write general case arrive weighting iteration Figure perf comparison depth vs boost One major attractions boosting easy design computationally efficient weak learners A popular type weak learner shallow decision tree decision tree small depth limit Figure shows test error rates decision trees different maximum depths different curves run differing numbers boosting iterations x axis As willing boost iterations shallow trees effective In fact popular weak learner decision decision stump decision tree ask question This like silly model fact s combined boosting effective To understand suppose moment data consists binary features question decision tree ask form feature By concentrating decision stumps weak functions form f x s 2xd s d indexes feature Why functions form Now consider final form function learned AdaBoost We expand follow let fk denote single feature selected kth decision stump let sk denote sign f x sgn k k f k x sgn k ksk 2x fk sgn k 2kskx fk k ksk sgn w x b ensemble methods Algorithm RandomForestTrain D depth K k K t k complete binary tree depth depth random feature splits f k function computed t k leaves filled D end return f x sgn k f k x Return voted classifier wd k fk d 2ksk b k ksk Thus working decision stumps AdaBoost actually pro vides algorithm learning linear classifiers In fact con nection recently strengthened AdaBoost provides algorithm optimizing exponential loss However connection scope book As example consider case boosting linear classi fier In case let kth weak classifier parameterized w k b k overall predictor form f x sgn k ksgn w k x b k You notice layer neural network K hidden units Of course s classifically trained neural network learn w k update structure identical Random Ensembles One computationally expensive aspects ensembles decision trees training decision trees This fast de cision stumps deeper trees prohibitively expensive The expensive choosing tree structure Once tree struc ture chosen cheap fill leaves e predictions trees training data An efficient surprisingly effective alternative use trees fixed structures random features Collections trees called forests classifiers built like called random forests The random forest training algorithm shown Algo rithm short It takes arguments data desired depth decision trees number K total decision trees build The algorithm generates K trees independently makes easy parallelize For trees constructs binary tree depth depth The features branches course machine learning tree selected randomly typically replacement meaning feature appear multiple times branch The leaves tree predictions filled based training data This step point training data The resulting classifier voting K random trees The amazing thing approach actually works remarkably It tends work best features marginally relevant number features selected given tree small An intuitive reason works following Some trees query useless features These trees essentially random predictions But trees happen query good features good predictions leaves estimated based training data If trees random ones wash noise good trees effect final classification Exercises Exercise TODO EFFICIENT LEARNING Dependencies So far focus models learning basic al gorithms models We placed emphasis learn quickly The basic techniques learned far learning algorithms running tens hundreds thousands examples But want build algorithm web page ranking need deal millions billions examples hundreds thousands dimensions The basic approaches seen far insufficient achieve massive scale In chapter learn techniques scaling learning algorithms This useful billions training examples s nice program runs quickly You techniques speeding model training model prediction The focus chapter linear models simplicity learn applies generally What Does Mean Fast Everyone wants fast algorithms In context machine learning mean things You want fast training algorithms training algorithms scale large data sets instance ones fit main memory You want training algorithms easily parallelized Or care training efficiency offline process care quickly learned functions classification decisions It important separate desires If care efficiency training time asking efficient learning algorithms On hand care efficiency test time asking models quickly evaluated One issue covered chapter parallel learning Learning Objectives Understand able imple ment stochastic gradient descent algorithms Compare contrast small ver sus large batch sizes stochastic optimization Derive subgradients sparse regularizers Implement feature hashing course machine learning This largely currently understood area machine learning There aspects parallelism come play speed communication network shared memory etc Right general poor man s approach parallelization employ ensembles Stochastic Optimization During training learning algorithms consider entire data set simultaneously This certainly true gradient descent algorithms regularized linear classifiers recall Algorithm compute gradient entire training data simplicity consider unbiased case g n w yn w xn w y y loss function Then update weights w w g In algorithm order single update look training example When billions training examples bit silly look Perhaps basis examples start learning Stochastic optimization involves thinking training data big distribution examples A draw distribution corresponds picking example uniformly random data set Viewed way optimization problem stochastic optimization problem trying optimize function regularized linear classifier probability distribution You derive intepretation directly follows w arg max w n yn w xn R w definition arg max w n yn w xn N R w R inside sum arg max w n N yn w xn N2 R w divide N arg max w E y x D y w x N R w write expectation D training data distribution Given framework following general form efficient learning Algorithm StochasticGradientDescent F D S K z initialize variable optimizing k K D k S random data points D g k zF D k z k compute gradient sample z k z k k g k step gradient end return z K optimization problem min z E F z In example denotes random choice examples dataset z denotes weight vector F w denotes loss example plus fraction regularizer Stochastic optimization problems formally harder regu lar deterministic optimization problems access exact function values gradients The access function F wish optimize noisy mea surements governed distribution Despite lack information run gradient based algorithm simply compute local gradients current sample data More precisely draw data point random data set This analogous drawing single value distribution You compute gradient F point In case norm regularized linear model simply g w y w x 1N w y x random point selected Given estimate gradient s estimate s based single random draw small gradient step w w g This stochastic gradient descent algorithm SGD In prac tice taking gradients respect single data point myopic In cases useful use small batch data Here draw random examples training data compute small gradient estimate based examples g 10m w ym w xm N w need include counts regularizer Popular batch sizes single points The generic SGD algorithm depicted Algorithm takes K steps batches S examples In stochastic gradient descent imperative choose good step sizes It important steps smaller time reasonable slow rate In particular convergence guaranteed learning rates form k k fixed initial step size typically depending quickly ex course machine learning pect algorithm converge Unfortunately comparisong gradient descent stochastic gradient sensitive selection good learning rate There practical issues related use SGD learning algorithm select random point subset random points step stream data order The answer akin answer question perceptron algorithm Chapter If permute data bad things happen If permute data multiple passes permutation converge slowly In theory permute iteration If data small fit memory big deal pay cache misses However data large memory resides magnetic disk slow seek time randomly seeking new data points example prohibitivly slow likely need forgo permuting data The speed hit convergence speed certainly recovered speed gain having seek disk routinely Note story different solid state disks random accesses efficient Sparse Regularization For learning algorithms test time efficiency governed features prediction This reason de cision trees tend fastest predictors use small number features Especially cases actual com putation features expensive cutting number test time yield huge gains efficiency Moreover memory predictions typically governed number features Note true kernel methods like support vector machines dominant cost number support vectors Furthermore simply believe learning problem solved small number features reasonable form inductive bias This idea sparse models particular sparse regularizers One disadvantages norm regularizer linear models tend produce weights exactly zero They close zero hit To understand weight wd approaches zero gradient approaches zero Thus weight zero essentially constantly shrinking gradient This suggests alternative regularizer required yield sparse inductive bias An ideal case zero norm regular efficient learning izer simply counts number non zero values vector w d wd If minimize regularizer explicitly minimizing number non zero features Un fortunately zero norm non convex s discrete Optimizing NP hard A reasonable middle ground norm w d wd It convex fact tighest p norm convex Moreover gradients zero norm Just hinge loss tightest convex upper bound zero error norm tighest convex upper bound zero norm At point content You subgradi ent optimizer arbitrary functions plug norm regularizer The norm surely non differentiable wd simply choose value range subgradi ent point You choose zero Unfortunately work way expect The issue gradient overstep zero end solution particularly sparse For example end gradient step w3 Your gradient g6 gradient step assuming update new w3 In subsequent iteration g6 step w3 This observation leads idea trucated gradients The idea simple gradient step wd set wd In easy case learning rate means sign wd gd different sign wd truncate gradient step simply set wd In words gd larger wd Once incorporate learning rates express gd gd wd gd k wd gd wd gd k wd This works case subgradient descent It works somewhat case stochastic subgradient descent The problem arises stochastic case choose stop optimizing touched single example small batch examples increase weights lot features regularizer time shrink zero You end somewhat sparse solutions sparse There algorithms dealing situation heuristic flavor scope book course machine learning Feature Hashing As speed bottleneck prediction memory usage If large number features memory takes store weights prohibitive especially wish run algorithm small de vices Feature hashing incredibly simple technique reducing memory footprint linear models small sacrifices accuracy The basic idea replace features hashed ver sions features reducing space D fea ture weights P feature weights P range hash function You actually think hashing random ized feature mapping RD RP P D The idea follows First choose hash function h domain D D range P Then receive feature vector x RD map shorter feature vector x RP Algorithmically think mapping follows Initialize x For d D Hash d position p h d b Update pth position adding xd x p x p xd Return x Mathematically mapping looks like x p d h d p xd d h p xd h p d h d p In unrealistic case P D h simply encodes mutation mapping change learning problem All rename features In practice P D collisions In context collision means features different end looking learning algorithm For instance sunny today favorite sports team win night mapped location hashing The hope learning algorithm sufficiently robust noise handle case efficient learning Consider kernel defined hash mapping Namely K hash x z x z p d h d p xd d h d p zd p d e h d p h e p xdze d e h h d xdze x z d e d e h h d xdze This hash kernel form linear kernel plus small number quadratic terms The particular quadratic terms exactly given collisions hash function There things notice The collisions actually bad things In sense giving little extra representational power In particular hash function happens select feature pairs benefit paired better representation The second doesn t happen quadratic term kernel small effect overall prediction In particular assume hash function pairwise independent common assumption hash functions expected value quadratic term zero variance decreases rate O P In words choose P variance order Exercises Exercise TODO UNSUPERVISED LEARNING Dependencies If access labeled training data know This supervised setting teacher telling right answers Unfortunately finding teacher difficult expensive right impossible In cases want able analyze data labels Unsupervised learning learning teacher One basic thing want data visualize Sadly difficult visualize things dimensions data hundreds dimensions Dimension ality reduction problem taking high dimensional data embedding lower dimension space Another thing want automatically derive partitioning data clusters You ve learned basic approach k means algorithm Chapter Here analyze algorithm works You learn advanced clustering approaches K Means Clustering Revisited The K means clustering algorithm presented Algorithm There basic questions algorithm converge quickly sensitive initializa tion The answers questions detailed yes converges converges quickly practice slowly theory yes sensitive initialization good ways initialize Consider question convergence The following theorem states K Means algorithm converges quickly happens The method proving convergence specify clustering quality objective function K Means algorithm converges local optimum objective function The particular objective function K Means Learning Objectives Explain difference linear non linear dimensionality reduction Relate view PCA maximiz ing variance view minimizing reconstruction error Implement latent semantic analysis text data Motivate manifold learning perspective reconstruction error Understand K means clustering distance minimization Explain importance initial ization k means furthest heuristic Implement agglomerative clustering Argue spectral cluster ing clustering algorithm dimensionality reduction algorithm unsupervised learning Algorithm K Means D K k K k random location randomly initialize mean kth cluster end repeat n N zn argmink k xn assign example n closest center end k K k mean xn zn k estimate mean cluster k end converged return z return cluster assignments optimizing sum squared distances data point assigned center This natural generalization definition mean mean set points single point minimizes sum squared distances mean point data Formally K Means objective L z D n xn zn k n zn k xn k Theorem K Means Convergence Theorem For dataset D number clusters K K means algorithm converges finite num ber iterations convergence measured L ceasing change Proof Theorem The proof works follows There points K means algorithm changes values z lines We operations increase value L Assuming true rest argu ment follows After pass data finitely possible assignments z z discrete finite number values means subset data Furthermore L lower bounded zero Together means L decrease finite number times Thus stop decreasing point point algorithm converged It remains lines decrease L For line looking example n suppose previous value zn new value b It case xn b xn b Thus changing b decrease L For line consider second form L Line computes k mean data points zn k precisely point minimizes squared sitances Thus update k decrease L There aspects K means unfortunate First convergence local optimum L In practice course machine learning means usually run times different initial izations pick minimal resulting L Second input datasets initializations exponential time converge Fortu nately cases happen practice fact recently shown roughly limit floating point pre cision machine K means converge polynomial time local optimum techniques smoothed analysis The biggest practical issue K means initialization If clus ter means initialized poorly convergence uninter esting solutions A useful heuristic furthest heuristic This gives way perform semi random initialization attempts pick initial means far possible The heuristic sketched Pick random example m set xm For k K Find example m far possible previ ously selected means m arg maxm mink k xm k set k xm In heuristic bit randomness selection data point After completely deterministic rare case multiple equidistant points step 2a It extremely important selecting 3rd mean select point maximizes minimum distance closest mean You want point s far away previous means possible The furthest heuristic heuristic It works practice somewhat sensitive outliers selected initial means However outlier sensitivity usually reduced iteration K means algorithm Despite heuristic useful practice You turn heuristic algorithm adding bit randomness This idea K means algorithm simple randomized tweak furthest heuristic The idea select kth mean instead choosing absolute furthest data point choose data point random probability proportional distance squared This formal Algorithm If use K means initialization K means able achieve approximation guarantee final value unsupervised learning Algorithm K Means D K xm m chosen uniformly random randomly initialize point k K dn mink k xn k n compute distances p n nd d normalize probability distribution m random sample p pick example random k xm end run K Means initial centers objective This doesn t tell reach global optimum tell reasonably close In particular L value obtained running K means far L opt true global minimum Theorem K means Approximation Guarantee The expected value objective returned K means O log K optimal close O optimal Even case 2K random restarts restart O optimal high probability Formally E L log K L opt Moreover data suited clustering E L O L opt The notion suited clustering informally states advantage going K clusters K clusters large Formally means LK opt e2LK opt LK opt optimal value clustering K clusters e desired degree approximation The idea condition hold shouldn t bother clustering data One biggest practical issues K means clustering choosing K Namely hands dataset asks cluster clusters produce This difficult increasing K decrease LK opt K N simply L notion goodness insuffi cient analogous overfitting supervised setting A number information criteria proposed try address problem They effectively boil regularizing K model grow complicated The popular Bayes Information Criteria BIC Akaike Information Criteria AIC defined context K means BIC arg min K L K K log D AIC arg min K L K 2KD The informal intuition criteria increasing K going LK However doesn t s worth In case BIC course machine learning means proportional log D case AIC s proportional 2D Thus AIC provides stronger penalty clusters BIC especially high dimensions A formal intuition BIC following You ask question I wanted send data network bits I need send Clearly simply send N examples roughly log D bits send This gives N log D send data Alternatively cluster data send cluster centers This K log D bits Then data point send center deviation center It turns cost exactly L K bits Therefore BIC precisely measuring bits send data K clusters The K minimizes number bits optimal value Linear Dimensionality Reduction Dimensionality reduction task taking dataset high di mensions reducing low dimensions retaining important characteristics data Since unsupervised setting notion important characteristics difficult define Consider dataset Figure lives high dimensions want reduce low dimensions In case linear dimensionality reduction thing project data vector use projected distances embeddings Figure shows projection data vector points direction maximal variance original dataset Intuitively reasonable notion importance direction information encoded data For rest section assume data centered mean data origin This sim ply math easier Suppose dimensional data x1 xN looking vector u points direc tion maximal variance You compute projecting point u looking variance result In order projection sense need constrain u In case projections x1 u xN u Call values p1 pN The goal compute variance pn s choose u maximize variance To compute variance need compute mean Because mean xns zero unsupervised learning usual MATH REVIEW EIGENVALUES AND EIGENVECTORS Figure mean ps zero This seen follows n pn n xn u n xn u u The variance pn n p2n Finding optimal u perspective variance maximization reduces following optimization problem max u n xn u subj u In problem apparent keeping u unit length important u simply stretch infinite length maximize objective It helpful write collection datapoints xn N D matrix X If matrix X multiply u dimensions D end N vector values exactly values p The objective Eq squared norm p This simplifies Eq max u Xu subj u constraint rewritten amenable con structing Lagrangian Doing taking gradients yields L u Xu u uL 2X Xu 2u u X X u You solve expression u X Xu computing eigenvector eigenvalue matrix X X This gives solution projection dimensional space To second dimension want find new vector v data maximal variance However avoid redundancy want v orthogonal u u v This gives max v Xv subj v u v Following procedure construct La course machine learning Algorithm PCA D K mean X compute data mean centering D X X compute covariance vector ones k uk K eigenvalues eigenvectors D return X U project data U grangian differentiate L v Xv u 2u v uL 2X Xv 21v 22u 1v X X v 2u However know u eigenvector X X solution problem v given second eigen value eigenvector pair X X Repeating analysis inductively tells want project K mutually orthogonal dimensions simply need K eigenvectors matrix X X This matrix called data covariance matrix X X j n m xn ixm j sample covariance features j This leads technique principle components analysis PCA For completeness depicted Algorithm The important thing note eigenanalysis gives projection directions It embedded data To embed data point x need compute embedding x u1 x u2 x uK If write U D K matrix XU There alternative derivation PCA informative based reconstruction error Consider dimensional case looking single projection direction u If use direction projected data Z Xu Each Zn gives position nth datapoint u You project dimensional data original space multiplying u This gives reconstructed values Zu Instead maximizing variance instead want minimize reconstruction error defined X Zu X Xuu definition Z X Xuu 2X Xuu quadratic rule unsupervised learning X Xuu 2u X Xu quadratic rule X X 2u X Xu u unit vector C Xu join constants rewrite term Minimizing final term equivalent maximizing Xu exactly form maximum variance derivation PCA Thus maximizing variance identical minimiz ing reconstruction error The question K arises dimension ality reduction clustering If purpose dimensionality reduction visualize K However alter native purpose dimensionality reduction avoid curse dimensionality For instance labeled data worthwhile reduce dimensionality applying super vised learning essentially form regularization In case question optimal K comes In case criteria AIC BIC clustering PCA The difference quality measure changes sum squared distances means clustering sum squared distances original data points PCA In particular BIC reconstruction error plus K log D AIC recon struction error plus 2KD Manifolds Graphs manifold graph construction Non linear Dimensionality Reduction isomap lle mvu mds Non linear Clustering Spectral Methods spectrum spectral clustering course machine learning Exercises Exercise TODO EXPECTATION MAXIMIZATION Dependencies Suppose building naive Bayes model text cate gorization problem After boss told prohibitively expensive obtain labeled data You probabilistic model assumes access labels don t labels Can Amazingly You treat labels hidden variables attempt learn time learn param eters model A broad family algorithms solving problems like expectation maximization family In chapter derive expectation maximization EM algorithms clustering dimensionality reduction EM works Clustering Mixture Gaussians In Chapter learned probabilitic models classification based density estimation Let s start fairly simple classifica tion model assumes labeled data We shortly remove assumption Our model state K classes data class k drawn Gaussian mean k vari ance 2k The choice classes parameterized The generative story model For example n N Choose label yn Disc b Choose example xn Nor yn yn This generative story directly translated likelihood p D n Mult yn Nor xn yn yn Learning Objectives Explain relationship parameters hidden variables Construct generative stories clustering dimensionality reduction Draw graph explaining EM works constructing convex lower bounds Implement EM clustering mixtures Gaussians contrast ing k means Evaluate differences betweem EM gradient descent hidden variable models course machine learning example n yn choose label 22yn D2 exp 22yn xn yn choose feature values If access labels good obtain closed form solutions maximum likelihood estimates parameters taking log taking gradients log likelihood k fraction training examples class k N n yn k k mean training examples class k n yn k xn n yn k 2k variance training examples class k n yn k xn k n yn k Suppose don t labels Analogously K means You able derive maximum likelihood solution sults formally algorithm potential solution iterate You start guesses values unknown variables iteratively improve time In K means approach assign examples labels clusters This time instead making hard assignments example belongs cluster ll soft signments example belongs half cluster quarter cluster quarter cluster So confuse ll introduce new variable zn zn zn K sums denote fractional assignment examples clusters Figure em piecharts A figure showing pie charts This notion soft assignments visualized Figure Here ve depicted example pie chart s coloring denotes degree s assigned clusters The size pie pieces correspond zn values Formally zn k denotes probability example n assigned cluster k zn k p yn k xn p yn k xn p xn Zn Mult k Nor xn k k Here normalizer Zn ensure zn sums Given set parameters s s 2s fractional signments zn k easy compute Now akin K means given expectation maximization Algorithm GMM X K k K k random location randomly initialize mean kth cluster 2k initialize variances k K cluster equally likely priori end repeat n N k K zn k k 22k D2 exp 22k xn k compute unnormalized fractional assignments end zn k zn k zn normalize fractional assignments end k K k 1N n zn k estimate prior probability cluster k k n zn k xn n zn k estimate mean cluster k 2k n zn k xn k n zn k estimate variance cluster k end converged return z return cluster assignments fractional assignments need recompute estimates model parameters In analogy maximum likelihood solution Eqs counting fractional points points This gives following estimation updates k fraction training examples class k N n zn k k mean fractional examples class k n zn kxn n zn k 2k variance fractional examples class k n zn k xn k n zn k All happened hard assignments yn k replaced soft assignments zn k As bit fore shadowing come ve essentially replace known labels expected labels expectation maxi mization Putting yields Algorithm This GMM Gaussian Mixture Models algorithm probabilitic model learned describes dataset drawn mix ture distribution component distribution course machine learning Gaussian Aside fact GMMs use soft assignments K means uses hard assignments differences approaches What Just K means algorithm approach succeptible local optima quality initialization The heuristics comput ing better initializers K means useful The Expectation Maximization Framework At point ve seen method learning particular prob abilistic model hidden variables Two questions remain apply idea generally reason able thing Expectation maximization family algorithms performing maximum likelihood estimation probabilistic mod els hidden variables Figure em lowerbound A figure showing successive lower bounds The general flavor proceed follows We want maximize log likelihood L turn diffi cult directly Instead ll pick surrogate function L s lower bound L e L L s hopefully easier maximize We ll construct surrogate way increas ing force true likelihood After maximizing L ll construct new lower bound optimize This process shown pictorially Figure To proceed consider arbitrary probabilistic model p x y x denotes observed data y denotes hidden data denotes parameters In case Gaussian Mixture Models x data points y unknown labels included cluster prior probabilities cluster means cluster vari ances Now given access number examples x1 xN like estimate parameters model Probabilistically means variables un known need marginalize sum possible values Now data consists X x1 x2 xN x y pairs D You write likelihood p X y1 y2 yN p X y1 y2 yN marginalization y1 y2 yN n p xn yn examples independent n yn p xn yn algebra At point natural thing logs start taking gradients However start taking logs run expectation maximization problem log eat sum L X n log yn p xn yn Namely log gets stuck outside sum decompose rest likelihood term The step apply somewhat strange strangely useful trick multiplying In particular let q arbitrary probability distribution We multiply p term q yn q yn valid step long q zero This leads L X n log yn q yn p xn yn q yn We construct lower bound Jensen s inequality This useful easy prove result states f ixi f xi long b c f concave If looks familiar s s direct result definition concavity Recall f concave f ax f x b f x b Prove Jensen s inequality definition concavity induc tion You apply Jensen s inequality log likelihood identifying list q yn s s log f concave x p q term This yields L X n yn q yn log p xn yn q yn n yn q yn log p xn yn q yn log q yn L X Note inequality holds choice function q long non negative sums In particular needn t function q n We need advantage properties We succeeded goal constructing lower bound L When optimize lower bound matters term The second term q log q drops function This means maximization need able compute fixed qns new arg max n yn qn yn log p xn yn This exactly sort maximization Gaussian mixture models recomputed new means variances cluster prior probabilities course machine learning The second question qn actually Any rea sonable q lead lower bound order choose q need criterion Recall hoping max imize L instead maximizing lower bound In order ensure increase lower bound implies increase L need ensure L X L X In words L lower bound L makes contact current point This shown Figure including case lower bound contact guarantee increase L increase L EM versus Gradient Descent computing gradients marginals step size Dimensionality Reduction Probabilistic PCA derivation advantages pca Exercises Exercise TODO SEMI SUPERVISED LEARNING Dependencies You find setting access labeled data unlabeled data You like use labeled data learn classifier wasteful throw unlabeled data The key question unlabeled data aid learning And assumptions order helpful One idea try use unlabeled data learn better deci sion boundary In discriminative setting accomplish trying find decision boundaries don t pass closely unlabeled data In generative setting simply treat labels observed hidden This semi supervised learning An alternative idea spend small money labels subset unlabeled data However like money like pay labels useful This active learning EM Semi Supervised Learning naive bayes model Graph based Semi Supervised Learning key assumption graphs manifolds label prop Loss based Semi Supervised Learning density assumption loss function non convex Learning Objectives Explain cluster assumption semi supervised discriminative learning necessary Dervive EM algorithm generative semi supervised text categorization Compare contrast query uncertainty query committee heuristics active learning course machine learning Active Learning motivation qbc qbu Dangers Semi Supervised Learing unlab overwhelms lab biased data active Exercises Exercise TODO GRAPHICAL MODELS Dependencies None Exercises Exercise TODO Learning Objectives foo ONLINE LEARNING Dependencies All learning algorithms know point based idea training model data evaluating data This batch learning model How find situation students con stantly rating courses constantly asking recommenda tions Online learning focuses learning stream data predictions continually You actually seen example online learning algorithm perceptron However use perceptron analysis performance batch setting In chapter formalization online learning differs batch learning formalization algorithms online learning different properties Online Learning Framework regret follow leader agnostic learning algorithm versus problem Learning Features change littlestone analysis gd egd Passive Agressive Learning pa algorithm online analysis Learning Objectives Explain experts model hard compete single best expert Define means online learning algorithm regret Implement follow leader algorithm Categorize online learning algo rithms terms measure changes parameters measure error online learning Learning Lots Irrelevant Features winnow relationship egd Exercises Exercise TODO STRUCTURED LEARNING TASKS Dependencies Hidden Markov models viterbi Hidden Markov models forward backward Maximum entropy Markov models Structured perceptronn Conditional random fields M3Ns Exercises Exercise TODO Learning Objectives TODO BAYESIAN LEARNING Dependencies Exercises Exercise TODO Learning Objectives TODO CODE AND DATASETS Rating Easy AI Sys Thy Morning y y n y n y y n y n n y n n n n n n y n n y y n y y y n n n y y n y n n y n y n n n n n y y n n y y n y n y n y y y y y y y y n y n n y y n n n y n y y n y n y n n y y n n y y n y y n y n n y n y n y NOTATION BIBLIOGRAPHY Sergey Brin Near neighbor search large metric spaces In Confer ence Very Large Databases VLDB Tom M Mitchell Machine Learning McGraw Hill Frank Rosenblatt The perceptron A probabilistic model infor mation storage organization brain Psychological Review Reprinted Neurocomputing MIT Press INDEX K nearest neighbors e ball p norms loss absolute loss activation function activations active learning AdaBoost algorithm pairs versus architecture selection area curve AUC AVA averaged perceptron propagation bag words bagging base learner batch batch learning Bayes error rate Bayes optimal classifier Bayes optimal error rate Bernouilli distribution bias binary features bipartite ranking problems boosting bootstrap resampling bootstrapping categorical features chain rule chord circuit complexity clustering clustering quality collective classification complexity concave concavity concept confidence intervals constrained optimization problem contour convergence rate convex cross validation cubic feature map curvature data covariance matrix data generating distribution decision boundary decision stump decision tree decision trees development data dimensionality reduction discrete distribution distance dominates dot product dual problem dual variables early stopping embedding ensemble error driven error rate Euclidean distance evidence example normalization examples expectation maximization expected loss exponential loss feasible region feature combinations feature mapping feature normalization feature scale feature space feature values feature vector features forward propagation fractional assignments furthest heuristic Gaussian distribution Gaussian kernel Gaussian Mixture Models generalize generative story geometric view global minimum GMM gradient gradient ascent gradient descent graph hard margin SVM hash kernel held data hidden units hidden variables hinge loss histogram course machine learning hyperbolic tangent hypercube hyperparameter hyperplane hyperspheres hypothesis hypothesis class hypothesis testing d assumption imbalanced data importance weight independently independently identically dis tributed indicator function induce induced distribution induction inductive bias iteration jack knifing Jensen s inequality joint K nearest neighbors Karush Kuhn Tucker conditions kernel kernel trick kernels KKT conditions label Lagrange multipliers Lagrange variable Lagrangian layer wise leave cross validation level set license likelihood linear classifier linear classifiers linear decision boundary linear regression linearly separable link function log likelihood log posterior log probability log likelihood ratio logarithmic transformation logistic loss logistic regression LOO cross validation loss function margin margin data set marginal likelihood maximum posteriori maximum depth maximum likelihood estimation Mercer s condition model modeling multi layer network naive Bayes assumption nearest neighbor neural network neural networks neurons noise non convex non linear Normal distribution normalize null hypothesis objective function versus versus rest online online learning optimization problem output unit OVA overfitting oversample p value PAC paired t test parametric test parity function patch representation PCA perceptron perpendicular pixel representation polynomial kernels positive semi definite posterior precision precision recall curves predict preference function primal variables principle components analysis prior probabilistic modeling Probably Approximately Correct projected gradient psd radial basis function random forests RBF kernel RBF network recall receiver operating characteristic reconstruction error reductions redundant features regularized objective regularizer representer theorem ROC curve sample complexity semi supervised learning sensitivity separating hyperplane SGD shallow decision tree shape representation sigmoid sigmoid function sigmoid network sign single layer network slack slack parameters smoothed analysis soft assignments soft margin SVM span sparse specificity squared loss stacking index StackTest statistical inference statistically significant stochastic gradient descent stochastic optimization strong learner strong learning algorithm strongly convex structural risk minimization sub sampling subderivative subgradient subgradient descent support vector machine support vectors surrogate loss symmetric modes t test test data test error test set text categorization curse dimensionality threshold Tikhonov regularization training data training error trucated gradients layer network unbiased underfitting unit hypercube unsupervised learning validation data Vapnik Chernovenkis dimension variance VC dimension vector visualize vote voted perceptron voting weak learner weak learning algorithm weighted nearest neighbors weights zero loss About Book How Use Book Why Another Textbook Organization Auxilary Material Acknowledgements Decision Trees What Does Mean Learn Some Canonical Learning Problems The Decision Tree Model Learning Formalizing Learning Problem Inductive Bias What We Know Before Data Arrives Not Everything Learnable Underfitting Overfitting Separation Training Test Data Models Parameters Hyperparameters Chapter Summary Outlook Exercises Geometry Nearest Neighbors From Data Feature Vectors K Nearest Neighbors Decision Boundaries K Means Clustering Warning High Dimensions Scary Extensions KNN Exercises The Perceptron Bio inspired Learning Error Driven Updating The Perceptron Algorithm Geometric Intrepretation Interpreting Perceptron Weights Perceptron Convergence Linear Separability Improved Generalization Voting Averaging Limitations Perceptron Exercises Practical Issues The Importance Good Features Irrelevant Redundant Features Feature Pruning Normalization Combinatorial Feature Explosion Evaluating Model Performance Cross Validation Hypothesis Testing Statistical Significance Debugging Learning Algorithms Exercises Beyond Binary Classification Learning Imbalanced Data Multiclass Classification Ranking Collective Classification Exercises Linear Models The Optimization Framework Linear Models Convex Surrogate Loss Functions Weight Regularization Optimization Gradient Descent From Gradients Subgradients Closed form Optimization Squared Loss Support Vector Machines Exercises Probabilistic Modeling Classification Density Estimation Statistical Estimation Naive Bayes Models Prediction Generative Stories Conditional Models Regularization Priors Exercises Neural Networks Bio inspired Multi Layer Networks The Back propagation Algorithm Initialization Convergence Neural Networks Beyond Two Layers Breadth versus Depth Basis Functions Exercises Kernel Methods From Feature Combinations Kernels Kernelized Perceptron Kernelized K means What Makes Kernel Support Vector Machines Understanding Support Vector Machines Exercises Learning Theory The Role Theory Induction Impossible Probably Approximately Correct Learning PAC Learning Conjunctions Occam s Razor Simple Solutions Generalize Complexity Infinite Hypothesis Spaces Learning Noise Agnostic Learning Error versus Regret Exercises Ensemble Methods Voting Multiple Classifiers Boosting Weak Learners Random Ensembles Exercises Efficient Learning What Does Mean Fast Stochastic Optimization Sparse Regularization Feature Hashing Exercises Unsupervised Learning K Means Clustering Revisited Linear Dimensionality Reduction Manifolds Graphs Non linear Dimensionality Reduction Non linear Clustering Spectral Methods Exercises Expectation Maximization Clustering Mixture Gaussians The Expectation Maximization Framework EM versus Gradient Descent Dimensionality Reduction Probabilistic PCA Exercises Semi Supervised Learning EM Semi Supervised Learning Graph based Semi Supervised Learning Loss based Semi Supervised Learning Active Learning Dangers Semi Supervised Learing Exercises Graphical Models Exercises Online Learning Online Learning Framework Learning Features Passive Agressive Learning Learning Lots Irrelevant Features Exercises Structured Learning Tasks Exercises Bayesian Learning Exercises Code Datasets Notation Bibliography Index\n",
            "4 []\n",
            "4 My title Bayesian Reasoning Machine Learning David Barber c Notation List V calligraphic symbol typically denotes set random variables dom x Domain variable x x The variable x state x p x tr probability event variable x state true p x fa probability event variable x state false p x y probability x y p x y probability x y p x y probability x y p x y The probability x conditioned y X Y Z Variables X independent variables Y conditioned variables Z X Y Z Variables X dependent variables Y conditioned variables Z x f x For continuous variables shorthand f x dx discrete vari ables means summation states x x f x I S Indicator value statement S true pa x The parents node x ch x The children node x ne x Neighbours node x dim x For discrete variable x denotes number states x f x p x The average function f x respect distribution p x b Delta function For discrete b Kronecker delta b continuous b Dirac delta function b dim x The dimension vector matrix x x s y t The number times x state s y state t simultaneously xy The number times variable x state y D Dataset n Data index N Number dataset training points S Sample Covariance matrix x The logistic sigmoid exp x erf x The Gaussian error function xa b xa xa xb j The set unique neighbouring edges graph I am The m m identity matrix II DRAFT April Preface The data explosion We live world rich data increasing scale This data comes different sources science bioinformatics astronomy physics environmental monitoring commerce customer databases financial transactions engine monitoring speech recognition surveillance search Possessing knowledge process extract value data key increasingly important skill Our society expects ultimately able engage computers natural manner computers talk humans understand comprehend visual world These difficult large scale information processing tasks represent grand challenges computer science related fields Similarly desire control increasingly complex systems possibly containing interacting parts robotics autonomous navigation Successfully mastering systems requires understanding processes underlying behaviour Processing making sense large amounts data complex systems pressing modern day concern likely remain foreseeable future Machine Learning Machine Learning study data driven methods capable mimicking understanding aiding human biological information processing tasks In pursuit related issues arise compress data interpret process Often methods necessarily directed mimicking directly human processing enhance predicting stock market retrieving information rapidly In probability theory key inevitably limited data understanding problem forces address uncertainty In broadest sense Machine Learning related fields aim learn useful environment agent operates Machine Learning closely allied Artificial Intelligence Machine Learning placing emphasis data drive adapt model In early stages Machine Learning related areas similar techniques discovered relatively isolated research communities This book presents unified treatment graphical models marriage graph probability theory facilitating transference Machine Learning concepts different branches mathematical computational sciences Whom book The book designed appeal students modest mathematical background undergraduate calculus linear algebra No formal computer science statistical background required follow book basic familiarity probability calculus linear algebra useful The book appeal students variety backgrounds including Computer Science Engineering applied Statistics Physics Bioinformatics wish gain entry probabilistic approaches Machine Learning In order engage students book introduces fundamental concepts inference III minimal reference algebra calculus More mathematical techniques postponed required concept primary mathematics secondary The concepts algorithms described aid worked examples The exercises demonstrations accompanying MATLAB toolbox enable reader experiment deeply understand material The ultimate aim book enable reader construct novel algorithms The book places emphasis skill learning collection recipes This key aspect modern applications specialised require novel methods The approach taken describe problem graphical model translated mathematical framework ultimately leading algorithmic implementation BRMLtoolbox The book primarily aimed final year undergraduates graduates significant experience mathematics On completion reader good understanding techniques practicalities philosophies probabilistic aspects Machine Learning equipped understand advanced research level material The structure book The book begins basic concepts graphical models inference For independent reader chapters form good introduction probabilistic reasoning modelling Machine Learning The material chapters advanced remaining material specialised interest Note chapter level material varying difficulty typically challenging material placed end chapter As introduction area probabilistic modelling course constructed material indicated chart The material parts I II successfully courses Graphical Models I taught introduction Probabilistic Machine Learning material largely III indicated These courses taught separately useful approach teach Graphical Models course followed separate Probabilistic Machine Learning course A short course approximate inference constructed introductory material I advanced material V indicated The exact inference methods I covered relatively quickly material V considered depth A timeseries course primarily material IV possibly combined material I students unfamiliar probabilistic modelling approaches Some material particularly chapter advanced deferred end course considered advanced course The references generally works level consistent book material readily available Accompanying code The BRMLtoolbox provided help readers mathematical models translate actual MAT LAB code There large number demos lecturer wish use adapt help illustrate material In addition exercises use code helping reader gain confidence concepts application Along complete routines Machine Learning methods philosophy provide low level routines composition intuitively follows mathematical de scription algorithm In way students easily match mathematics corresponding algorithmic implementation IV DRAFT April Probabilistic Reasoning Basic Graph Concepts Belief Networks Graphical Models Efficient Inference Trees The Junction Tree Algorithm Making Decisions Statistics Machine Learning Learning Inference Naive Bayes Learning Hidden Variables Bayesian Model Selection Machine Learning Concepts Nearest Neighbour Classification Unsupervised Linear Dimension Reduction Supervised Linear Dimension Reduction Linear Models Bayesian Linear Models Gaussian Processes Mixture Models Latent Linear Models Latent Ability Models Discrete State Markov Models Continuous State Markov Models Switching Linear Dynamical Systems Distributed Computation Sampling Deterministic Approximate Inference G ra p h ic l M o d el s C o u rs e P ro b b il ti c M ch e L ea rn g C o u rs e A p p ro x I am te In fe n ce S h o rt C o u rs e T I am e se ri es S h o rt C o u rs e P ro b b il ti c M o d el li n g C o u rs e Part I Inference Probabilistic Models Part II Learning Probabilistic Models Part III Machine Learning Part IV Dynamical Models Part V Approximate Inference Website The BRMLtoolbox electronic version book available www cs ucl ac uk staff D Barber brml Instructors seeking solutions exercises find information website additional teaching materials DRAFT April V http www cs ucl ac uk staff D Barber brml Other books area The literature Machine Learning vast relevant literature contained statistics en gineering physical sciences A small list specialised books referred deeper treatments specific topics Graphical models Graphical models S Lauritzen Oxford University Press Bayesian Networks Decision Graphs F Jensen T D Nielsen Springer Verlag Probabilistic Networks Expert Systems R G Cowell A P Dawid S L Lauritzen D J Spiegelhalter Springer Verlag Probabilistic Reasoning Intelligent Systems J Pearl Morgan Kaufmann Graphical Models Applied Multivariate Statistics J Whittaker Wiley Probabilistic Graphical Models Principles Techniques D Koller N Friedman MIT Press Machine Learning Information Processing Information Theory Inference Learning Algorithms D J C MacKay Cambridge Uni versity Press Pattern Recognition Machine Learning C M Bishop Springer Verlag An Introduction To Support Vector Machines N Cristianini J Shawe Taylor Cambridge University Press Gaussian Processes Machine Learning C E Rasmussen C K I Williams MIT press Acknowledgements Many people helped book way terms reading feedback general insights allowing present work plain motivation Amongst I like thank Dan Cornford Massimiliano Pontil Mark Herbster John Shawe Taylor Vladimir Kolmogorov Yuri Boykov Tom Minka Simon Prince Silvia Chiappa Bertrand Mesot Robert Cowell Ali Taylan Cemgil David Blei Jeff Bilmes David Cohn David Page Peter Sollich Chris Williams Marc Toussaint Amos Storkey Za kria Hussain Le Chen Seraf n Moral Milan Studeny Luc De Raedt Tristan Fletcher Chris Vryonides Yannis Haralambous particularly help example Tom Furmston Ed Challis Chris Bracegirdle I like thank students helped improve material lectures years I m particularly grateful Taylan Cemgil allowing GraphLayout package bundled BRMLtoolbox The staff Cambridge University Press delight work I especially like thank Heather Bergman initial endeavors wonderful Diana Gillooly continued enthu siasm A heartfelt thankyou parents sister I hope small token proud I m fortunate able acknowledge support generosity friends Finally I d like thank Silvia worthwhile VI DRAFT April BRMLtoolbox The BRMLtoolbox lightweight set routines enables reader experiment concepts graph theory probability theory Machine Learning The code contains basic routines manipulating discrete variable distributions limited support continuous variables In addition hard coded standard Machine Learning algorithms The website contains complete list teaching demos related exercise material BRMLTOOLKIT Graph Theory ancestors Return ancestors nodes x DAG A ancestralorder Return ancestral order DAG A oldest descendents Return descendents nodes x DAG A children return children variable x given adjacency matrix A edges Return edge list adjacency matrix A elimtri Return variable elimination sequence triangulated graph connectedComponents Find connected components adjacency matrix istree Check graph singly connected neigh Find neighbours vertex v graph adjacency matrix G noselfpath return path excluding self transitions parents return parents variable x given adjacency matrix A spantree Find spanning tree edge list triangulate Triangulate adjacency matrix A triangulatePorder Triangulate adjacency matrix A according partial ordering Potential manipulation condpot Return potential conditioned variable changevar Change variable names potential dag Return adjacency matrix zeros diagonal Belief Network deltapot A delta function potential disptable Print table potential divpots Divide potential pota potb drawFG Draw Factor Graph A drawID plot Influence Diagram drawJTree plot Junction Tree drawNet plot network evalpot Evaluate table potential variables set exppot exponential potential eyepot Return unit potential grouppot Form potential based grouping variables groupstate Find state group variables corresponding given ungrouped state logpot logarithm potential markov Return symmetric adjacency matrix Markov Network pot maxpot Maximise potential variables maxsumpot Maximise Sum potential variables multpots Multiply potentials single potential numstates Number states variables potential VII orderpot Return potential variables reordered according order orderpotfields Order fields potential creating blank entries necessary potsample Draw sample single potential potscontainingonly Returns potential numbers contain required variables potvariables Returns information variables set potentials setevpot Sets variables potential evidential states setpot sets potential variables specified states setstate set potential s specified joint state specified value squeezepots Eliminate redundant potentials contained wholly sumpot Sum potential pot variables sumpotID Return summed probability utility tables ID sumpots Sum set potentials table Return potential table ungrouppot Form potential based ungrouping variables uniquepots Eliminate redundant potentials contained wholly whichpot Returns potentials contain set variables Routines extend toolbox deal Gaussian potentials multpotsGaussianMoment m sumpotGaussianCanonical m sumpotGaussianMoment m multpotsGaussianCanonical m See demoSumprodGaussCanon m demoSumprodGaussCanonLDS m demoSumprodGaussMoment m Inference absorb Update potentials absorption message passing Junction Tree absorption Perform round absorption Junction Tree absorptionID Perform round absorption Influence Diagram ancestralsample Ancestral sampling Belief Network binaryMRFmap MAP assignment binary MRF positive W bucketelim Bucket Elimination set potentials condindep Conditional Independence check graph variable interactions condindepEmp Compute empirical log Bayes Factor MI independence dependence condindepPot Numerical conditional independence measure condMI conditional mutual information I x y z potential FactorConnectingVariable Factor nodes connecting set variables FactorGraph Returns Factor Graph adjacency matrix based potentials IDvars probability decision variables partial order jtassignpot Assign potentials cliques Junction Tree jtree Setup Junction Tree based set potentials jtreeID Setup Junction Tree based Influence Diagram LoopyBP loopy Belief Propagation sum product algorithm MaxFlow Ford Fulkerson max flow min cut algorithm breadth search maxNpot Find N probable values states potential maxNprodFG N Max Product algorithm Factor Graph Returns Nmax probable States maxprodFG Max Product algorithm Factor Graph MDPemDeterministicPolicy Solve MDP EM deterministic policy MDPsolve Solve Markov Decision Process MesstoFact Returns message numbers connect factor potential metropolis Metropolis sample mostprobablepath Find probable path Markov Chain mostprobablepathmult Find source sink probable paths Markov Chain sumprodFG Sum Product algorithm Factor Graph represented A Specific Models ARlds Learn AR coefficients Linear Dynamical System ARtrain Fit autoregressive AR coefficients order L v BayesLinReg Bayesian Linear Regression training basis functions phi x BayesLogRegressionRVM Bayesian Logistic Regression Relevance Vector Machine CanonVar Canonical Variates post rotation variates cca canonical correlation analysis covfnGE Gamma Exponential Covariance Function EMbeliefnet train Belief Network Expectation Maximisation EMminimizeKL MDP deterministic policy solver Finds optimal actions EMqTranMarginal EM marginal transition MDP EMqUtilMarginal Returns term proportional q marginal utility term EMTotalBetaMessage backward information needed solve MDP process message passing EMvalueTable MDP solver calculates value function MDP current policy FA Factor Analysis VIII DRAFT April GMMem Fit mixture Gaussian data X EM GPclass Gaussian Process Binary Classification GPreg Gaussian Process Regression HebbML Learn sequence Hopfield Network HMMbackward HMM Backward Pass HMMbackwardSAR Backward Pass beta method Switching Autoregressive HMM HMMem EM algorithm HMM HMMforward HMM Forward Pass HMMforwardSAR Switching Autoregressive HMM switches updated Tskip timesteps HMMgamma HMM Posterior smoothing Rauch Tung Striebel correction method HMMsmooth Smoothing Hidden Markov Model HMM HMMsmoothSAR Switching Autoregressive HMM smoothing HMMviterbi Viterbi likely joint hidden state HMM kernel A kernel evaluated points Kmeans K means clustering algorithm LDSbackward Full Backward Pass Latent Linear Dynamical System RTS correction method LDSbackwardUpdate Single Backward update Latent Linear Dynamical System RTS smoothing update LDSforward Full Forward Pass Latent Linear Dynamical System Kalman Filter LDSforwardUpdate Single Forward update Latent Linear Dynamical System Kalman Filter LDSsmooth Linear Dynamical System Filtering Smoothing LDSsubspace Subspace Method identifying Linear Dynamical System LogReg Learning Logistic Linear Regression Using Gradient Ascent BATCH VERSION MIXprodBern EM training Mixture product Bernoulli distributions mixMarkov EM training mixture Markov Models NaiveBayesDirichletTest Naive Bayes prediction having Dirichlet prior training NaiveBayesDirichletTrain Naive Bayes training Dirichlet prior NaiveBayesTest Test Naive Bayes Bernoulli Distribution Max Likelihood training NaiveBayesTrain Train Naive Bayes Bernoulli Distribution Max Likelihood nearNeigh Nearest Neighbour classification pca Principal Components Analysis plsa Probabilistic Latent Semantic Analysis plsaCond Conditional PLSA Probabilistic Latent Semantic Analysis rbf Radial Basis function output SARlearn EM training Switching AR model SLDSbackward Backward pass Mixture Gaussians SLDSforward Switching Latent Linear Dynamical System Gaussian Sum forward pass SLDSmargGauss compute single Gaussian weighted SLDS mixture softloss Soft loss function svdm Singular Value Decomposition missing values SVMtrain train Support vector Machine General argmax performs argmax returning index value assign Assigns values variables betaXbiggerY p x y x Beta b y Beta c d bar3zcolor Plot 3D bar plot matrix Z avsigmaGauss Average logistic sigmoid Gaussian cap Cap x absolute value c chi2test inverse chi square cumulative density count data matrix column datapoint return state counts condexp Compute normalised p proportional exp logp condp Make conditional distribution matrix dirrnd Samples Dirichlet distribution field2cell Place field structure cell GaussCond Return mean covariance conditioned Gaussian hinton Plot Hinton diagram ind2subv Subscript vector linear index ismember_sorted True member sorted set lengthcell Length cell entry logdet Log determinant positive definite matrix computed numerically stable manner logeps log x eps logGaussGamma unnormalised log Gauss Gamma distribution logsumexp Compute log sum exp b valid large logZdirichlet Log Normalisation constant Dirichlet distribution parameter u majority Return majority values column matrix maxarray Maximise multi dimensional array set dimensions maxNarray Find highest values states array set dimensions DRAFT April IX mix2mix Fit mixture Gaussians mixture Gaussians mvrandn Samples multi variate Normal Gaussian distribution mygamrnd Gamma random variate generator mynanmean mean values nan mynansum sum values nan mynchoosek binomial coefficient v choose k myones ones x x scalar interprets ones x myrand rand x x scalar interprets rand x myzeros zeros x x scalar interprets zeros x normp Make normalised distribution array randgen Generates discrete random variables given pdf replace Replace instances value value sigma exp x sigmoid exp beta x sqdist Square distance vectors x y subv2ind Linear index subscript vector sumlog sum log x cutoff 10e Miscellaneous compat Compatibility object F position h image v grid Gx Gy logp The logarithm specific non Gaussian distribution placeobject Place object F position h grid Gx Gy plotCov return points plotting ellipse covariance pointsCov unit variance contours 2D Gaussian mean m covariance S setup run initialisation checks bugs matlab initialises path validgridposition Returns point defined grid X DRAFT April Contents Front Matter I Notation List II Preface II BRML toolbox VII Contents XI I Inference Probabilistic Models Probabilistic Reasoning Probability Refresher Interpreting Conditional Probability Probability Tables Probabilistic Reasoning Prior Likelihood Posterior Two dice individual scores Summary Code Basic Probability code General utilities An example Exercises Basic Graph Concepts Graphs Numerically Encoding Graphs Edge list Adjacency matrix Clique matrix Summary Code Utility routines Exercises Belief Networks The Benefits Structure Modelling independencies Reducing burden specification Uncertain Unreliable Evidence Uncertain evidence XI CONTENTS CONTENTS Unreliable evidence Belief Networks Conditional independence The impact collisions Graphical path manipulations independence d Separation Graphical distributional dependence Markov equivalence belief networks Belief networks limited expressibility Causality Simpson s paradox The calculus Influence diagrams calculus Summary Code Naive inference demo Conditional independence demo Utility routines Exercises Graphical Models Graphical Models Markov Networks Markov properties Markov random fields Hammersley Clifford Theorem Conditional independence Markov networks Lattice Models Chain Graphical Models Factor Graphs Conditional independence factor graphs Expressiveness Graphical Models Summary Code Exercises Efficient Inference Trees Marginal Inference Variable elimination Markov chain message passing The sum product algorithm factor graphs Dealing Evidence Computing marginal likelihood The problem loops Other Forms Inference Max Product Finding N probable states Most probable path shortest path Mixed inference Inference Multiply Connected Graphs Bucket elimination Loop cut conditioning Message Passing Continuous Distributions Summary Code Factor graph examples Most probable shortest path XII DRAFT April CONTENTS CONTENTS Bucket elimination Message passing Gaussians Exercises The Junction Tree Algorithm Clustering Variables Reparameterisation Clique Graphs Absorption Absorption schedule clique trees Junction Trees The running intersection property Constructing Junction Tree Singly Connected Distributions Moralisation Forming clique graph Forming junction tree clique graph Assigning potentials cliques Junction Trees Multiply Connected Distributions Triangulation algorithms The Junction Tree Algorithm Remarks JTA Computing normalisation constant distribution The marginal likelihood Some small JTA examples Shafer Shenoy propagation Finding Most Likely State Reabsorption Converting Junction Tree Directed Network The Need For Approximations Bounded width junction trees Summary Code Utility routines Exercises Making Decisions Expected Utility Utility money Decision Trees Extending Bayesian Networks Decisions Syntax influence diagrams Solving Influence Diagrams Messages ID Using junction tree Markov Decision Processes Maximising expected utility message passing Bellman s equation Temporally Unbounded MDPs Value iteration Policy iteration A curse dimensionality Variational Inference Planning Financial Matters Options pricing expected utility Binomial options pricing model Optimal investment Further Topics DRAFT April XIII CONTENTS CONTENTS Partially observable MDPs Reinforcement learning Summary Code Sum Max partial order Junction trees influence diagrams Party Friend example Chest Clinic Decisions Markov decision processes Exercises II Learning Probabilistic Models Statistics Machine Learning Representing Data Categorical Ordinal Numerical Distributions The Kullback Leibler Divergence KL q p Entropy information Classical Distributions Multivariate Gaussian Completing square Conditioning system reversal Whitening centering Exponential Family Conjugate priors Learning distributions Properties Maximum Likelihood Training assuming correct model class Training assumed model incorrect Maximum likelihood empirical distribution Learning Gaussian Maximum likelihood training Bayesian inference mean variance Gauss Gamma distribution Summary Code Exercises Learning Inference Learning Inference Learning bias coin Making decisions A continuum parameters Decisions based continuous intervals Bayesian methods ML II Maximum Likelihood Training Belief Networks Bayesian Belief Network Training Global local parameter independence Learning binary variable tables Beta prior Learning multivariate discrete tables Dirichlet prior Structure learning PC algorithm XIV DRAFT April CONTENTS CONTENTS Empirical independence Network scoring Chow Liu Trees Maximum Likelihood Undirected models The likelihood gradient General tabular clique potentials Decomposable Markov networks Exponential form potentials Conditional random fields Pseudo likelihood Learning structure Summary Code PC algorithm oracle Demo empirical conditional independence Bayes Dirichlet structure learning Exercises Naive Bayes Naive Bayes Conditional Independence Estimation Maximum Likelihood Binary attributes Multi state variables Text classification Bayesian Naive Bayes Tree Augmented Naive Bayes Learning tree augmented Naive Bayes networks Summary Code Exercises Learning Hidden Variables Hidden Variables Missing Data Why hidden missing variables complicate proceedings The missing random assumption Maximum likelihood Identifiability issues Expectation Maximisation Variational EM Classical EM Application Belief networks General case Convergence Application Markov networks Extensions EM Partial M step Partial E step A failure case EM Variational Bayes EM special case variational Bayes An example VB Asbestos Smoking Cancer network Optimising Likelihood Gradient Methods Undirected models Summary Code Exercises DRAFT April XV CONTENTS CONTENTS Bayesian Model Selection Comparing Models Bayesian Way Illustrations coin tossing A discrete parameter space A continuous parameter space Occam s Razor Bayesian Complexity Penalisation A continuous example curve fitting Approximating Model Likelihood Laplace s method Bayes information criterion BIC Bayesian Hypothesis Testing Outcome Analysis Outcome analysis Hindep model likelihood Hsame model likelihood Dependent outcome analysis Is classifier A better B Summary Code Exercises III Machine Learning Machine Learning Concepts Styles Learning Supervised learning Unsupervised learning Anomaly detection Online sequential learning Interacting environment Semi supervised learning Supervised Learning Utility Loss Using empirical distribution Bayesian decision approach Bayes versus Empirical Decisions Summary Exercises Nearest Neighbour Classification Do As Your Neighbour Does K Nearest Neighbours A Probabilistic Interpretation Nearest Neighbours When nearest neighbour far away Summary Code Exercises Unsupervised Linear Dimension Reduction High Dimensional Spaces Low Dimensional Manifolds Principal Components Analysis Deriving optimal linear reconstruction Maximum variance criterion PCA algorithm PCA nearest neighbours classification Comments PCA XVI DRAFT April CONTENTS CONTENTS High Dimensional Data Eigen decomposition N D PCA Singular value decomposition Latent Semantic Analysis Information retrieval PCA With Missing Data Finding principal directions Collaborative filtering PCA missing data Matrix Decomposition Methods Probabilistic latent semantic analysis Extensions variations Applications PLSA NMF Kernel PCA Canonical Correlation Analysis SVD formulation Summary 10Code 11Exercises Supervised Linear Dimension Reduction Supervised Linear Projections Fisher s Linear Discriminant Canonical Variates Dealing nullspace Summary Code Exercises Linear Models Introduction Fitting A Straight Line Linear Parameter Models Regression Vector outputs Regularisation Radial basis functions The Dual Representation Kernels Regression dual space Linear Parameter Models Classification Logistic regression Beyond order gradient ascent Avoiding overconfident classification Multiple classes The Kernel Trick Classification Support Vector Machines Maximum margin linear classifier Using kernels Performing optimisation Probabilistic interpretation Soft Zero One Loss Outlier Robustness Summary Code Exercises DRAFT April XVII CONTENTS CONTENTS Bayesian Linear Models Regression With Additive Gaussian Noise Bayesian linear parameter models Determining hyperparameters ML II Learning hyperparameters EM Hyperparameter optimisation gradient Validation likelihood Prediction model averaging Sparse linear models Classification Hyperparameter optimisation Laplace approximation Variational Gaussian approximation Local variational approximation Relevance vector machine classification Multi class case Summary Code Exercises Gaussian Processes Non Parametric Prediction From parametric non parametric From Bayesian linear models Gaussian processes A prior functions Gaussian Process Prediction Regression noisy training outputs Covariance Functions Making new covariance functions old Stationary covariance functions Non stationary covariance functions Analysis Covariance Functions Smoothness functions Mercer kernels Fourier analysis stationary kernels Gaussian Processes Classification Binary classification Laplace s approximation Hyperparameter optimisation Multiple classes Summary Code Exercises Mixture Models Density Estimation Using Mixtures Expectation Maximisation Mixture Models Unconstrained discrete tables Mixture product Bernoulli distributions The Gaussian Mixture Model EM algorithm Practical issues Classification Gaussian mixture models The Parzen estimator K Means Bayesian mixture models XVIII DRAFT April CONTENTS CONTENTS Semi supervised learning Mixture Experts Indicator Models Joint indicator approach factorised prior Polya prior Mixed Membership Models Latent Dirichlet allocation Graph based representations data Dyadic data Monadic data Cliques adjacency matrices monadic binary data Summary Code Exercises Latent Linear Models Factor Analysis Finding optimal bias Factor Analysis Maximum Likelihood Eigen approach likelihood optimisation Expectation maximisation Interlude Modelling Faces Probabilistic Principal Components Analysis Canonical Correlation Analysis Factor Analysis Independent Components Analysis Summary Code Exercises Latent Ability Models The Rasch Model Maximum likelihood training Bayesian Rasch models Competition Models Bradley Terry Luce model Elo ranking model Glicko TrueSkill Summary Code Exercises IV Dynamical Models Discrete State Markov Models Markov Models Equilibrium stationary distribution Markov chain Fitting Markov models Mixture Markov models Hidden Markov Models The classical inference problems Filtering p ht v1 t Parallel smoothing p ht v1 T Correction smoothing Sampling p h1 T v1 T Most likely joint state DRAFT April XIX CONTENTS CONTENTS Prediction Self localisation kidnapped robots Natural language models Learning HMMs EM algorithm Mixture emission The HMM GMM Discriminative training Related Models Explicit duration model Input Output HMM Linear chain CRFs Dynamic Bayesian networks Applications Object tracking Automatic speech recognition Bioinformatics Part speech tagging Summary Code Exercises Continuous state Markov Models Observed Linear Dynamical Systems Stationary distribution noise Auto Regressive Models Training AR model AR model OLDS Time varying AR model Time varying variance AR models Latent Linear Dynamical Systems Inference Filtering Smoothing Rauch Tung Striebel correction method The likelihood Most likely state Time independence Riccati equations Learning Linear Dynamical Systems Identifiability issues EM algorithm Subspace Methods Structured LDSs Bayesian LDSs Switching Auto Regressive Models Inference Maximum likelihood learning EM Summary Code Autoregressive models Exercises XX DRAFT April CONTENTS CONTENTS Switching Linear Dynamical Systems Introduction The Switching LDS Exact inference computationally intractable Gaussian Sum Filtering Continuous filtering Discrete filtering The likelihood p v1 T Collapsing Gaussians Relation methods Gaussian Sum Smoothing Continuous smoothing Discrete smoothing Collapsing mixture Using mixtures smoothing Relation methods Reset Models A Poisson reset model Reset HMM LDS Summary Code Exercises Distributed Computation Introduction Stochastic Hopfield Networks Learning Sequences A single sequence Multiple sequences Boolean networks Sequence disambiguation Tractable Continuous Latent Variable Models Deterministic latent variables An augmented Hopfield network Neural Models Stochastically spiking neurons Hopfield membrane potential Dynamic synapses Leaky integrate fire models Summary Code Exercises V Approximate Inference Sampling Introduction Univariate sampling Rejection sampling Multivariate sampling Ancestral Sampling Dealing evidence Perfect sampling Markov network Gibbs Sampling Gibbs sampling Markov chain DRAFT April XXI CONTENTS CONTENTS Structured Gibbs sampling Remarks Markov Chain Monte Carlo MCMC Markov chains Metropolis Hastings sampling Auxiliary Variable Methods Hybrid Monte Carlo Swendson Wang Slice sampling Importance Sampling Sequential importance sampling Particle filtering approximate forward pass Summary Code Exercises Deterministic Approximate Inference Introduction The Laplace approximation Properties Kullback Leibler Variational Inference Bounding normalisation constant Bounding marginal likelihood Bounding marginal quantities Gaussian approximations KL divergence Marginal moment matching properties minimising KL p q Variational Bounding Using KL q p Pairwise Markov random field General mean field equations Asynchronous updating guarantees approximation improvement Structured variational approximation Local KL Variational Approximations Local approximation KL variational approximation Mutual Information Maximisation A KL Variational Approach The information maximisation algorithm Linear Gaussian decoder Loopy Belief Propagation Classical BP undirected graph Loopy BP variational procedure Expectation Propagation MAP Markov networks Pairwise Markov networks Attractive binary Markov networks Potts model 10Further Reading 11Summary 12Code 13Exercises End Matter VI Appendix A Background Mathematics XXII DRAFT April CONTENTS CONTENTS A Linear Algebra A Vector algebra A The scalar product projection A Lines space A Planes hyperplanes A Matrices A Linear transformations A Determinants A Matrix inversion A Computing matrix inverse A Eigenvalues eigenvectors A Matrix decompositions A Multivariate Calculus A Interpreting gradient vector A Higher derivatives A Matrix calculus A Inequalities A Convexity A Jensen s inequality A Optimisation A Multivariate Optimisation A Gradient descent fixed stepsize A Gradient descent line searches A Minimising quadratic functions line search A Gram Schmidt construction conjugate vectors A The conjugate vectors algorithm A The conjugate gradients algorithm A Newton s method A Constrained Optimisation Lagrange Multipliers A Lagrange Dual Bibliography Index DRAFT April XXIII CONTENTS CONTENTS XXIV DRAFT April Part I Inference Probabilistic Models Introduction Part I Probabilistic models explicitly account uncertainty deal imperfect knowledge world Such models fundamental significance Machine Learning understanding world limited observations understanding We focus initially probabilistic models kind expert system In Part I assume model fully specified That given model environment use answer questions interest We relate complexity inferring quantities interest structure graph describing model In addition describe operations terms manipulations corresponding graphs As provided graphs simple tree like structures quantities interest computed efficiently Part I deals manipulating mainly discrete variable distributions forms background later material book DRAFT April Graphical Models Directed Directed Factor Graph Bayesian Networks Dynamic Bayes nets Markov chains HMM LDS Latent variable models Discrete Mixture models cluster ing Continuous dimen reduct complete repres Influence diagrams Strong JT Decision theory Chain Graphs Undirected Graphs Markov network input dependent CRF Pairwise Boltz machine disc Gauss Process cont Clique Graphs Junction treeClique tree Factor Graphs Some members graphical models family uses Nodes Graphical Models root node loosely speaking specialised versions parents We discuss models Part I specialised models deferred later Parts book DRAFT April Graphical model Multiply connected decomposable cliques small messages tractable message passing messages intractable approx required cliques large approx required non decomposable JTA cliques small absorption Shafer Shenoy cliques big mess intract approx required cutset conditioning inefficient tractable special cases Gaussian attractive binary MRF MAP planar binary pure interaction MRF Singly connected message updates tractable sum max product message updates intractable approx required EP Bucket elimination inefficient Graphical models associated marginal inference methods Specific inference methods highlighted red Loosely speaking provided graph corresponding model singly connected standard marginal inference methods tractable Multiply connected graphs generally problematic special cases remain tractable DRAFT April DRAFT April CHAPTER Probabilistic Reasoning We intuition uncertainty works simple cases To reach sensible conclusions complicated situations possibly related events possible outcomes need formal calculus extends intuitive notions The concepts mathematical language rules probability formal framework need In chapter review basic concepts probability particular conditional probability Bayes rule workhorses machine learning Another strength language probability structures problems form consistent computer implementation We introduce basic features BRMLtoolbox support manipulating probability distributions Probability Refresher Variables States Notational Shortcuts Variables denoted upper case X lower case x set variables typically denoted calligraphic symbol example V B c The domain variable x written dom x denotes states x States typically represented sans serif font For example coin c dom c heads tails p c heads represents probability variable c state heads The meaning p state clear specific reference variable For example discussing experiment coin c meaning p heads clear context shorthand p c heads When summing variable x f x interpretation states x included e x f x s dom x f x s Given variable x domain dom x specification probability values variable states p x distribution x Sometimes fully specify distribution certain properties variables x y p x y p x p y unspecified p x p y When clarity required distributions structure p x p y distribution class p x p y For purposes events expressions random variables Two heads coin tosses Two events mutually exclusive true For example events The coin heads The coin tails mutually exclusive One think defining new variable named event example p The coin tails interpreted p The coin tails true We use shorthand p x tr probability event variable x state true p x fa probability variable x state false Definition Rules Probability Discrete Variables Probability Refresher The probability p x x variable x state x represented value p x x means certain x state x Conversely p x x means certain x state x Values represent degree certainty state occupancy The summation probability states x dom x p x x This called normalisation condition We usually conveniently write x p x Two variables x y interact p x y b p x p y b p x y b Or generally write p x y p x p y p x y We use shorthand p x y p x y Note p y x p x y p x y p y x Definition Set notation An alternative notation terms set theory write p x y p x y p x y p x y Definition Marginals Given joint distribution p x y distribution single variable given p x y p x y Here p x termed marginal joint probability distribution p x y The process computing marginal joint distribution called marginalisation More generally p x1 xi xi xn xi p x1 xn Definition Conditional Probability Bayes Rule The probability event x conditioned knowing event y shortly probability x given y defined p x y p x y p y If p y p x y defined From definition p x y p y x immediately arrive Bayes rule p x y p y x p x p y Since Bayes rule trivially follows definition conditional probability loose language use terms Bayes rule conditional probability synonymous As shall book Bayes rule plays central role probabilistic reasoning helps DRAFT April Probability Refresher invert probabilistic relationships translating p y x p x y Definition Probability Density Functions For continuous variable x probability density f x defined f x f x dx probability x falls interval b given p x b b f x dx As shorthand write x f x particularly want expression valid continuous discrete variables The multivariate case analogous integration real space probability x belongs region space defined accordingly Unlike probabilities probability densities positive values greater Formally speaking continuous variable speak probability x probability single value zero However shall write p x continuous variables distinguishing probabilities probability density function values Whilst appear strange nervous reader simply replace p x notation x f x dx small region centred x This defined probabilistic sense limit small approximately f x If consistently use occurrences pdfs simply common prefactor expressions Our strategy simply ignore values end relative probabilities relevant write p x In way standard rules probability carry including Bayes Rule Remark Subjective Probability Probability contentious topic wish bogged debate apart pointing necessarily rules probability contentious interpretation place In cases potential repetitions experiment envisaged long run frequentist definition probability probabilities defined respect potentially infinite repetition experiments makes sense For example coin tossing probability heads interpreted If I repeat experiment flipping coin random limit number heads occurred number tosses defined probability head occurring Here s problem typical kind scenario face machine learning situation A film enthusiast joins new online film service Based expressing films user likes dislikes online company tries estimate probability user like films database If define probability limiting case infinite repetitions experiment wouldn t sense case t repeat experiment However assume user behaves manner consistent users able exploit large data users ratings reasonable guess consumer likes This degree belief Bayesian subjective interpretation probability sidesteps non repeatability issues s framework manipulating real values consistent intuition probability Interpreting Conditional Probability Conditional probability matches intuitive understanding uncertainty For example imagine circular dart board split equal sections labelled Randy dart thrower hits sections uniformly random Hence probability dart thrown Randy occurs regions p region A friend Randy tells hasn t hit region What probability Randy hit region Conditioned information regions remain possible preference Randy hit regions probability The DRAFT April Probability Refresher conditioning means certain states inaccessible original probability subsequently distributed remaining accessible states From rules probability p region region p region region p region p region p region giving intuitive result An important point clarify p A B b interpreted Given event B b occurred p A B b probability event A occurring In contexts explicit temporal causality implied1 correct interpretation p A B b probability A state constraint B state b The relation conditional p A B b joint p A B b normalisation constant p A B b distribution A words p A B b To distribution need divide p A B b p A B b summed sum Indeed definition p A B b Definition Independence Variables x y independent knowing state value continuous case variable gives extra information variable Mathematically expressed p x y p x p y Provided p x p y independence x y equivalent p x y p x p y x p y If p x y p x states x y variables x y said independent If p x y kf x g y constant k positive functions f g x y independent write x y Example Independence Let x denote day week females born y denote day males born dom x dom y It reasonable expect x independent y We randomly select woman phone book Alice find born Tuesday We randomly select male random Bob Before phoning Bob asking knowing Alice s birth day add day think Bob born Under independence assumption answer Note doesn t mean distribution Bob s birthday necessarily uniform means knowing Alice born doesn t provide extra information knew Bob s birthday p y x p y Indeed distribution birthdays p y p x non uniform statistically fewer babies born weekends suggest x y dependent Deterministic Dependencies Sometimes concept independence little strange Consider following variables x y binary domains consist states We define distribution x y certain joint state p x y p x y p x b y p x b y Are x y dependent The reader p x p x b p y p y Hence p x p y p x y states x y x y independent 1We discuss issues related causality section DRAFT April Probability Refresher This strange know sure relation x y joint state independent Since distribution trivially concentrated single joint state knowing state x tells didn t know state y vice versa This potential confusion comes term independent suggest relation objects discussed The best way think statistical independence ask knowing state variable y tells knew variable x knew means working joint distribution p x y figure know x p x Definition Conditional Independence X Y Z denotes sets variables X Y independent provided know state set variables Z For conditional independence X Y independent given states Z Formally means p X Y Z p X Z p Y Z states X Y Z In case conditioning set write X Y X Y case X unconditionally independent Y If X Y conditionally independent conditionally dependent This written X Y Z Similarly X Y written X Y Intuitively x conditionally independent y given z means given z y contains additional information x Similarly given z knowing x tell y Note X Y Z X Y Z X X Y Y Remark Independence implications It s tempting think independent b b independent c independent c b b c c However follow Consider example distribution form p b c p b p c From p b c p b c p b c p c Hence p b function b multiplied function b independent Similarly b c independent However necessarily independent c distribution p c set arbitrarily Similarly s tempting think b dependent b c dependent c dependent b b c c However follow We explicit numerical example exercise Finally note conditional independence x y z imply marginal independence x y See exercise DRAFT April Probabilistic Reasoning Probability Tables Based populations England E Scotland S Wales W priori probability randomly selected person combined countries live England Scotland Wales approximately respectively We write vector probability table p Cnt E p Cnt S p Cnt W component values sum The ordering components vector arbitrary long consistently applied For sake simplicity assume Mother Tongue languages exist English Eng Scottish Scot Welsh Wel conditional probabilities given country residence England E Scotland S Wales W We write fictitious conditional probability table p MT Eng Cnt E p MT Eng Cnt S p MT Eng Cnt W p MT Scot Cnt E p MT Scot Cnt S p MT Scot Cnt W p MT Wel Cnt E p MT Wel Cnt S p MT Wel Cnt W From form joint distribution p Cnt MT p MT Cnt p Cnt This written matrix columns indexed country rows indexed Mother Tongue The joint distribution contains information model environment By summing columns table marginal p Cnt Summing rows gives marginal p MT Similarly easily infer p Cnt MT p MT Cnt p Cnt joint distribution dividing entry equation row sum For joint distributions larger number variables xi D variable xi taking Ki states table describing joint distribution array D 1Ki entries Explicitly storing tables requires space exponential number variables rapidly impractical large number variables We discuss deal issue chapter chapter A probability distribution assigns value joint states variables For reason p T J R S considered equivalent p J S R T reordering variables case joint setting variables simply different index probability This situation clear set theoretic notation p J S T R We abbreviate set theoretic notation commas careful confuse use indexing type notation functions f x y general dependent variable order Whilst variables left conditioning bar written order equally right conditioning bar written order moving variables bar generally equivalent p x1 x2 p x2 x1 Probabilistic Reasoning The central paradigm probabilistic reasoning identify relevant variables x1 xN envi ronment probabilistic model p x1 xN interaction Reasoning inference performed introducing evidence sets variables known states subsequently computing proba bilities interest conditioned evidence The rules probability combined Bayes rule complete reasoning system includes traditional deductive logic special case In examples number variables environment small In chapter discuss DRAFT April Probabilistic Reasoning reasoning networks containing variables graphical notations chapter play central role Example Hamburgers Consider following fictitious scientific information Doctors find people Kreuzfeld Jacob disease KJ invariably ate hamburgers p Hamburger Eater KJ The probability individual having KJ currently low Assuming eating lots hamburgers widespread p Hamburger Eater probability hamburger eater Kreuzfeld Jacob disease This computed p KJ Hamburger Eater p Hamburger Eater KJ p Hamburger Eater p Hamburger Eater KJ p KJ p Hamburger Eater If fraction people eating hamburgers small p Hamburger Eater probability regular hamburger eater Kreuzfeld Jacob disease Repeating calculation given This higher scenario sure eating hamburgers related illness Example Inspector Clouseau Inspector Clouseau arrives scene crime The victim lies dead room alongside possible murder weapon knife The Butler B Maid M inspector s main suspects inspector prior belief Butler murderer prior belief Maid murderer These beliefs independent sense p B M p B p M It possible Butler Maid murdered victim The inspector s prior criminal knowledge formulated mathematically follows dom B dom M murderer murderer dom K knife knife p B murderer p M murderer p knife B murderer M murderer p knife B murderer M murderer p knife B murderer M murderer p knife B murderer M murderer In addition p K B M p K B M p B p M Assuming knife murder weapon probability Butler murderer Remember murderer Using b states B m states M p B K m p B m K m p B m K p K m p K B m p B m m b p K b m p b m p B m p K B m p m b p b m p K b m p m DRAFT April Probabilistic Reasoning fact model p B M p B p M Plugging values demoClouseau m p B murderer knife Hence knowing knife murder weapon strengthens belief butler Remark The role p knife Inspector Clouseau example because confusion In p knife b p b m p knife b m p m computed But surely p knife given question Note quantity p knife relates prior probability model assigns knife absence information If know knife posterior p knife knife p knife knife p knife p knife p knife naturally case Example Who s bathroom Consider household people Alice Bob Cecil Cecil wants bathroom finds occupied He goes Alice s room sees Since Cecil knows Alice Bob bathroom infers Bob bathroom To arrive conclusion mathematical framework define following events A Alice bedroom B Bob bedroom O Bathroom occupied We encode information Alice Bob bedrooms bathroom bathroom p O tr A fa B p O tr A B fa The term expresses bathroom occupied Alice bedroom Bob Similarly second term expresses bathroom occupancy long Bob bedroom Then p B fa O tr A tr p B fa O tr A tr p O tr A tr p O tr A tr B fa p A tr B fa p O tr A tr p O tr A tr p O tr A tr B fa p A tr B fa p O tr A tr B tr p A tr B tr Using fact p O tr A tr B fa p O tr A tr B tr encodes Alice room Bob bathroom occupied similarly Alice Bob rooms bathroom occupied p B fa O tr A tr p A tr B fa p A tr B fa This example interesting required probabilistic model case thanks limiting nature probabilities don t need specify p A B The situation common limiting situations probabilities corresponding traditional logic systems DRAFT April Probabilistic Reasoning Example Aristotle Modus Ponens According logic statements All apples fruit All fruits grow trees lead conclusion All apples grow trees This kind reasoning form transitivity statements A F F T infer A T To deduced Bayesian assume All apples fruit corresponds p F tr A tr All fruit grows trees corresponds p T tr F tr We want implies p T tr A tr Showing equivalent showing p T fa A tr assuming p A tr turn equivalent showing p T fa A tr Consider p T fa A tr p T fa A tr F tr p T fa A tr F fa We terms right zero First consider p T fa A tr F tr p T fa F tr p T fa F tr p F tr This zero assumption p T fa F tr p T tr F tr Similarly p T fa A tr F fa p A tr F fa p F fa A tr p A tr assumption p F fa A tr Example Aristotle Inverse Modus Ponens According Logic statement If A true B true deduce B false A false To fits probabilistic reasoning system express statement If A true B true p B tr A tr Then infer p A fa B fa p A tr B fa p B fa A tr p A tr p B fa A tr p A tr p B fa A fa p A fa This follows p B fa A tr p B tr A tr annihilating second term Both examples intuitive expressions deductive logic The standard rules Aristotelian logic seen limiting cases probabilistic reasoning Example Soft XOR Gate A standard XOR logic gate given table right If observe output XOR gate A B In case A B A B This means don t know state A equally likely A B A xor B DRAFT April Probabilistic Reasoning Consider soft version XOR gate given right gate stochastically outputs C depending inputs additionally A B p A p B What p A C A B p C A B p A C B p A B C B p C A B p A p B p A p C A B p B p C A B p B p A C B p A B C B p C A B p A p B p A p C A B p B p C A B p B Then p A C p A C p A C p A C Example Larry Larry typically late school If Larry late denote L late L late When mother asks late school admits late The response Larry gives RL represented follows p RL late L late p RL late L late The remaining values determined normalisation p RL late L late p RL late L late Given RL late probability Larry late e p L late RL late Using Bayes p L late RL late p L late RL late p RL late p L late RL late p L late RL late p L late RL late In p L late RL late p RL late L late p L late p L late RL late p RL late L late p L late Hence p L late RL late p L late p L late p L late p L late Where normalisation step p L late p L late This result intuitive Larry s mother knows admits late belief late unchanged regardless Larry actually says DRAFT April Probabilistic Reasoning Example Larry Sue Continuing example Larry s sister Sue tells truth mother Larry late School p RS late L late p RS late L late The remaining values determined normalisation p RS late L late p RS late L late We assume p RS RL L p RS L p RL L We write p RL RS L p RL L p RS L p L Given RS late RL late probability Larry late Using Bayes rule p L late RL late RS late Z p RS late L late p RL late L late p L late normalisation Z given p RS late L late p RL late L late p L late p RS late L late p RL late L late p L late Hence p L late RL late RS late p L late p L late p L late This result intuitive Since Larry s mother knows Sue tells truth matter Larry says knows late Example Luke Luke told s lucky won prize lottery There prizes available value The prior probabilities winning prizes p1 p2 p3 p4 p5 p0 prior probability winning prize Luke asks eagerly Did I win I m afraid sir response lottery phone operator Did I win asks Luke Again I m afraid sir What probability Luke won Note p0 p1 p2 p3 p4 p5 We denote W prize W remaining prizes W prize We need compute p W W W W p W W W W p W W W p W p W W W p3 p1 p2 p3 term denominator computed fact events W mutually exclusive win prize This result makes intuitive sense removed impossible states W probability Luke wins prize proportional prior probability prize normalisation simply total set possible probability remaining DRAFT April Prior Likelihood Posterior b c Figure Noisy observations displacements x1 x100 pendulum b The prior belief possible values c The posterior belief Prior Likelihood Posterior Much science deals problems form tell variable given I observed data D knowledge underlying data generating mechanism Our interest quantity p D p D p p D p D p p D p This shows forward generative model p D dataset coupled prior belief p variable values appropriate infer posterior distribution p D vari able light observed data The probable posteriori MAP setting maximises posterior arg max p D For flat prior p constant changing MAP solution equivalent maximum likelihood maximises likelihood p D model generating observed data We return discussion summaries posterior parameter learning chapter This use generative model sits physical models world typically postulate generate observed phenomena assuming know model For example postulate generate time series displacements swinging pendulum unknown mass length damping constant Using generative model given displacements infer unknown physical properties pendulum Example Pendulum As prelude scientific inference use continuous variables consider idealised pendulum xt angular displacement pendulum time t Assuming measurements independent given knowledge parameter problem likelihood sequence observations x1 xT given p x1 xT T t p xt If model correct measurement displacements x perfect physical model xt sin t represents unknown physical constants pendulum g L g gravitational attraction L length pendulum If assume poor instrument measure displacements known variance chapter xt sin t t DRAFT April Prior Likelihood Posterior t zero mean Gaussian noise variance We consider set possible parameters place prior p expressing prior belief seeing measurements appropriateness different values The posterior distribution given p x1 xT p T t e xt sin t Despite noisy measurements posterior assumed possible values strongly peaked large number measurements fig Two dice individual scores Two fair dice rolled Someone tells sum scores What posterior distribution dice scores2 The score die denoted sa dom sa similarly sb The variables involved sa sb total score t sa sb A model variables naturally takes form p t sa sb p t sa sb likelihood p sa sb prior The prior p sa sb joint probability score sa score sb knowing Assuming dependency rolling mechanism p sa sb p sa p sb Since dice fair p sa p sb uniform distributions p sa p sb p sa p sb sa sa sa sa sa sa sb sb sb sb sb sb Here likelihood term p t sa sb I t sa sb states total score given sa sb Here I A indicator function defined I A statement A true p t sa sb sa sa sa sa sa sa sb sb sb sb sb sb Hence complete model p t sa sb p t sa sb p sa p sb terms right explicitly defined p t sa sb p sa p sb sa sa sa sa sa sa sb sb sb sb sb sb The posterior given p sa sb t p t sa sb p sa p sb p t p t sa sb p t sa sb p sa p sb p sa sb t sa sa sa sa sa sa sb sb sb sb sb sb The term p t sa sb p t sa sb p sa p sb Hence posterior given equal mass non zero elements shown 2This example Taylan Cemgil DRAFT April Code Summary The standard rules probability consistent logical way reason uncertainty Bayes rule mathematically encodes process inference A useful introduction probability given The interpretation probability contentious refer reader detailed discussions The website understandinguncertainty org contains entertaining discussions reasoning uncertainty Code The BRMLtoolbox code accompanying book intended reader insight repre senting discrete probability tables performing simple inference We provide briefest descriptions code reader encouraged experiment demos understand better routines purposes Basic Probability code At simplest level need basic routines One multiplying probability tables called potentials code summing probability table Potentials represented structure For example code corresponding Inspector Clouseau example demoClouseau m define probability table pot ans variables table 2x2x2 double This says potential depends variables entries stored array given table field The size array informs states variable takes order given variables The order variables defined potential irrelevant provided indexes array consistently A routine help setting table entries setstate m For example pot setstate pot means potential table entry variable state variable state variable state set value The philosophy code information required perform computations minimum Additional information labels variables domains useful interpret results actually required carry computations One specify domain variable example variable ans domain murderer murderer butler The variable domain information Clouseau example stored structure variable helpful display potential table disptable pot variable knife maid murderer butler murderer knife maid murderer butler murderer knife maid murderer butler murderer DRAFT April Code knife maid murderer butler murderer knife maid murderer butler murderer knife maid murderer butler murderer knife maid murderer butler murderer knife maid murderer butler murderer Multiplying Potentials In order multiply potentials arrays tables potential dimensionally consistent number states variable potentials This checked ing potvariables m This consistency required basic operations summing potentials multpots m Multiplying potentials divpots m Dividing potential Summing Potential sumpot m Sum marginalise potential set variables sumpots m Sum set potentials Making conditional Potential condpot m Make potential conditioned variables Setting Potential setpot m Set variables potential given states setevpot m Set variables potential given states return identity potential given states The philosophy BRMLtoolbox information variables local read potential Using setevpot m enables set variables state whilst maintaining information number states variable Maximising Potential maxpot m Maximise potential set variables See maxNarray m maxNpot m return N highest values associated states Other potential utilities setstate m Set potential state given value table m Return table potential whichpot m Return potentials contain set variables potvariables m Variables number states set potentials orderpotfields m Order fields potential structure uniquepots m Merge redundant potentials multiplication return unique ones numstates m Number states variable domain squeezepots m Find unique potentials rename variables normpot m Normalise potential form distribution General utilities condp m Return table p x y p x y condexp m Form conditional distribution log value logsumexp m Compute log sum exponentials numerically precise way normp m Return normalised table unnormalised table assign m Assign values multiple variables maxarray m Maximize multi dimensional array subset DRAFT April Exercises An example The following code highlights use routines solving Inspector Clouseau example reader invited examine code familiar numerically represent proba bility tables demoClouseau m Solving Inspector Clouseau example Exercises Exercise Prove p x y z p x z p y x z p x y z p y x z p x z p y z Exercise Prove Bonferroni inequality p b p p b Exercise Adapted There boxes Box contains red white balls box contains red white balls A box chosen random p box p box ball chosen random box turns red What posterior probability red ball came box Exercise Adapted Two balls placed box follows A fair coin tossed white ball placed box head occurs red ball placed box The coin tossed red ball placed box tail occurs white ball placed box Balls drawn box times succession replacing drawn ball box It found occasions red ball drawn What probability balls box red A secret government agency developed scanner determines person terrorist The scanner fairly reliable scanned terrorists identified terrorists upstanding citizens identified An informant tells agency exactly passenger aboard aeroplane seated terrorist The police haul plane person scanner tests positive What probability person terrorist Exercise Consider variable distributions admit factorisation p b c p b p b c p c variables binary How parameters needed specify distributions form Exercise Repeat Inspector Clouseau scenario example restriction maid butler murderer Explicitly probability maid murderer butler probability butler murderer maid Modify demoClouseau m implement Exercise Prove p b c p b p c p b c Exercise Prove p x z y p x y z p y z y w p x w y z p w y z p y z DRAFT April Exercises Exercise As young man Mr Gott visits Berlin He s surprised cross East Berlin wall separating halves city He s told wall erected years previously He reasons The wall finite lifespan ignorance means arrives uniformly random time lifespan wall Since time arrive lifespan wall asserts confidence wall survive years In Professor Gott pleased find prediction correct promotes prediction method prestigious journals This delta t method widely adopted form predictions range scenarios researchers totally ignorant Would buy prediction Prof Gott Explain carefully reasoning Exercise Implement soft XOR gate example BRMLtoolbox You find condpot m use Exercise Implement hamburgers example scenarios BRMLtoolbox To need define joint distribution p hamburgers KJ dom hamburgers dom KJ tr fa Exercise Implement dice example section BRMLtoolbox Exercise A redistribution lottery involves picking correct numbers replacement example possible The order picked numbers irrelevant Every week million people play game paying enter numbers popular people chooses numbers Given million pounds prize money split equally winners different numbers come random expected money players choosing win week The popular set numbers people choosing How profit week average Do think skill involved playing lottery Exercise In test psychometry car keys wrist watches people given medium The medium attempts match wrist watch car key person What expected number correct matches medium chance What probability medium obtain correct match Exercise Show function f x p x y f y f y Explain general x p x y f x y x f x y Exercise Inspired singingbanana com Seven friends decide order pizzas telephone Pizza4U based flyer pushed letterbox Pizza4U kinds pizza person chooses pizza independently Bob phones Pizza4U places combined pizza order simply stating pizzas kind required Unfortunately precise order lost chef makes seven randomly chosen pizzas passes delivery boy How different combined orders possible What probability delivery boy right order Exercise Sally new area listens friends discussing female friend Sally knows talking Alice Bella doesn t know From previous conver sations Sally knows independent pieces information She s sure Alice white car doesn t know Bella s car white black Similarly s sure Bella likes sushi doesn t know Alice likes sushi Sally hears conversation person discussed hates sushi drives white car What probability friends talking Alice Assume maximal uncertainty absence knowledge probabilities DRAFT April Exercises Exercise The weather London summarised rains day s chance rain following day s sunny day s chance sunny following day Assuming prior probability rained yesterday probability raining yesterday given s sunny today If weather follows pattern day day probability rain day based effectively infinite number days observing weather Use result new prior probability rain yesterday recompute proba bility raining yesterday given s sunny today Exercise A game Battleships played pixel grid There pixel length ships placed uniformly random grid subject constraints ships lap ii ship vertical horizontal After unsuccessful misses locations calculate pixel highest proba bility containing ship State pixel value highest probability DRAFT April CHAPTER Basic Graph Concepts Often good reason believe event affects conversely events independent Incorporating knowledge produce models better specified computation ally efficient Graphs describe objects linked provide convenient picture describing related objects We ultimately introduce graph structure variables probabilistic model produce graphical model captures relations variables uncertainties In chapter introduce required basic concepts graph theory Graphs Definition Graph A graph G consists nodes called vertices edges called links nodes Edges directed arrow single direction undirected Edges associated weights A graph edges directed called directed graph edges undirected called undirected graph A B CD E An directed graph G consists directed edges nodes A B CD E An undirected graph G consists undirected edges nodes Graphs edge weights model networks flows pipes distances cities node represents city We use concepts chapter chap ter Our main use graphs endow probabilistic interpretation develop connection directed graphs probability chapter Undirected graphs play central role modelling reasoning uncertainty Essentially variables independent linked path graph We discuss detail consider Markov networks chapter Definition Path ancestors descendants A path A B node A node B sequence nodes connects A B That path form A0 A1 An An A0 A An B Graphs edge Ak Ak k n graph A directed path sequence nodes follow direction arrows leads A B In directed graphs nodes A A B B A ancestors B The nodes B A B B A descendants A Definition Cycle loop chord A cycle directed path starts returns node b z A loop path containing nodes irrespective edge direction starts returns node For example fig 2b forms loop graph acyclic contains cycles A chord edge connects non adjacent nodes loop example edge chord loop fig 2a Definition Directed Acyclic Graph DAG A DAG graph G directed edges arrows link nodes following path nodes node direction edge path revisit node In DAG ancestors B nodes directed path ending B Conversely descendants A nodes directed path starting A Definition Relationships DAG x4 x1 x2 x3 x5 x6 x7x8 The parents x4 pa x4 x1 x2 x3 The children x4 ch x4 x5 x6 The family node parents The Markov blanket node parents children parents children excluding In case Markov blanket x4 x1 x2 x3 x5 x6 x7 DAGs play central role modelling environments variables particular belief networks describe following chapter One view directed links graph direct dependencies parent child variables Naively acyclic condition prevents circular reasoning These connections discussed detail chapter Definition Neighbour For undirected graph G neighbours x ne x nodes directly connected x Definition Clique A B CD E Given undirected graph clique fully connected subset nodes All members clique neighbours maximal clique larger clique contains clique For example graph maximal cliques C1 A B C D C2 B C E Whilst A B C fully connected non maximal clique larger fully connected set A B C D contains A non maximal clique called cliquo Cliques play central role modelling inference In modelling describe variables dependent chapter In inference describe sets variables simpler structure describing relationship simpler efficient inference procedure likely exist We discuss issue detail chapter chapter DRAFT April Numerically Encoding Graphs d c b e f g d c b e f g b Figure Singly connected graph b Multiply connected graph Definition Connected graph An undirected graph connected path pair nodes e isolated islands For graph connected connected components subgraphs connected Definition Singly Connected Graph A graph singly connected path node A node B Otherwise graph multiply connected This definition applies regardless edges graph directed An alternative singly connected graph tree A multiply connected graph called loopy Definition Spanning Tree A spanning tree undirected graph G singly connected subset existing edges resulting singly connected graph covers nodes G On right graph associated spanning tree A maximum weight spanning tree spanning tree sum weights edges tree large spanning tree G Procedure Finding maximal weight spanning tree An algorithm find spanning tree maximal weight follows Start picking edge largest weight add edge set Then pick candidate edge largest weight add edge set results edge set cycles reject candidate edge propose largest edge weight Note maximal weight spanning tree Numerically Encoding Graphs Our ultimate goal computational implementations inference Therefore want incorpo rate graph structure models need express graphs way computer understand manipulate There equivalent possibilities Edge list As suggests edge list simply lists node node pairs graph For fig 2a edge list L Undirected edges listed twice direction DRAFT April Numerically Encoding Graphs b Figure An undirected graph represented symmetric adjacency matrix b A directed acyclic graph nodes labelled ancestral order corresponds triangular adjacency matrix Adjacency matrix An alternative use adjacency matrix A Aij edge node node j graph Some authors include self connections place s diagonal definition An undirected graph symmetric adjacency matrix Provided nodes labelled ancestral order parents come children directed graph fig 2b represented triangular adjacency matrix T Adjacency matrix powers Adjacency matrices wasteful entries zero However useful property redeems For N N adjacency matrix A powers adjacency matrix Ak ij specify paths node node j k edge hops If include s diagonal A AN ij non zero path connecting j graph If A corresponds DAG non zero entries jth row AN correspond descendants node j Clique matrix For undirected graph N nodes maximal cliques C1 CK clique matrix N K matrix column ck zeros ones entries describing clique For example C clique matrix fig 2a A cliquo matrix relaxes constraint cliques required maximal1 A cliquo matrix containing node cliques called incidence matrix For example Cinc 1The term cliquo non maximal clique attributed Julian Besag DRAFT April Code incidence matrix fig 2a It straightforward CincC T inc equal adjacency matrix diagonals contain degree node number edges touches Similarly cliquo matrix diagonal entry CCT ii expresses number cliquos columns node occurs Off diagonal elements CCT ij contain number cliquos nodes j jointly inhabit Remark Graph Confusions Graphs widely differ markedly represent Two potential pitfalls described State Transition Diagrams Such representations Markov chains finite state automata Each state node directed edge node node j associated weight pij represents transition state state j occur probability pij From graphical models perspective use directed graph x t x t represent Markov chain The state transition diagram provides detailed graphical description conditional probability table p x t x t Neural Networks Neural networks nodes edges In general neural networks graphical representations functions graphical models representations distributions Summary A graph nodes edges use represent variables relations A DAG acyclic graph useful representing causal relationships variables Neighbouring nodes undirected graph represent dependent variables A graph singly connected path node graph multiply connected A clique group nodes connected The adjacency matrix machine readable description graph Powers adjacency matrix information paths nodes Good reference graphs associated theories uses Code Utility routines drawNet m Draw graph based adjacency matrix ancestors m Find ancestors node DAG edges m Edge list adjacency matrix ancestralorder m Ancestral order DAG connectedComponents m Connected Components parents m Parents node given adjacency matrix children m Children node given adjacency matrix neigh m Neighbours node given adjacency matrix A connected graph tree number edges plus equal number nodes However disconnected graph case The code istree m deals possibly disconnected case The routine based observation singly connected graph possess simplicial DRAFT April Exercises node leaf node eliminated reveal smaller singly connected graph istree m If graph singly connected return elimination sequence spantree m Return spanning tree ordered edge list singleparenttree m Find directed tree parent undirected tree Additional routines basic graph manipulations given end chapter Exercises Exercise Consider adjacency matrix A elements A ij reach state state j timestep Show matrix Ak ij represents number paths lead state j k timesteps Hence derive algorithm find minimum number steps state j state Exercise For N N symmetric adjacency matrix A describe algorithm find connected components You wish examine connectedComponents m Exercise Show connected graph singly connected number edges E equal number nodes minus E V Give example graph E V singly connected Hence condition E V necessary sufficient condition graph singly connected Exercise Describe procedure determine graph singly connected Exercise Describe procedure determine ancestors set nodes DAG Exercise WikiAdjSmall mat contains random selection Wiki authors link authors know snap stanford edu data wiki Vote html Assume knows j j knows Plot histogram separation length path users graph corresponding adjacency matrix users based separations That bin n s histogram contains number pairs separation s Exercise The file cliques mat contains list cliques defined graph nodes Your task return set unique maximal cliques eliminating cliques wholly contained Once found clique represent binary form example says clique contains variables reading left right Converting binary representation decimal rightmost bit units leftmost corresponds number Using decimal representation write list unique cliques ordered lowest decimal representation highest Describe fully stages algorithm use find unique cliques Hint find examining uniquepots m useful Exercise Explain construct graph N nodes N contains N maximal cliques Exercise Let N divisible Construct graph N nodes partitioning nodes N subsets subset containing nodes Then connect nodes provided subset Show graph 3N maximal cliques This shows graph exponentially large number maximal cliques Exercise A jewel stolen room party2 Each guests A B C D E F enters room stays time leaves The testimonies gave police 2This question modified version known problem disguised looking solution harder Apologies original author lack citation DRAFT April Exercises A I present room E B B I present room A F E C I present room F D D I present room A F E I present room C F I present room C E One statements testimonies false Which Notes If people present room time report fact present room person Each testimony interpreted example A I present room E I present room B It necessarily mean A simultaneously present room E B A B present room time t1 A E present room time t2 The testimonies similar interpretation The testimonies partially false If X says present room Y Z statement X present room Y true X present room Z false DRAFT April Exercises DRAFT April CHAPTER Belief Networks We connection probability graph theory A belief network introduces structure probabilistic model graphs represent independence assumptions vari ables Probability operations marginalizing conditioning correspond simple operations graph details model read graph There benefit terms computational efficiency Belief networks capture possible relations variables However natural representing causal relations family graphical models study chapter The Benefits Structure It s tempting think feeding mass undigested data probability distributions computer getting good predictions useful insights extremely complex environments However unfor tunately naive approach likely fail The possible ways variables interact extremely large sensible assumptions unlikely useful model Independently specifying entries table p x1 xN binary variables xi takes O N space impractical handful variables This clearly infeasible machine learning related application areas need deal distributions potentially hundreds millions variables Structure important computational tractability inferring quantities interest Given distribution N binary variables p x1 xN computing marginal p x1 requires summing N states variables Even optimistically fast supercomputer far long N variable system The way deal large distributions constrain nature variable interactions manner render specification ultimately inference systems tractable The key idea specify variables independent leading structured factorisation joint probability distribution For distribution chain p x1 x100 xi xi computing marginal p x1 computed blink eye modern computers Belief networks con venient framework representing independence assumptions We discuss belief networks formally section discussing natural role causal models Belief networks called Bayes networks Bayesian belief networks way depict indepen dence assumptions distribution Their application domain widespread ranging troubleshooting expert reasoning uncertainty machine learning Before formally define Belief Network BN example help motivate development1 1The scenario adapted The Benefits Structure J R T S B A E R b Figure Belief network structure wet grass example Each node graph rep resents variable joint distribution variables feed parents variable represent variables right conditioning bar b Belief network Burglar model Modelling independencies One morning Tracey leaves house realises grass wet Is overnight rain forget turn sprinkler night Next notices grass neighbour Jack wet This explains away extent possibility sprinkler left concludes probably raining We model situation defining variables wish include model In situation natural variables R R means raining S S means Tracey forgotten turn sprinkler J J means Jack s grass wet T T means Tracey s Grass wet A model Tracey s world corresponds probability distribution joint set variables interest p T J R S order variables irrelevant Since variables example states appear naively specify values states e g p T J R S etc However normalisation conditions probabilities need specify state probabilities To states need specified consider following decomposition Without loss generality repeatedly definition conditional probability write p T J R S p T J R S p J R S p T J R S p J R S p R S p T J R S p J R S p R S p S That write joint distribution product conditional distributions The term p T J R S requires specify values need p T J R S joint states J R S The value p T J R S given normalisation p T J R S p T J R S Similarly need values factors making total values In general distribution n binary variables need specify 2n values range The important point number values need specified general scales exponentially number variables model impractical general motivates simplifications Conditional independence The modeler knows constraints system For example scenario assume Tracey s grass wet depends directly raining sprinkler That conditional independence assumption p T J R S p T R S Similarly assume Jack s grass wet influenced directly raining write p J R S p J R DRAFT April The Benefits Structure Furthermore assume rain directly influenced sprinkler p R S p R means model equation p T J R S p T R S p J R p R p S We represent conditional independencies graphically fig 1a This reduces number values need specify saving previous values case conditional independencies assumed To complete model need numerically specify values conditional probability table CPT Let prior probabilities R S p R p S We set remaining probabilities p J R p J R Jack s grass wet unknown effects rain p T R S p T R S p T R S s small chance sprinkler left didn t wet grass noticeably p T R S Inference Now ve model environment perform inference Let s calculate probability sprinkler overnight given Tracey s grass wet p S T To use p S T p S T p T J R p T J R S J R S p T J R S J R p J R p T R S p R p S J R S p J R p T R S p R p S R p T R S p R p S R S p T R S p R p S posterior belief sprinkler increases prior probability evidence grass wet Note equation summation J numerator unity function f R summation form J p J R f R equals f R This follows definition distribution p J R sum fact f R depend J A similar effect occurs summation J denominator Let calculate probability Tracey s sprinkler overnight given grass wet Jack s grass wet p S T J We use conditional probability p S T J p S T J p T J R p T J R S R S p T J R S R p J R p T R S p R p S R S p J R p T R S p R p S The probability sprinkler given extra evidence Jack s grass wet lower probability grass wet given Tracey s grass wet This occurs fact Jack s grass wet increases chance rain played role making Tracey s grass wet Naturally don t wish carry inference calculations hand time General purpose algorithms exist junction tree algorithm chapter DRAFT April The Benefits Structure Example Was Burglar Here s example binary variables adapted Sally comes home find burglar alarm sounding A Has burgled B alarm triggered earthquake E She turns car radio news earthquakes finds radio broadcasts earthquake alert R Using Bayes rule write loss generality p B E A R p A B E R p B E R We repeat p B E R continue p B E A R p A B E R p R B E p E B p B However alarm surely directly influenced report Radio p A B E R p A B E Similarly conditional independence assumptions p B E A R p A B E p R E p E p B depicted fig 1b Specifying conditional probability tables Alarm Burglar Earthquake Radio Earthquake The remaining tables p B p E The tables graphical structure fully specify distribution Now consider happens observe evidence Initial Evidence The Alarm sounding p B A E R p B E A R B E R p B E A R E R p A B E p B p E p R E B E R p A B E p B p E p R E Additional Evidence The Radio broadcasts Earthquake warning A similar calculation gives p B A R Thus initially Alarm sounds Sally thinks s burgled However probability drops dramatically hears Earthquake That Earthquake explains away extent fact Alarm ringing See demoBurglar m Remark Causal intuitions Belief networks ve defined ways express independence statements Nevertheless expressing independencies useful potentially misleading think causes In example chose ordering variables reading right left B E R A equation B E considered root causes A R effects Reducing burden specification Consider discrete variable y discrete parental variables x1 xn fig 2a Formally structure graph implies form parameterisation table p y x1 xn If parent xi dim xi states constraint table table p y x1 xn DRAFT April Uncertain Unreliable Evidence x1 x2 x3 x4 x5 y x1 x2 x3 x4 x5 z1 z2 y b x1 x2 x3 x4 x5 z1 z2 z3 z4 z5 y c Figure If variables binary states required specify p y x1 x5 b Here states required c Noisy logic gates contains dim y dim xi entries If stored explicitly state require potentially huge storage An alternative constrain table simpler parametric form For example write decomposition limited number parental interactions required called divorcing parents For example fig 2b p y x1 x5 z1 z2 p y z1 z2 p z1 x1 x2 x3 p z2 x4 x5 Assuming variables binary number states requiring specification compared states unconstrained case Logic gates Another technique constrain tables uses simple classes conditional tables For example fig 2c use logical OR gate binary zi p y z1 z5 zi state We table p y x1 x5 including additional terms p zi xi When xi binary total quantities required specifying p y x In case fig 2c represent noisy logic gate noisy OR noisy AND number parameters required specify noisy gate linear number parents The noisy OR particularly common disease symptom networks diseases x rise symptom y provided diseases present probability symptom present high Uncertain Unreliable Evidence In following distinction evidence uncertain evidence unreliable Uncertain evidence In soft uncertain evidence evidence variable state strength belief state given probabilities For example x states dom x red blue green vector represents belief respective states In contrast hard evidence certain variable particular state In case probability mass vector components example Performing inference soft evidence straightforward achieved Bayes rule For example model p x y consider soft evidence y variable y wish DRAFT April Uncertain Unreliable Evidence know effect variable x wish compute p x y From Bayes rule assumption p x y y p x y p x y y p x y y y p x y y p y y y p x y p y y p y y represents probability y state soft evidence This generalisation hard evidence vector p y y zero component values single component This procedure define model conditioned evidence average distribution evidence known Jeffrey s rule In BN use dashed circle represent variable soft evidence state x y Example soft evidence Revisiting burglar scenario example imagine sure heard burglar alarm sounding For binary variable case represent soft evidence states A What probability burglary soft evidence p B A A p B A p A A p B A p B A The probabilities p B A p B A calculated Bayes rule p B A This lower probability having burgled sure heard alarm Holmes Watson Mrs Gibbon An entertaining example uncertain evidence given Pearl adapt purposes The environment contains variables B tr fa B tr means Holmes house burgled A tr fa A tr means Holmes house Alarm went W tr fa W tr means Watson heard alarm G tr fa G tr means Mrs Gibbon heard alarm The BN scenario depicted fig 3a p B A G W p A B p B p W A p G A Watson states heard alarm sounding Mrs Gibbon little deaf sure heard alarm sure heard This dealt soft evidence technique fig 3b From Jeffrey s rule uses original model equation compute model conditioned evidence p B tr W tr G p B tr W tr G p W tr G A p G A p W tr A p A B tr p B tr B A p G A p W tr A p A B p B uses soft evidence p G G G tr G fa compute p B tr W tr G p B tr W tr G tr p G tr G p B tr W tr G fa p G fa G A calculation requires numerically specify terms equation example exercise DRAFT April Uncertain Unreliable Evidence B A G W B A G W b B A H W c B A G H d Figure Mr Holmes burglary worries given B urglar A larm W atson Mrs G ibbon b Mrs Gibbon s uncertain evidence represented dashed circle c Virtual evidence replacement unreliable evidence represented dashed line d Mrs Gibbon uncertain evidence Holmes replaces unreliable Watson interpretation Unreliable evidence Holmes telephones Mrs Gibbon realises doesn t trust evidence suspects s drinking interpretation alarm sounded probable resulted Mrs Gibbon stating heard alarm didn t sound chance Mrs Gibbon stated heard Note Mrs Gibbon sure heard alarm soft evidence effect calculations contain term p G A equation Holmes wishes discard simply replace interpretation events Mr Holmes achieve replacing term p G A called virtual evidence term p G A p H A p H A A tr A fa Here state H arbitrary fixed This modify joint distribution p B A H W p A B p B p W A p H A fig 3c When compute p B tr W tr H effect Mr Holmes judgement count factor times favour alarm sounding The values table entries irrelevant normalisation constants absorbed proportionality constant Note p H A distribution A normalisation required This form evidence called likelihood evidence Uncertain unreliable evidence To demonstrate combine effects unreliable uncertain evidence consider situation Mrs Gibbon uncertain evidence Mr Holmes feels Watson s evidence unreliable wishes replaces interpretation fig 3d To account deal unreliable evidence p B A W G p B A H G p B p A B p G A p H A Using modified model use Jeffrey s rule compute model conditioned evidence p B A H G p B p A B p G A p H A A B p B p A B p G A p H A We include uncertain evidence G form final model p B A H G G p B A H G p G G compute marginal p B H G p B H G A p B A H G DRAFT April Belief Networks Belief Networks Definition Belief network A belief network distribution form p x1 xD D p xi pa xi pa xi represent parental variables variable xi Represented directed graph arrow pointing parent variable child variable belief network corresponds Directed Acyclic Graph DAG ith node graph corresponding factor p xi pa xi Remark Graphs distributions A somewhat subtle point belief network corresponds specific instance distribution given definition requiring numerical specification conditional probability tables refers distribution consistent specified structure In potentially distinguish belief network distribution containing numerical specification belief network graph contains numerical specification Normally issue arise book potentially important clarifying scope independence dependence statements In Wet Grass Burglar examples choice recursively Bayes rule In general variable case choose factorisation p x1 x2 x3 x4 p x1 x2 x3 x4 p x2 x3 x4 p x3 x4 p x4 An equally valid choice fig p x1 x2 x3 x4 p x3 x4 x1 x2 p x4 x1 x2 p x1 x2 p x2 In general different graphs represent independence assumptions discuss section If wishes independence assumptions choice factorisation significant The observation distribution written cascade form fig gives algorithm constructing BN variables x1 xn write n node cascade graph label nodes variables order successive independence statement corresponds deleting edges More formally corresponds ordering variables loss generality write x1 xn Then Bayes rule p x1 xn p x1 x2 xn p x2 xn p x1 x2 xn p x2 x3 xn p x3 xn p xn n p xi xi xn The representation BN Directed Acyclic Graph DAG Every probability distribution written BN correspond fully connected cascade DAG The particular role BN structure DAG corresponds set conditional independence assumptions ancestral parental variables sufficient specify conditional probability table Note mean non parental variables influ ence For example distribution p x1 x2 p x2 x3 p x3 DAG x1 x2 x3 imply p x2 x1 x3 p x2 x3 The DAG specifies conditional independence statements variables cestors ancestors direct causes variable The effects given descendants variable generally dependent variable See remark DRAFT April Belief Networks x1 x2 x3 x4 x3 x4 x1 x2 b Figure Two BNs variable distribution Both graphs b represent distribution p x1 x2 x3 x4 Strictly speaking represent lack independence assumptions graphs content tables The extension cascade variables clear results Directed Acyclic Graph Remark Dependencies Markov Blanket Consider distribution set variables X For variable xi X corresponding belief network represented DAG G let MB xi variables Markov blanket xi Then variable y Markov blanket xi y X xi MB xi xi y MB xi That Markov blanket xi carries information xi As example fig 2b MB z1 x1 x2 x3 y z2 z1 x4 MB z1 The DAG corresponds statement conditional independencies model To complete speci fication BN need define elements conditional probability tables p xi pa xi Once graphical structure defined entries conditional probability tables CPTs p xi pa xi expressed For possible state parental variables pa xi value states xi needs specified determined normalisation For large number parents writing table values intractable tables usually parameterised low dimensional manner This central topic discussion application BNs machine learning Conditional independence Whilst BN corresponds set conditional independence assumptions immediately clear DAG set variables conditionally independent set variables definition For example fig x1 x2 independent given state x4 The answer yes p x1 x2 x4 p x4 x3 p x1 x2 x3 x4 p x4 x3 p x1 x4 p x2 x3 x4 p x3 p x4 p x1 x4 x3 p x2 x3 x4 p x3 Now p x2 x4 p x4 x1 x3 p x1 x2 x3 x4 p x4 x1 x3 p x1 x4 p x2 x3 x4 p x3 p x4 x3 p x2 x3 x4 p x3 Combining results p x1 x2 x4 p x1 x4 p x2 x4 x1 x2 x3 x4 Figure p x1 x2 x3 x4 p x1 x4 p x2 x3 x4 p x3 p x4 DRAFT April Belief Networks x1 x3 x2 x1 x3 x2 b x1 x3 x2 c x1 x3 x2 d Figure By dropping connection variables x1 x2 reduce possible BN graphs variables The fully connected cascade graphs correspond x1 x2 x2 x1 b x1 x2 b x2 x1 c x1 x2 d x2 x1 Any graphs cyclic distributions x1 x2 independent conditioned x4 We like general algorithm allow avoid tedious manipulations reading result directly graph To help develop intuition constructing algorithm consider variable distribution p x1 x2 x3 We write ways p x1 x2 x3 p xi1 xi2 xi3 p xi2 xi3 p xi3 i1 i2 i3 permutations Whilst factorisation produces different DAG represent distribution makes independence statements If DAGs cascade form independence assumptions The minimal independence assumptions correspond dropping single link cascade graph This gives rise DAGs fig Are graphs equivalent sense represent distribution Applying Bayes rule gives p x2 x3 p x3 x1 p x1 graph c p x2 x3 p x3 x1 p x3 p x1 x3 p x2 x3 p x1 x3 p x3 x2 p x2 graph d p x1 x3 p x2 x3 p x3 graph b DAGs b c d represent conditional independence CI assumptions given state variable x3 variables x1 x2 independent x1 x2 x3 However graph represents fundamentally different p x1 x2 p x1 p x2 There way transform distribution p x3 x1 x2 p x1 p x2 Remark Graphical Dependence Belief network graphs good encoding conditional indepen dence suited encoding dependence For example consider graph b This appear encode relation b dependent However specific numerical instance belief network distribution p b p b b The lesson DAG appears graphical dependence instances distributions depen dence follow The caveat holds Markov networks section We discuss issue depth section The impact collisions Definition Given path P collider node c P neighbours b P c b Note collider path specific fig DRAFT April Belief Networks x z y x z y b x z y c x y w z d Figure In graphs b variable z collider c Variable z collider Graphs b represent conditional independence x y z In graphs c d x y graphically conditionally dependent given variable z In general BN check x y z In fig 7a x y independent conditioned z p x y z p x z p y z Similarly fig 7b x y independent conditioned z p x y z p z x p x p y z function x multiplied function y In fig 7c x y graphically dependent p x y z p z x y p x p y situation variable z collider arrows neighbours pointing What fig 7d In d condition z x y graphically dependent p x y z p x y z p z p z w p z w p w x y p x p y p x z p y z The inequality holds term p w x y special cases p w x y const x y independent Intuitively variable w dependent value z x y conditionally dependent w conditionally dependent z If non collider z conditioned path x y fig b path induce dependence x y Similarly path x y contains collider provided collider conditioning set descendants path x y dependent If path x y contains colliders conditioning variables path d connects x y Note collider defined relative path In fig 8a variable d collider path b d c path b d e relative path arrows point inwards d Consider BN A B C Here A C unconditionally independent However conditioning B makes graphically dependent Intuitively whilst believe root causes independent given value observation tells state causes coupling making generally dependent In definition describe effect condition ing marginalisation graph remaining variables Definition Some properties belief networks It useful understand effect conditioning marginalising variable belief network We state operations effect remaining variables graph use intuition develop complete description section A B C p A B C p C A B p A p B From causal perspective models causes A B priori independent determining effect C DRAFT April Belief Networks d b c e The variable d collider path b d c path b d e Is e b e d connected colliders path e non collider b conditioning set Hence e d separated b e b b c e d b The variable d collider path d e path b c d e Is e c There paths e d e b c d e The path d e blocked d collider path d conditioning set descendant collider d conditioning set c For path b c d e node c collider path c conditioning set For path d collider Hence path blocked e graphically dependent given c Figure Collider examples d separation d connection A B C A B Marginalising C makes A B independent A B unconditionally independent p A B p A p B In absence information effect C retain belief A B C A B Conditioning C makes A B graphically de pendent general p A B C p A C p B C Al causes priori independent knowing effect C general tells causes colluded bring effect observed A B C D A B Conditioning D descendent collider C makes A B graphically dependent general p A B D p A D p B D A B C p A B C p A C p B C p C Here because C independent effects A B A B C A B Marginalising C makes A B graphically de pendent In general p A B p A p B Although don t know because effects neverthe dependent DRAFT April Belief Networks A B C A B Conditioning C makes A B independent p A B C p A C p B C If know because C know effect occurs independent effect This true reversing arrow A C case A because C C because B Conditioning C blocks ability A influence B A B C A B C A B C These graphs express conditional independence assump tions Graphical path manipulations independence Intuitively tools need understand x independent y conditioned z Examining rules definition need look path x y Colouring x red y green conditioning node z yellow need examine path x y adjust edges following intuitive rules fig d Separation The description intuitive A formal treatment amenable computational implemen tation straightforward obtain intuitions First define DAG concepts d separation d connection central determining conditional independence BN structure given DAG Definition d connection d separation If G directed graph X Y Z disjoint sets vertices X Y d connected Z G exists undirected path U vertex X vertex Y collider C U C descendent C Z non collider U Z X Y d separated Z G d connected Z G One phrase follows For variable x X y Y check path U x y A path U said blocked node w U w collider w descendants Z w collider U w Z If paths blocked X Y d separated Z If variable sets X Y d separated Z independent conditional Z probability distributions graph represent Remark Bayes Ball The Bayes Ball algorithm provides linear time complexity algorithm given set nodes X Z determines set nodes Y X Y Z Y called set irrelevant nodes X given Z Graphical distributional dependence We shown X Y d separated Z X Y Z distributions consistent belief network structure DRAFT April Belief Networks x y z u x y z u If z collider path undirected links neighbours collider x y w z u x y w z u If z descendant collider induce dependence retain links making undi rected x y z u x y z u If collider conditioning set upper path cut links collider variable In case upper path x y blocked x y z u x y z u If non collider conditioning set path cut link neigh bours non collider induce depen dence x y The path blocked x y z u x y z u In case path contributes dependence x y z Both paths blocked x u z y w x u z y w Whilst z collider conditioning set w collider con ditioning set This means path x y x y independent given z Figure Graphical manipulations determine independence x y z After manipulations undirected path x y x y independent conditioned z Note graphical rules differ definition considered effect graph having eliminated variable conditioning marginalisation Here consider rules determining independence based graphical representation variables remain graph In words takes instance distribution P factorises according belief network structure writes list LP conditional independence statements obtained P X Y d separated Z list contain statement X Y Z Note list LP contain statements obtained graph For example belief network graph p b c p c b p p b DRAFT April Belief Networks b g f st b g f st u b Figure t f d connected g b b f d separated u representable DAG c b b graphical independence statement Consider distribution consistent equation example binary variables dom dom b dom c p c b b p p b numerically b distribution p Indeed list L contains statement b On hand consider distribution p c b p p b L b c b c In case L contains statements b An interesting question d connection similarly implies dependence That distri butions P consistent belief network possess dependencies implied graph If consider belief network structure equation b d connected c graphically b dependent conditioned c For specific instance p numerically b c list dependence statements p contains graphical dependence statement Now consider p The list dependence statements p Hence graphical dependence statements necessarily found distributions consistent belief network Hence X Y d connected Z X Y Z distributions consistent belief network structure See exercise This shows belief networks powerful ensuring distributions necessarily obey independence assumptions expect graph However belief networks suitable ensuring distributions obey desired dependency statements Example Consider graph fig 10a Are variables t f unconditionally independent e t f Here colliders g s conditioning set t f d separated unconditionally independent What t f g There path t f colliders conditioning set Hence t f d connected g t f graphically dependent conditioned g Example Is b f u fig 10b Since conditioning set path b f u contains collider b f unconditionally independent u Markov equivalence belief networks We invested lot effort learning read conditional independence relations DAG Happily determine DAGs represent set conditional independence statements don t know relatively simple rule DRAFT April Belief Networks h t1 y1 t2 y2 t1 y1 t2 y2 b Figure Two treatments t1 t2 corresponding outcomes y1 y2 The health patient represented h This DAG embodies conditional independence state ments t1 t2 y2 t2 t1 y1 treat ments effect b One repre sent effect marginalising h bi directional edge Definition Markov equivalence Two graphs Markov equivalent represent set conditional independence statements This definition holds directed undirected graphs Example Consider belief network edges A C B set conditional independence statements A B For belief network edges A C B A B set conditional independence statements In case belief networks Markov equivalent Procedure Determining Markov equivalence Define immorality DAG configuration nodes A B C C child A B A B directly connected Define skeleton graph removing directions arrows Two DAGs represent set independence assumptions Markov equivalent skeleton set immoralities Using procedure fig BNs b c d skeleton immoralities equivalent However BN immorality equivalent BNs b c d Note unmarried parents having children considered immoral scope book Belief networks limited expressibility Belief networks fit intuitive notion modelling causal independencies However formally speaking necessarily graphically represent independence properties given distribu tion Consider DAG fig 11a This DAG represent successive ex periments t1 t2 treatments y1 y2 represent outcomes interest h underlying health status patient treatment effect second outcome edge y1 y2 Now consider implied independencies marginal distribution p t1 t2 y1 y2 obtained marginalising distribution h There DAG containing vertices t1 y1 t2 y2 represents independence relations imply independence relation implied fig 11a Consequently DAG vertices t1 y1 t2 y2 fail represent independence relation p t1 t2 y1 y2 impose additional independence restriction implied DAG In example p t1 t2 y1 y2 p t1 p t2 h p y1 t1 h p y2 t2 h p h general expressed product functions defined limited set variables However case conditional independence conditions t1 t2 y2 t2 t1 y1 hold p t1 t2 y1 y2 DRAFT April Causality encoded form conditional probability tables It independence present structure marginalised graph naturally infer larger graph p t1 t2 y1 y2 h For example BN link y2 y1 t1 t2 y2 true distribution Similarly BN link y1 y2 implied statement t1 t2 y1 true This example demonstrates BNs express conditional independence statements set variables set conditional independence statements increased consid ering additional variables This situation general sense graphical model limited expressibility terms independence statements It worth bearing mind BNs appropriate framework express s independence assumptions intu itions A natural consideration use bi directional arrow variable marginalised For fig 11a depict marginal distribution bi directional edge fig 11b For discussion extensions BNs bi directional edges Causality Causality contentious topic purpose section reader aware pitfalls occur rise erroneous inferences The reader referred details The word causal contentious particularly cases model data contains explicit tem poral information formally correlations dependencies inferred For distribution p b write p b p b ii p b p In think b causes ii causes b Clearly meaningful represent exactly distribution fig Formally BNs independence statements causal ones Neverthe constructing BNs helpful think dependencies terms causation intuitive understanding usually framed variable influences First discuss classic conundrum highlights potential pitfalls arise Simpson s paradox Simpson s paradox cautionary tale causal reasoning BNs Consider medical trial patient treatment outcome recovered Two trials conducted females males The data summarised table The question Does drug because increased recovery According table males answer males recovered given drug Similarly females recovered given drug recovered given drug The conclusion appears drug beneficial aids subpopulation However ignoring gender information collating male female data combined table find people recovered given drug Hence drug doesn t work males females work overall Should recommend drug A B A B b R W c R W d Figure Both b represent distribution p b p b p b p b p c The graph represents p rain grasswet p grasswet rain p rain d We equally written p rain grasswet p grasswet appears causally non sensical DRAFT April Causality Males Recovered Not Recovered Rec Rate Given Drug Not Given Drug Females Recovered Not Recovered Rec Rate Given Drug Not Given Drug Combined Recovered Not Recovered Rec Rate Given Drug Not Given Drug Table Table Simpson s Paradox G D R FD G D R b Figure A DAG relation tween Gender G Drug D Recovery R table b Influence diagram No decision vari able required G G parents Resolution paradox The paradox occurs asking causal interventional question If drug happens performing observational calculation Pearl remind difference given observational evidence given interventional evidence We want model causal experiment intervene setting drug state observe effect recovery A model Gender Drug Recovery data makes conditional independence assumptions fig 13a p G D R p R G D p D G p G In causal interpretation intervene drug term p D G equation play role experiment decide drug independent gender The term p D G needs replaced term reflects set experiment We use idea atomic intervention single variable set particular state In atomic causal intervention set D deal modified distribution p G R D p R G D p G terms right hand equation taken original BN data To denote intervention use p R G D p R G D p R G D p G R p R G D p G p R G D One consider G interventional case doesn t matter fact variable G parents means distribution conditional G prior factor p G present Using equation males given drug recover versus recovery given drug For females given drug recover versus recovery given drug Similarly p R D p R D G p R G D p G R G p R G D p G G p R G D p G DRAFT April Causality Using post intervention distribution equation p recovery drug p recovery drug Hence infer drug overall helpful intuitively expect consistent results subpopulations Summarising argument p G D R p R G D p G p D means choose Male Female patient drug independent gender absence term p D G joint distribution One way think models consider draw sample joint distribution random variables cases clarify role causality experiment In contrast interventional calculation observational calculation makes conditional independence assumptions This means example term p D G plays role calculation reader wish verify result given combined data table equivalent inferring distribution equation The calculus In making causal inferences ve seen adjust model reflect causal experi mental conditions In setting variable particular state need surgically remove parental links variable Pearl calls operator contrasts observational inference p x y causal inference p x y Definition Pearl s Do Operator Let variables X XC XC written terms intervention variables XC non intervention variables XC For Belief Network p X p Xi pa Xi inferring effect setting variables Xc1 XcK ck C states xc1 xcK equivalent standard evidential inference post intervention distribution p XC Xc1 xc1 XcK xcK j C p Xj pa Xj parental variable included intervention set set intervention state An alternative notation p XC xc1 xcK In words variables causally intervene set particular state corresponding terms p Xci pa Xci removed original Belief Network Graphically effect consider intervention variable cut connections parents set intervention variable intervention state For variables evidential non causal corresponding factors removed distribution The interpretation post intervention distribution corresponds experiment causal variables set non causal variables subsequently observed For Belief Network causal interpretation means ancestral order variables correspond temporal order This means start variables parents come time children coming later etc Ancestral sampling causal Belief Network corresponds temporal evolution physical experiment Influence diagrams calculus Another way represent intervention modify basic BN appending parental decision variable FX variable X intervention giving rise called influence diagram DRAFT April Code For example2 Simpson s paradox example use fig 13b p D G R FD p D FD G p G p R G D p FD p D FD G p D pa D p D FD d G D d Hence decision variable FD set state variable D determined standard observational term p D pa D If decision variable set state D variable puts probability single state D d This effect replacing conditional probability term unit factor instances D set variable interventional state3 A potential advantage influence diagram approach calculus conditional independence statements derived standard techniques augmented BN Additionally learning standard techniques apply decision variables set condition data sample collected causal non causal sample Remark Learning edge directions In absence data causal experiments justifiably sceptical learning causal networks Nevertheless prefer certain direction link based assumptions simplicity CPTs This preference come physical intuition whilst root causes uncertain relationship because effect clear In sense measure complexity CPT required entropy Such heuristics numerically encoded edge directions learned Markov equivalent graph See exercise Summary We reason certain uncertain evidence repeated application Bayes rule A belief network represents factorisation distribution conditional probabilities variables de pendent parental variables Belief networks correspond directed acyclic graphs Variables conditionally independent x y z p x y z p x z p y z absence link belief network corresponds conditional independence statement If graph representing belief network variables independent independent distribution consistent belief network structure Belief networks natural representing causal influences Causal questions addressed appropriate causal model Code Naive inference demo demoBurglar m Was Burglar demo demoChestClinic m Naive Inference Chest Clinic See exercise 2Here influence diagram distribution variables including decision variables contrast application IDs chapter 3More general cases considered variables placed distribution states DRAFT April Exercises U P D H A Figure Party animal Here variables binary P Been Party H Got Headache D Demotivated work U Underperform work A Boss Angry Shaded variables observed true state x d e t l b s x Positive X ray d Dyspnea Shortness breath e Either Tuberculosis Lung Cancer t Tuberculosis l Lung Cancer b Bronchitis Visited Asia s Smoker Figure Belief network structure Chest Clinic example Conditional independence demo The following demo determines X Y Z Chest Clinic network fig checks result numerically4 The independence test based Markov method section This alternative d separation method general deals conditional independence Markov Networks belief networks Running demo code happen numerical dependence small p X Y Z p X Z p Y Z X Y Z This highlights difference structural numerical independence condindepPot m Numerical measure conditional independence demoCondindep m Demo conditional independence Markov method Utility routines dag m Find DAG structure belief network Exercises Exercise Party Animal The party animal problem corresponds network fig The boss angry worker headache probability worker party To complete specifications probabilities given follows p U tr P tr D tr p U tr P fa D tr p H tr P tr p U tr P tr D fa p U tr P fa D fa p H tr P fa p A tr U tr p A tr U fa p P tr p D tr Exercise Consider distribution p b c p c b p p b Is b ii Is b c Exercise The Chest Clinic network concerns diagnosis lung disease tuberculosis lung cancer fig In model visit Asia assumed increase probability tuberculosis State following conditional independence relationships true false tuberculosis smoking shortness breath 4The code graphical conditional independence given chapter DRAFT April Exercises Gauge Battery Fuel Turn Over Start Figure Belief network car starting exercise lung cancer bronchitis smoking visit Asia smoking lung cancer visit Asia smoking lung cancer shortness breath Exercise Consider Chest Clinic belief network fig Calculate hand values p d p d s tr p d s fa The table values p tr p s tr p t tr tr p t tr fa p l tr s tr p l tr s fa p b tr s tr p b tr s fa p x tr e tr p x tr e fa p d tr e tr b tr p d tr e tr b fa p d tr e fa b tr p d tr e fa b fa p e tr t l t l fa Exercise If interpret Chest Clinic network exercise causally help doctor answer question If I cure patients Bronchitis affect patients chance short breath How compare p d tr b fa non causal interpretation mean Exercise The network fig concerns probability car starting p b bad p f p g b good f p g b good f p g b bad f p g b bad f p t fa b good p t fa b bad p s fa t tr f p s fa t tr f p s fa t fa f p s fa t fa f Calculate P f s probability fuel tank conditioned observation car start Exercise There synergistic relationship Asbestos A exposure Smoking S Cancer C A model describing relationship given p A S C p C A S p A p S Is A S Is A S C How adjust model account fact people work building industry higher likelihood smokers higher likelihood asbestos exposure Exercise Consider belief network right represents Mr Holmes burglary worries given fig 3a B urglar A larm W atson Mrs G ibbon B A G W DRAFT April Exercises All variables binary states tr fa The table entries p B tr p A tr B tr p A tr B fa p W tr A tr p W tr A fa p G tr A tr p G tr A fa Compute hand e working p B tr W tr b p B tr W tr G fa Consider situation evidence uncertain Mrs Gibbon thinks state G fa probability Similarly Dr Watson believes state W fa value Compute hand posteriors uncertain soft evidences p B tr W b p B tr W G Exercise A doctor gives patient D rug drug drug dependent A ge old young G ender male female Whether patient R ecovers recovers doesn t recover depends D A G In addition A G Write belief network situation Explain compute p recover drug Explain compute p recover drug young Exercise Implement Wet Grass scenario section BRMLtoolbox Exercise LA Burglar Consider Burglar scenario example We wish model fact Los Angeles probability burgled increases earthquake Explain include effect model Exercise Given belief networks represented DAGs associated adjacency matrices A B write MATLAB function MarkovEquiv A B m returns A B Markov equivalent zero Exercise The adjacency matrices belief networks given ABmatrices mat State Markov equivalent A B Exercise There computers indexed Computer send message timestep computer j Cij Cij There fault network task find information communication matrix C C necessarily symmetric To Thomas engineer run tests reveal computer send message computer j t timesteps t This expressed Cij t Cij Cij For example know C13 meaning according test message sent computer arrive computer timesteps Note message different routes directly timestep indirectly You assume Cii A priori Thomas thinks probability Cij j assumes connection independent rest Given test information C C12 C23 compute posteriori probability vector p C12 C p C13 C p C23 C p C32 C p C21 C p C31 C DRAFT April Exercises Exercise A belief network models relation variables oil inf eh bp rt stand price oil inflation rate economy health British Petroleum Stock price retailer stock price Each variable takes states low high bp states low high normal The belief network model variables tables p eh low p bp low oil low p bp normal oil low p bp low oil high p bp normal oil high p oil low eh low p oil low eh high p rt low inf low eh low p rt low inf low eh high p rt low inf high eh low p rt low inf high eh high p inf low oil low eh low p inf low oil low eh high p inf low oil high eh low p inf low oil high eh high Draw belief network distribution Given BP stock price normal retailer stock price high probability inflation high Exercise There set C potentials potential c defined subset variables Xc If Xc Xd merge multiply potentials c d variables potential c contained potential d With reference suitable graph structures describe efficient algorithm merge set potentials new set potentials potential contained Exercise This exercise explores distinction d connection dependence Consider distribution class p b c p c b p b p d connected c One expect means c dependent c Our interest non trivial distributions c Consider dom dom c dom b For p p b p c b c Consider p b c Z b b c positive function Z b c b b c Defining matrices M N elements Mij b j Nkj b j c k marginal distribution p c k represented matrix elements p c k Z MNT ik Show MNT m0n T vectors m0 n0 c DRAFT April Exercises Writing M m1 m2 m3 N n1 n2 n3 dimensional vectors mi ni MNT m1n T m2n T m3n T Show setting m2 m1 n3 n1 n2 scalar MNT written m0n T m0 m1 m3 n0 n1 n2 Hence construct example tables different specified p p b p c b c Verify examples explicitly BRMLtoolbox Exercise Alice Bob share bank account contains priori unknown total money T Whenever Alice goes cash machine available withdrawal A Alice total T Similarly Bob goes cash machine available withdrawal B Bob total T Whatever bank Alice Bob check available amounts withdrawal independently Draw belief network expresses situation A B Exercise Assume day week females born x independent day week y males born Assume old rhyme true personality dependent day week born If represents female personality type b male personality type x b y b Whether male female married m depends strongly personality types m b independent x y know b Draw belief network represent setting What graphical dependency days week John Jane born given married Exercise A survey households husband wife car The survey states household income inc high low There car types dom h dom w cheap expensive The survey finds given household income types cars owned husband wife independent wife s car type husband s car type family income Specifically p inc low p w inc low p w inc high p h inc low p h inc high Use BRMLtoolbox find marginal p w h whilst h w inc case h w Exercise BinGame player game win lose outcome There players A B C competing tournament following outcomes A beat B times B beat C times DRAFT April Exercises A beat C times C beat A time Player D enters tournament loses twice player C We assume player skill level given skill levels players sA sB probability player A beats player B exp sB sA Assume priori skill levels players independent place equal probability skills levels Additionally outcome games given skill levels independent Using tournament information Draw belief network represents independence assumptions Are skill levels players posteriori e given game outcomes independent Calculate probability player D beat player A game BinGame Calculate expected skill level players Exercise Before watching video free website Daily Fail forces watch video advert There fact adverts available Daily Fail randomly selects advert links present viewer After clicking advert links advert plays desired news video watched The data Daily Fail website visitors follows ai aj ak means advert clicked preference adverts j k Assume advert implicit interest value s affects probability viewers click video If sA sB sC interest levels randomly chosen adverts A B C probability viewer clicks advert A assumed proportional exp sA max sB sC Assuming interest levels priori independent uniform viewer clicks adverts independently given advert interest levels calculate expected interest levels advert DRAFT April CHAPTER Graphical Models In chapter saw belief networks represent statements independence variables probabilistic model Belief networks simply way unite probability graphical representation Many exist general heading graphical models Each specific strengths weaknesses Broadly graphical models fall classes useful modelling belief networks useful inference This chapter survey popular models class Graphical Models Graphical Models GMs depictions independence dependence relationships distributions Each class GM particular union graph probability constructs details form independence assumptions represented GMs useful provide framework studying wide class prob abilistic models associated algorithms In particular help clarify modelling assumptions provide unified framework inference algorithms different communities related It needs emphasised forms GM limited ability graphically express conditional dependence statements As ve seen belief networks useful modelling ancestral condi tional independence In chapter ll introduce types GM suited representing different assumptions Here ll focus Markov networks chain graphs marry Belief Markov networks factor graphs There inhabitants zoo graphical models The general viewpoint adopt describe problem environment probabilistic model reasoning corresponds performing probabilistic inference This process Modelling After identifying potentially relevant variables problem environment task describe variables interact This achieved structural assumptions form joint probability distribution variables typically corresponding assumptions independence variables Each class graphical model corresponds factorisation property joint distribution Inference Once basic assumptions variables interact formed e probabilistic model constructed questions interest answered performing inference distribution This computationally non trivial step coupling GMs accurate inference algorithms central successful graphical modelling Whilst strict separation GMs tend fall broad classes useful modelling useful representing inference algorithms For modelling belief networks Markov networks chain graphs influence diagrams popular For inference typically compiles model Markov Networks x1 x2 x3 x4 x1 x2 x3 x4 b x1 x2 x3 x4 x5 x6 c Figure x1 x2 x2 x3 x3 x4 x4 x1 Za b x1 x2 x3 x4 Zb c x1 x2 x4 x2 x3 x4 x3 x5 x3 x6 Zc suitable GM algorithm readily applied Such inference GMs include factor graphs junction trees Markov Networks Belief networks correspond special kind factorisation joint probability distribution factors distribution An alternative factorisation example p b c Z b b c b b c potentials Z constant ensures normalisation called partition function Z b c b b c Definition Potential A potential x non negative function variable x x A joint potential x1 xn non negative function set variables A distribution special case potential satisfying normalisation x x This holds similarly continuous variables summation replaced integration We typically use convention ordering variables potential relevant distribution joint variables simply index element potential table Markov Networks defined products potentials defined maximal cliques undirected graph fig Definition Markov Network For set variables X x1 xn Markov network defined product potentials subsets variables Xc X p x1 xn Z C c c Xc The constant Z ensures distribution normalised Graphically represented undirected graph G Xc c C maximal cliques G For case clique potentials strictly positive called Gibbs distribution Definition Pairwise Markov network In special case graph contains cliques size distribution called pairwise Markov Network potentials defined link variables DRAFT April Markov Networks b Figure b By global Markov property path passes Whilst Markov network formally defined maximal cliques practice authors use term refer non maximal cliques For example graph right maximal cliques x1 x2 x3 x2 x3 x4 graph describes distribution p x1 x2 x3 x4 x1 x2 x3 x2 x3 x4 Z In pairwise network potentials assumed cliques giving p x1 x2 x3 x4 x1 x2 x1 x3 x2 x3 x2 x4 x3 x4 Z x1 x2 x3 x4 Example Boltzmann machine A Boltzmann machine MN binary variables dom xi form p x Z w b e j wijxixj bixi interactions wij weights bi biases This model studied machine learning community basic model distributed memory computation The graphical model Boltzmann machine undirected graph link nodes j wij Consequently specially constrained W graph multiply connected inference typically intractable Definition Properties Markov Networks A B C p A B C AC A C BC B C Z A B C A B Marginalising C makes A B graphically dependent In general p A B p A p B A B C A B Conditioning C makes A B independent p A B C p A C p B C Markov properties We consider informally properties Markov networks reader referred detailed proofs Consider MN fig 2a use shorthand p p x1 x1 x2 x3 etc We use undirected graph demonstrate conditional independence properties Note dividing potentials order ensure defined assume potentials positive For positive potentials following local pairwise global Markov properties equivalent DRAFT April Markov Networks x1 x3 x4 x2 x3 x4 b x1 x3 x4 c x2 x3 x4 d x1 x2 x3 x4 e Figure d Local conditional distributions Note distribution implied parents variable That given conditional p x4 x1 x3 read graph imply x1 x3 marginally independent e The Markov network consistent local distributions If local distributions positive Hammersley Clifford theorem joint distribution consistent local distributions Gibbs distribution structure given e Definition Separation A subset S separates subset A subset B disjoint A B path member A member B passes S If path member A member B A separated B If S provided path exists A B A B separated Definition Global Markov Property For disjoint sets variables A B S S separates A B G A B S As example global Markov property consider fig 2a p p This implies p p p This inferred paths pass Procedure An algorithm independence The separation property implies simple algorithm deciding A B S We simply remove links neighbour set variables S If path member A member B A B S true section For positive potentials called local Markov property holds p x X x p x ne x That conditioned neighbours x independent remaining variables graph In addition called pairwise Markov property holds non adjacent vertices x y x y X x y Markov random fields A MRF set conditional distributions indexed location DRAFT April Markov Networks Definition Markov Random Field A MRF defined set distributions p xi ne xi n indexes distributions ne xi neighbours variable xi subset variables x1 xn distribution variable xi depends The term Markov indicates proper subset variables A distribution MRF respect undirected graph G p xi x p xi ne xi ne xi neighbouring variables variable xi according undirected graph G The notation x shorthand set variables X excluding variable xi X xi set notation Hammersley Clifford Theorem An undirected graph G specifies set independence statements An interesting challenge find general functional form distribution satisfies independence statements A trivial example graph x1 x2 x3 x1 x3 x2 From requirement p x1 x2 x3 p x1 x2 Hence p x1 x2 x3 p x1 x2 x3 p x2 x3 p x1 x2 p x2 x3 x1 x2 x2 x3 potentials More generally decomposable graph G definition start edge work inwards reveal functional form product potentials cliques G For example fig 2a start variable x1 corresponding local Markov statement x1 x4 x5 x6 x7 x2 x3 write p x1 x7 p x1 x2 x3 p x2 x3 x4 x5 x6 x7 Now consider x1 eliminated neighbours x1 x2 x3 The graph specifies x1 x2 x3 independent x5 x6 x7 given x4 p x1 x2 x3 x4 x5 x6 x7 p x1 x2 x3 x4 By summing sides x1 p x2 x3 x4 x5 x6 x7 p x2 x3 x4 Hence p x2 x3 x4 x5 x6 x7 p x2 x3 x4 x5 x6 x7 p x4 x5 x6 x7 p x2 x3 x4 p x4 x5 x6 x7 p x1 x7 p x1 x2 x3 p x2 x3 x4 p x4 x5 x6 x7 Having eliminated x2 x3 neighbour s remaining graph x4 Continuing way necessarily end distribution form p x1 x7 p x1 x2 x3 p x2 x3 x4 p x4 x5 x6 p x5 x6 x7 p x7 The pattern clear shows Markov conditions mean distribution expressible product potentials defined cliques graph That G F F factorisation clique potentials G The converse easily shown given factorisation clique potentials Markov conditions G implied Hence G F It clear decomposable G holds work inwards edges graph The Hammersley Clifford theorem stronger result shows factorisation property holds undirected graph provided potentials positive For formal proof reader referred An informal argument considering specific example DRAFT April Markov Networks cycle x1 x2 x3 x4 x1 fig 1a The theorem states positive potentials Markov conditions implied graph mean distribution form p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 One readily verify distribution form x1 x3 x2 x4 Consider including additional term links x1 variable member cliques x1 inhabits That include term x1 x3 Our aim distribution form p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 satisfy Markov property x1 x3 x2 x4 To examine p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 x1 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 x1 x3 If assume potential weakly dependent x1 x3 x1 x3 x1 x3 p x1 x2 x3 x4 given x1 x2 x4 x1 x1 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 By expanding f f O retaining terms order obtain p x1 x2 x3 x4 x1 x2 x4 x1 x1 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 O The factor independent x3 required Markov condition However second term varies function x3 The reason find function x1 x3 x1 x3 x1 x1 x2 x4 x1 x1 x3 x1 x1 x2 x4 x1 term x1 x3 left functionally dependent x1 term right function x1 Hence way ensure Markov condition holds connection x1 x3 One generalise argument graph potentials distribution contains link present G distribution corresponding Markov condition hold Informally G F The converse F G trivial The Hammersley Clifford theorem helps resolve questions set positive local conditional distributions p xi pa xi form consistent joint distribution p x1 xn Each local conditional distribution p xi pa xi corresponds factor set variables xi pa xi include term joint distribution The MN form joint distribution consistent local conditional distributions p x1 xn factorises according p x1 xn Z exp c Vc Xc DRAFT April Markov Networks b c d e f g j k h b c d e f b b c d e f c Figure Belief network interested checking conditional independence b d b Ancestral graph c Ances tral moralised separated graph b d There path red green node b independent given d sum cliques Vc Xc real function defined variables clique indexed c Equation equivalent c Xc MN positive clique potentials The graph cliques defined undirected graph constructed taking local conditional distribution p xi pa xi drawing clique xi pa xi This repeated local conditional distributions fig Note HC theorem mean given set conditional distributions form consistent joint distribution states functional form joint distribution conditionals consistent joint exercise Conditional independence Markov networks For X Y Z collections variables section discussed algorithm determine X Y Z belief networks An alternative general method handles directed undirected graphs uses procedure See fig example Procedure Ascertaining independence Markov belief networks For Markov Networks final separation criterion needs applied Ancestral Graph Identify ancestors A nodes X Y Z Retain nodes X Y Z remove nodes A edges nodes Moralisation Add link remaining nodes common child connected arrow Then remove remaining arrowheads Separation Remove links neighbouring Z In undirected graph constructed look path joins node X Y If path deduce X Y Z Note ancestral step procedure belief networks intuitive given set nodes X ancestors A remaining nodes D form contribution distribution form p D X A p X A summing D simply effect removing variables DAG Lattice Models Undirected models long history different branches science especially statistical mechanics lattices recently models visual processing models encourage neighbouring vari ables states DRAFT April Markov Networks Consider model desire states binary valued variables x1 x9 arranged lattice right prefer neighbouring variables state p x1 x9 Z j ij xi xj j denotes set indices j neighbours undirected graph x1 x2 x3 x4 x5 x6 x7 x8 x9 The Ising model A set potentials equation encourages neighbouring variables state ij xi xj e 2T xi xj xi This corresponds known model physics magnetic systems called Ising model consists mini magnets prefer aligned state depending temperature T For high T variables behave independently global magnetisation appears For low T strong preference neighbouring mini magnets aligned generating strong macro magnet Remarkably large dimensional lattice called Curie temperature Tc variables system admits phase change large fraction variables aligned Tc average variables unaligned This depicted fig M Ni xi N average alignment variables That phase change happens non zero temperature driven considerable research related areas Global coherence effects arise weak local constraints present systems admit emergent behaviour Similar local constraints popular image restoration algorithms clean noise assumption noise local spatial coherence whilst signal Example Cleaning images Consider binary image defined set pixels xi D We observe noise corrupted version yi pixel xi state yi opposite xi probability Here filled nodes indicate observed noisy pixels unshaded nodes latent clean pixels Our interest clean observed dirty image Y find likely joint clean image X A model situation p X Y Z D xi yi j xi xj xi yi exiyi xi xj exixj j indicates set latent variables neighbours The potential encourages noisy clean pixel state Similarly potential xi xj encourages neighbouring pixels state To find likely clean image need compute argmax X p X Y argmax X p X Y This computationally difficult task approximated iterative methods section T Tc M Figure Onsager magnetisation As temperature T decreases critical temperature Tc phase transition occurs large fraction variables aligned state DRAFT April Chain Graphical Models b c d b cd b b g d c e fh c aedfh c bg d Figure Chain graphs The chain components identified deleting directed edges identifying remaining connected components Chain components b c d written BN cluster variables b c Chain components e d f h b g c cluster BN representation d On left clean image noisy corrupted image Y formed middle The likely restored image given right See demoMRFclean m Note parameter straightforward set given knowledge corruption probability pcorrupt p yi xi xi pcorrupt Setting complex relating p xi xj straightforward section In demonstration set pcorrupt Chain Graphical Models Chain Graphs CGs contain directed undirected links To develop intuition consider fig 6a The terms unambiguously specify depiction p p b mixed interaction directed undirected edges b vertices By probability p b c d p p b p c d b Looking graph expect interpretation p c d b c d p c p d b However ensure normalisation retain generality interpret p c d b c d p c p d b b b c d c d p c p d b This leads interpretation CG DAG chain components Definition Chain Component The chain components graph G obtained Forming graph G directed edges removed G Then connected component G constitutes chain component DRAFT April Chain Graphical Models Each chain component represents distribution variables component conditioned parental components The conditional distribution product cliques undirected component moralised parental components including factor ensure normalisation chain component Definition Chain Graph distribution The distribution associated chain graph G found identifying chain components associated variables X Then p x p X pa X p X pa X d D p xd pa xd c C Xc C denotes union cliques component associated functions defined clique D set variables component correspond directed terms p xd pa xd The proportionality factor determined implicitly constraint distribution sums BNs CGs connected components singletons MNs CGs chain components simply connected components undirected graph CGs useful expressive CI statements belief networks Markov networks The reader referred details Example Chain graphs expressive Belief Markov networks Consider chain graph fig 7a chain component decomposition p b c d e f p p b p c d e f b p c d e f b p c c e e f d f p d b b normalisation requirement b c d e f p c c e e f d f p d b The marginal p c d e f given c e e f d f b b p p b p c p d b c d Since marginal distribution p c d e f undirected cycle DAG express CI statements contained marginal p c d e f Similarly undirected distribution skeleton fig 7a express b independent unconditionally e p b p p b DRAFT April Factor Graphs b c d e f c d e f b c d e f c Figure The CG expresses b d e c f No directed graph express conditions marginal distribution p c d e f undirected cycle b Any DAG cycle contain collider c express different set CI statements b Similarly connected Markov network express unconditional independence expresses CI statements belief network Markov network express Factor Graphs Factor Graphs FGs mainly inference algorithms1 Definition Factor Graph Given function f x1 xn Xi The FG node represented square factor variable node represented circle variable xj For xj Xi undirected link factor variable xj When represent distribution p x1 xn Z Xi normalisation constant Z X Xi assumed Here X represents variables distribution For factor Xi conditional distribution p xi pa xi use directed links parents factor node directed link factor node child xi This structure undirected FG preserves information factors distributions Factor graphs useful preserve information form distribution belief network Markov network chain graph Consider distribution p b c b c b c Represented MN single clique given fig 8c However fig 8c equally represent unfactored clique potential b c factorised structure clique lost In sense FG representation fig 8b precisely conveys form distribution equation An unfactored clique potential b c represented FG fig 8a Hence different FGs MN information structure clique potential lost MN Similarly belief network fig 8d represent standard undirected FG information independence preserved directed FG representation fig 8e One consider partially directed FGs contain directed undirected edges requires specification structure normalised use approach analogous chain graph details 1Formally FG alternative graphical depiction hypergraph vertices represent variables hyperedge factor function variables associated hyperedge A FG hypergraph additional interpretation graph represents function defined products associated hyperedges Many thanks Robert Cowell observation DRAFT April Expressiveness Graphical Models bc b c b b c c b c d b c e Figure b c b b b c c c b c Both b undirected graphical model c d undirected FG d e Directed FG BN d A directed factor represents term p children parents The advantage e information marginal independence variables b c clear graph e ascertain examination numerical entries factors graph Conditional independence factor graphs Conditional independence questions addressed rule works directed undirected partially directed FGs To determine variables independent given set condi tioned variables consider paths connecting variables If paths blocked variables conditionally independent A path blocked following conditions satisfied One variables path conditioning set One variables factors path incoming edges path variable factor collider variable factor descendants conditioning set Expressiveness Graphical Models It clear directed distributions represented undirected distributions asso ciate normalised factor joint distribution potential For example distribution p b p b c p c factored b b c b p b b c p b c p c Z Hence belief network represented MN simple identification factors distributions However general associated undirected graph corresponds moralised directed graph contain additional links independence information lost For example MN p c b p p b single clique b c graphically infer b The converse question undirected model represented BN readily derived link structure Consider example fig In case directed model link structure express dependencies undirected graph Naturally probability distribution represented BN necessarily simple structure fully connected cascade style graph In sense DAG graphically represent independence properties hold undirected distribution Definition Independence Maps A graph independence map I map given distribution P conditional independence statement derive graph G true distribution P That X Y ZG X Y ZP disjoint sets X Y Z DRAFT April Expressiveness Graphical Models b c d b c d b Figure An undirected model wish find directed equivalent b Every DAG structure undirected model situation arrows point node node d cyclic graph Summing states variable d leave DAG variables b c link c This represent undirected model marginalises d adds link c Similarly graph dependence map D map given distribution P conditional independence statement derive P true graph G That X Y ZG X Y ZP disjoint sets X Y Z A graph G I map D map P called perfect map X Y ZG X Y ZP disjoint sets X Y Z In case set conditional independence dependence statements expressible graph G consistent P vice versa Note contraposition dependence map equivalent X Y ZG X Y ZP meaning X Y graphically dependent given Z dependent distribution One way think distribution P write list LP independence statements For graph G writes list possible independence statements LG Then LP LG Dependence Map D map LP LG Independence Map I map LP LG Perfect Map In assume statement l contained L consistent derived independence statements L One discuss distribution class associated map That numerical instances distributions consistent specified form obey constraints required map To numerical instance distribution Pi consistent given class P write list LPi independence statements One takes intersection LP iLPi lists possible distribution instances This list equation determine associated map For example distribution class p x y z p z x y p x p y directed perfect map x z y However undirected graph class equation fully connected LG For distribution consistent equation x y independent statement contained LG undirected D map perfect undirected map class represented equation Example Consider distribution class defined variables t1 t2 y1 y2 p t1 t2 y1 y2 p t1 p t2 h p y1 t1 h p y2 t2 h p h DRAFT April Expressiveness Graphical Models In case list independence statements distribution instances consistent p LP y1 t2 t1 y2 t1 t2 t2 t1 Consider graph BN p y2 y1 t1 t2 p y1 t1 p t1 p t2 For LG y1 t2 t1 t2 t1 Hence LG LP BN I MAP independence statement BN true distribution class equation However D MAP LP LG In case perfect MAP BN MN represent Remark Forcing dependencies Whilst graphical models defined ensure specified independencies inappropriate ensuring specified dependencies Consider undirected graph x y z Graphically expresses x z dependent However numerical instances distributions hold example p x y z x y y z Z1 x y const One complain pathological case graphical representation particular instance contains link x y Maybe force potentials non trivial functions arguments ensure dependency Consider x y x y y z yz In case potentials non trivial sense truly functionally dependent arguments Hence undirected network contains genuine links x y y z Nevertheless p2 x y z x y y z Z2 x y yz xz Hence p2 x z xz x z So forcing local non trivial functions guarantee dependence path connected variables In case algebraic cancellation clear problem trivial p2 x y y z assume x z remark However cases algebraic simplifications highly non trivial true See example exercise construct p x y z x y y z x y y z x z Summary Graphical modelling discipline representing probability models graphically Belief networks intuitively describe variables causally influence represented directed graphs A Markov network represented undirected graph Intuitively linked variables Markov network graphically dependent describing local cliques graphically dependent variables DRAFT April Exercises Markov networks historically important physics understand global collabo rative phenomena emerge local dependencies Graphical models generally limited ability represent possible logical consequences probabilistic model Some special probabilistic models perfectly mapped graphically Factor graphs describe factorisation functions necessarily related probability distribu tions A detailed discussion axiomatic logical basis conditional independence given Code condindep m Conditional Independence test p X Y Z p X Z p Y Z Exercises Exercise Consider pairwise Markov network p x x1 x2 x2 x3 x3 x4 x4 x1 Express terms following p x1 x2 x4 p x2 x1 x3 p x3 x2 x4 p x4 x1 x3 For set local distributions defined p1 x1 x2 x4 p2 x2 x1 x3 p3 x3 x2 x4 p4 x4 x1 x3 possible find joint distribution p x1 x2 x3 x4 consistent local conditional distributions Exercise Consider Markov network p b c ab b bc b c Nominally summing b variables c dependent For binary b explain situation case marginally c independent Exercise Show Boltzmann machine defined binary variables xi p x Z W b exp xTWx xTb assume loss generality W WT Exercise The restricted Boltzmann machine Harmonium constrained Boltzmann machine bipartite graph consisting layer visible variables v v1 vV hidden variables h h1 hH p v h Z W b exp vTWh aTv bTh All variables binary taking states h1 h2 v1 v2 v3 DRAFT April Exercises Show distribution hidden units conditional visible units factorises p h v p hi v p hi v bi j Wjivj x ex ex By symmetry arguments write form conditional p v h Is p h p hi Can partition function Z W b computed efficiently RBM Exercise You given x y z u u z Derive general form probability distribution p x y z u consistent statements Does distribution simple graphical model Exercise The undirected graph represents Markov network nodes x1 x2 x3 x4 x5 counting clockwise pentagon potentials xi xj Show joint distribution written p x1 x2 x3 x4 x5 p x1 x2 x5 p x2 x4 x5 p x2 x3 x4 p x2 x5 p x2 x4 express marginal probability tables explicitly functions potentials xi xj Exercise Consider belief network right Write Markov network p x1 x2 x3 Is Markov network perfect map p x1 x2 x3 h1 h2 x1 x2 x3 Exercise Two research labs work independently relationship discrete variables x y Lab A proudly announces ascertained distribution pA x y data Lab B proudly announces ascertained pB y x data Is possible find joint distribution p x y consistent results labs Is possible define consistent marginals p x p y sense p x y pA x y p y p y x pB y x p x If explain find marginals If explain Exercise Research lab A states findings set variables x1 xn list LA conditional independence statements Lab B similarly provides list conditional independence statements LB Is possible find distribution consistent LA LB If lists contain dependence statements attempt find distribution consistent lists Exercise Consider distribution p x y w z p z w p w x y p x p y Write p x z formula involving p z w p w x y p x p y Write p y z formula involving p z w p w x y p x p y DRAFT April Exercises Using results derive explicit condition x y z explain satisfied distribution Exercise Consider distribution p t1 t2 y1 y2 h p y1 y2 t1 t2 h p y2 t2 h p t1 p t2 p h Draw belief network distribution Does distribution p t1 t2 y1 y2 h p y1 y2 t1 t2 h p y2 t2 h p t1 p t2 p h perfect map belief network Show p t1 t2 y1 y2 defined t1 y2 Exercise Consider distribution p b c d ab b bc b c cd c d da d potentials Draw Markov network distribution Explain distribution represented non complete belief network Derive explicitly c Exercise Show singly connected Markov network construct Markov equivalent belief network Exercise Consider pairwise binary Markov network defined variables si N p s ij E ij si sj E given edge set potentials ij arbitrary Explain translate Markov network Boltzmann machine Exercise Our interest possible find joint distribution p rise set specified consistent distributions q subsets variables We wish construct set distributions q12 x1 x2 q13 x1 x3 q23 x2 x3 binary variables xi marginally consistent x2 q12 x1 x2 x3 q13 x1 x3 x1 q12 x1 x2 x3 q23 x2 x3 x1 q13 x1 x3 x2 q23 x2 x3 q distributions non negative summing By writing q12 x1 x2 y1 q12 x1 x2 y2 etc equations represented linear system My c yi M suitably defined matrix Hence solving system linear programming find set marginally consistent distributions q DRAFT April Exercises Given set marginally consistent q interest find joint distribution p rise marginals q That x1 p x1 x2 x3 q23 x2 x3 x2 p x1 x2 x3 q13 x1 x3 x3 p x1 x2 x3 q12 x1 x2 Show writing states p z1 z8 expressed linear system Az y zi zi A suitably defined matrix For marginally consistent y numerically possible find joint distribution z rise marginals The set marginals consistent distribution p called marginal polytope p The question asking given marginal set q marginal polytope p Whilst straightforward solve linear programming example n variable system size linear system exponential n resulting computationally hard problem This issue important reasons Imagine different research labs asked examine different aspects problem lab summarising results distribution subset variables The question exists single joint distribution consistent marginals ii In certain deterministic approximation schemes section objective depends set marginals question characterise marginal polytope As question intimates generally difficult However chapter singly connected structures representing distribution terms locally consistent marginals straightforward Exercise The question concerns cleaning binary image given file xnoisy mat The objective function maximise j Wi jI xi xj bixi xi Wi j neighbouring pixels left right image Wi j Using y represent noisy image bi 2yi Plot final cleaned image x maximum objective function found cleaned image DRAFT April CHAPTER Efficient Inference Trees In previous chapters discussed set models Inference corresponds operations summing subsets variables In machine learning related areas deal distributions containing hundreds variables In general inference computationally expensive useful understand graphical structures cheap order models subsequently compute In chapter discuss inference cheap case trees links classical algorithms different fields computer science dynamic programming physics transfer matrix methods Marginal Inference Given distribution p x1 xn inference process computing functions distribution Marginal inference concerned computation distribution subset variables possibly conditioned subset For example given joint distribution p x1 x2 x3 x4 x5 evidence x1 tr marginal inference calculation p x5 x1 tr x2 x3 x4 p x1 tr x2 x3 x4 x5 Marginal inference discrete models involves summation focus development In principle algorithms carry continuous variable models lack closure continuous distributions marginalisation Gaussian notable exception direct transference algorithms continuous domain problematic The focus efficient inference algorithms marginal inference singly connected structures An efficient algorithm multiply connected graphs considered chapter Variable elimination Markov chain message passing A key concept efficient inference message passing information graph summarised local edge information To develop idea consider variable Markov chain Markov chains b c d Figure A Markov chain form p xT T t p xt xt assignment variables labels xt Variable Elimi nation carried time linear number variables chain Marginal Inference discussed depth section p b c d p b p b c p c d p d given fig task calculate marginal p For simplicity assume variables domain Then p b c d p b c d b c d p b p b c p c d p d We carry computation simply summing probabilities states variables b c d This require addition numbers calls A efficient approach push summation d far right possible p b c p b p b c d p c d p d d c d c state potential Defining d c requires addition numbers calls state c Similarly distribute summation c far right possible p b p b c p b c d c c b Then finally p b p b c b By distributing summations addition numbers calls compared naive approach Whilst saving appear important point number computations chain length T linear 2T opposed exponential 2T naive approach This procedure called variable elimination time sum states variable elimi nate distribution We perform variable elimination chain efficiently natural way distribute summations working inwards edges Note case potentials fact distributions recursively computing marginal distribution right leaf chain One view elimination variable passing message information neighbouring node graph We calculate univariate marginal tree singly connected graph starting leaf tree eliminating variable working inwards nibbling time leaf remaining tree Provided perform elimination leaves inwards structure remaining graph simply subtree original tree albeit conditional probability table entries modified This guaranteed enable calculate marginal p xi number summations scales linearly number variables tree Finding conditional marginals chain Consider following inference problem fig Given p b c d p b p b c p c d p d find p d This computed p d b c p b c d b c p b p b c p c d p d c b p b p b c b c p c d p d c d DRAFT April Marginal Inference The missing proportionality constant found repeating computation states variable d Since know p d kc d c d unnormalised result summation use fact d p d infer k d c d In example potential b c distribution c c d In general view variable elimination passing messages form potentials nodes neighbours For belief networks variable elimination passes messages distributions following direction edge non normalised potentials passing messages direction edge Remark Variable elimination trees matrix multiplication Variable elimination related associativity matrix multiplication For equation define matrices Mab j p b j Mbc j p b c j Mcd j p c d j Md p d Ma p Then marginal Ma written Ma MabMbcMcdMd Mab Mbc McdMd matrix multiplication associative This matrix formulation calculating marginals called transfer matrix method particularly popular physics literature Example Where fly You live house rooms labelled There door rooms rooms One directly pass rooms time step An annoying fly buzzing room smelly cheese room attract fly Using xt indicate room fly time t dom xt movement fly described transition p xt xt j Mij Mij element transition matrix M The matrix M called stochastic meaning required conditional probability table columns sum 1Mij Given fly room time t probability room occupancy time t Assume Markov chain defined joint distribution p x1 xT p x1 T t p xt xt We asked compute p x5 x1 given x4 x3 x2 p x5 x4 p x4 x3 p x3 x2 p x2 x1 Since graph distribution Markov chain easily distribute summation terms This easily transfer matrix method giving p x5 x1 M4v DRAFT April Marginal Inference v vector components T reflecting evidence time t fly room Computing decimal places accuracy M4v Similarly time t occupancy probabilities The room occupancy probability converging particular distribution stationary distribution Markov chain One ask fly infinite number time steps That interested large t behaviour p xt xt p xt xt p xt At convergence p xt p xt Writing p vector describing stationary distribution means p Mp In words p eigenvector M eigenvalue Computing numerically stationary distribution Note software packages usually return eigenvectors e unit eigenvector usually require normalisation probability ei The sum product algorithm factor graphs Both Markov belief networks represented factor graphs For reason convenient derive marginal inference algorithm FGs applies Markov belief networks This termed sum product algorithm compute marginals need distribute sum variable states product factors In texts referred belief propagation Non branching graphs variable variable messages Consider distribution p b c d f1 b f2 b c f3 c d f4 d factor graph represented fig To compute marginal p b c variable d occurs locally use p b c d p b c d d f1 b f2 b c f3 c d f4 d f1 b f2 b c d f3 c d f4 d d c c Here d c c defines message node d node c function variable c Similarly p b c p b c f1 b c f2 b c d c c c b b Hence c b b c f2 b c d c c It clear recurse definition messages chain n variables marginal node computed time linear n The term c b b interpreted carrying DRAFT April Marginal Inference b c d f1 f2 f3 f4 Figure For singly connected structures branches simple messages variable neighbour defined form efficient marginal inference scheme marginal information graph c For simple linear structures branching messages variables variables sufficient However general structures branching useful consider types messages variables factors vice versa General singly connected factor graphs The slightly complex example p b p b c d p c p d p e d factor graph depicted fig f1 b f2 b c d f3 c f4 d e f5 d The marginal p b represented amputated graph message p b f1 b c d f2 b c d f3 c f5 d e f4 d e f2 b b f2 b b message factor variable This message constructed messages arriving branches c d f2 b b c d f2 b c d f3 c c f2 c f5 d e f4 d e d f2 d Similarly interpret d f2 d f5 d f5 d d e f4 d e f4 d d To complete interpretation identify c f2 c f3 c c In non branching link simply use variable variable message To compute marginal p p b f1 b f2 b b f1 b c d e f1 f2 f3 f4 f5 Figure For branching singly connected graph useful define messages factors variables variables factors DRAFT April Marginal Inference For consistency interpretation view f1 b f1 b f2 b b b f1 b We message factor node formed summing product incoming node factor messages Similarly message node factor given product incoming factor node messages A convenience approach messages reused evaluate marginal inferences For example clear p b given p b f1 b f1 b b f2 b b If additionally desire p c need define message f2 c f2 c c b d f2 b c d b f2 b d f2 d b f2 b f1 b b This demonstrates reuse computed message d f2 compute marginal p c Definition Message schedule A message schedule specified sequence message updates A valid schedule message sent node node received requisite messages neighbours In general valid updating schedule Sum Product algorithm The sum product algorithm described messages updated function incoming messages One proceeds computing messages schedule allows computation new message based previously computed messages messages factors variables vice versa computed Procedure Sum Product messages Factor Graphs Given distribution defined product subsets variables p X Z f f Xf provided factor graph singly connected carry summation variables efficiently Initialisation Messages leaf node factors initialised factor Messages leaf variable nodes set unity Variable Factor message x f x g ne x f g x x f f1 f2 f3 x x f x f x x f2 x x f3 x x DRAFT April Marginal Inference Factor Variable message f x x Xf x f Xf y ne f x y f y We write Xf x denote summation states set variables Xf x x y1 y2 y3 f f x x y f y y2 f y2 y3 f y3 Marginal p x f ne x f x x f1 f2 f3x f1 x x f2 x x x f3 x For marginal inference important information relative size message states renormalise messages wish Since marginal proportional incoming messages variable normalisation constant trivially obtained fact marginal sum However wish compute normalisation constant messages normalise messages global information lost Dealing Evidence For distribution splits evidential non evidential variables X Xe Xn marginal non evidential variable p xi Xe given summing variables Xn xi Xe set evidential states There ways reconcile factor graph formalism Either simply setting variables Xe define new factor graph Xn pass messages new factor graph Alternatively define potentials contain variables Xe multiplying potential contains evidential variable delta function indicator zero variable xe specified evidential state When perform factor variable message sum modified potential evidential variable states zero state corresponds evidential setting Another way view sum factor variable message non evidential variables evidential variables potential set evidential states Computing marginal likelihood For distribution defined products potentials f Xf p X Z f f Xf normalisation given Z X f f Xf To compute summation efficiently product incoming messages arbitrarily chosen variable x sum states variable Z x f ne x f x x If factor graph derived setting subset variables BN evidential states summation non evidential variables yield marginal visible evidential variables For example p b d c p b p b c p c d p d DRAFT April Marginal Inference This interpreted requiring sum product suitably defined factors Hence readily find marginal likelihood evidential variables singly connected BNs Log messages For method work absolute relative values messages required prohibits renormalisation stage message passing procedure However normalisation numerical value messages small particularly large graphs numerical precision issues occur A remedy situation work log messages log For variable factor messages x f x g ne x f g x x simply x f x g ne x f g x x More care required factors variable messages defined f x x Xf x f Xf y ne f x y f y Naively write f x x log Xf x f Xf exp y ne f x y f y However exponentiation log messages because potential numerical precision problems A solution numerical difficulty obtained finding largest value incoming log messages y f max y ne f x y f y Then f x x y f log Xf x f Xf exp y ne f x y f y y f By construction terms exp y ne f x y f y y f term equal This ensures dominant numerical contributions summation computed accu rately Log marginals readily found log p x f ne x f x x DRAFT April Other Forms Inference b cd f1 f2 f3 f4 b c f1 f2 f5 b Figure Factor graph loop b Elimi nating variable d adds edge c demon strating general perform marginal ference loopy graphs simply passing messages existing edges original graph The problem loops Loops because problem variable elimination message passing techniques variable eliminated structure amputated graph general changes For example consider FG p b c d f1 b f2 b c f3 c d f4 d depicted fig 4a The marginal p b c given p b c f1 b f2 b c d f3 c d f4 d f5 c adds link ac amputated graph fig 4b This means account information variable d simply updating potentials links original graph needs account fact structure graph changes The junction tree algorithm chapter deals combining variables new singly connected graph graph structure remains singly connected variable elimination Other Forms Inference Max Product A common interest likely state distribution That argmax x1 x2 xn p x1 x2 xn To compute efficiently trees exploit factorisation structure distribution analogous sum product algorithm That aim distribute maximization local computations required To develop algorithm consider function represented undirected chain f x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 wish find joint state x x x x maximises f Firstly calculate maximum value f Since potentials non negative write max x f x max x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 max x1 x2 x3 x1 x2 x2 x3 max x4 x3 x4 x3 max x1 x2 x1 x2 max x3 x2 x3 x3 x2 max x1 x2 x1 x2 x2 max x1 max x2 x1 x2 x2 x1 The final equation corresponds solving single variable optimisation determines optimal value function f optimal state x argmax x1 x1 Given x optimal x2 given x argmax x2 x x2 x2 similarly x argmax x3 x x3 x3 x argmax x4 x x4 This procedure called backtracking Note equally started end chain DRAFT April Other Forms Inference defining messages pass information xi xi The chain structure function ensures maximal value state computed time scales linearly number factors function There requirement function f corresponds probability distribution factors non negative Example Consider distribution defined binary variables p b c p b p b c p c p tr b tr p tr b fa p b tr c tr p b tr c fa p c tr What likely joint configuration argmax b c p b c Naively evaluate p b c joint states b c select states highest probability An alternative message passing approach define c b max c p b c p c For state b tr p b tr c tr p c tr p b tr c fa p c fa Hence c b tr Similarly b fa p b fa c tr p c tr p b fa c fa p c fa Hence c b fa We consider b max b p b c b For tr state b tr value p tr b tr c b tr state b fa value p tr b fa c b fa Hence b tr Similarly fa state b tr value p fa b tr c b tr state b fa value p fa b fa c b fa giving b fa Now compute optimal state argmax b fa Given optimal state backtrack giving b argmax b p fa b c b fa c argmax c p b fa c p c fa Note backtracking process information required computation messages DRAFT April Other Forms Inference If want find likely state variable centre chain pass messages end followed backtracking This approach example taken Viterbi algorithm HMMs section Alternatively send messages carrying result maximisations simultaneously ends chain read maximal state variable state maximises product incoming messages The sequential procedure passed messages chain backtrack The second parallel procedure messages sent concurrently The approach represented factor graph described Using factor graph One use factor graph compute joint probable state Provided schedule message passing occurred product messages variable equals maximum value joint function respect variables One simply read probable state maximising local potential Procedure Max Product messages Factor Graphs Given distribution defined product subsets variables p X Z f f Xf provided factor graph singly connected carry maximisation variables efficiently Initialisation Messages leaf node factors initialised factor Messages leaf variable nodes set unity Variable Factor message x f x g ne x f g x x f f1 f2 f3 x x f x f x x f2 x x f3 x x Factor Variable message f x x max Xf x f Xf y ne f x y f y x y1 y2 y3 f f x x y f y y2 f y2 y3 f y3 Maximal State x argmax x f ne x f x x f1 f2 f3x f1 x x f2 x x x f3 x This algorithm called belief revision Finding N probable states It interest calculate likely joint state N probable states particularly cases optimal state slightly probable states This interesting problem tackled variety methods A general technique given Nilson based junction tree formalism chapter construction candidate DRAFT April Other Forms Inference lists example For singly connected structures approaches developed For hidden Markov model section simple algorithm N Viterbi approach stores N probable messages stage propagation For general singly connected graphs extend max product algorithm N max product algorithm retaining stage N probable messages N max product The algorithm N max product minor modification standard max product algorithm Compu tationally straightforward way accomplish introduce additional variable message index likely messages We develop message passing undi rected graph Consider distribution e b c d p b c d e e b b c b d wish find probable values Using notation max x f x ith highest value f x maximisation d expressed message d b max d b d d b max d b d Defining messages similarly likely values p b c d e computed max b c d e e b b c b d max e ma max mb e max b mc md b max c b c c b mc max d b d d b md b mb e ma ma mb mc md index highest values At final stage table dim e entries compute highest joint states e ma Given likely joint states e m backtracks find likely state mb arg max mb e b mb One continues backtracking finding likely state b mc md finally c d One restart backtracking second likely state e ma continue find likely states given rise leading second likely joint state The translation factor graph formalism straightforward contained maxNprodFG m Essentially modification required define extended messages contain N likely messages computed stage A variable factor message consists product extended mes sages For factor variable message extended messages neighbours multiplied large table The N probable messages retained defining new extended message The N probable states variable read finding variable state maximises product incoming extended messages Branches bottleneck computation Consider term larger system z b c z b c DRAFT April Other Forms Inference Figure State transition diagram weights shown The shortest unweighted path state state Considered Markov chain random walk probable path state state The path longer probable path probability exiting state state assuming transition equally likely Note example states absorbing enter exit states This correspond adding self loops nodes drawn improve clarity See demoMostProbablePath m We like pass message branch b c rest graph To compute likely value find max z max b b max c c represents efficient approach maximisation carried branch separately However find second likely value write max z max b b max c c For fixed z erroneously force different state state corresponding likely value Similarly assume second likely state corresponds finding second likely state factor Unlike single likely state case distribute maximisations branch search branch contributions concurrently This corresponds exponentially complex search N highest joint branch states Whilst non branching links non problematic degree D variable s node FG undirected representation contributes additional exponential term ND computational complexity Most probable path shortest path What likely path state state b N state Markov chain Note necessarily shortest path explained fig If consider path length T probability p s2 s1 p s3 s2 p sT b sT Finding probable path readily solved max product max sum algorithm log transitions simple serial factor graph To deal issue don t know optimal T approach redefine probability transitions desired state b absorbing state chain enter state leave With redefinition probable joint state correspond probable state product N transitions This approach demonstrated demoMostProbablePath m direct approaches described An alternative cleaner approach follows Markov chain dispense variable factor factor variable messages use variable variable messages If want find likely set states s2 sT b computed defining maximal path probability E b T b T timesteps E b T max s2 sT p s2 s1 p s3 s2 p s4 s3 p sT b sT max s3 sT max s2 p s2 s1 p s3 s2 s3 p s4 s3 p sT b sT To compute efficiently define messages t t st max st t t st p st st t s2 p s2 s1 DRAFT April Other Forms Inference point E b T max sT T T sT p sT b sT T T sT b We proceed find maximal path probability timestep T Since messages time T need compute additional message T T sT E b T max sT T T sT p sT b sT T T sT b We proceed manner reach E b N N number nodes graph We don t need number steps necessarily contain non simple paths A simple path include state The optimal time t given E b E b N maximal Given t begin backtrack1 Since E b t max st t t st p st b st know optimal state s t argmax st t t st p st b st We continue backtrack s t argmax st t t st p s t st See mostprobablepath m In derivation use properties probability p non negative sign changes flip sequence probability local message recursion longer applies One consider algorithm finding optimal product path b It straightforward modify algorithm solve single source single sink shortest weighted path problem One way replace Markov transition probabilities exp u st st u st st edge weight infinite edge st st This approach taken shortestpath m able deal positive negative edge weights This method general known Dijkstra s algorithm requires weights positive If negative edge cycle exists code returns shortest weighted length N path N number nodes graph See demoShortestPath m The algorithm efficient single source single sink scenario messages contain N states meaning overall storage O N2 As stands algorithm numerically impractical messages recursively multiplied values usually case probabilities One quickly run numerical underflow possibly overflow case non probabilities method To fix final point best work defining logarithm E Since monotonic transformation probable path defined logE obtained E In case L b T max s2 sT log p s2 s1 p s3 s2 p s4 s3 p sT b sT max s2 sT log p s2 s1 T t log p st st log p sT b sT 1An alternative finding t define self transitions probability use fixed time T N Once desired state b reached self transition preserves chain state b remaining timesteps This procedure mostprobablepathmult m DRAFT April Inference Multiply Connected Graphs We define new messages t t st max st t t st log p st st One proceeds finding probable t defined L backtracks Remark A possible confusion optimal paths efficiently found graph loopy Note graph fig state transition diagram graphical model The graphical model corresponding simple Markov chain Belief Network t p st st linear serial structure Hence underlying graphical model simple chain explains computation efficient Most probable path multiple source multiple sink If need probable path states b run single source single sink algorithm b A computationally efficient approach observe define message starting state t t st max st t t st p st st continue find maximal path probability matrix getting state state b T timesteps E b T max sT T T sT p sT b sT Since know message T T sT states readily compute probable path starting states states b T steps This requires passing N N matrix message We proceed timestep T Since messages time T need compute additional message T T sT E b T max sT T T sT p sT b sT In way efficiently compute optimal path probabilities starting states end states b t timesteps To find optimal corresponding path backtracking proceeds mostprobablepathmult m demoMostProbablePathMult m One use algorithm solve multiple source multiple sink shortest weighted path problem exponentiated negative edge weights This variant Floyd Warshall Roy algorithm Mixed inference An encountered situation infer likely state joint marginal possibly given evidence For example given distribution p x1 xn find argmax x1 x2 xm p x1 x2 xm argmax x1 x2 xm xm xn p x1 xn In general tree structured p x1 xn optimal marginal state computed efficiently One way summation resulting joint marginal structured factored form products simpler functions marginal variables Finding probable joint marginal requires search joint marginal states task exponential m An approximate solution provided EM algorithm section exercise Inference Multiply Connected Graphs We briefly discuss relatively straightforward approaches dealing multiply connected graphs conceptually straightforward build repeated use singly connected structures We discuss general algorithm chapter DRAFT April Inference Multiply Connected Graphs Algorithm Compute marginal p x1 evidence distribution p x f f x f Assumes non evidential variables ordered x1 xn procedure Bucket Elimination p x f f x f Initialize bucket potentials unity Fill buckets There potentials left distribution For potential f find highest variable xj according ordering Multiply f potential bucket j remove f distribution end bucket n Empty buckets For bucket sum states variable xi potential Identify highest variable xh potential Multiply existing potential bucket h end The marginal p x1 evidence proportional return p x1 evidence The conditional marginal end procedure Bucket elimination We consider general conditional marginal variable elimination method works distribu tion including multiply connected graphs Bucket elimination presented algorithm considered way organise distributed summation The algorithm best explained simple example given Example Bucket Elimination Consider problem calculating marginal p f p b c d e f g p f d p g d e p c p d b p p b p e fig 1a Whilst singly connected serves explain general procedure p f b c d e g p b c d e f g b c d e g p f d p g d e p c p d b p p b p e We distribute summation terms follows e b c end nodes sum values p f d g p f d p b p d b p b c p c e p g d e p e For convenience let s write terms brackets b p d b p b b d e p g d e p e e d g The term c p c equal unity eliminate node directly Rearranging terms write p f d g p f d p b d e d g If think graphically effect summing b c e effectively remove eliminate variables We carry summing g end points new graph p f d p f d p b d g e d g Again defines new potentials d g d final answer found p f d p f d d g d DRAFT April Inference Multiply Connected Graphs e p e p g d e c p c b p b p d b p b p d b g E d g E d g p p p B d p B d d p f d p f d p f d p f d G d p f d G d A d f D f Figure The bucket elimination algorithm applied graph fig At stage node eliminated graph The second stage eliminating c trivial c p c skipped bucket send message We illustrate fig Initially define ordering variables beginning wish find marginal suitable ordering f d g b c e Then starting highest bucket e according ordering f d g b c e potentials mention e e bucket Continuing highest bucket c remaining potentials mention c c bucket etc The result initialisation procedure terms conditional distributions DAG distributed buckets shown left column fig Eliminating highest bucket e pass message node g Immediately eliminate bucket c sums unity In column fewer buckets eliminate highest remaining bucket time b passing message bucket There important observations bucket elimination To compute p x2 evidence need order variables required marginal variable labelled x1 repeat bucket elimination Hence query calculation marginal case requires running algorithm It efficient reuse messages recalculating time In general bucket elimination constructs multi variable messages bucket bucket The storage requirements multi variable message exponential number variables message For trees choose variable ordering render computational complexity linear number variables Such ordering called perfect definition shown perfect ordering easily found singly connected graphs However orderings exist bucket elimination extremely inefficient Loop cut conditioning For multiply connected distributions run difficulty message passing routines sum product algorithm designed work singly connected graphs One way solve difficulties multiply connected loopy graphs identify nodes removed reveal singly connected subgraph Consider example fig Imagine wish calculate DRAFT April Message Passing Continuous Distributions d c b e f g d c b e f g b Figure A multiply con nected graph reduced singly connected graph b conditioning variable c marginal p d Then p d c b e f g p c p p p d b p b p f c d p f d p g d e p potentials necessarily distributions For state c product new po tentials variables b e f g singly connected standard singly connected message passing perform inference We need perform inference state variable c state defining new singly connected graph structure modified potentials More generally define set variables C called loop cut set run singly connected inference joint state cut set variables C This finding likely state multiply connected joint distribution Hence computational price exponential loop cut size calculate marginals likely state multiply connected distribution However determining small cut set general difficult guarantee small given graph Whilst method able handle loops general manner particularly elegant concept messages applies conditioned cut set variables use messages inference additional quantities interest unclear We discuss alternative method handling multiply connected distributions chapter Message Passing Continuous Distributions For parametric continuous distributions p x x message passing corresponds passing parameters distributions For sum product algorithm requires operations multiplication integration variables closed respect family distributions This case example Gaussian distribution marginal integral Gaussian Gaussian product Gaussians Gaussian section This means implement sum product algorithm based passing mean covariance parameters To implement requires tedious algebra compute appropriate message parameter updates At stage complex ities performing calculations potential distraction interested reader refer demoSumprodGaussMoment m demoSumprodGaussCanon m demoSumprodGaussCanonLDS m chapter examples message passing Gaussians For general exponential family distri butions message passing essentially straightforward specifics updates tedious work In cases operations marginalisation products closed family distributions need projected chosen message family Expectation propagation section relevant case Summary For tree structured factor graph non mixed inference essentially linear number nodes graph provided variables discrete inference operations form tractable closed family Computation trees achieved local message passing algorithms analogous dynamic programming DRAFT April Code The sum product max product algorithm particularly useful computing marginal likely inferences respectively Message passing holds continuous variables based passing messages update parameters distribution Shortest path problems solved message passing approaches Inference non trees multiply connected distributions complex fill effect variables eliminated adds additional links graph Inference multiply connected graphs achieved techniques cut set conditioning conditioning subset variables reveal singly connected structure However generally inefficient messages readily A home message chapter non mixed inference singly connected structures usu ally computationally tractable Notable exceptions message passing operations closed message family representing messages explicitly requires exponential space This happens example distribution contain discrete continuous variables Switching Linear Dynamical system discuss chapter Broadly speaking inference multiply connected structures complex intractable However want impression case Notable exceptions finding likely state attractive pairwise MN section finding likely state marginals binary planar MN pure interactions example For N variables graph naive use general purpose routines junction tree algorithm inferences result O 2N computation clever algorithms able return exact results O N3 operations Of interest bond propagation intuitive node elimination method perform marginal inference pure interaction Ising models Code The code implements message passing tree structured factor graph The FG stored adjacency matrix message FG node FG node j given Ai j FactorGraph m Return factor graph adjacency matrix message numbers sumprodFG m Sum Product algorithm factor graph In general recommended work log space Max Product case particularly large graphs product messages small The code provided work log space work large graphs writing log messages straightforward leads readable code An implementation based log messages left exercise interested reader maxprodFG m Max Product algorithm factor graph maxNprodFG m N Max Product algorithm factor graph Factor graph examples For distribution fig following code finds marginals likely joint states The number states variable chosen random demoSumprod m Test Sum Product algorithm demoMaxprod m Test Max Product algorithm demoMaxNprod m Test Max N Product algorithm Most probable shortest path mostprobablepath m Most probable path demoMostProbablePath m Most probable versus shortest path demo DRAFT April Exercises demoShortestPath m The shortest path demo works positive negative edge weights If negative weight cycles exist code finds best length N shortest path mostprobablepathmult m Most probable path multi source multi sink demoMostProbablePathMult m Demo probable path multi source multi sink Bucket elimination The efficiency Bucket Elimination depends critically elimination sequence chosen In demon stration find marginal variable Chest Clinic exercise randomly chosen elimination order The desired marginal variable specified eliminated For comparison use elimination sequence based decimating triangulated graph model discussed section constraint variable decimated marginal variable interest For smarter choice elimination sequence complexity computing single marginal roughly Junction Tree algorithm triangulation bucketelim m Bucket Elimination demoBucketElim m Demo Bucket Elimination Message passing Gaussians The following code hints message passing implemented continuous distributions The reader referred BRMLtoolbox details section algebraic manipu lations required perform marginalisation products Gaussians The principle holds family distributions closed products marginalisation reader wish implement specific families following method outlined Gaussians demoSumprodGaussMoment m Sum product message passing based Gaussian Moment parameterisation Exercises Exercise Given pairwise singly connected Markov network form p x Z j xi xj explain efficiently compute normalisation factor called partition function Z function potentials Exercise Consider pairwise Markov network defined binary variables p x x1 x100 xi xi Is possible compute argmax x1 x100 p x efficiently Exercise You employed web start company designs virtual environments players rooms The rooms accessible time step given matrix M stored virtualworlds mat Mij means door rooms j Mij Mji Mij means door rooms j Mii meaning time step stay room You visualise matrix typing imagesc M Write list rooms reached room time steps The manager complains takes time steps room room Is true Find likely path sequence rooms room room If single player jump randomly room stay room preference rooms probability time t player room Assume effectively infinite time passed player began room t DRAFT April Exercises If players jumping randomly rooms staying room explain compute probability infinite time room Assume players begin room Exercise Consider hidden Markov model p v1 vT h1 hT p h1 p v1 h1 T t p vt ht p ht ht dom ht H dom vt V t T Draw belief network representation distribution Draw factor graph representation distribution Use factor graph derive Sum Product algorithm compute marginals p ht v1 vT Explain sequence order messages passed factor graph Explain compute p ht ht v1 vT Show belief network p h1 hT simple linear chain whilst p v1 vT fully connected cascade belief network Exercise For singly connected Markov Network p x p x1 xn computation marginal p xi carried efficiently Similarly likely joint state x arg maxx1 xn p x computed efficiently Explain likely joint state marginal computed efficiently e circumstances efficiently O m time compute argmax x1 x2 xm p x1 xm m n Exercise Consider internet webpages labelled N If webpage j link webpage place element matrix Lij Lij By considering random jump webpage j webpage given transition probability Mij Lij Lij probability infinite random surfing ends webpage How relate potential relevance webpage terms search engine Exercise A special time homogeneous hidden Markov model given p x1 xT y1 yT h1 hT p x1 h1 p y1 h1 p h1 T t p ht ht p xt ht p yt ht The variable xt states dom xt A C G T numerically labelled states The variable yt states dom yt A C G T The hidden latent variable ht states dom ht The HMM models following fictitious process In humans Z factor proteins sequence states variables x1 x2 xT In bananas Z factor proteins present represented different sequence y1 y2 yT Given sequence x1 xT human task find corresponding sequence y1 yT banana finding likely joint latent sequence likely banana sequence given optimal latent sequence That require argmax y1 yT p y1 yT h h T h h T argmax h1 hT p h1 hT x1 xT The file banana mat contains emission distributions pxgh p x h pygh p y h transition phtghtm p ht ht The initial hidden distribution given ph1 p h1 The observed x sequence given x DRAFT April Exercises Explain mathematically detail compute optimal y sequence stage procedure stated Write MATLAB routine computes displays optimal y sequence given observed x sequence Your routine use Factor Graph formalism Explain computationally tractable compute argmax y1 yT p y1 yT x1 xT Bonus question By considering y1 yT parameters explain EM algorithm section find argmax y1 yT p y1 yT x1 xT Implement approach suitable initiali sation optimal parameters y1 yT Exercise There set firstnames david anton fred jim barry numbered david barry set surnames barber ilsung fox chain fitzwilliam quinceadams grafvonunterhosen numbered barber grafvonunterhosen A string generated randomly sampling character z Then continue random character sampling probability start generate firstname A firstname chosen uniformly random After generating firstname generate character random We continue generate letter random probability start generate surname chosen uniformly set surnames We continue letter surname generated The process goes start end timepoint T For example generate dtyjimdfilsungffdavidmjfox The catch process character generation noisy The character want generate generated correctly probability probability character uniformly random generated Given character sequence file noisystring mat decode noisystring find likely intended clean sequence Once sequence able find set firstname surname pairs traverse clean sequence Construct matrix m j counts number occurrences pair firstname surname j clean sequence display matrix Exercise A security company employed watch behaviour people moving train station Using video cameras able track x y position people The matrix drunkproblemX mat contains position people time matrix representing person occupying position This matrix generated program drunkmover m wish examine If person moves grid moved random position grid described drunkmover m All people single neighbouring point x y grid time step However person fast dangerous drunk want track The drunk x y x y time step Devise method track think dangerous drunk list x y coordinates single likely path drunk took station Exercise The file BearBulldata contains price asset T timepoints The price takes values If market bear state price changes time t t probability transition matrix pbear t t If market bull state price changes time t t transition matrix pbull t t If market bear state remain probability If market bull state remain probability You assume timestep market uniformly bear bull state price distribution timestep uniform Use model compute probability price time T given observed prices time T p price T price T Using probability compute expected gain price T price T price asset standard deviation price gain price T price T Exercise You find adrift outer space space station disaster Your rocket suit simple controls allow accelerate time t ai t DRAFT April Exercises dimensions space independently At time t command spacesuit provide acceleration vector a1 t a2 t a3 t ai t According discrete time Newton s laws changes velocity vt position xt according Markovian updates vt vt xt xt vt given value corresponding unit real time Each time acceleration applied unit fuel Your task origin x1 T time rendezvous rescue station x102 T time minimum fuel doesn t matter speed long rendezvous rescue station At time t stationary v1 T The total fuel t ai t For spatial dimensions write single equation relates ai t t rescue station x102 What minimum fuel rendezvous rescue station time Assuming sequence a1 a100 obtain rendezvous rescue station time explain believe efficient way calculate minimum fuel If efficient algorithm state Otherwise explain efficient algorithm available Exercise An algorithm exists example Dijkstra s algorithm finds minimum weighted path specified nodes graph algorithm generally works weights non negative You minimum weighted path problem weights uij negative A friend suggests use Dijkstra s algorithm making new set edge weights non negative wij uij u u minimum value uij weights Explain generally correct minimum weight path Exercise It s year tax collector Simo Hurtta needs trip starting planet ending planet Interplanetary travel possible certain planets cost going planet planet j equal Euclidian distance planets xi xj minus tax collect planet j Given set planet positions x interplanetary travel possibilities matrix Aij possible planet planet j collectable taxes planet t SimoHurtta mat minimum cost travelling planet planet Assume Hurrta collect taxes planet DRAFT April Exercises DRAFT April CHAPTER The Junction Tree Algorithm When distribution multiply connected useful generic inference approach efficient reuse messages In chapter discuss important structure junction tree clustering variables enables perform message passing efficiently structure message passing occurs consist intractably large clusters The important thing junction tree based different message passing procedures considered The junction tree helps forge links computational complexity inference fields computer science statistics physics Clustering Variables In chapter discussed efficient inference singly connected graphs variable elimination message passing schemes appropriate In multiply connected case general perform inference passing messages existing links graph The idea junction tree algorithm JTA form new representation graph variables clustered resulting singly connected graph cluster variables albeit different graph The main focus development marginal inference similar techniques apply different inferences finding probable state distribution At stage important point JTA magic method deal intractabilities resulting multiply connected graphs simply way perform correct inference multiply connected graph transforming singly connected structure Carrying inference resulting junction tree computationally intractable For example junction tree representation general dimensional Ising model single supernode containing variables Inference case exponentially complex number variables Nevertheless cases implementing JTA intractable JTA provides useful insight representation distributions form basis approximate inference In sense JTA key understanding issues related representations complexity inference central development efficient inference algorithms Reparameterisation Consider chain p b c d p b p b c p c d p d From definition conditional probability reexpress p b c d p b p b p b c p c p c d p d p d p b p b c p c d p b p c Clique Graphs d b c b c b c b c d b Figure Markov network b c b c d b Clique graph representation A useful insight distribution written product marginal distributions di vided product intersection marginal distributions Looking numerator p b p b c p c d distribution b c d overcounting b c overcounting b arises overlap sets b b c b intersection Similarly overcounting c arises overlap sets b c c d Intuitively need correct overcounting dividing distribution intersections Given transformed representation product marginals divided product intersection right equation marginal p b read directly factors new expression The aim junction tree algorithm form representation distribution contains marginals explicitly We want way works belief Markov networks deals multiply connected case In order appropriate way parameterise distribution terms clique graph described section Clique Graphs Definition Clique Graph A clique graph consists set potentials X n X n defined set variables X For neighbouring cliques graph defined sets variables X X j intersection X s X X j called separator corresponding potential s X s A clique graph represents function c c X c s s X s For notational simplicity usually drop clique potential index c Graphically clique potentials represented circles ovals separator potentials rectangles X X X X The graph left represents X X X X Clique graphs translate Markov networks structures convenient carrying inference Consider Markov network fig 1a p b c d b c b c d Z An equivalent representation given clique graph fig 1b defined product numerator clique potentials divided product separator potentials In case separator potential set normalisation constant Z By summing Zp b c b c d b c d Zp b c d b c d b c Multiplying expressions Z2p b c p b c d b c d b c d b c d b c Z2p b c d d p b c d DRAFT April Clique Graphs In words p b c d p b c p b c d p c b The important observation distribution written terms marginals variables original cliques clique graph structure All changed original clique potentials replaced marginals distribution separator marginal defined separator variables b c p b c b c d p b c d Z p c b The usefulness representation interested marginal p b c read transformed clique potential To use representation require systematic way transforming clique graph potentials end transformation new potentials contain marginals distribution Remark Note whilst visually similar Factor Graph Clique Graph different rep resentations In clique graph nodes contain sets variables share variables nodes Absorption Consider neighbouring cliques V W sharing variables S common In case distribution variables X V W p X V W S V S W aim find new representation p X V W S V S W potentials given V p V W p W S p S In example explicitly work new potentials function old potentials computing marginals follows p W V S p X V S V W S W V S V S p V W S p X W S V W S V W S W S There symmetry present equations interchanging V W One way describe equations absorption We cluster W absorbs information cluster V following updating procedure First define new separator S V S V refine W potential W W S S DRAFT April Clique Graphs A B C D E F Figure An example absorption schedule clique tree Many valid schedules exist con straint messages passed neigh bour messages received The advantage interpretation new representation valid clique graph representation distribution V W S V W S S S V W S p X For simple clique graph W absorbs information V W p W verified comparing right equation equation With new updated potentials graph W V After V absorbs information W V contains marginal p V After separator S participated absorption directions separator potential contain p S case single absorption To consider absorbing W V updated potentials W S S W S W W S W S S W V S W V S p S Continuing new potential V given V V S S V W S W S S S W S V W S p V Hence terms equation new representation V V S S W W Definition Absorption V S W Let V W neighbours clique graph let S separator let V W S potentials Absorption V toW S replaces tables S W S V S V W W S S We clique W absorbs information clique V The potentials written clique graph left highlight potential updating Absorption schedule clique trees Having defined local message propagation approach need define update ordering absorption In general node V send exactly message neighbour W sent V received message neighbours We continue sequence absorptions message passed directions link See example fig Note valid message passing schemes case DRAFT April Junction Trees x1 x2x3 x4 x1 x4 x2 x4x3 x4 x4x4 x4 b x1 x4 x2 x4x3 x4 x4x4 c Figure Singly connected Markov network b Clique graph c Clique tree Definition Absorption Schedule A clique send message neighbour provided received messages neighbours Junction Trees There stages need order transform distribution appropriate structure inference Initially explain singly connected structures moving multiply connected case Consider singly connected Markov network fig 3a p x1 x2 x3 x4 x1 x4 x2 x4 x3 x4 The clique graph singly connected Markov network multiply connected fig 3b sep arator potentials set unity Nevertheless let s try reexpress Markov network terms marginals First relations p x1 x4 x2 x3 p x1 x2 x3 x4 x1 x4 x2 x2 x4 x3 x3 x4 p x2 x4 x1 x3 p x1 x2 x3 x4 x2 x4 x1 x1 x4 x3 x3 x4 p x3 x4 x1 x2 p x1 x2 x3 x4 x3 x4 x1 x1 x4 x2 x2 x4 Taking product marginals p x1 x4 p x2 x4 p x3 x4 x1 x4 x2 x4 x3 x4 x1 x1 x4 x2 x2 x4 x3 x3 x4 p x4 This means Markov network expressed terms marginals p x1 x2 x3 x4 p x1 x4 p x2 x4 p x3 x4 p x4 p x4 Hence valid clique graph given representation fig 3c Indeed variable x4 occurs separator clique graph loop remove variable arbitrarily chosen separator loop If leaves separator simply remove This shows cases transform clique graph clique tree e singly connected clique graph Provided original Markov network singly connected form clique tree manner DRAFT April Junction Trees The running intersection property Sticking example consider clique tree fig x3 x4 x1 x4 x2 x4 x4 x4 representation distribution set x4 x4 match Now perform absorption clique tree We absorb x3 x4 x1 x4 The new separator x4 x3 x3 x4 new potential x1 x4 x1 x4 x4 x4 x1 x4 x4 Now x1 x4 x2 x4 The new separator x4 x1 x1 x4 new potential x2 x4 x2 x4 x4 x4 x2 x4 x4 Since ve hit buffers terms message passing potential x2 x4 updated Let s examine carefully value new potential x2 x4 x2 x4 x4 x2 x4 x1 x1 x4 x2 x4 x1 x1 x4 x3 x3 x4 x1 x3 p x1 x2 x3 x4 p x2 x4 Hence new potential x2 x4 contains marginal p x2 x4 To complete round message passing need passed messages valid schedule directions separator To continue follows We absorb x2 x4 x1 x4 The new separator x4 x2 x2 x4 x1 x4 x1 x4 x4 x4 Note x4 x2 x2 x4 x2 p x2 x4 p x4 absorbing directions separator contains marginal p x4 The reader x1 x4 p x1 x4 Finally absorb x1 x4 x3 x4 The new separator x4 x1 x1 x4 p x4 DRAFT April Junction Trees x3 x4 x3 x4 x4 x4 p x3 x4 Hence round message passing new potentials contain correct marginals The new representation consistent sense necessarily neighbouring cliques V W intersection I corresponding potentials V W V I V W I W Note bidirectional absorption following valid schedule guarantees local consistency neighbouring cliques example provided started clique tree correct representation distribution To ensure global consistency variable occurs cliques present cliques path connecting cliques An extreme example removed link cliques x3 x4 x1 x4 In case clique tree forest global consistency guaranteed information required clique x3 x4 consistent rest graph reach clique Formally requirement propagation local global consistency clique tree junction tree defined See exercise Definition Junction Tree A clique tree junction tree pair nodes V andW nodes path V W contain intersection V W This called running intersection property From definition local consistency passed neighbours distribution globally consistent Proofs results contained Example A consistent junction tree To gain intuition meaning consistency consider junction tree fig 4d After round message passing tree link consistent product potentials divided product separator potentials original distribution Imagine interested calculating marginal node abc That requires summing variables defgh If consider summing h link consistent h e h e ratio h e h e unity effect summing node h link eh dce removed separator The happens link node eg dce cf abc The nodes remaining dce abc separator c far unaffected summations We need sum d e Again link consistent de d c e c ratio de d c e c The result summation variables abc produces unity cliques separators summed potential representation reduces simply potential b c marginal p b c It clear similar effect happen nodes We obtain marginals individual variables simple brute force summation variables potential example p f c c f DRAFT April Constructing Junction Tree Singly Connected Distributions c b d e f g h c b d e f g h b dce abc cf eg eh c c c e e e c dce abc cf eg eh c c e e d Figure Belief Network b Moralised version c Clique graph b d A junction tree This satisfies running intersection property nodes contain variable common clique path linking nodes contains variable Constructing Junction Tree Singly Connected Distributions Moralisation For belief networks initial step required required case undirected graphs Definition Moralisation For variable x add undirected link parents x replace directed link x parents undirected links This creates moralised Markov network Forming clique graph The clique graph formed identifying cliques Markov network adding link cliques non intersection Add separator intersecting cliques Forming junction tree clique graph For singly connected distribution maximal weight spanning tree clique graph junction tree Definition Junction Tree A junction tree obtained finding maximal weight spanning tree clique graph The weight tree defined sum separator weights tree separator weight number variables separator If clique graph contains loops separators loop contain variable By continuing remove loop links tree revealed obtain junction tree DRAFT April Junction Trees Multiply Connected Distributions b cd b c b b cd c b cd d abc acdac e Figure An undirected graph loop b Eliminating node d adds link c subgraph c The induced representation graph d Equivalent induced representation e Junction tree Example Forming Junction Tree Consider Belief Network fig 4a The moralisation procedure gives fig 4b Identifying cliques graph linking gives clique graph fig 4c There possible junction trees obtain clique graph given fig 4d Assigning potentials cliques Definition Clique Potential Assignment Given junction tree function defined product set potentials X X n valid clique potential assignment places potentials JT cliques variables contain product JT clique potentials divided JT separator potentials equal function A simple way achieve assignment list potentials order JT cliques arbitrarily Then potential search JT cliques encountered potential variables subset JT clique variables Subsequently potential JT clique taken product clique potentials assigned JT clique Lastly assign JT separators unity This approach taken jtassignpot m Note instances junction tree clique assigned unity Example For belief network fig 4a wish assign potentials junction tree fig 4d In case assignment unique given abc p p b p c b dce p d p e d c cf p f c eg p g e eh p h e All separator potentials initialised unity Junction Trees Multiply Connected Distributions When distribution contains loops construction outlined section result junction tree The reason loops variable elimination changes structure remaining graph DRAFT April Junction Trees Multiply Connected Distributions To consider following distribution p b c d b b c c d d shown fig 5a Let s try clique graph We choice variable marginalise Let s choose d p b c b b c d c d d The remaining subgraph extra connection c fig 5b We express joint terms marginals p b c d p b c d c d d c d d To continue transformation marginal form let s try replace numerator terms probabil ities We considering p c d c d d b b b c Plugging equation p b c d p b c p c d d c d d b b b c We recognise denominator simply p c p b c d p b c p c d p c This means valid clique graph distribution fig 5a contain cliques larger original distribution To form JT based products cliques divided products separators start induced representation fig 5c Alternatively marginalised variables c ended equivalent representation fig 5d Generally result variable elimination representation terms induced graph link added variables loop length chord This called triangulation A Markov network triangulated graph written terms product marginals divided product separators Armed new induced representation form junction tree Example A slightly complex loopy distribution depicted fig 6a p b c d e f b b c c d d e e f f b e There different induced representations depending variables decide eliminate The reader convince induced representation given fig 6b b c def b c def b Figure Loopy ladder Markov network b Induced rep resentation DRAFT April Junction Trees Multiply Connected Distributions Definition Triangulated Decomposable Graph An undirected graph triangulated loop length chord An equivalent term graph decomposable chordal From definition undirected graph triangulated clique graph junction tree Triangulation algorithms When variable eliminated graph links added neighbours eliminated variable A triangulation algorithm produces graph exists variable elimination order introduces extra links graph For discrete variables complexity inference scales exponentially clique sizes triangulated graph absorption requires computing tables cliques It interest find triangulated graph small clique sizes However finding triangulated graph smallest maximal clique computationally hard problem general graph heuristics unavoidable Below describe simple algorithms generically reasonable cases alternative algorithm considerably efficient Remark Triangles Note triangulated graph squares original graph triangles triangulated graph Whilst case fig 6b true fig 10d The term triangulation refers fact square e loop length triangle edges added criterion satisfied See fig Greedy variable elimination An intuitive way think triangulation start simplicial nodes eliminated introduce extra links remaining graph Next consider non simplicial node remaining graph minimal number neighbours Then add link neighbours node eliminate node graph Continue nodes eliminated This procedure corresponds Rose Tarjan Elimination particular node elimination choice By labelling nodes eliminated sequence obtain perfect ordering In case discrete variables different numbers states refined version choose non simplicial node eliminated leaves smallest clique table size product size state dimensions neighbours node See fig example Procedure Variable Elimination In Variable Elimination simply picks non deleted node x graph adds links neighbours x Node x deleted One repeats nodes deleted Definition Perfect Elimination Order Let n variables Markov network ordered n The ordering perfect node neighbours later ordering form maximal clique This means eliminate variables sequence n additional links induced remaining marginal graph A graph admits perfect elimination order decomposable vice versa b c d e Figure This graph triangulated despite triangular appearance The loop b c d chord DRAFT April The Junction Tree Algorithm b c d e f g h j k l b c d e f g h j k l b b c d e f g h j k l c b c d e f g h j k l d b c d e f g h j k l e b c d e f g h j k l b c d e f g h j k l f Figure Markov network seek triangulation greedy variable elimination We eliminate simplicial nodes e l b We eliminate variables b d add single extra link induced graph c There simplicial nodes stage choose eliminate f elimination adding single link d We eliminate g h simplicial e The remaining variables c j k eliminated order f Final triangulation The variable elimination partial order e l b d f g h c j k brackets indicate order variables inside bracket eliminated irrelevant Compared triangulation produced max cardinality checking approach fig 10d triangulation parsimonious Whilst variable elimination guarantees triangulated graph efficiency depends heavily se quence nodes chosen eliminated Several heuristics proposed including corresponds choosing x node minimal number neighbours Maximum cardinality checking Algorithm terminates success graph triangulated Not sufficient condition graph triangulated necessary It processes node time process node quadratic number adjacent nodes This triangulation checking algorithm suggests triangulation construction algorithm simply add link neighbours caused algorithm FAIL restart algorithm The algorithm restarted beginning continued current node This important new link change connectivity previously labelled nodes See fig example1 The Junction Tree Algorithm We steps required inference multiply connected graphs given procedure Procedure Junction tree algorithm 1This example David Page www cs wisc edu dpage cs731 DRAFT April The Junction Tree Algorithm abf bcfg cdhi deibf di cfgj chik cjk jkl cfg chi cj ck jk Figure Junction tree formed triangulation fig 8f One verify satisfies running intersection prop erty Algorithm A check graph decomposable triangulated The graph triangulated cycling n nodes graph FAIL criterion encountered Choose node graph label n Choose node labeled neighbours label If labeled neighbours adjacent FAIL end Where node labeled neighbours tie broken arbitrarily Moralisation Marry parents This required directed distributions Note parents variable married common error marry neighbouring parents Triangulation Ensure loop length chord Junction Tree Form junction tree cliques triangulated graph removing unnecessary links loop cluster graph Algorithmically achieved finding tree maximal spanning weight weight wij given number variables separator cliques j Alternatively given clique elimination order lowest cliques eliminated connect clique single neighbouring clique j greatest edge weight wij Potential Assignment Assign potentials junction tree cliques set separator potentials unity Message Propagation Carry absorption updates passed directions link JT The clique marginals read JT An example given fig Remarks JTA The algorithm provides upper bound computation required calculate marginals graph There exist efficient algorithms particular cases generally believed efficient approaches JTA approach perform triangulation One particular special case marginal inference binary variable Markov network dimensional lattice containing pure quadratic interactions In case complexity computing marginal inference O n3 n number variables distribution This contrast pessimistic exponential complexity suggested JTA One think class distributions essentially linear time algorithm available singly connected distributions However decomposable graphs cliques limited size meaning inference tractable For example extended version ladder fig 6a simple induced decomposable representation fig 6b marginal inference linear number rungs ladder Effectively structures hyper trees complexity related tree width graph DRAFT April The Junction Tree Algorithm b c d Figure Starting Markov network maximum cardinality check algorithm proceeds b additional link required c One continues fully triangulated graph d found c b d e f g h c b d e f g h b Figure Original loopy Belief Network b The moralisation links dashed nodes e f nodes f g The additional links come triangulation The clique size resulting clique tree shown Ideally like find triangulated graph minimal clique size However shown computationally hard problem find efficient triangulation In practice general purpose triangulation algorithms chosen provide reasonable clearly optimal generic performance Numerical flow issues occur repeated multiplication potentials If care marginals avoid numerical difficulties normalising potentials step missing normalisation constants found normalisation constraint If required store values local renormalisations example global malisation constant distribution required section After clamping variables evidential states running JTA returns joint distribution non evidential variables Xc clique evidential variables clamped evidential states p Xc evidence From conditionals straightforward calculate Representing marginal distribution set variables X contained single clique general computationally difficult Whilst probability state p X computed efficiently general exponential number states A classical example regard HMM section singly connected joint distribution p V H However marginal distribution p V fully connected This means example whilst entropy p V H straightforward compute entropy marginal p V intractable Computing normalisation constant distribution For Markov network p X Z Xi find Z efficiently If JTA unnormalised distribution Xi equivalent representation p X Z c Xc s Xs DRAFT April The Junction Tree Algorithm s c separator clique indices Since distribution normalise obtain Z Z X c Xc s Xs For consistent JT summing variables simplicial JT clique including separator variables marginal clique cancel corresponding separator unity term clique separator removed This forms new JT eliminate simplicial clique Continuing manner left single numerator potential Z Xc Xc This true clique c makes sense choose small number states resulting raw summation efficient Hence order compute normalisation constant distribution runs JT algorithm unnormalised distribution global normalisation given local normalisation clique Note graph disconnected isolated cliques normalisation product connected component normalisation constants The marginal likelihood Our interest computation p V V X subset variable set X Naively carry computation summing non evidential variables hidden variables H X V explicitly In cases computationally impractical alternative use p H V p V H p V One view product clique potentials divided normalisation p V general method section directly applied See demoJTree m Some small JTA examples Example A simple example JTA Consider running JTA simple graph p b c p b p b c p c b c The moralisation triangulation steps trivial JTA given immediately figure right A valid assignment b p b b b c p b c p c ab bcb To find marginal p b run JTA Absorbing ab b new separator b b p b The new potential b c given b c b c b b p b c p c Absorbing bc b new separator b c b c c p b c p c DRAFT April The Junction Tree Algorithm The new potential b given b b b b p b c p b c p c This equal marginal c p b c p b The new separator b contains marginal p b b c p b c p c c p b c p b Example Finding conditional marginal Continuing distribution example consider compute p b c First clamp evidential variables states Then claim effect running JTA produce set clique variables X marginals cliques p X V We demonstrate In general new separator given b b p b However clamped state summation carried instead b p b The new potential b c clique given b c b c b b p b c p c p b The new separator normally given b c b c However c clamped state instead b p b c p c p b The new potential b given b b b b p b p b c p c p b p b p b p b c p c joint distribution p b c The effect clamping set variables V evidential states running JTA clique contains set non evidential variables Hi consistent potential JTA contains marginal p Hi V Finding conditional marginal straightforward ensuring normalisation Example finding likelihood p c One use JTA compute marginal likelihood variables clique effect clamping variables evidential states running JTA produces joint marginals b p b c Then calculating likelihood easy sum non evidential variables converged potential p c b b b p b c DRAFT April Finding Most Likely State Shafer Shenoy propagation Consider Markov network fig 12a junction tree given fig 12b We use obvious notation shortcut writing variable indices In absorption procedure essentially store result message passing potentials separators An alternative message passing scheme junction tree derived follows Consider computing marginal variables involves summing variables p In general clique potential Vi neighbouring clique j potential Vj provided received messages neighbours send message j Vi Vj Vi k j k Once round message passing completed marginal clique given product incoming messages This message passing scheme called Shafer Shenoy propagation property division potentials required unlike absorption On hand compute message need product incoming messages absorption required effect message passing stored clique potentials The separators required Shafer Shenoy approach use indicate variables messages depend Both absorption Shafer Shenoy propagation valid message passing schemes junction tree relative efficacy approaches depends topology junction tree Finding Most Likely State It interest compute likely joint state distribution argmax x1 xn p x1 xn b c Figure Markov network b Junction tree Under absorption absorbed result absorptions stored new potential After absorb operating potential sending information c In Shafer Shenoy updating send message clique neighbouring clique based product incoming messages DRAFT April Reabsorption Converting Junction Tree Directed Network Since development JTA based variable elimination procedure max operator distributes distribution eliminating variable maximising variable effect graph structure summation This means junction tree appropriate structure perform max operations Once JT constructed uses Max Absorption procedure perform maximisation variables After round absorption carried cliques contain distribution variables clique remaining variables set optimal states The optimal local states found explicit optimisation clique potential separately Note procedure holds non distributions sense example general dynamic programming procedure applied case underlying graph multiply connected This demonstrates efficiently compute optimum multiply connected function defined product potentials Definition Max Absorption V S W Let V W neighbours clique graph let S separator let V W S potentials Absorption replaces tables S W S max V S V W W S S Once messages passed directions separators according valid schedule likely joint state read maximising state clique potentials This implemented absorb m absorption m flag switch sum max absorption Reabsorption Converting Junction Tree Directed Network It useful able convert consistent JT round message passing occurred BN desired form For example wishes draw samples Markov network achieved ancestral sampling equivalent directed structure section Definition Reabsorption V S W V W S Let V W neighbouring cliques directed consistent JT clique tree parent Furthermore let S separator V W S potentials Reabsorption W removes separator forms set conditional distribution p W S V W S We clique W reabsorbs separator S Revisiting example fig JT given fig 13a To find valid directed represen tation orient JT edges consistently away chosen root node singleparenttree m forming directed JT property clique parent clique Consider fig 13a represents p b c d e f g h p e g p d c e p b c p c f p e h p e p c p c p e DRAFT April The Need For Approximations dce abc cf eg eh c c e e dce abc cf eg eh c c e e b b c e d f g h c Figure Junction tree b Directed junction tree edges consistently oriented away clique abc c A set chain formed junction tree reabsorbing separator child clique We choices clique absorbs separator One choice p b c d e f g h p g e p d e c p b c p f c p h e This represented called set chain fig 13c set chains generalise Belief Networks product clusters variables conditioned parents By writing set conditional proba bilities local conditional BNs form BN For example given decomposition p c b p b p p g e p f c p h e p d e c p e c The Need For Approximations The JTA provides upper bound complexity marginal max inference attempts exploit structure graph reduce computations However great deal interesting applications use JTA algorithm result clique sizes triangulated graph prohibitively large A classical situation arise disease symptom networks For example graph fig triangulated graph diseases fully connected meaning simplification occur general This situation common bipartite networks children small number parents Intuitively eliminates parent links added parents diated common children Unless graph highly regular analogous form hidden Markov model fill effect rapidly results large cliques intractable computations Dealing large cliques triangulated graph active research topic ll discuss strategies approximate inference chapter Bounded width junction trees In applications liberty choose structure Markov network For example wish fit Markov network data wish use complex Markov network computationally afford In cases desire clique sizes resulting triangulated Markov network smaller specified tree width considering corresponding junction tree hypertree This results thin junction tree A simple way start graph include randomly chosen edge provided size cliques resulting triangulated graph specified maximal width See demoThinJT m makeThinJT m assumes initial graph G graph candidate edges C iteratively expanding G maximal tree width limit reached See discussion learning appropriate Markov structure based data DRAFT April Code d1 d2 d3 d4 d5 s1 s2 s3 Figure diseases giving rise symptoms The triangulated graph contains clique diseases Summary The junction tree structure clusters variables inference operations marginalisation junction tree structure remains invariant This resolves fill issue message passing multiply connected graph The key stages moralisation triangulation potential assignment message passing There different propagation algorithms including absorption Shafer Shenoy These valid message passing algorithms junction tree differ efficiency depending branch structure junction tree The junction tree algorithm difficult inference problem necessarily easier It simply way organise computations required correctly carry message passing The computational complexity dominated clique size guarantee find cliques small sizes general The junction tree algorithm clever clairvoyant It provides upper bound com putational complexity inference It problems possess additional structure immediately apparent exploited reduce computational complexity inference suggested junction tree approach Code absorb m Absorption update V S W absorption m Full absorption schedule tree jtree m Form junction tree triangulate m Triangulation based simple node elimination Utility routines Knowing undirected graph tree returning valid elimination sequence useful A connected graph tree number edges plus equal number nodes However possibly discon nected graph case The code istree m deals possibly disconnected case returning valid elimination sequence graph singly connected The routine based observation singly connected graph possess simplicial node eliminated reveal smaller singly connected graph istree m If graph singly connected return elimination sequence elimtri m Node elimination triangulated graph given end node demoJTree m Chest clinic demo DRAFT April Exercises Exercises Exercise Show Markov network perfect elimination ordered perfect elimination labelling graph Exercise Consider following distribution p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 Draw clique graph represents distribution indicate separators graph Write alternative formula distribution p x1 x2 x3 x4 terms marginal prob abilities p x1 x2 p x2 x3 p x3 x4 p x2 p x3 Exercise Consider distribution p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 Write junction tree distribution Carry absorption procedure demonstrate gives correct result marginal p x1 Exercise Consider distribution p b c d e f g h p p b p c p d p e b p f c p g d p h e f p f g Draw belief network distribution Draw moralised graph Draw triangulated graph Your triangulated graph contain cliques smallest size pos sible Draw junction tree graph verify satisfies running intersection property Describe suitable initialisation clique potentials Describe absorption procedure write appropriate message updating schedule Exercise This question concerns distribution p b c d e f p p b p c b p d c p e d p f e Draw Belief Network distribution Draw moralised graph Draw triangulated graph Your triangulated graph contain cliques smallest size pos sible Draw junction tree graph verify satisfies running intersection property Describe suitable initialisation clique potentials Describe Absorption procedure appropriate message updating schedule Show distribution expressed form p f p b c p c d p d e p e f p f DRAFT April Exercises Exercise For undirected graph square lattice shown draw triangulated graph smallest clique sizes possible Exercise Consider binary variable Markov Random Field p x Z j xi xj defined n n lattice xi xj eI xi xj neighbour j lattice j A naive way perform inference stack variables tth column cluster variable Xt shown The resulting graph singly connected What complexity computing normalisation constant based cluster representation Compute logZ n X1 X2 X3 Exercise Given consistent junction tree round message passing occurred explain form belief network junction tree Exercise The file diseaseNet mat contains potentials disease bi partite belief network diseases d1 d20 symptoms s1 s40 The disease variables numbered symptoms Each disease symptom binary variable symptom connects parent diseases Using BRMLtoolbox construct junction tree distribution use compute marginals symptoms p si Explain compute marginals p si way efficient junction tree formalism By implementing method compare results junction tree algorithm Symptoms present state symptoms present state rest known Compute marginal p di s1 diseases Exercise Consider distribution p y x1 xT p x1 T t p xt xt variables binary Draw junction tree distribution explain computational complexity computing p xT suggested junction tree algorithm By approach different plain JTA explain p xT computed time scales linearly T Exercise Analogous jtpot absorption jtpot jtsep infostruct write routine jtpot jtmess ShaferShenoy jtpot infostruct returns clique marginals messages junction tree Shafer Shenoy updating Modify demoJTree m additionally output results marginals conditional marginals alongside obtained absorption Exercise Clique elimination Since junction tree tree extremal edge clique X1 connects clique separator S1 More generally able label cliques elimination order n eliminating clique cliques left label greater For separator connected cliques j k number separator lower values min j k Using elimination ordering cliques write JT form X represent clique potentials clique variables X S represent separator potentials separator variables S p X X1 S1 c c Xc s s Ss DRAFT April Exercises Now eliminate clique summing variables clique separator S1 Show gives new JT cliques c separators s form d Xd c c d c Xc s s Ss d clique neighbouring clique d Xd d Xd X1 S1 X1 S1 relate absorption procedure Show continuing manner eliminating cliques clique contain marginal p Xn Explain reversing elimination schedule eliminating cliques updating potentials similar manner Equation updated cliques j Xj contain marginals p Xj Hence explain updating cliques according forward reversed elimination schedules cliques globally consistent DRAFT April Exercises DRAFT April CHAPTER Making Decisions So far ve considered modelling inference distributions In cases need decisions uncertainty need additionally express useful making right decision In chapter particularly interested case sequence decisions need taken The corresponding sequential decision theory problems solved general decision tree approach ex ploiting structure problem based extending belief network framework corresponding inference routines The framework related problems control theory reinforcement learning Expected Utility This chapter concerns situations decisions need taken uncertainty Consider following scenario asked wish bet outcome tossing fair coin If bet win gain If bet lose lose If don t bet cost zero We set state variable x dom x win lose decision variable d dom d bet bet utilities follows U win bet U lose bet U win bet U lose bet Since don t know state x order decision bet arguably best work expected winnings losses situations betting betting If bet expect gain U bet p win U win bet p lose U lose bet If don t bet expected gain zero U bet Based taking decision maximises expected utility advised bet Definition Subjective Expected Utility The utility decision U d U d x p x p x distribution outcome x d represents decision Utility money You wealthy individual bank account You asked like participate fair coin tossing bet win bank account Decision Trees Rain Rain Party ye s yes yes Figure A decision tree containing chance nodes denoted ovals decision nodes denoted rectangles utility nodes denoted diamonds Note decision tree graphical representation belief network additional nodes Rather decision tree explicit enumeration possible choices beginning leftmost decision node probabilities links chance nodes However lose bank account contain Assuming coin fair bet If bet expected bank balance U bet If don t bet bank balance remain Based expected utility advised bet Note considers instead win lose difference expected utility betting betting exercise Whilst makes mathematical sense people millionaires likely willing risk losing order billionaire This means subjective utility money simply quantity money In order better reflect situation utility money need non linear function money growing slowly large quantities money decreasing rapidly small quantities money exercise Decision Trees Decision trees DTs way graphically organise sequential decision process A decision tree contains decision nodes branches alternative decisions Chance nodes random variables appear tree utility branch computed leaf branch The expected utility decision computed basis weighted summation branches decision leaves branch Example Party Consider decision problem ahead fund raising garden party If ahead party subsequently rains lose money people hand don t ahead party doesn t rain free fun To characterise numerically use p Rain rain p Rain rain The utility defined U party rain U party rain U party rain U party rain We represent situation fig The question ahead party Since don t know actually happen weather compute expected utility decision U party Rain U party Rain p Rain U party Rain U party Rain p Rain DRAFT April Decision Trees Based expected utility advised ahead party The maximal expected utility given demoDecParty m max Party Rain p Rain U Party Rain Example Party Friend An extension Party problem decide ahead party opportunity visit friend However sure friend The question ahead party We need quantify uncertainties utilities If ahead party utilities Uparty party rain Uparty party rain p Rain rain p Rain rain If decide ahead party consider going visit friend In making decision ahead party utilities Uparty party rain Uparty party rain The probability friend depends weather according p Friend rain p Friend rain The probabilities determined normalisation We additionally Uvisit friend visit Uvisit friend visit remaining utilities zero The sets utilities add overall utility decision sequence Uparty Uvisit The decision tree Party Friend problem shown fig For decision sequence utility sequence given corresponding leaf DT Note leaves contain total utility Uparty Uvisit Solving DT corresponds finding decision node maximal expected utility possible optimising future decisions At point tree choosing action leads child highest expected utility lead optimal strategy Using find optimal expected utility value given going ahead party demoDecPartyFriend m Mathematically express optimal expected utility Party Friend example summing un revealed variables optimising future decisions max Party Rain p Rain max V isit Friend p Friend Rain Uparty Party Rain Uvisit V isit Friend I Party term I Party effect curtailing DT party goes ahead To answer question ahead party state Party corresponds maximal expected utility The way read equation start decision needs taken case V isit When V isit stage assume previously decision Party observed raining However DRAFT April Decision Trees Party Rain V isit Friend Friend V isit Friend Friend Rain n o ye s n o ye s ye s ye s ye s n o Party Rain V isit Friend Friend V isit Friend Friend Rain n o ye s n o ye s ye s ye s ye s n o b Figure Solving Decision Tree Decision Tree Party Friend problem example b Solving DT cor responds making decision highest expected future utility This achieved starting leaves utilities For chance parent node x utility parent expected utility variable For example DT Rain variable children probability probabil ity Hence expected utility Rain node For decision node value node optimum child values One curses backwards leaves root For example value Rain chance node lower branch given The opti mal decision sequence given decision node finding child node maximal value Hence overall best decision decide ahead party If decided rain best decision visit friend expected utility A compact description problem given influence diagram fig See demoDecPartyFriend m don t know friend compute expected utility averaging unknown We optimal decision maximising V isit Subsequently decision assuming future optimal Since future taken decision uncertain Friend variable current decision taken uncertainty Rain maximising expected optimal utility Party Note sequence maximisations summations matters changing order general result different problem different expected utility1 For Party Friend example DT asymmetric decide ahead party visit friend curtailing decisions present lower half tree Whilst DT approach flexible handle decision problems arbitrary structure drawback nodes repeated decision tree For longer sequence decisions number branches tree grow exponentially number decisions making representation impractical 1If sequence summations order summations irrelevant likewise case maximi sations However summation maximisation operators general commute DRAFT April Extending Bayesian Networks Decisions RainParty Utility Figure An influence diagram contains random vari ables denoted ovals circles Decision nodes denoted rectangles Utility nodes denoted diamonds Con trasted fig compact representation structure problem The diagram represents expres sion p rain u party rain In addition diagram denotes ordering variables party rain according convention given equation Extending Bayesian Networks Decisions An influence diagram Bayesian Network additional Decision nodes Utility nodes The decision nodes associated distribution utility nodes deterministic functions parents The utility decision nodes continuous discrete simplicity examples decisions discrete A benefit decision trees general explicitly encode utilities probabilities associated decision event In addition readily solve small decision problems decision trees However sequence decisions increases number leaves decision tree grows representing tree exponentially complex problem In cases useful use Influence Diagram ID An ID states information required order decision order decisions The details probabilities utilities specified ID enable compact description decision problem Syntax influence diagrams Information Links An information link random variable decision node X D d indicates state variable X known decision D taken Information links decision node d D similarly indicate decision d known decision D taken We use dashed link denote decision D functionally related parents Random Variables Random variables depend states parental random variables belief networks decision node states YXD As decisions taken states random variables revealed To emphasise typically shade node denote state revealed sequential decision process Utilities A utility node deterministic function parents The parents random variables decision nodes XUD In party example BN trivially consists single node Influence Diagram given fig The complex Party Friend problem depicted fig The ID generally provides compact representation structure problem DT details specific probabilities utilities present ID DRAFT April Extending Bayesian Networks Decisions Rain Party Uparty V isit Friend Uvisit Figure An influence diagram Party Friend problem example The partial ordering Party Rain V isit Friend The dashed link party visit strictly nec essary retained order satisfy convention directed path connecting decision nodes Partial ordering An ID defines partial ordering nodes We begin writing variables X0 states known evidential variables decision D1 We find set variables X1 states revealed second decision D2 Subsequently set variables Xt revealed decision Dt The remaining fully unobserved variables placed end ordering X0 D1 X1 D2 Xn Dn Xn Xk variables revealed decision Dk Dk The term partial refers fact order implied variables set Xn For notational clarity points indicate decision variables reinforce maximise variables sum non starred variables Where sets omit writing In fig 5a consider oil exploration situation decision taken carry seismic test associated utility cost U1 The result test represented variable Seismic depends oil present Based seismic result subsequent decision taken drill oil associated utility U2 The ordering Test Seismic Drill Oil The optimal decision D1 determined computing U D1 X0 X1 max D2 Xn max Dn Xn I p xi pa xi j J Uj pa uj state decision D1 given X0 In equation I denotes set indices random variables J indices utility nodes For state conditioning variables optimal decision D1 found argmax D1 U D1 X0 Remark Reading partial ordering Sometimes tricky read partial ordering ID A method identify decision D1 variables X0 need observed decision Then identify decision D2 variables X1 revealed decision D1 taken decision D2 taken etc This gives partial ordering X0 D1 X1 D2 Place unrevealed variables end ordering Implicit explicit information links The information links potential source confusion An information link specifies explicitly quantities known decision taken2 We implicitly assume forgetting assumption past decisions revealed variables available current decision revealed variables 2Some authors prefer write information links possible prefer leave implicit Here largely implicit approach For purposes computation required partial ordering view basic information links superficial DRAFT April Extending Bayesian Networks Decisions Test DrillU1 U2Oil Seismic Test DrillU1 U2Oil Seismic b Figure The partial ordering Test Seismic Drill Oil The explicit information links Test Seismic Seismic Drill fundamental sense removing results different partial ordering The shaded node emphasises state variable revealed sequential decision process Conversely non shaded node observed b Based ID implicit link Test Drill decision Test taken Seismic revealed necessarily parents future decision nodes If include information links IDs potentially messy In fig explicit implicit information links demon strated We information link fundamental removal alter partial ordering Causal consistency For Influence Diagram consistent current decision affect past This means random variable descendants decision D ID come later partial ordering Asymmetry IDs convenient corresponding DT symmetric However forms asymmetry relatively straightforward deal ID framework For Party Friend example DT asymmetric However easily dealt ID link Party Uvisit removes contribution Uvisit party goes ahead More complex issues arise set variables observed depends decision sequence taken In case DT asymmetric In general Influence Diagrams suited modelling asymmetries effects mediated careful use additional variables extending ID notation See details issues possible resolutions Example Should I PhD Consider decision PhD education E Taking PhD incurs costs UC terms fees terms lost income However PhD likely win Nobel Prize P certainly likely boost Income I subsequently benefitting finances UB This setup depicted fig 6a The ordering excluding sets E I P dom E PhD PhD dom I low average high dom P prize prize The probabilities p win Nobel prize PhD p win Nobel prize PhD p low PhD prize p average PhD prize p high PhD prize p low PhD prize p average PhD prize p high PhD prize p low PhD prize p average PhD prize p high PhD prize p low PhD prize p average PhD prize p high PhD prize DRAFT April Extending Bayesian Networks Decisions E UC P I UB SE UC P USI UB b Figure Education E incurs cost gives chance win pres tigious science prize Both affect likely Incomes corresponding long term financial benefits b The start scenario The utilities UC PhD UC PhD UB low UB average UB high The expected utility Education U E I P p I E P p P E UC E UB I U phd whilst taking PhD U phd making average beneficial PhD See demoDecPhD m Example PhDs Start companies Influence Diagrams particularly useful sequence decisions taken For example fig 6b model new situation dropping link E I simply reduce table size decided PhD Ten years later career decide start company This decision based won Nobel Prize The start decision modelled S dom S tr fa If start cost money terms investment However potential benefit terms income high We model required table entries taken example p low start prize p average start prize p high start prize p low start prize p average start prize p high start prize p low start prize p average start prize p high start prize p low start prize p average start prize p high start prize US start US start Our interest advise desirable terms expected utility PhD bearing mind later win Nobel Prize start company The ordering eliding sets E P S I DRAFT April Solving Influence Diagrams The expected optimal utility state E U E P max S I p I S P p P E US S UC E UB I assume optimal decisions taken future Computing find U PhD U PhD Hence better PhD See demoDecPhd m Solving Influence Diagrams Solving influence diagram means computing optimal decision sequence decisions The direct variable elimination approach equation perform required sequence summations maximisations explicitly Due causal consistency requirement future influence past To help matters notation order variables decisions write belief network influence diagram p x1 T d1 T T t p xt x1 t d1 t For general utility u x1 T d1 T solving ID corresponds carrying operations max d1 x1 max dT xT T t p xt x1 t d1 t u x1 T d1 T Let s look eliminating xT dT Our aim write new ID reduced variables x1 T d1 T Since xT dT appear final factor belief network write max d1 x1 max dT xT T t p xt x1 t d1 t max dT xT p xT x1 T d1 T u x1 T d1 T new ID max d1 x1 max dT xT T t p xt x1 t d1 t u x1 T d1 T modified potential u x1 T d1 T max dT xT p xT x1 T d1 T u x1 T d1 T This doesn t exploit fact utilities typically structure Without loss gen erality write utility independent xT dT depends xT dT u x1 T d1 T ua x1 T d1 T ub x1 T d1 T Then eliminating xT dT updates utility u x1 T d1 T ua x1 T d1 T max dT xT p xT x1 T d1 T ub x1 T d1 T DRAFT April Solving Influence Diagrams Messages ID For ID sets variables X1 X2 associated decision sets D1 D2 write belief network p X D p X2 X1 D1 D2 p X1 D1 D1 X1 D2 X2 corresponding utilities u X D u X1 D u X1 X2 D The optimal utility given uopt max D1 X1 max D2 X2 p X D u X D After eliminating X2 D2 obtain ID p X1 D1 u X1 D max D2 X2 p X2 X1 D1 D2 u X1 X2 D express terms original distribution p X D X D p X D u X1 D X D p X D X D p X D u X1 X2 D Y refers summing chance variables Y maximising decision variables Y These updates define ID reduced set variables viewed messages The potential usefulness equation applied IDs causally consistent future decisions affect past expressed directly causal form Using junction tree In complex IDs computational efficiency carrying series summations maximisations issue seeks exploit structure ID It intuitive form junction tree style algorithm applicable The treatment inspired related approach deals general chain graphs given We represent ID decision potentials consist parts defined Definition Decision Potential A decision potential clique C contains potentials probability potential C utility potential C The joint potentials junction tree defined C C C C C C junction tree representing term In case constraints triangulation imposed inverse partial ordering restricts variables elimination sequence This results called strong Junction Tree The sequence steps required construct JT ID given following procedure Procedure Making strong junction tree Remove Information Edges Parental links decision nodes removed Moralization Marry parents remaining nodes DRAFT April Solving Influence Diagrams Remove Utility Nodes Remove utility nodes parental links Strong Triangulation Form triangulation based elimination order obeys partial ordering variables Strong Junction Tree From strongly triangulated graph form junction tree orient edges strong root clique appears elimination sequence The cliques ordered according sequence eliminated The separator prob ability cliques initialised identity separator utilities initialised zero The probability cliques initialised placing conditional probability factors lowest available clique probability factors placed cliques closest leaves tree furthest root contain similarly utilities Remaining probability cliques set identity utility cliques zero Example Junction Tree An example junction tree ID given fig 7a The moralisation triangulation links given fig 7b The orientation edges follows partial ordering leaf cliques disappear sequence summations maximisations A product steps cliques describe fundamental dependencies previous decisions observations In fig 7a example information link f D2 present moralised triangulated graph fig 7b associated cliques fig 7c This e revealed utility U4 independent f giving rise branch structure fig 7b Nevertheless information link f D2 fundamental specifies f revealed removing link change partial ordering Absorption By analogy definition messages section neighbouring cliques C1 C2 C1 closer strong root JT clique defined elimination order define S C2 S C2 S C2 S C2C2 newC1 C1S new C1 C1 S S In C generalised marginalisation operation sums elements clique C random variables maximises decision variables clique The order sequence sums maximisations follows partial ordering defined Absorption carried leaves inwards root strong JT The optimal setting decision D1 computed root clique Subsequently backtracking applied infer optimal decision trajectory The optimal decision D obtained working clique containing D closest strong root setting previously taken decisions revealed observations evidential states See demoDecAsia m example DRAFT April Solving Influence Diagrams b c d e f g D2 D1 U1 D4 l U4 h j k D3 U2 U3 b c d e f g D2 D1 D4 l h j k D3 b b D1 e f d b e d c b c f D3 h D3 h k h k j e D2 g D2 g D4 D4 l b c b e d e D2 g D4 f D3 h h k c Figure Influence Diagram adapted Causal consistency satisfied directed path linking decisions sequence The partial ordering b D1 e f D2 D3 g D4 c d h j k l b Moralised strongly triangulated graph Moralisation links green strong triangulation links red c Strong Junction Tree Absorption passes information leaves tree root Example Absorption chain For ID fig moralisation triangulation steps trivial JT x1 x2 d1 x2 x2 x3 d2 x3 x3 x4 d3 cliques indexed according elimination order The probability utility cliques initialised x1 x2 d1 p x2 x1 d1 x1 x2 d1 x2 x3 d2 p x3 x2 d2 x2 x3 d2 u x2 x3 x4 d3 p x4 x3 d3 x3 x4 d3 u x3 u x4 separator cliques initialised x3 x3 x2 x2 Updating separator new probability potential x3 max d3 x4 x3 x4 d3 DRAFT April Markov Decision Processes utility potential x3 max d3 x4 x3 x4 d3 x3 x4 d3 max d3 x4 p x4 x3 d3 u x3 u x4 max d3 u x3 x4 p x4 x3 d3 u x4 At step update probability potential x2 x3 d2 x2 x3 d2 x3 p x3 x2 d2 utility potential x2 x3 d2 x2 x3 d2 x3 x3 u x2 max d3 u x3 x4 p x4 x3 d3 u x4 The separator decision potential x2 max d2 x3 x2 x3 d2 x2 max d2 x3 x2 x3 d2 x2 x3 d2 max d2 x3 p x3 x2 d2 u x2 max d3 u x3 x4 p x4 x3 d3 u x4 Finally end root decision potential x1 x2 d1 x1 x2 d1 x2 p x2 x1 d1 x1 x2 d1 x2 x1 d1 x2 x2 max d2 x3 p x3 x2 d2 u x2 max d3 u x3 x4 p x4 x3 d3 u x4 From final decision potential expression x1 x2 d1 x1 x2 d1 equivalent obtained simply distributing summations maximi sations original ID At special case verified JT approach yields correct root clique potentials Markov Decision Processes Consider Markov chain transition probabilities p xt xt j At time t consider action decision affects state time t We describe p xt xt j dt k Associated state xt utility u xt schematically depicted fig More generally consider utilities depend transitions decisions u xt xt j dt k DRAFT April Markov Decision Processes x1 x2 x3 x4 u2 u3 u4 d1 d2 d3 Figure Markov Decision Process These model planning problems form I I want incurring lowest total cost They readily solvable message passing algorithm time dependent versions pt xt xt j dt k ut xt xt j dt k We ll stick time independent stationary case generalisations conceptually straightforward expense notational complexity Markov Decision Processes MDPs solve planning tasks desired goal state quickly possible For positive utilities total utility state decision path x1 T d1 T defined assuming know initial state x1 U x1 T T t u xt probability happens given p x2 T x1 d1 T T t p xt xt dt At time t want decision d1 lead maximal expected total utility U d1 x1 x2 max d2 x3 max d3 x4 max dT xT p x2 T x1 d1 T U x1 T Our task compute U d1 x1 state d1 choose state maximal expected total utility To carry summations maximisations efficiently use junction tree approach described previous section However case ID sufficiently simple direct message passing approach compute expected utility Maximising expected utility message passing Consider time dependent decisions non stationary policy MDP T t p xt xt dt T t u xt For specific example fig joint model BN utility p x4 x3 d3 p x3 x2 d2 p x2 x1 d1 u x2 u x3 u x4 To decide optimal decision need compute U d1 x1 x2 max d2 x3 max d3 x4 p x4 x3 d3 p x3 x2 d2 p x2 x1 d1 u x2 u x3 u x4 Since u x4 depends x4 explicitly write U d1 x1 x2 max d2 x3 p x3 x2 d2 p x2 x1 d1 u x2 u x3 max d3 x4 p x4 x3 d3 u x4 DRAFT April Markov Decision Processes Defining message corresponding value u3 x3 max d3 x4 p x4 x3 d3 u x4 v x3 u x3 u3 x3 write U d1 x1 x2 max d2 x3 p x3 x2 d2 p x2 x1 d1 u x2 v x3 In similar manner term depends x3 U d1 x1 x2 p x2 x1 d1 u x2 max d2 x3 p x3 x2 d2 v x3 Defining similarly value v x2 u x2 max d2 x3 p x3 x2 d2 v x3 U d1 x1 x2 p x2 x1 d1 v x2 Given U d1 x1 find optimal decision d1 d x1 argmax d1 U d1 x1 Bellman s equation In Markov Decision Process define utility messages recursively ut t xt max dt xt p xt xt dt u xt ut t xt It common define value state xt vt xt u xt ut t xt vT xT u xT write equivalent recursion vt xt u xt max dt xt p xt xt dt vt xt The optimal decision d t given d t xt argmax dt xt p xt xt dt vt xt Equation called Bellman s equation 3The continuous time analog long history physics called Hamilton Jacobi equation DRAFT April Temporally Unbounded MDPs Figure States defined dimensional grid In square left value state number right utility state An agent state neighbouring state indicated The task solve problem position state knows optimally maximise expected utility This means need goal states states non zero utility See demoMDP Temporally Unbounded MDPs In previous discussion MDPs assumed given end time T propagate messages end chain The infinite T case appear I shall defined sum utilities u x1 u x2 u xT general unbounded There simple way avoid difficulty If let u maxs u s largest value utility consider sum modified utilities chosen discount factor T t tu xt u T t t u T result geometric series In limit T means summed modified utility tu xt finite The modification required previous discussion include factor message definition Assuming convergence define value v xt s dependent state s time This means replace time dependent Bellman s value recursion equation time independent equation v s u s max d s p xt s xt s dt d v s We need solve equation value v s states s The optimal decision policy state xt s given d s argmax d s p xt s xt s dt d v s For deterministic transition p e decision d state s available means best decision takes accessible state highest value Equation straightforward solve However max operation means equations non linear value v closed form solution available Two popular techniques solving equation Value Policy iteration describe When number states S large approximate solutions required Sampling state dimension reduction techniques described Value iteration A naive procedure iterate equation convergence assuming initial guess values uniform One value iteration procedure guaranteed converge unique optimum The convergence rate depends smaller faster convergence An example value iteration given fig DRAFT April Temporally Unbounded MDPs Figure Value Iteration set states corresponding dimensional grid De terministic transitions allowed neighbours grid stay left right There goal states utility states utility Plotted value v s updates Value Iteration states index point x y grid The optimal decision state grid neighbouring state highest value See demoMDP Policy iteration In policy iteration assume know optimal decision d s state s We use equation v s u s s p xt s xt s d s v s The maximisation d disappeared assumed know optimal decision state s For fixed d s equation linear value Defining value v utility u vectors transition matrix P v s v s u s u s P s s p s s d s matrix notation equation v u PTv I PT v u v I PT u These linear equations readily solved Gaussian Elimination Using optimal policy recomputed equation The steps solving value recomputing policy iterated convergence The procedure initialised guessing initial d s solve linear equations value alternatively guessing initial values solving initial policy Example A grid world MDP We define set states N N grid corresponding utilities state given example fig N The agent able deterministically neighbouring grid state time step After initialising value grid state unity converged value state obtained An example grid given fig utilities zero states The optimal policy given moving neighbouring grid state highest value A curse dimensionality Consider following Tower Hanoi problem There pegs b c d disks numbered You single disk peg allowed bigger numbered disk smaller numbered disk Starting disks peg peg d minimal number moves This appear straightforward Markov decision process transitions allowed disk moves If use x represent state disks pegs naively states DRAFT April Variational Inference Planning equivalent permutation pegs reduces factor This large number states renders naive approach computationally problematic Many interesting real world problems suffer large number states issue naive approach find best decision computationally infeasible Finding efficient exact approximate state representations key aspect solving large scale MDPs example Variational Inference Planning For finite horizon stationary policy MDP learning optimal policy addressed variety methods Two popular approaches policy gradients EM style procedures example section For MDPs interest optimal policy deterministic methods explicitly seek deterministic policies interest For reason remain close discussion policy value iteration involved deterministic policies focus case brief discussion referring reader texts details non deterministic case For time independent deterministic policy d s maps state s decision d write short expected utility U T t xt ut xt x1 t t p x x d x convention p x1 x0 d x0 p x1 Viewed factor graph simply set chains policy expected utility computed easily In principle attempt optimise U respect policy directly An alternative use EM style procedure To define trans dimensional distribution p x1 t t ut xt Z t p x x d x The normalisation constant Z distribution T t x1 t ut xt t p x x d x T t x1 t ut xt t p x x d x U If define variational distribution q x1 t t consider KL q x1 t t p x1 t t gives lower bound logU H q x1 t t log ut xt t p x x d x q x1 t t H q x1 t t entropy distribution q x1 t t In terms EM algorithm M step requires dependency E T t t log p x x d x q x x t T t t q x s x s t log p x s x s d x d For given state s attempt find optimal decision d corresponds maximising E d s s T t t q x s x s t log p s s d DRAFT April Financial Matters Defining q s s T t t q x s x s t given s constant E d s Kullback Leibler divergence q s s p s s d optimal decision d given index distribution p s s d closely aligned q s s d s argmin d KL q s s p s s d The E step concerns computation marginal distributions required M step The optimal q distribution proportional p evaluated previous decision function d q x1 t t ut xt t p x x d x For constant discount factor time step time independent utility4 ut xt tu xt q x1 t t tu xt t p x x d x For t simple Markov chain pairwise transition marginals required M step equation straightforward This requires inference series Markov models different lengths This efficiently single forward backward pass EM related methods follow closely spirit inference graphical models exhibit disappoint ingly slow convergence More recently alternative method Lagrange Duality shows promising performance reader referred details Note EM algorithm formally fails case deterministic environment transition p xt xt dt deterministic exercise explanation exercise possible resolution Remark Solving MDP easy hard The discussion section highlights solving linear chain influence diagram finding optimal decision timestep straightforward achieved simple message passing algorithm scaling linearly length chain In contrast finding optimal time independent policy typically complex reason different algorithms attempt find optimal policies areas time independent control reinforcement learning games Mathematically reason difference constraint time independent case policy time steps leads graphical structure longer chain timepoints connected single In case general simple linear time message passing algorithm available optimise resulting objective Financial Matters Utility decision theory play major role finance terms setting prices based expected future gains determining optimal investments In following sections briefly outline basic applications 4In standard MDP framework common define ut xt t 1u xt comparison standard Policy Value routines needs divide expected utility DRAFT April Financial Matters T S Sd S Su Figure Options pricing An asset market value S time t We assume asset market value Su Sd time T The owner options buyer client agree market value greater strike price S time T client right purchase asset S The question owner asset charge client privilege having right purchase asset Options pricing expected utility An owner asset currently priced market S The owner wants opportunity purchase asset time T agreed price S At time T market price goes strike price Su decide purchase asset owner S sell asset increased value fig If price remains strike price Sd walk away leaving owner asset The question owner charge C option able buy asset agreed price time T To help answer need know risk free investment time period e putting money safe bank We assume interest rate R known time period T We people owner client buy asset Two possibilities case For simplicity assume asset prices Su Sd time T We assume know probabilities events Let s work expected utilities parties Asset goes U client Su S immediate profit selling C option cost CR lost interest option cost U owner S S immediate loss sale C option cost CR gained interest option cost SR lost interest The final SR term comes entering option deal owner sold asset time resulting sum bank Asset goes U client C option cost CR lost interest option cost The follows case client doesn t act option sell asset U owner Sd S change asset value C option cost CR gained interest option cost SR lost interest The expected utility client U client U client U client Su S C CR C CR Su S C R The expected utility owner U owner U owner U owner S S C CR SR Sd S C CR SR S Sd Sd S C R SR DRAFT April Financial Matters It reasonable assume client owner expected benefit U client U owner Hence Su S C R S Sd Sd S C R SR Solving C find C Su 2S Sd Sd S R R All quantities required price option assumed known One way set described Setting It reasonable expect set process describes true probability price increase However given value knowing possible prices Su Sd owner compute expected utility holding asset This Su Sd S Alternatively owner sell asset S money bank collecting interest RS time T In fair market expected reward holding asset risk free return asset Su Sd S RS S R Sd Su Sd Using value price option equation ensures expected gain offering option offering option owner furthermore option available expected reward parties Binomial options pricing model If timepoints readily extend It s easiest assume time t price change possibilities price factor u factor d For T timesteps set possible values asset time T For client sell asset ST S We need work expected gains owner client assuming know probability price increasing factor u time step For sequence n ups T n downs price ST Su ndT n If greater S client sell asset utility I SundT n S SundT n S C R The probability n ups T n downs T n T n n T n T n binomial coefficient Hence total expected utility client upside U client T n T n I SundT n S SundT n S C R Similarly U client C R T n T n I SundT n S DRAFT April Financial Matters The total expected utility client U client U client U client Similarly owner U owner S S R C R T n T n I SundT n S U owner T n T n I SundT n S SundT n S R C R U owner U owner U owner Setting U client U owner results simple linear equation C Setting To set use similar logic timestep case First compute expected value asset time T T n T n SundT n equate expected gain equal gain risk free investment asset T n T n SundT n S RS T n T n undT n R Knowing u d solve use equation C We learn u d past observed data literature related observed variance prices The binomial options pricing approach relatively simplistic way price options The celebrated Black Scholes method essentially limiting case number timepoints infinite Optimal investment Another example utility finance related Markov decision processes issue best invest wealth order maximise future criterion We ll consider simple setup readily extended complex scenarios We assume assets b prices time t given sat s b t The prices assumed independently follow Markovian updating sat s t t p sat sat sat sat Dirac delta function The price increments follow Markov transition p b t bt p p bt bt Using model effects price increments likely stay bank interest variable stock market DRAFT April Financial Matters b c Figure Two assets time risky asset value fluctuates wildly stable asset grows slowly b The wealth portfolio time based investing hope achieve wealth T c The optimal investment decisions time corresponding placing wealth safe asset placing money risky asset We investment decision dt says fraction current wealth wt buy asset time t We invest rest wealth asset b If asset priced sat decide use fraction dt current wealth wt purchase quantity q t asset quantity q b t asset b given qat dtwt sat qbt wt dt sbt At time step t prices assets b changed sat s b t new wealth wt q t s t q b ts b t dtwts t sat wt dt sbt sbt wt dt t dt bt This expressed transition p wt wt bt dt wt wt dt t dt bt At end time T utility u wT expresses satisfaction wealth Given start wealth w1 assume know b want find best decision d1 maximise expected utility time T To bear mind intermediate time t T adjust fraction dt wealth asset The Markov chain given section p a1 T b T w2 T a1 b1 w1 d1 T T t p p bt bt p wt wt bt dt The expected utility decision d1 U d1 a1 b1 w1 a2 b w2 max dT T b T wT max dT T b T wT p a1 T b T w2 T a1 b1 w1 d1 T u wT corresponding influence diagram corresponds ordering d1 a2 b w2 d2 aT b T wT dT aT b T wT To compute U d1 a1 b1 w1 carry operations T message T T T b T wT max dT T b T wT p aT aT p bT bT p wT wT aT bT dT u wT And generally t t t b t wt max dt b t wt p p bt bt p wt wt bt dt t t bt wt DRAFT April Further Topics h1 h2 h3 h4 v1 v2 v3 v4 u2 u3 u4 d1 d2 d3 Figure An example Partially Observable Markov Decision Process POMDP The hidden variables h observed In solving Influence Diagram required sum variables observed couple past observed variables decisions decision time t depend previous decisions Note forgetting principle means need explicitly write decision depends previous observations implicitly assumed U d1 a1 b1 w1 a2 b w2 p a2 a1 p b2 b1 p w2 w1 a2 b2 d1 a2 b2 w2 Note process equivalent myopic strategy investment decision maximise expected period wealth For continuous wealth wt messages difficult represent A simple strategy discretise wealth values price changes b t investment decisions dt exercise In case needs approximate delta function x distribution zero state discrete state closest real value x Example Optimal investment We demonstrate simple optimal portfolio investment problem fig safe bank asset risky stock market asset We start unit wealth wish obtain wealth time t If place money bank able reach desired place wealth risky asset In beginning stock market poorly wealth correspondingly poor The stock market picks sufficiently t longer need risks place money bank confident reach investment objective Further Topics Partially observable MDPs In POMDP states observed This seemingly innocuous extension MDP case lead computational difficulties Let s consider situation fig attempt compute optimal expected utility based sequence summations maximisations The sum hidden variables couples decisions observations meaning longer simple chain structure remaining maximisations For POMDP length t leads intractable problem complexity exponential t An alternative view recognise past decisions observations v1 t d1 t summarised terms belief current latent state p ht v1 t d1 t This suggests instead having actual state MDP case need use distribution states represent current knowledge One write effective MDP albeit belief distributions opposed finite states Approximate techniques required solve resulting infinite state MDPs reader referred specialised texts study approximation procedures See example Reinforcement learning Reinforcement Learning deals mainly time independent Markov Decision Processes The added twist transition p s s d possibly utility unknown Initially agent begins explore DRAFT April Further Topics set states utilities rewards associated taking decisions The set accessible states rewards populates agent traverses environment Consider example maze problem given start goal state unknown maze structure The task start goal minimum number moves maze Clearly balance required curiosity acting maximise expected reward If curious don t optimal decisions given currently available information maze structure continue exploring possible maze routes bad On hand don t explore possible maze states realise optimal short cut follow based current knowledge This exploration exploitation tradeoff central difficulties RL See extensive discussion reinforcement learning From model based model free learning Consider MDP state transitions p xt xt dt policy p dt xt For simplicity consider utilities depend state xt The expected utility taking decision dt state xt U dt xt derived similar argument section discount factor Analogous equation including discounting U dt xt xt p xt xt dt u xt max d x p x xt d v x Using value recursion v xt u xt max d x p x xt d v x write U dt xt xt p xt xt dt v xt Substituting equation v xt u xt max d U d xt Substituting equation equation obtain U dt xt xt p xt xt dt u xt max d U d xt If know model p xt xt dt solve equation U d x Given solution state x optimal policy decision d arg maxd U d x In case wish explicitly store describe model p xt xt dt use sample transition approximate equation state xt decision dt environment returns sample xt This gives sample estimate equation U dt xt u xt max d U d xt This gives highly stochastic update ensure convergence preferable use U t dt xt t U t dt xt t u xt max d U t d xt written U t dt xt U t dt xt t u xt max d U t d xt U t dt xt DRAFT April Further Topics learning rate satisfies t t t t For example t t This gives procedure called Q learning updating approximation U based samples environment5 This simple powerful scheme popular model free methods inforcement learning A complicating factor select decision based d arg maxd U d x influences sample drawn Nevertheless certain conditions essentially decisions repeatedly sampled state sample estimate U t d x converges exact U d x limit t Bayesian reinforcement learning For given set environment data X observed transitions utilities aspect RL problem considered finding policy maximises expected reward given prior belief environment observed decisions states If assume know utility function transition write U X U p X represents environment state transition p xt xt dt Given set observed states decisions p X p X p p prior transition Similar techniques EM style training carried case Rather policy function state environment optimally needs consider policy p dt xt b function state belief environment b p X This means example belief environment high entropy agent recognise explicitly carry decisions actions explore environment A complication RL data collected X depends policy If write t episode policy t followed data Xt collected utility policy given historical information U t X1 t U p X1 t t Depending prior environment long episode different posteriors environment parameters If set t argmax U t X1 t affects data collect episode Xt In way trajectory policies different depending episodes priors Summary One way decisions decision maximises expected utility decision Sequential decision problems modelled decision trees These powerful unwieldy long decision sequences Influence diagrams extend belief networks decision arena Efficient inference approaches carry case including extensions strong junction tree formalism The sequence information revealed decisions taken specified influence diagram The optimal utility invariant corresponding partial ordering 5The standard notation Q learning literature uses Q x d place U d x DRAFT April Code x d e tdx Ux l dh Uh b s s Smoking x Positive X ray d Dyspnea Shortness breath e Either Tuberculosis Lung Cancer t Tuberculosis l Lung Cancer b Bronchitis Visited Asia dh Hospitalise dx Take X ray Figure Influence Diagram Chest Clinic Decision example Markov decision processes correspond simple chain like influence diagram inference straightforward corresponds classical Bellman equations Reinforcement learning considered extension Markov decision framework model environment agent acts needs learned basis experience In chapter discussed planning control inference problem particular attention discrete variables See example application approximate inference continuous control Code Sum Max partial order maxsumpot m Generalised elimination operation according partial ordering sumpotID m Sum max ID probability decision potentials demoDecParty m Demo summing maxing ID Junction trees influence diagrams There need specify information links provided partial ordering given In code jtreeID m check partial ordering consistent influence diagram In case step junction tree formulation section required Also moralisation removal utility nodes easily dealt defining utility potentials including moralisation process The strong triangulation found simple variable elimination scheme seeks eliminate variable number neighbours provided variable eliminated according specified partial ordering The junction tree constructed based elimination clique sequence C1 CN obtained triangulation routine The junction tree obtained connecting clique Ci clique j connected clique Clique Ci eliminated graph In manner junction tree connected cliques formed We require separators influence diagram absorption computed discarded fly Note code computes messages leaves root junction tree sufficient taking decisions root If desires optimal decision non root need absorb probabilities clique contains decision required These extra forward probability absorptions required information unobserved variables affected decisions DRAFT April Code observations past This extra forward probability schedule given code left exercise interested reader jtreeID m Junction Tree Influence Diagram absorptionID m Absorption Influence Diagram triangulatePorder m Triangulation based partial ordering demoDecPhD m Demo utility Doing PhD Startup Party Friend example The code implements Party Friend example text To deal asymmetry V isit utility zero Party state yes demoDecPartyFriend m Demo Party Friend Chest Clinic Decisions The table Chest Clinic Decision network fig taken exercise There slight modification p x e table If x ray taken information x available However decision x ray information x available This form asymmetry A straightforward approach case dx parent x variable set distribution x uninformative dx fa p tr p s tr p t tr tr p t tr fa p l tr s tr p l tr s fa p b tr s tr p b tr s fa p x tr e tr dx tr p x tr e fa dx tr p x tr e tr dx fa p x tr e fa dx fa p d tr e tr b tr p d tr e tr b fa p d tr e fa b tr p d tr e fa b fa The utilities designed reflect costs benefits taking x ray hospitalising patient dh tr t tr l tr dh tr t tr l fa dh tr t fa l tr dh tr t fa l fa dh fa t tr l tr dh fa t tr l fa dh fa t fa l tr dh fa t fa l fa dx tr t tr dx tr t fa dx fa t tr dx fa t fa We assume know patient Asia deciding taking x ray The partial ordering dx d x dh b e l s t The demo demoDecAsia m produces results utility table asia yes takexray yes asia takexray yes asia yes takexray asia takexray shows optimally x ray patient Asia demoDecAsia m Junction Tree Influence Diagram demo DRAFT April Exercises Markov decision processes In demoMDP m consider simple dimensional grid agent grid square left right current square stay current square We defined goal states grid squares high utility having zero utility demoMDPclean m Demo Value policy iteration simple MDP MDPsolve m MDP solver value policy iteration Routines efficient MDP variational solvers available book website There code fast Lagrange duality techniques scope discussion Exercises Exercise You play game probability p winning If win game gain S lose game lose S Show expected gain playing game 2p S Exercise It suggested utility money based relative people Assume distribution p incomes histogram bins bin representing income range Use histogram roughly reflect distribution incomes society incomes average wealthy extremely poor people Now define utility income x chance income x higher randomly chosen income y distribution defined relate cumulative distribution p Write program compute probability plot resulting utility function income Now repeat coin tossing bet section wins bet s new income placed histogram bin whilst loses s new income lowest bin Compare optimal expected utility decisions situations s original income average ii higher average Exercise Derive partial ordering ID right explain ID differs fig Test DrillU1 U2Oil Seismic Exercise This question follows closely demoMDP m represents problem pilot wishes land airplane The matrix U x y file airplane mat contains utilities position x y crude model runway taxiing area The airspace represented grid Gx Gy notation employed demoMDP m The matrix U represents position desired parking bay airplane vertical height airplane taken account The positive values U represent runway areas airplane allowed Zero utilities represent neutral positions The negative values represent unfavourable positions airplane By examining matrix U airplane preferably veer runway avoid small villages close airport At timestep plane perform following actions stay left right For stay airplane stays x y position For airplane moves x y position For airplane moves x y position For left airplane moves x y position For right airplane moves x y position A takes airplane airspace allowed plane remains current position x y For example issue right action stay x y x y airspace DRAFT April Exercises The airplane begins point x y Assuming action deterministically results intended grid find optimal xt yt sequence times t position aircraft The pilot tells fault airplane right action Provided x y airspace current position x y If x y airspace right x y probability If x y airspace right x y probability x y probability Assuming airplane begins point x y return optimal xt yt sequence times t position aircraft Exercise The influence diagram depicted describes stage game The decision variable dom d1 play play indicates decision play stage If decide play cost c1 play C1 cost oth erwise c1 play The variable x1 describes win lose game dom x1 win lose probabilities p x1 win d1 play p1 p x1 win d1 play The utility winning losing u1 x1 win W1 u1 x1 lose x1 u1 c1 d1 Show expected utility gain playing game U d1 play p1W1 C1 Exercise Exercise describes stage new stage game If win stage x1 win decision d2 play second stage dom d2 play play If win stage enter second stage If decide play second stage win probability p2 p x2 win x1 win d2 play p2 If decide play second stage chance win p x2 win x1 win d2 play The cost playing second stage c2 d2 play C2 c2 d2 play utility winning losing second stage u2 x2 win W2 u2 x2 lose Draw Influence Diagram describes stage game A gambler needs decide enter stage stage game Show based taking optimal future decision d2 expected utility based decision U d1 play p1 p2W2 C2 p1W1 C1 p2W2 C2 p1W1 C1 p2W2 C2 Exercise You B bank account You asked like participate bet win bank account W However lose bank account contain L You win bet probability pw DRAFT April Exercises Assuming utility given number pounds bank account write formula expected utility taking bet U bet expected utility taking bet U bet The situation formulated differently If win bet gain W B If lose bet lose B L Compute expected money gain bet Ugain bet don t bet Ugain bet Show U bet U bet Ugain bet Ugain bet Exercise Consider objective F x U x p x positive function U x task maximise F respect An Expectation Maximisation style bounding approach section derived defining auxiliary distribution p x U x p x F considering KL q x p x variational distribution q x obtain bound logF log q x q x logU x q x log p x q x The M step states optimal q distribution given q x p x old At E step algorithm new parameters new given maximising energy term new argmax log p x p x old Show deterministic distribution p x x f E step fails giving new old Exercise Consider objective F x U x p x positive function U x p x x f n x arbitrary distribution n x Our task maximise F respect As previous exercise showed attempt EM algorithm limit deterministic model updating occurs EM algorithm fails find optimises F0 Show F F0 x n x U x F new F old F0 new F0 old DRAFT April Exercises Show find new F new F old necessarily F0 new F0 old Using result derive EM style algorithm guarantees increase F optimum guarantees increase F0 Hint use p x U x p x F consider KL q x p x variational distribution q x Exercise The file IDjensen mat contains probability utility tables influence diagram fig 7a Using BRMLtoolbox write program returns maximal expected utility ID strong junction tree approach check result explicit summation maximisation Similarly program output maximal expected utility states d1 check computation strong junction tree agrees result explicit summation maximisation Exercise For POMDP explain structure strong junction tree relate complexity inference POMDP Exercise Define partial order ID depicted ii Draw strong junction tree ID c b d e f g h u1 u2 Exercise exerciseInvest m contains parameters simple investment problem prices assets b follow Markovian updating section The transition matrices given end time T initial wealth w1 initial price movements b wealth investment states Write function form d1 val optdec epsilonA1 epsilonB1 desired T w1 pars desired desired wealth level time T The end utility defined u wT wT 5w1 wT 5w1 Using routine compute optimal expected utility decision time Round wealth values nearest possible discrete values given exerciseInvest m Draw influence diagram describes Markov decision problem Exercise Tom writes cheques value twice You t value cheque asked pick The select value You cash cheque exchange cheque Your friend Randy explains doesn t matter chance having hand higher value cheque What think What don t look value cheque select DRAFT April Part II Learning Probabilistic Models Introduction Part II In Part II address learn model data In particular discuss learning model form inference extended distribution taking account parameters model Learning model model parameters data forces deal uncertainty limited data certain correct model We address structure model parameters principle learned In Part II learning achieved simplifying assumptions maximum likelihood set parameters likely reproduce observed data We discuss problems arise case missing data Together Part I Part II prepares basic material required embark standing models Machine Learning having tools required learn models data subsequently query answer questions interest DRAFT April Graphical model Undirected complete data decomposable unconstr counting constr limited cliques IPF exp form IS gradient non decomposable gradient difficult incomplete data decomposable exp form IS gradient non decomposable IPF difficult gradient difficult gradient Directed complete data counting incomplete data Variational EM Maximum likelihood algorithms learning graphical models The leaf nodes denote specific algorithms described Part II DRAFT April CHAPTER Statistics Machine Learning In chapter discuss classical distributions manipulations In previous chapters ve assumed know distributions concentrated inference problem In machine learning typically fully know distributions need learn available data This means need familiarity standard distributions data later set parameters Representing Data The numeric encoding data significant effect performance understanding options representing data considerable importance We briefly outline central encodings Categorical For categorical nominal data observed value belongs number classes intrinsic ordering represented simply integer An example categorical variable description type job e g healthcare education financial services transport homeworker unemployed engineering etc represented values Another way transform data numerical values use m encoding For example kinds jobs soldier sailor tinker spy represent soldier sailor tinker spy In encoding distance vectors representing different professions constant Note m encoding induces dependencies profession attributes attributes zero Ordinal An ordinal variable consists categories ordering ranking categories e g cold cool warm hot In case preserve ordering use cold cool warm hot This choice somewhat arbitrary bear mind results dependent numerical coding Numerical Numerical data takes values real numbers e g temperature measured thermometer salary earns Distributions Distributions Distributions discrete variables section focus book point Here discuss distributions continuous variables concepts marginalisation conditioning carry discrete case simply replacing summation discrete states integration continuous domain variable Definition Probability Density Functions For continuous variable x probability density p x defined p x p x dx p x b b p x dx We refer continuous probability densities distributions Definition Averages Expectation f x p x denotes average expectation f x respect distribution p x A common alternative notation E f x When context clear drop notational dependency p x The notation f x y shorthand average f x conditioned knowing state variable y e average f x respect distribution p x y An advantage expectation notations hold distribution continuous discrete variables In discrete case f x x f x x p x x continuous variables f x f x p x dx The reader wonder x means x discrete For example dom x apple orange pear associated probabilities p x states x refer Clearly f x makes sense f x x maps state x numerical value For example f x apple f x orange f x pear f x meaningful Unless states discrete variable associated numerical value x meaning Result Change variables For univariate continuous random variable x distribution p x transformation y f x f x monotonic function distribution p y p x df dx x f y For multivariate x bijection f x y f x distribution p y p x f y det f x DRAFT April Distributions Jacobian matrix elements f x ij fi x xj Sometimes needs consider transformations different dimensions For example z lower dimension x introduce additional variables z define new multivariate y z z dimension x Then applies transformation distribution joint variables y p z obtained marginalisation Definition Moments The kth moment distribution given average xk distribution xk p x For k mean typically denoted x Definition Cumulative Distribution Function For univariate distribution p x CDF defined cdf y p x y I x y p x For unbounded domain cdf cdf Definition Moment Generating Function For distribution p x define moment generating function g t g t etx p x The usefulness differentiating g t generate moments lim t dk dtk g t xk p x Definition Mode The mode x distribution p x state x distribution takes highest value x arg maxx p x A distribution mode multi modal A widespread abuse terminology refer isolated local maximum p x mode Definition Variance Correlation x x p x The variance measures spread distribution mean The square root variance called standard deviation natural length scale suggesting far typical values drawn p x mean The notation var x emphasise variable variance computed The reader equivalent expression x2 x DRAFT April Distributions For multivariate distribution matrix elements ij xi xj j xi called covariance matrix The diagonal entries covariance matrix contain variance variable An equivalent expression ij xixj xi xj The correlation matrix elements ij xi xj j j deviation variable xi The correlation normalised form covariance element bounded ij The reader note resemblance correlation coefficient scalar product equation A See exercise For independent variables xi xj xi xj covariance ij zero Similarly independent variables zero correlation uncorrelated Note converse generally true variables uncorrelated dependent A special case xi xj Gaussian distributed independence equivalent uncorrelated exercise Definition Skewness Kurtosis The skewness measure asymmetry distribution x x p x variance x respect p x A positive skewness means distribution heavy tail right Similarly negative skewness means distribution heavy tail left The kurtosis measure peaked mean distribution x x p x A distribution positive kurtosis mass mean Gaussian mean variance These called super Gaussian Similarly negative kurtosis sub Gaussian distribution mass mean corresponding Gaussian The kurtosis defined Gaussian zero kurtosis accounts term definition Definition Delta function For continuous x define Dirac delta function x x0 zero x0 spike x x0 dx x x0 f x dx f x0 One view Dirac delta function infinitely narrow Gaussian x x0 lim 0N x x0 The Kronecker delta x x0 similarly zero x0 x0 The Kronecker delta equivalent x x0 I x x0 We use expression x x0 denote Dirac Kronecker delta depending context DRAFT April Distributions Definition Empirical Distribution For set datapoints x1 xN states random variable x empirical distribution probability mass distributed evenly datapoints zero For discrete variable x empirical distribution fig p x N N n I x xn N number datapoints For continuous distribution p x N N n x xn x Dirac Delta function The mean empirical distribution given sample mean datapoints N N n xn Similarly variance empirical distribution given sample variance N N n xn For vectors sample mean vector elements N N n xni sample covariance matrix elements ij N N n xni xnj j The Kullback Leibler Divergence KL q p The Kullback Leibler divergence KL q p measures difference distributions q p Definition KL divergence For distributions q x p x KL q p log q x log p x q x Figure Empirical distribution discrete variable states The empirical samples consist n samples states 2n samples state n On normalising gives distribution values states DRAFT April Distributions The KL divergence The KL divergence widely important understand divergence positive To consider following linear bound function log x log x x plotted figure right Replacing x p x q x bound p x q x log p x q x Since probabilities non negative multiply sides q x obtain p x q x q x log p x q x log q x We integrate sum case discrete variables sides Using p x dx q x dx log p x log q x q x Rearranging gives log q x log p x q x KL q p The KL divergence zero distributions exactly Definition divergence For distributions q x p x real divergence defined D p q p x q x p x The Kullback Leibler divergence KL p q corresponds D1 p q KL q p D0 p q readily verified L Ho pital s rule Entropy information For discrete continuous variables entropy defined H p log p x p x For continuous variables called differential entropy exercise The entropy measure uncertainty distribution One way H p KL p u const u uniform distribution Since KL p u like uniform distribution p smaller entropy Or vice versa similar p uniform distribution greater entropy Since uniform distribution contains information priori state p x entropy measure priori uncertainty state occupancy For discrete distribution permute state labels changing entropy For discrete distribution entropy positive differential entropy negative The mutual information measure dependence sets variables X Y conditioned variables Z DRAFT April Classical Distributions Definition Mutual Information MI X Y Z KL p X Y Z p X Z p Y Z p Z If X Y Z true MI X Y Z zero vice versa When Z average p Z absent writes MI X Y Classical Distributions Definition Bernoulli Distribution The Bernoulli distribution concerns discrete binary variable x dom x The states merely symbolic real values p x From normalisation follows p x From x p x p x The variance given var x Definition Categorical Distribution The categorical distribution generalises Bernoulli distribu tion symbolic states For discrete variable x symbolic states dom x C p x c c c c The Dirichlet conjugate categorical distribution Definition Binomial Distribution The Binomial describes distribution discrete state variable x dom x states symbolic The probability n Bernoulli Trials independent samples x1 xn k success states observed p y k n k k n k y n I xi n k n k n k binomial coefficient The mean variance y n var y n The Beta distribution conjugate prior Binomial distribution Definition Multinomial Distribution Consider multi state variable x dom x K corresponding state probabilities K We draw n samples distribution The probability observing state y1 times state y2 times state K yK times n samples p y1 yK n y1 yK K yi n K 1yi yi ni var yi ni yiyj yi yj nij j DRAFT April Classical Distributions b Figure Exponential distribution b Laplace double exponential distribution The Dirichlet distribution conjugate prior multinomial distribution Definition Poisson Distribution The Poisson distribution model situations expected number events scales length interval events occur If expected number events unit interval distribution number events x interval t p x k k e t t k k For unit length interval t x var x The Poisson distribution derived limiting case Binomial distribution success probability scales n limit n Definition Uniform distribution For variable x distribution uniform p x const domain variable Definition Exponential Distribution For x fig 2a p x e x One rate x var x The alternative parameterisation b called scale Definition Gamma Distribution Gam x x e x x called shape parameter scale parameter Gamma function defined ta 1e tdt DRAFT April Classical Distributions b Figure Gamma distribution varying fixed b varying fixed The parameters related mean variance s s2 mean distribution s standard deviation The mode given fig An alternative parameterisation uses inverse scale Gamis x Gam x x 1e x Definition Inverse Gamma distribution InvGam x x e x This mean variance Definition Beta Distribution p x B x B x x x Beta function defined B b Figure Beta distribution The parameters written terms mean variance leading alternative parameterisation exercise DRAFT April Classical Distributions Figure Top datapoints x1 x200 drawn Gaussian distribution Each vertical line denotes datapoint corresponding x value hori zontal axis Middle Histogram equally spaced bins datapoints Bottom Gaussian distribution N x datapoints drawn In limit infinite data limitingly small bin size normalised histogram tends Gaussian probability density function x Gamma function Note distribution flipped interchanging x x equivalent interchanging See fig The mean variance given x var x Definition Laplace Distribution p x e b x For scale b x var x 2b2 The Laplace distribution known Double Exponential distribution fig 2b Definition Univariate Gaussian Distribution p x N x e x mean distribution variance This called normal distribution One parameters correspond x N x x N x For Gaussian called standard normal distribution See fig depiction univariate Gaussian samples therefrom Definition Student s t distribution p x Student x x mean degrees freedom scales distribution The variance given var x For distribution tends Gaussian mean variance As decreases tails distribution fatter DRAFT April Classical Distributions x x x x x x b x x x c x x x d Figure Dirichlet distribution parameter u1 u2 u3 displayed simplex x1 x2 x3 x1 x2 x3 Black denotes low probability white high probability b c d The t distribution derived scaled mixture p x b N x Gamis b d e x 2bae ba d ba b x This matches equation setting 2a b Definition Dirichlet Distribution The Dirichlet distribution distribution probability distri butions Q p Z u Q Q q uq q I q Z u Q q uq Q q uq It conventional denote distribution Dirichlet u The parameter u controls strongly mass distribution pushed corners simplex Setting uq q corresponds uniform distribution fig In binary case Q equivalent Beta distribution The product Dirichlet distributions Dirichlet distribution Dirichlet u1 Dirichlet u2 Dirichlet u1 u2 The marginal Dirichlet Dirichlet j Dirichlet u Dirichlet j u j DRAFT April Multivariate Gaussian b Figure Bivariate Gaussian mean covariance Plotted vertical axis probability density value p x b Probability density contours bivariate Gaussian Plotted unit eigenvectors scaled square root eigenvalues The marginal single component Beta distribution p B ui j uj Multivariate Gaussian The multivariate Gaussian plays central role data analysis discuss properties detail Definition Multivariate Gaussian Distribution p x N x det e x T x mean vector distribution covariance matrix The inverse covariance called precision One x N x x x T N x Note det M Ddet M M D D matrix explains dimension independent notation normalisation constant definition The moment representation uses parameterise Gaussian The alternative canonical repre sentation p x b M c ce xTMx xTb related moment representation M M 1b det ce bTM 1b DRAFT April Multivariate Gaussian The multivariate Gaussian widely instructive understand geometric picture This achieved viewing distribution different co ordinate system First use fact real symmetric matrix D D eigen decomposition EET ETE I diag D In case covariance matrix eigenvalues positive This means use transformation y ET x x T x x T E 1ET x yTy Under transformation multivariate Gaussian reduces product D univariate zero mean unit variance Gaussians Jacobian transformation constant This means view multivariate Gaussian shifted scaled rotated version standard zero mean unit covariance Gaussian centre given mean rotation eigenvectors scaling square root eigenvalues depicted fig 7b A Gaussian covariance I scalar example isotropic meaning rotation For isotropic distribution contours equal probability spherical origin Result Product Gaussians The product Gaussians Gaussian multi plicative factor exercise N x N x N x exp T S det 2S S mean covariance given 1S 2S 1S Completing square A useful technique manipulating Gaussians completing square For example expression exp xTAx bTx transformed follows First complete square xTAx bTx x A 1b T A x A 1b bTA 1b Hence exp xTAx bTx N x A 1b A det 2A exp bTA 1b From derive exp xTAx bTx dx det 2A exp bTA 1b DRAFT April Multivariate Gaussian Result Linear Transform Gaussian Let y linearly related x y Mx x N x N x x Then marginal p y x p y x p x Gaussian p y N y Mx MxM T Result Partitioned Gaussian Consider distribution N z defined jointly vectors x y potentially differing dimensions z x y corresponding mean partitioned covariance x y xx xy yx yy yx Txy The marginal distribution given p x N x x xx conditional p x y N x x xy yy y y xx xy 1yy yx Result Gaussian average quadratic function xTAx N x TA trace A Conditioning system reversal For joint Gaussian distribution p x y consider conditional p x y The formula Gaussian given equation An equivalent useful way write result consider reversed linear system form x Ay N marginal reverse noise equivalent conditioning That Gaussian p x y x Ay p p N suitably defined A To need statistics x linear system match given conditioning operation The mean covariance linear system equation given x Ay xx We match equation setting A xy yy xx xy 1yy yx x xy 1yy y This means write explicit linear system form equation parameters given terms statistics original system This particularly useful deriving results inference Linear Dynamical Systems section DRAFT April Exponential Family Whitening centering For set data x1 xN dim xn D transform data y1 yN zero mean centering yn xn m mean m data given m N N n xn Furthermore transform values z1 zN zero mean unit covariance whitening zn S xn m covariance S data given S N N n xn m xn m T An equivalent approach compute SVD decomposition matrix centered datapoints USVT Y Y y1 yN D N matrix Z Ndiag S1 SD D U TY columns Z z1 zN zero mean unit covariance exercise Result Entropy Gaussian The differential entropy multivariate Gaussian p x N x H x log p x p x log det D D dim x Note entropy independent mean Exponential Family A theoretically convenient class distributions exponential family contains standard distributions including Gaussian Gamma Poisson Dirichlet Wishart Multinomial Definition Exponential Family For distribution possibly multidimensional variable x continuous discrete exponential family model form p x h x exp Ti x parameters Ti x test statistics log partition function ensures normali sation log x h x exp Ti x One transform parameters form case distribution canonical form p x h x exp TT x DRAFT April Learning distributions For example univariate Gaussian written exp x exp x2 x log Defining t1 x x t2 x x2 h x log Note parameterisation necessarily unique example rescale functions Ti x inversely scale arrive equivalent representation Conjugate priors For exponential family likelihood p x h x exp TT x prior hyperparameters p exp T posterior p x p x p exp T T x p T x prior equation conjugate exponential family likelihood equation posterior form prior modified hyperparameters Whilst likelihood exponential family conjugate prior necessarily exponential family Learning distributions For distribution p x parameterised data X x1 xN learning corresponds inferring best explains data X There criteria defining Bayesian Methods In examines posterior p X p X p This gives rise distribu tion The Bayesian method says best summarise posterior Maximum A posteriori This summarisation posterior MAP argmax p X Maximum Likelihood Under flat prior p const MAP solution equivalent setting value maximises likelihood observing data ML argmax p X Moment Matching Based empirical estimate moment mean set moment moments distribution matches empirical moment Pseudo Likelihood For multivariate x x1 xN sets parameters based argmax N n D log p xni xn The pseudo likelihood method likelihood p x difficult compute DRAFT April Learning distributions In seeking best single parameter required carry numerical optimisation This necessarily trivial step considerable effort spent attempting define models resulting computational difficulties minimal finding good approximations find useful optima complex objective functions section A In book focus Bayesian methods maximum likelihood We reiterate basic ground covered section Bayesian maximum likelihood methods related Definition Prior Likelihood Posterior For data X variable Bayes rule tells update prior beliefs variable light data posterior belief p X posterior p X likelihood p prior p X evidence The evidence called marginal likelihood Note term evidence unfortunately marginal likelihood observations observations The term likelihood probability model generates observed data More fully condition model M p X M p X M p M p X M role likelihood p X M model likelihood p X M The probable posteriori MAP setting maximises posterior MAP argmax p X M For flat prior p M constant MAP solution equivalent maximum likelihood maximises p X M ML argmax p X M Definition conjugacy If posterior parametric form prior prior conjugate distribution likelihood distribution That prior parameter hyperparameter p posterior given data D form prior updated hyperparameters p D p Definition Independent Identically distributed For variable x set d observations x1 xN conditioned assume dependence observations p x1 xN N n p xn For non Bayesian methods return single value based data X interesting know good procedure Concepts help case bias consistency The bias DRAFT April Properties Maximum Likelihood measures estimate correct average The property estimator parameter converges true model parameter sequence data increases termed consistency Definition Unbiased estimator Given data X x1 xN formed d samples distribution p x use data X estimate parameter generate data The estimator function data write X For unbiased parameter estimator X p X More generally consider function distribution p x scalar value example mean x p x Then X unbiased estimator respect data distribution p X X p X A classical example estimator bias mean variance Let X N N n xn This unbiased estimator mean x p x iid data X p X N N n xn p xn N N x p x x p x On hand consider estimator variance X N N n xn X This biased omitting lines algebra X p X N N n xn X N N Properties Maximum Likelihood A crude summary posterior given distribution mass single likely state MAP definition In making approximation potentially useful information concern ing reliability parameter estimate lost In contrast posterior reflects beliefs range possibilities associated credibilities The term maximum likelihood refers parameter observed data likely generated model One motivate MAP decision theoretic perspective If assume utility zero correct U true I true expected utility U true I true p true X p X This means maximum utility decision return highest posterior value DRAFT April Properties Maximum Likelihood When flat prior p const MAP parameter assignment equivalent maximum likelihood setting ML argmax p X Since logarithm strictly increasing function positive function f opt argmax f opt argmax log f MAP parameters found optimising MAP objective equivalently logarithm log p X log p X log p log p X normalisation constant p X function The log likelihood convenient d assumption summation data terms log p X n log p xn log p log p X quantities derivatives log likelihood w r t straightforward compute Training assuming correct model class Consider dataset X xn n N generated underlying parametric model p x Our interest fit model p x form correct underlying model p x examine limit large data parameter learned maximum likelihood matches correct parameter Our derivation non rigorous highlights essence argument Assuming data d scaled log likelihood L N log p X N N n log p xn In limit N sample average replaced average respect distribution generating data L N log p x p x KL p x p x log p x p x Up negligible constant Kullback Leibler divergence distributions x different parameter settings The maximises L minimises Kullback Leibler divergence In limit large data principle learn correct parameters assuming know correct model class That maximum likelihood consistent estimator Training assumed model incorrect We write q x assumed model p x correct generating model Repeating calculations case assumed model correct limit large data scaled log likelihood L log q x p x KL p x q x log p x p x Since q p form setting necessarily minimise KL p x q x necessarily optimize L DRAFT April Learning Gaussian Maximum likelihood empirical distribution Given dataset discrete variables X x1 xN define empirical distribution q x N N n I x xn case x vector variables I x xn I xi xni The Kullback Leibler divergence empirical distribution q x distribution p x KL q p log q x q x log p x q x Our interest functional dependence KL q p p Since entropic term log q x q x indepen dent p x consider constant focus second term Hence KL q p log p x q x const N N n log p xn const We recognise N n log p x n log likelihood model p x assuming data d This means setting parameters maximum likelihood equivalent setting parameters minimising Kullback Leibler divergence empirical distribution parameterised distribution In case p x unconstrained optimal choice set p x q x maximum likelihood optimal distribution corresponds empirical distribution Learning Gaussian Given importance Gaussian distribution instructive explicitly consider maximum likelihood Bayesian methods fitting Gaussian data Maximum likelihood training Given set training data X x1 xN drawn Gaussian N x unknown mean covariance find parameters Assuming data drawn d log likelihood L N n log p x N n xn T xn N log det Optimal Taking partial derivative respect vector obtain vector derivative L N n xn Equating zero gives optimum log likelihood N n 1xn N optimally given sample mean N N n xn DRAFT April Learning Gaussian Prior b Posterior Figure Bayesian approach inferring mean precision inverse variance Gaussian based N randomly drawn datapoints A Gauss Gamma prior b Gauss Gamma posterior conditional data For comparison sample mean data maximum likelihood optimal variance computed N normalisation The datapoints drawn Gaussian mean variance See demoGaussBayes m Optimal The derivative L respect matrix requires work It convenient isolate dependence covariance parameterise inverse covariance L trace N n xn xn T M N2 log det Using M MT obtain L M N Equating derivative zero matrix solving gives sample covariance N N n xn xn T Equations define maximum likelihood solution mean covariance training data X Consistent previous results fact equations simply set parameters sample statistics empirical distribution That mean set sample mean data covariance sample covariance Bayesian inference mean variance For simplicity deal univariate case Assuming d data likelihood p X N exp N n xn For Bayesian treatment require posterior parameters p X p X p p X p p Our aim find conjugate priors mean variance A convenient choice prior mean Gaussian centred p exp DRAFT April Learning Gaussian The posterior p X N exp n xn p It convenient write form p X p X p X Since equation quadratic contributions exponent conditional posterior p X Gaussian To identify Gaussian multiply terms exponent arrive exp a2 2b c N b n x n c n xn Using identity a2 2b c b c b write p X aexp b p X exp c b N p p X We encounter difficulty attempting find conjugate prior term b2 simple expression For reason constrain fixed hyperparameter Defining constants N b n xn c n xn c b c b Using expression equation obtain p X N exp c b p An inverse Gamma distribution prior p conjugate For Gauss Inverse Gamma prior p N InvGam posterior Gauss Inverse Gamma p X N b InvGam N c b DRAFT April Learning Gaussian Gauss Gamma distribution It common use prior precision defined inverse variance If use Gamma prior p Gam 1e posterior p X Gam N c b The Gauss Gamma prior distribution p N Gam conjugate prior Gaussian unknown mean precision The posterior prior Gauss Gamma distribution parameters p X N b Gam N The marginal p X Student s t distribution An example Gauss Gamma prior posterior given fig The maximum likelihood solution recovered limit flat prior exercise The unbiased estimators mean variance given prior exercise For multivariate case extension techniques uses multivariate Gaussian distribution conjugate prior mean Inverse Wishart distribution conjugate prior covariance Summary Classical univariate distributions include exponential Gamma Beta Gaussian Poisson A classical distribution distributions Dirichlet distribution Multivariate distributions difficult deal computationally A special case multivariate Gaussian marginals normalisation constants computed time cubic number variables model A useful measure difference distributions Kullback Leibler divergence Bayes rule enables achieve parameter learning translating priori parameter belief posterior parameter belief based observed data Bayes rule says best summarise posterior distribution Conjugate distributions prior posterior distribution different parameters Maximum likelihood corresponds simple summarisation posterior flat prior DRAFT April Exercises Provided correct model class maximum likelihood learn optimal parameters limit large data guarantee Code demoGaussBayes m Bayesian fitting univariate Gaussian logGaussGamma m Plotting routine Gauss Gamma distribution Exercises Exercise In public lecture following phrase uttered Professor Experimental Psy chology In recent data survey people claim average intelligence clearly nonsense Audience Laughs Is theoretically possible people average intelli gence If example explain What median intelligence Exercise Consider distribution defined real variables x y p x y x2 y2 2e x2 y2 dom x dom y Show x y Furthermore x y uncorrelated xy x y Whilst x y uncorrelated dependent Exercise For variable x dom x p x n independent draws x1 xn distribution probability observing k states Binomial distribution n k k n k Exercise Normalisation constant Gaussian The normalisation constant Gaussian distribu tion related integral I e x2dx By considering I2 e x2dx e y2dy e x y2 dxdy transforming polar coordinates x r cos y r sin dxdy rdrd r I e x dx Exercise For univariate Gaussian distribution x N x x N x DRAFT April Exercises Exercise Using xTAx trace AxxT derive result Exercise Show marginal Dirichlet distribution Dirichlet distribution j Dirichlet u Dirichlet j u j Exercise For Beta distribution xk B k B k k k k x x x Exercise For moment generating function g t etx p x lim t dk dtk g t xk p x Exercise Change variables Consider dimensional continuous random variable x corresponding p x For variable y f x f x monotonic function distribution y p y p x df dx x f y More generally vector variables y f x p y p x f y det f x Jacobian matrix elements f x ij fi x xj Exercise Normalisation Multivariate Gaussian Consider I exp x T x dx By transformation z x I det Exercise Consider partitioned matrix M A B C D wish find inverse M We assume A m m invertible D n n invertible By definition partitioned inverse M P Q R S DRAFT April Exercises satisfy A B C D P Q R S I am In I am m m identity matrix zero matrix dimension D Using derive results P A BD 1C Q A 1B D CA 1B R D 1C A BD 1C S D CA 1B Exercise Show Gaussian distribution p x N x skewness kurtosis zero Exercise Consider small interval time t let probability event occurring small interval t Derive distribution expresses probability event interval t Exercise Consider vector variable x x1 xn T set functions defined component x xi For example x x1 x2 T x1 x1 x2 x22 Consider distribution p x Z exp T x x vector function ith component xi parameter vector Each component tractably integrable sense exp ii xi dxi computed analytically acceptable numerical accuracy Show xi xj The normalisation constant Z tractably computed Consider transformation x My invertible matrix M Show distribution p y M tractable normalisation con stant known general yi yj Explain significance deriving tractable multivariate distributions Exercise Show reparameterise Beta distribution definition writing parameters functions mean m variance s m m s Exercise Consider function f DRAFT April Exercises lim f log log d lim f d f d Using result log B logB B Beta function Show additionally log B logB Using fact B x gamma function relate averages digamma function defined x d dx log x Exercise Using similar generating function approach exercise explain compute log Dirichlet u Exercise Consider function f x n x ui d1 dn Show Laplace transform f s e sxf x dx f s n e si ui di s ui n ui By inverse Laplace transform s1 q xq q f x n ui ui x ui Hence normalisation constant Dirichlet distribution parameters u given n ui ui Exercise Derive formula differential entropy multi variate Gaussian Exercise Show gamma distribution Gam x mode given x provided Exercise Consider distribution p x distribution p x small DRAFT April Exercises Take Taylor expansion KL p x p x small equal log p x p x More generally distribution parameterised vector elements small change parameter results j ij Fij Fisher Information matrix defined Fij j log p x p x Show Fisher information matrix positive semidefinite expressing equivalently Fij log p x j log p x p x Exercise Consider joint prior distribution p N Gam Show prior distribution flat independent Show settings mean variance jointly maximise posterior equation given standard maximum likelihood settings N n xn N n xn Exercise Show equation limit jointly optimal mean variance obtained argmax p X given N n xn N n xn Note correspond standard unbiased estimators mean variance Exercise For Gauss Gamma posterior p X given equation compute marginal posterior p X What mean distribution Exercise This exercise concerns derivation equation By considering p y p y x p x dx exp y Mx T y Mx x x T 1x x x dx p y exp yT 1y exp xTAx xT By c suitably defined A B c DRAFT April Exercises Using equation p y exp yT 1y By c T A By c This establishes p y Gaussian We need find mean covariance Gaussian We lengthly process completing square Alternatively appeal y M x Mx By considering y y y y T Mx Mx Mx Mx T independence x derive formula covariance p y Exercise Consider multivariate Gaussian distribution p x N x vector x components x1 xn p x det e x T x Calculate p xi x1 xi xi xn Hint use equation Exercise Observations y0 yn noisy d measurements underlying variable x p x N x p yi x N yi x n Show p x y0 yn Gaussian mean n20 n20 y y y0 y1 yn n variance n 2n n Exercise Consider set data X x1 xN xn independently drawn Gaussian known mean unknown variance Assume gamma distribution prior p Gamis b Show posterior distribution p X Gamis N b N n xn Show distribution x p x X p x p X d Student x b 2a N b b N n x n Exercise The Poisson distribution discrete distribution non negative integers p x e x x x You given sample n observations x1 xn independently drawn distribution Determine maximum likelihood estimator Poisson parameter DRAFT April Exercises Exercise For Gaussian mixture model p x piN x pi pi p x mean x pii covariance pi T pii j pj T j Exercise Show whitened data matrix given equation ZZT NI Exercise Consider uniform distribution pi N defined states N Show entropy distribution H N pi log pi logN number states N increases infinity entropy diverges infinity Exercise Consider continuous distribution p x x We form discrete approximation probabilities pi continuous distribution identifying continuous value N state N With pi p N p N entropy H pi log pi given H p N p N log p N log p N Since continuous distribution p x dx discrete approximation integral bins size N gives N N p N Hence large N H p x log p x dx const constant tends infinity N Note result says continuous distribu tion essentially infinite number states uncertainty distribution infinite alternatively need infinite number bits specify continuous value This motivates definition differential entropy neglects infinite constant limiting case discrete entropy Exercise Consider multivariate Gaussians N x N x DRAFT April Exercises Show log product Gaussians given xT x xT T1 T log det det Defining A b write x A 1b T A x A 1b bTA 1b T1 T log det det Writing A A 1b product Gaussians Gaussian covariance mean log prefactor bTA 1b T1 T log det det log det Show written N x N x N x exp T S det 2S S Exercise Show log p x p x Exercise Using f2 x f x suitably chosen function f x p x q x p x distributions q x p x This related divergence Exercise Show D p p D p q distributions p x q x Exercise Show D dimensional Gaussians 2KL N x N x trace T log det D Exercise For data pairs xn yn n N correlation considered measure extent linear relationship holds x y For simplicity consider x y zero mean wish consider validity linear relation x y A measure discrepancy linear assumption given E N n xn yn DRAFT April Exercises Show minimises E given c 2y c N n xnyn 2y N n yn A measure linearity x y E E linear relation perfect linear relation E Show correlation coefficient definition given E E Defining vectors x x1 xN T y y1 yN T correlation coefficient cosine angle x y xTy x y Show general relation x y constant offset setting minimise E N n xn yn effect simply replacing xn xn x yn yn y x y mean x y data respectively Exercise For variables x y z x y correlation coefficients related x z x y With reference correlation coefficient angle vectors explain x z x y geometrically obvious Exercise Consider Boltzman machine distribution binary variables xi D p x W Zp W exp xTWx wish fit distribution q form p q x U Zq U exp xTUx Show argmin U KL p q argmax U trace UC logZq U Cij xixj p Hence knowing cross moment matrix C p sufficient fully specify p DRAFT April Exercises Generalise result models exponential family Exercise Consider distribution N z defined jointly vectors x y potentially differing dimensions z x y corresponding mean partitioned covariance x y xx xy yx yy yx Txy By considering p x y p x y p x y exp x x T P x x x x T Q y y xx xy yx yy P Q QT S By completing square p x y exp x x P 1Q y y T P x x P 1Q y y Using partitioned matrix inversion yxP yyQ T P 1Q xy 1yy Show P xx xy 1yy yx Hence p x y N x x xy yy y y xx xy 1yy yx Exercise As question consider joint Gaussian distribution p z Consider p x p x y dy p z dy Show p x exp x x T P x x exp x x T Q y y y y T S y y dy By completing square x x T Q y y y y T S y y y y S 1QT x x T S y y S 1QT x x x x T QS 1QT x x DRAFT April Exercises Using fact exp y y S 1QT x x T S y y S 1QT x x dy det 2S p x exp x x T P QS 1QT x x Since xx xy yx yy P Q QT S Use partitioned matrix inversion xx P QS 1QT Hence p x N x x xx Exercise Consider necessarily square matrix T Gaussian distributed x p x N x We interested distribution variable y Tx dim y dim x Define z y x T I M x I matrix M square invertible Show p z p z x p x dx z Mx p x dx exp M 1z T M 1z Show p z N z M MMT Using p y p z dx result marginal Gaussian Gaussian exercise p y N y T TTT DRAFT April CHAPTER Learning Inference In previous chapters largely assumed distributions fully specified inference tasks In machine learning related fields distributions need learned basis data Learning problem integrating data domain knowledge model environment In chapter discuss learning phrased inference problem Learning Inference Learning bias coin Consider data expressing results tossing coin We write vn toss n coin comes heads vn tails Our aim estimate probability coin head p vn called bias coin For fair coin The variables environment v1 vN require model probabilistic interaction variables p v1 vN Assuming dependence observed tosses belief network p v1 vN p N n p vn depicted fig The assumption observation independent identically dis tributed called d assumption Learning refers observations v1 vN infer In context interest p v1 vN p v vN p v1 vN p v1 vN p p v1 vN We need fully specify prior p To avoid complexities resulting continuous variables ll consider discrete possible states Specifically assume p p p shown fig 2a This prior expresses belief coin fair belief coin biased land heads belief coin biased land tails The distribution given data beliefs p v1 vN p N n p vn p N n I v n I v n p N n I v n N n I v n Learning Inference v1 v2 v3 vN vn N b Figure Belief network coin tossing model b Plate notation equiv alent A plate replicates quanti ties inside plate number times specified plate In N n I v n number occurrences heads conveniently denote NH Likewise N n I v n number tails NT Hence p v1 vN p NH NT For experiment NH NT posterior distribution p V k k p V k k p V k k V shorthand v1 vN From normalisation requirement k p V p V p V shown fig 2b These posterior parameter beliefs In case asked choose single posteriori likely value confidence low posterior belief appreciable This result intuitive observed Tails Heads prior belief likely coin fair Repeating NH NT posterior changes p V p V p V fig 2c posterior belief dominates This reasonable situation tails heads unlikely occur fair coin Even priori thought coin fair posteriori evidence change minds Making decisions In Bayesian posterior merely represents beliefs says best summarise beliefs In situations decisions need taken uncertainty need additionally specify utility decision chapter b c Figure Prior encoding liefs coin biased heads b Posterior having seen NH heads NT tails c Posterior having seen NH heads NT tails Assuming value possible ML setting NH NT NH NT DRAFT April Learning Inference In coin tossing scenario assumed setup decision problem follows If correctly state bias coin gain points incorrect loses points We write U 10I 20I true value bias The expected utility decision coin U U p V U p V U p V Plugging numbers equation obtain U Similarly U U The best highest utility coin unbiased Repeating calculations NH NT arrive U U U best decision case choose As information distribution p v available posterior p V increas ingly peaked aiding decision making process A continuum parameters In section considered possible values Here discuss continuum parameters Using flat prior We examine case flat uniform prior p k constant k For continuous variables normalisation requires p d Since represents probability p d k Repeating previous calculations flat continuous prior p V c NH NT c constant determined normalisation c NH NT d B NH NT B Beta function See fig example DRAFT April Learning Inference Figure Posterior p V assuming flat prior blue NH NT red NH NT In cases probable state posterior maximum posteriori makes intuitive sense fraction Heads Tails cases Where data posterior certain sharpens probable value Using conjugate prior Determining normalisation constant continuous distribution requires integral unnor malised posterior carried For coin tossing case clear prior form Beta distribution posterior parametric form For prior p B posterior p V NH NT p V B NH NT The prior posterior form Beta distributions simply different parameters Hence Beta distribution conjugate Binomial distribution Decisions based continuous intervals To illustrate use continuous variables decision making consider simple decision problem The result coin tossing experiment NH heads NT tails You need decision win dollars correctly guess way coin biased heads tails If guess incorrect lose million dollars What decision Assume uninformative prior We need quantities guess truth Then utility saying Heads U p V U p V In p V p V d0 B NH NT NH NT d I0 NH NT Ix b regularised incomplete Beta function For case NH NT flat prior p V I0 NH NT Since events exclusive p V Hence expected utility saying heads likely Similarly utility saying tails likely computed DRAFT April Bayesian methods ML II c s cn sn c s n N b Figure A model relationship lung Cancer Asbestos exposure Smoking b Plate notation replicating observed n datapoints CPTs tied datapoints v v b Figure Standard ML learning The best parameter found maximising probability model generates observed data opt arg max p v b ML II learning In cases prior preference parameters unspecified hyperparameter find opt arg max p v arg max p v p Since expected utility deciding tails highest better taking decision coin likely come tails If modify lose million dollars guess tails fact heads expected utility saying tails In case confident coin likely come tails pay penalty making mistake saying tails fact better heads Bayesian methods ML II Consider parameterised distribution p v wish learn optimal parameters given data The model p v depicted fig 5a dot indicates distribution present variable For single observed datapoint v setting maximum likelihood corresponds finding parameter maximises p v In cases idea parameters appropriate express prior preference distribution p If prior fully specified learn p v fully known However cases practice unsure exact parameter settings prior specify parametersised prior distribution p hyperparameter This depicted fig 5b Learning corresponds finding optimal maximises likelihood p v p v p This known ML II procedure corresponds maximum likelihood higher hyperparameter level By treating parameters variables view learning hidden variables methods chapter applicable We encounter examples ML II procedure later example section DRAFT April Maximum Likelihood Training Belief Networks s c Figure A database containing information Asbestos exposure signifies exposure Smoker signifies individual smoker lung Cancer signifies individual lung Cancer Each row contains information seven individuals database Maximum Likelihood Training Belief Networks Consider following model relationship exposure asbestos smoker s incidence lung cancer c p s c p c s p p s depicted fig 4a Each variable binary dom dom s dom c We assume direct relationship Smoking exposure Asbestos This kind assumption able elicit medical experts Furthermore assume list patient records fig row represents patient s data To learn table entries p c s counting number times variable c state parental states s p c s p c s p c s p c s Similarly based counting p p s These CPTs complete distribution specification Setting CPT entries way counting relative number occurrences corresponds mathemat ically maximum likelihood learning d assumption Maximum likelihood corresponds counting For BN constraint form p x p x K p xi pa xi To compute maximum likelihood setting term p xi pa xi shown section equivalently minimise Kullback Leibler divergence empirical distribution q x p x For BN p x empirical distribution q x KL q p K log p xi pa xi q x const K log p xi pa xi q xi pa xi const This follows general result f Xi q X f Xi q Xi says function f depends subset variables need know marginal distribution subset variables order carry average Since q x fixed add entropic terms q equivalently mimimize KL q p K log q xi pa xi q xi pa xi log p xi pa xi q xi pa xi const K KL q xi pa xi p xi pa xi q pa xi const DRAFT April Maximum Likelihood Training Belief Networks The final line positive weighted sum individual Kullback Leibler divergences The minimal Kullback Leibler setting corresponds maximum likelihood p xi pa xi q xi pa xi In terms original data p xi s pa xi t N n I xni s pa x n t This expression corresponds intuition table entry p xi pa xi set counting number times state xi s pa xi t occurs dataset t vector parental states The table given relative number counts state s compared states s fixed joint parental state t An alternative method derive intuitive result use Lagrange multipliers exercise For reader comfortable Kullback Leibler derivation direct example given makes use notation x1 s1 x2 s2 x3 s3 denote number times states x1 s1 x2 s2 x3 s3 occur training data See section examples Example We wish learn table entries distribution p x1 x2 x3 p x1 x2 x3 p x2 p x3 We address find CPT entry p x1 x2 x3 maximum likelihood For d data contribution p x1 x2 x3 log likelihood n log p xn1 xn2 xn3 The number times p x1 x2 x3 occurs log likelihood x1 x2 x3 number occurrences training set Since normalisation constraint p x1 x2 x3 p x1 x2 x3 total contribution p x1 x2 x3 log likelihood x1 x2 x3 log p x1 x2 x3 x1 x2 x3 log p x1 x2 x3 Using p x1 x2 x3 x1 x2 x3 log x1 x2 x3 log Differentiating expression w r t equating zero gives x1 x2 x3 x1 x2 x3 The solution optimal p x1 x2 x3 x1 x2 x3 x1 x2 x3 x1 x2 x3 corresponding intuitive counting procedure DRAFT April Maximum Likelihood Training Belief Networks x1 x2 xn xn y Figure A variable y large number parents x1 xn requires specification exponen tially large number entries conditional prob ability p y x1 xn One solution difficulty parameterise conditional p y x1 xn Conditional probability functions Consider binary variable y n binary parental variables x x1 xn fig There n entries CPT p y x infeasible explicitly store entries moderate values n To reduce complexity CPT constrain form table For example use function p y x w e w Tx need specify n dimensional parameter vector w In case maximum likelihood learn entries CPTs directly instead learn value parameter w Since number parameters w small n compared 2n unconstrained case hope small number training examples learn reliable value w Example Consider following variable model p x1 x2 x3 p x1 x2 x3 p x2 p x3 xi We assume CPT parameterised p x1 x2 x3 e x2 x3 One verify probability positive lies Due normalisation p x1 x2 x3 p x1 x2 x3 For unrestricted p x2 p x3 maximum likelihood setting p x2 x2 p x3 x3 The contribution log likelihood term p x1 x2 x3 assuming d data L N n I xn1 xn2 xn3 I xn1 log e xn2 xn3 This objective function needs optimised numerically find best The gradient dL d1 N n 2I xn1 2I xn1 1e x n x n e xn2 xn3 dL d2 N n 2I xn1 xn2 xn3 22I xn1 xn2 xn3 2e x n x n e xn2 xn3 The gradient standard optimisation procedure conjugate gradients section A find maximum likelihood parameters DRAFT April Bayesian Belief Network Training Bayesian Belief Network Training An alternative maximum likelihood training BN use Bayesian approach maintain distribution parameters We continue Asbestos Smoking Cancer scenario p c s p c s p p s represented fig 4a So far ve specified independence structure entries tables p c s p p s Given set visible observations V sn cn n N like learn appropriate distributions table entries To begin need notation table entries With variables binary parameters p p c s c 1c similarly remaining parameters c c c For example parameters s c c c c c In following section section describe useful independence assumptions general form prior variables making specific numerical prior specification section Global local parameter independence In Bayesian learning BNs need specify prior joint table entries Since general dealing multi dimensional continuous distributions computationally problematic useful specify uni variate distributions prior As pleasing consequence d data posterior factorises uni variate distributions Global parameter independence A convenient assumption prior factorises parameters For Asbestos Smoking Cancer example assume p s c p p s p c Assuming data d joint model p s c V p p s p c n p p sn s p cn sn c belief network given fig A convenience factorised prior BN posterior factorises p s c V p s c V p n p p s n p sn s p c n p cn sn c p Va p s Vs p c Vc consider parameter posterior separately In case learning involves computing posterior distributions p Vi Vi set training data restricted family variable The global independence assumption conveniently results posterior distribution factorises conditional tables However parameter c dimensional To simplify need assumption structure local table DRAFT April Bayesian Belief Network Training cn sn s c s n N s P Figure A Bayesian parameter model relationship lung Cancer Asbestos exposure Smoking factorised parameter priors The global parameter independence assumption means prior tables factorises priors conditional probability ta ble The local independence assumption case comes effect p c s means p c factorises s P p s c P Local parameter independence If assume prior table factorises states c p c p c p c p c p c posterior given p c Vc p Vc c p 0c p 0c p 1c p 1c 0c s c 0c s c p 0c p 0c Vc 1c s c 1c s c p 1c p 1c Vc 0c s c 0c s c p 0c p 0c Vc 1c s c 1c s c p 1c p 1c Vc posterior factorises parental states local conditional table Posterior marginal table A marginal probability table given example p c s V c p c s 0c p c Vc The integral tables equation unity left p c s V c p c s 0c p 0c Vc c 0c p c Vc Learning binary variable tables Beta prior We continue example section variables binary continuous valued table prior The simplest case start p requires univariate prior distribution p The likelihood depends table variable p total likelihood term DRAFT April Bayesian Belief Network Training The posterior p Va p This means prior form conjugacy hold mathematics integration straightforward This suggests convenient choice Beta distribution p B B 1a posterior Beta distribution p Va B The marginal table given following similar reasoning equation p Va p Va result mean Beta distribution definition The situation table p c s slightly complex need specify prior parental tables As convenient specify Beta prior parental states Let s look specific table p c s Assuming local independence property p c Vc given B 0c c s c s c s c s As marginal probability table given p c s Vc c s c s c s c s s s c s c s The prior parameters c s called hyperparameters A complete ignorance prior correspond setting fig It instructive examine Bayesian solution conditions No data limit N In limit data marginal probability table corresponds prior given case p c s c s c s c s For flat prior states c prior probability p c s Infinite data limit N In limit marginal probability tables dominated data counts typically grow proportion size dataset This means infinite large data limit p c s V c s c s c s corresponds maximum likelihood solution This effect large data limit Bayesian procedure corresponds maximum likelihood solution general prior pathologically strong effect DRAFT April Bayesian Belief Network Training Zero hyperparameter limit When c c marginal table equation corresponds maximum likelihood table setting data When c c Beta distribution places mass mass Note equivalence maximum likelihood solution marginal table zero hyperparameter values contrasts equivalence MAP table uniform hyperparameter values Example Asbestos Smoking Cancer Consider binary variable network p c s p c s p p s The data V given fig Using flat Beta prior conditional probability tables marginal posterior tables given p V N By comparison maximum likelihood setting The Bayesian result little cautious squares prior belief setting probability equally likely pulling posterior Similarly p s V s N p c s V c s c s c s p c s V c s c s c s p c s V c s c s c s p c s V c s c s c s Learning multivariate discrete tables Dirichlet prior The natural generalisation discussion Bayesian learning BNs consider variables states In case natural conjugate prior given Dirichlet distribution generalises Beta distribution states Again assume d data local global parameter prior independencies Since global parameter independence assumption posterior factorises variables equation concentrate posterior single variable DRAFT April Bayesian Belief Network Training No parents Let s consider variable v dom v I If denote probability v state e p v contribution posterior datapoint vn p vn I I vn I posterior given dataset V v1 vN p V p N n I I vn p I N n I v n It convenient use Dirichlet prior distribution hyperparameters u p Dirichlet u I ui Using prior posterior p V I ui I N n I v n I ui N n I v n means posterior given p V Dirichlet u c c count vector components ci N n I vn number times state observed training data The marginal table given integrating p v V p v p V ip V The single variable marginal distribution Dirichlet Beta distribution p V B ui ci j uj cj The marginal table given mean Beta distribution p v V ui ci j uj cj generalises binary state formula equation Parents To deal general case variable v parents pa v denote probability v state conditioned parents state j p v pa v j v j DRAFT April Bayesian Belief Network Training s c Figure A database patient records Asbestos exposure signifies ex posure Smoker signifies individual smoker lung Cancer signifies cancer signifies early stage cancer signifies late state cancer Each row contains information seven individuals database v j This forms components vector v j Note v K parents number parental states S exponential K Writing v v v S local parental state independence means p v j p v j global independence means p v p v v v V represents combined table variables Parameter posterior Thanks global parameter independence assumption posterior factorises posterior table variable Each posterior table variable v depends data D v family variable Assuming Dirichlet distribution prior p v j Dirichlet v j u v j posterior Dirichlet p v D v j Dirichlet v j u v j hyperparameter prior term updated observed counts u v j ui v j v pa v j By analogy parents case marginal table given p v pa v j D v u v j Example Consider p c s p s p asbestos example dom dom s variable c taking states dom c accounting different kinds cancer fig The marginal table Dirichlet prior given example p c s V u0 s c s ui s c s Assuming flat Dirichlet prior corresponds setting components u gives p c s V p c s V p c s V similarly tables p c s p c s p c s DRAFT April Structure learning Algorithm PC algorithm skeleton learning Start complete undirected graph G set V vertices repeat x V y Adj x Determine subset S size neighbours x including y x y S If set exists remove x y link graph G set Sxy S end end nodes neighbours Model likelihood For variable v d data D v vn pa vn n N family variable n p vn pa vn v p v n p vn pa vn v v j Z u v j v j ui v j n j v j I vn pa vn j j Z u v j v j v j ui v j v pa v j j Z u v j Z u v j Z u normalisation constant Dirichlet distribution hyperparameters u u given equation For belief network variables v v1 vD joint probability variables factorises local probabilities variable conditioned parents The likelihood complete set d data D v1 vN given p D k n p vnk pa vnk k j Z u vk j Z u vk j u given equation Expression written explicitly terms Gamma functions exercise In expression general number parental states differs variable vk implicit formula state product j goes number parental states variable vk Due local global parameter independence assumptions logarithm model likelihood product terms variable vk parental configuration j This called likelihood decomposable property Structure learning Up point assumed given structure distribution dataset D A complex task need learn structure network We ll consider case data complete e missing observations Since D variables exponentially large number D BN structures s clear search possible structures For reason structure learning computationally challenging problem rely constraints heuristics help guide search Whilst general structure learning intractable cele brated tractable special case network constrained parent section DRAFT April Structure learning For sparsest networks estimating dependencies accuracy requires large data making testing dependencies difficult Consider following simple situation independent variables p x y p x p y Based finite sample joint distributionD xn yn n N want try understand x independent y One way compute empirical mutual information I x y zero empirically x y independent However finite data variables typically non zero mutual information threshold needs set decide measured dependence significant finite sample section Other complexities arise concern Belief Markov Network visible variables parsimonious way represent observed data example latent variables driving observed dependencies We enter issues discussion limit presentation central approaches attempts network structure consistent local empirical dependencies PC algorithm builds structure probable global data network scoring PC algorithm The PC algorithm learns skeleton graph edges oriented form partially oriented DAG The procedure learn skeleton based empirical data test variables independent A variety approaches ascertain independence described section The PC algorithm begins round complete skeleton G attempts remove links possible At step test pairs x y If x y pair deemed independent link x y removed complete graph One repeats pairwise links In second round remaining graph examines x y link conditions single neighbour z x If x y z remove link x y One repeats way variables At round number neighbours conditioning set increased See algorithm fig demoPCoracle m A refinement algorithm known NPC necessary path PC limits number independence checks remove inconsistencies resulting empirical estimates condi tional mutual information Given learned skeleton partial DAG constructed algorithm Note necessary undirected graph G skeleton belief network independence assumptions discovered For example graph G x z y x y link removed basis x y Sxy As MN graph x z y graphically implies x y inconsistent discovery round x y This reason orientation consistency x z y x y x y z example See fig Example Skeleton orienting z x y x y z x y If x unconditionally independent y z collider marginalis ing z introduce dependence x y z x y x y z z x y If x independent y conditioned z z collider Any orientation ap propriate 1This example appears thanks Seraf n Moral online notes DRAFT April Structure learning t z w x y t z w x y b t z w x y c t z w x y d t z w x y e t z w x y f t z w x y g t z w x y h t z w x y t z w x y j t z w x y k t z w x y l t z w x y m t z w x y n t z w x y o t z w x y p t z w x y q t z w x y r Figure PC algorithm The BN data assumed generated conditional independence tests performed b The initial skeleton fully connected c l In round pairwise mutual informations x y checked link x y removed deemed independent green line m o We look connected subsets variables x y z remaining graph removing link x y x y z true Not steps shown p q We examine x y b The algorithm terminates round gets incremented nodes neighbours r Final skeleton During process sets Sx y Sx w Sz w y Sx t z w Sy t z w found See demoPCoracle m Algorithm Skeleton orientation algorithm returns DAG Unmarried Collider Examine undirected links x z y If z Sxy set x z y repeat x z y x z y For x y directed path x y orient x y If x z y w x w y w z w orient z w No edges oriented The remaining edges arbitrarily oriented provided graph remains DAG additional colliders introduced Example In fig describe processes PC algorithm learning structure belief network variables x y z w t In case data assess independence assume access oracle correctly answer independence question In practice course fortunate Once skeleton found orient skeleton fig Empirical independence Mutual information test Given data obtain estimate conditional mutual information empirical distri bution p x y z estimated simply counting occurrences data In practice finite data estimate empirical distribution This means data sampled distribution variables truly independent empirical mutual information neverthe typically greater zero An issue threshold use empirical conditional mutual information decide sufficiently far zero caused dependence A frequen tist approach compute distribution conditional mutual information DRAFT April Structure learning sample value compared distribution According null hypothesis vari ables independent 2NMI x y z Chi square distributed X Y Z degrees freedom dim x X dim y Y dim z Z This form hypothesis test sample value empirical mutual information significantly tails chi square distribution deem variables conditionally dependent This classical approach work large amounts data effective case small amounts data An alternative pragmatic approach estimate threshold based empirical samples MI controlled independent dependent conditions demoCondindepEmp m comparison approaches t z w x y t z w x y b t z w x y c t z w x y d Figure Skeleton orientation algorithm The skeleton Sx y Sx w Sz w y Sx t z w Sy t z w b z Sx y form collider c t Sz w form collider d Final partially oriented DAG The remaining edge oriented desired violating DAG condi tion See demoPCoracle m Bayesian conditional independence test A Bayesian approach testing independence comparing likelihood data independence hypothesis versus likelihood dependent hypothesis For independence hypothesis fig 12a joint distribution variables parameters p x y z Hindep p x z x z p y z y z p z z p x z p y z p z For categorical distributions convenient use prior Dirichlet u parameters assuming local global parameter independence For set assumed d data X Y Z xn yn zn n N likelihood given integrating parameters p X Y Z Hindep p Hindep n p xn yn zn Hindep Thanks conjugacy straightforward gives expression p X Y Z Hindep Z uz z Z uz z Z ux z x z Z ux z Z uy z y z Z uy z ux z hyperparameter matrix pseudo counts state x given state z Z v normalisation constant Dirichlet distribution vector parameter v For dependent hypothesis fig 12b p x y z Hdep p x y z x y z p x y z The likelihood p X Y Z Hdep Z ux y z x y z Z ux y z Assuming hypothesis equally likely Bayes Factor p X Y Z Hindep p X Y Z Hdep greater assume conditional independence holds assume variables con ditionally dependent demoCondindepEmp m suggests Bayesian hypothesis test tends outperform conditional mutual information approach particularly small sample size case fig DRAFT April Structure learning zn xn y n z x z y z N xn yn zn x y z N b Figure Bayesian conditional independence test Dirichlet priors tables A model Hindep conditional independence x y z b A model Hdep conditional dependence x y z By computing likelihood data model numerical score validity con ditional independence assumption formed See demoCondindepEmp m z1 x y z2 Figure Conditional independence test x y z1 z2 x y z1 z2 having states respectively From oracle belief network shown experiment tables drawn random examples sampled form dataset For dataset test carried determine x y independent conditioned z1 z2 correct answer independent Over experiments Bayesian conditional independence test correctly states variables con ditionally independent time compared accuracy chi square mutual information test See demoCondindepEmp m Network scoring An alternative local methods PC algorithm evaluate network structure set variables v That wish ascertain belief network particular structure p v k p vk pa vk fits data In probabilistic context given model structure M wish compute p M D p D M p M Some care needed fit model parameters p v M data D If maximum likelihood constraints end favouring model M complex structure assuming p M const This remedied Bayesian technique p D M p D M p M In case directed networks saw section assumptions local global parameter independence integrals tractable For discrete state network Dirichlet priors p D M given explicitly Bayesian Dirichlet score equation First specify hyperparameters u v j search structures M find best score p D M The simplest setting hyperparameters set unity Another setting uninformative prior ui v j dim v dim pa v dim x number states variable s x giving rise BDeu score equivalent sample size parameter A discussion settings given concept likelihood equivalence networks Markov equivalent score How dense resulting network sensitive Including explicit prior p M networks favour sparse connections sensible idea considers modified score p D M p M Searching structures computationally demanding task However log score decomposes additive terms involving family variable v compare networks differing single edge efficiently adjustment family terms outside family DRAFT April Structure learning x8 x3 x4 x1 x2 x6 x7 x5 x8 x3 x4 x1 x2 x6 x7 x5 b x8 x3 x4 x1 x2 x6 x7 x5 c Figure Learning structure Bayesian network The correct structure variables binary The ancestral order x2 x1 x5 x4 x3 x8 x7 x6 The dataset formed samples network b The learned structure based PC algorithm Bayesian empirical conditional independence test Undirected edges oriented arbitrarily provided graph remains acyclic c The learned structure based Bayes Dirichlet network scoring method See demoPCdata m demoBDscore m affected This means given candidate family find parents family connect child computation carried families independently To help find best families search heuristics based local addition removal reversal edges increase score popular In learnBayesNet m simplify problem demonstration purposes assume know ancestral order variables maximal number parents variable In practice unlikely large number parents influence variable case general require data exponential number parents ascertain One principle approach assuming parametric forms large tables common practice Example PC algorithm versus network scoring In fig compare PC algorithm network scoring based Dirichlet hyperparameters set unity samples known belief network The PC algorithm conditional independence test based Bayesian factor Dirichlet priors u In network scoring approach general need assume ancestral ordering However assume know correct ancestral ordering limit number parents variable In case easily search possible graph structures choosing highest posterior score This proceeds example looking contribution variable x7 family Since according given ancestral ordering x1 x2 x3 x4 x5 x8 possible parents x7 principle need search parental configurations family However assume maximally parents reduces parental configurations We perform optimisation parental structure variable x7 independently parental structure variables thanks likelihood decomposable property network score Similarly carry optimisation variables separately based possible parents according ancestral order In fig network scoring technique outperforms PC algorithm This partly explained network scoring technique provided correct ancestral order constraint variable maximally parents DRAFT April Structure learning Algorithm Chow Liu Trees D j D Compute mutual information pair variables xi xj wij MI xi xj end end For undirected graph G edge weights w find maximum weight undirected spanning tree T Choose arbitrary variable root node tree T Form directed tree orienting edges away root node Chow Liu Trees Consider multivariate distribution p x wish approximate distribution q x Furthermore constrain approximation q x Belief Network node parent fig First assume chosen particular labelling D variables children higher parent indices parents The DAG single parent constraint means q x D q xi xpa pa pa pa single parent index node To find best approximating distribution q constrained class minimise Kullback Leibler divergence KL p q log p x p x D log q xi xpa p xi xpa Since p x fixed term constant By adding term log p xi xpa p xi xpa depends p x write KL p q const D log q xi xpa p xi xpa log p xi xpa p xi xpa p xpa This enables recognise negligible constant overall Kullback Leibler divergence positive sum individual Kullback Leibler divergences optimal setting q xi xpa p xi xpa Plugging solution equation log p xi xpa log p xi xpa log p xpa obtain KL p q const D log p xi xpa p xi xpa D log p xpa p xpa We need find optimal parental structure pa minimises expression If add subtract entropy term write KL p q D log p xi xpa p xi xpa D log p xpa p xpa D log p xi p xi D log p xi p xi const x1 x2 x3 x4 Figure A Chow Liu Tree variable xi parent The variables indexed D DRAFT April Structure learning For variables xi xj distribution p xi xj mutual information definition written MI xi xj log p xi xj p xi p xj p xi xj seen Kullback Leibler divergence KL p xi xj p xi p xj non negative Using equation KL p q D MI xi xpa D log p xi p xi const Since task find optimal parental indices pa entropic term log p xi p xi fixed distribution p x independent mapping finding optimal mapping equivalent maximising summed mutual informations D MI xi xpa constraint pa Since need choose optimal initial labelling variables problem equivalent computing pairwise mutual informations wij MI xi xj finding maximal spanning tree graph edge weights w spantree m Once found need identify directed tree parent This achieved choosing node orienting edges consistently away node Maximum likelihood Chow Liu trees If p x empirical distribution p x N N n x xn KL p q const N n log q xn Hence distribution q minimises KL p q equivalent maximises likelihood data This means use mutual information found empirical distribution p xi xj b xi xj b Chow Liu tree produced corresponds maximum likelihood solution single parent trees An outline procedure given algorithm An efficient algorithm sparse data available Remark Learning Tree structured Belief Networks The Chow Liu algorithm pertains discus sion section learning structure Belief networks data Under special constraint variable parent Chow Liu algorithm returns maximum likelihood structure fit data DRAFT April Maximum Likelihood Undirected models Maximum Likelihood Undirected models Consider Markov network p X defined necessarily maximal cliques Xc X c C clique parameters C p X Z c c Xc c The term Z X c c Xc c ensures normalisation notation X indicating summation states set variables X Given set data X n n N assuming d data log likelihood L n log p X n n c log c X nc c N logZ Our interest find parameters maximise log likelihood L In general learning optimal parameters c c C awkward coupled Z Unlike BN objective function split set isolated parameter terms general need resort numerical methods In special cases exact results apply particular MN decomposable constraints placed form clique potentials discuss section More generally gradient based techniques insight properties maximum likelihood solution The likelihood gradient The gradient log likelihood respect clique parameter c given c L n c log c X nc c N c log c Xc c p Xc This obtained result c logZ Z X c c Xc c c c c Xc c c log c Xc c p Xc The gradient standard numerical optimisation package Exponential form potentials A common form parameterisation use exponential form c Xc exp Tc c Xc c vector parameters c Xc fixed feature function defined variables clique c From equation find gradient need c log c Xc c c Tc c Xc c Xc Using equation find L zero derivative N n c X nc c Xc p Xc DRAFT April Maximum Likelihood Undirected models Hence maximum likelihood solution satisfies empirical average feature function matches average feature function respect model By defining empirical distribution clique variables Xc Xc N N n I Xc X nc write equation compactly c Xc Xc c Xc p Xc An example learning exponential form given example We return learning parameters models section Example Boltzmann Machine learning We define BM p v W Z W e vTWv Z W v e vTWv symmetric W binary variables dom vi Given set training data D v1 vN log likelihood L W N n vn T Wvn N logZ W Differentiating w r t wij j wii gradients L wij N n vni v n j vivj p v W L wii N n vni vi p v W A simple algorithm optimise weight matrix W use gradient ascent wnewij w old ij L wij wnewii w old ii L wii learning rates The intuitive interpretation learning stop gradient zero second order statistics model vivj p v W match empirical distribution n v n v n j N BM learning difficult vivj p v W typically computationally intractable arbitrary interaction matrix W needs approximated Indeed compute likelihood L W exactly general matrix W monitoring performance difficult General tabular clique potentials For unconstrained clique potentials separate table states defined clique In writing log likelihood convenient use identity c X nc Yc c Yc I Yc X n c product states potential c This expression follows indicator zero single observed state X nc The log likelihood L c Yc n I Yc X nc log c Yc N logZ DRAFT April Maximum Likelihood Undirected models Z Yc c c Yc Differentiating log likelihood respect specific table entry c Yc obtain c Yc L n I Yc X nc c Yc N p Yc c Yc Equating zero rewriting terms variables X maximum likelihood solution obtained p Xc Xc empirical distribution defined equation That unconstrained optimal maximum likelihood solution given setting clique potentials marginal distribution clique p Xc matches empirical distribution clique Xc Note describes form optimal maximum likelihood solution doesn t closed form expression setting tables To find optimal tables case require numerical procedure gradient based methods based equation IPF method described Iterative proportional fitting According general result equation maximum likelihood solution clique marginals match empirical marginals Assuming absorb normalisation constant arbitrarily chosen clique drop explicitly representing normalisation constant For clique c requirement marginal p matches empirical marginal variables clique Xc X c d c Xd Xc Given initial setting potentials update Xc satisfy marginal requirement new Xc Xc X c d c Xd required states Xc By multiplying dividing right hand Xc equivalent ascertaining new Xc Xc Xc p Xc This called Iterative Proportional Fitting IPF update corresponds coordinate wise optimi sation log likelihood coordinate corresponds c Xc parameters fixed In case conditional optimum analytically given setting One proceeds selecting potential update continues updating convergence criterion met Note general update marginal p Xc needs recomputed computing marginals expensive width junction tree formed graph suitably limited Decomposable Markov networks Whilst general Markov networks require numerical methods find maximum likelihood solution important special case find optimal tables easily If MN corre sponding decomposable know junction tree representation express distribution form product local marginals divided separator distributions p X c p Xc s p Xs DRAFT April Maximum Likelihood Undirected models x1 x2 x3 x4 x5 x6 x1 x2 x2 x2 x3 x5 x5 x5 x6 x2 x4 x5 x2 x5 b x1 x2 x3 x5 x4 x6 c Figure A decomposable Markov network b A junction tree c Set chain formed choosing clique x2 x3 x5 root orienting edges consistently away root Each separator absorbed child clique form set chain Algorithm Learning unconstrained decomposable Markov network maximum likelihood We triangulated decomposable Markov network cliques c Xc c C empirical marginal distributions cliques separators Xc Xs Form junction tree cliques Initialise clique c Xc Xc separator s Xs Xs Choose root clique junction tree orient edges consistently away root For oriented junction tree divide clique parent separator Return new potentials clique maximum likelihood solution By reabsorbing separators numerator terms form set chain distribution section p X c p Xc X c Since directed provided constraint placed tables maximum likelihood solution learning tables given assigning set chain factor p Xc X c based counting instances dataset learnMarkovDecom m The procedure best explained example given See algorithm general description Example Given dataset X n n N corresponding empirical distribution X wish fit maximum likelihood MN form p x1 x6 Z x1 x2 x2 x3 x5 x2 x4 x5 x5 x6 potentials unconstrained tables fig 16a Since graph decomposable know admits factorisation clique potentials divided separators p x1 x6 p x1 x2 p x2 x3 x5 p x2 x4 x5 p x5 x6 p x2 p x2 x5 p x5 We convert set chain reabsorbing denominators numerator terms section For example choosing clique x2 x3 x5 root write p x1 x6 p x1 x2 x1 x2 p x2 x3 x5 x2 x3 x5 p x4 x2 x5 x2 x4 x5 p x6 x5 x5 x6 identified factors clique potentials normalisation constant Z unity fig 16b The advantage representation clique potentials independent distribution BN cluster variables The log likelihood d dataset L n log p xn1 xn2 log p xn2 xn3 xn5 log p xn4 xn2 xn5 log p xn6 xn5 DRAFT April Maximum Likelihood Undirected models x1 x2 x3 x4x5 x1 x2 b Figure Interpreted Markov network graph represents dis tribution x1 x4 x5 x1 x2 x4 x2 x4 x3 As pairwise MN graph represents x4 x5 x1 x4 x4 x5 x1 x2 x2 x4 x2 x3 x3 x4 b A junction tree pairwise MN We choice place pairwise cliques valid choice shorthand b b xa xb xa b xa xb terms independent parameter model The maximum likelihood solution corresponds BN case simply setting factor empirical distribution x1 x2 x1 x2 x2 x3 x5 x2 x3 x5 x2 x4 x5 x4 x2 x5 x5 x6 x6 x5 Constrained decomposable Markov networks If constraints forms maximal clique potentials Markov network ve seen learning straightforward Here interest functional form maximal clique constrained product potentials smaller cliques2 c Xc ic X ic constraint placed non maximal clique potentials ic X ic In general case write directly maximum likelihood solution non maximal clique potentials ic X ic Consider graph fig consider pairwise MN In case clique potentials con strained simply write solution unconstrained decomposable case Since graph decomposable computational savings case For empirical distribution maximum likelihood requires pairwise marginals MN match corresponding marginals obtained As explained fig choice junction tree clique potential assigned valid choice given fig 17b Let s consider updating potentials clique Keeping potentials cliques fixed update potentials Using bar denote fixed potentials marginal requirement MN marginal p x1 x2 x4 matches empirical marginal x1 x2 x4 written shorthand p1 We express requirement terms pairs variables x1 x2 x1 x4 clique p1 p1 2A Boltzmann machine form unconstrained binary pairwise potentials converted BM For cases ic constrained Iterative scaling place IPF DRAFT April Maximum Likelihood Undirected models Algorithm Efficient Iterative Proportional Fitting Given set I corresponding set reference empirical marginal distributions variables potential aim set marginals Markov network match given empirical marginals Given Markov network potentials I triangulate graph form cliques C1 CC Assign potentials cliques Thus clique set associated potentials Fc Initialise potentials example unity repeat Choose clique c root Propagate messages root compute separators boundary root repeat Choose potential clique c Fc Perform IPF update given fixed boundary separators potentials c Potentials clique c converge All Markov network marginals converge reference marginals Taking marginal requirement means p1 x3 x4 x5 expressed x4 x5 x3 The messages boundary separator tables choose central clique root carry absorption root Given fixed messages perform IPF updates root clique new1 x4 After making update subsequently update similarly constraint x2 x5 x3 new1 x2 We iterate updates convergence clique Given converged updates clique choose clique root propagate root compute separator cliques boundary root Given fixed boundary clique potentials perform IPF clique This efficient IPF procedure described generally algorithm empirical distribution More generally IPF minimises Kullback Leibler divergence given reference distribution Markov network See demoIPFeff m IPF m DRAFT April Maximum Likelihood Undirected models Figure Learning digits Simon Lucas algoval system Markov network Top row training examples Each example binary image pixels Second row training data missing pixels grey represents missing pixel Third row Reconstructions missing data thin junction tree MN maximum clique size Bottom row Reconstructions thin junction tree Boltzmann machine maximum clique size trained efficient IPF b Figure Based pair wise empirical entropies H xi xj edges ordered high entropy edges Shown adjacency matrix resulting Markov network junction tree cliques size white represents edge b Indicated number cliques pixel member dicating degree importance Note lowest clique membership value pixel member clique Example Learning structured Markov network In example aim fit Markov network data constrained inference Markov network computationally cheap ensuring junction tree Markov network limited clique sizes In fig examples binary pixel handwritten twos presented forming training set wish fit Markov network First pairwise empirical entropies H xi xj j computed rank edges highest entropy edges ranked Edges included graph G highest ranked provided triangulated G cliques size This resulted unique cliques adjacency matrix triangulated G presented fig 19a In fig 19b number times pixel appears cliques shown indicates degree importance pixel distinguishing examples Two models trained compute likely reconstruction based missing data p xmissing xvisible The model Markov network maximal cliques graph essentially training required settings clique potential obtained explained algorithm The model makes errors reconstruction missing pixels Note unfortunate effect reconstructing white pixel surrounded black pixels effect limited training data With larger amounts data model recognise effects occur In second model maximal cliques maximal clique potentials restricted product pairwise cliques maximal clique This equivalent structured Boltzmann machine trained efficient IPF approach algorithm The corresponding reconstruction error This performance worse model Boltzmann machine constrained Markov network struggles represent data See demoLearnThinMNDigit m DRAFT April Maximum Likelihood Undirected models Exponential form potentials For exponential form potentials c Xc exp Tc c Xc saw section compute derivatives use standard numerical optimisation proce dures In following section outline popular numerical technique Iterative scaling We consider Markov networks exponential form p X Z c ecfc Xc feature functions fc Xc c ranges non maximal cliques Xc X Note equation written form having multiple potentials clique The normalisation requirement Z X c exp cfc Xc A maximum likelihood training algorithm Markov network somewhat analogous EM approach section derived follows Consider bound positive x log x x log x x Hence log Z Z old Z Z old logZ logZ old Z Z old Then write bound log likelihood N L N c n cfc X nc logZ old Z Z old As stands bound general straightforward optimise parameters potential coupled Z term For convenience useful reparmameterise write c c oldc c oldc Then Z X exp c fc Xc c X exp c fc Xc oldc exp c fc Xc c One decouple additional bound derived considering exp c cfc Xc exp c pc c d fd Xd pc fc Xc d fd Xd DRAFT April Maximum Likelihood Undirected models Since pc c pc apply Jensen s inequality exp c cfc Xc c pc exp d fd Xd c Hence Z X exp c fc Xc oldc c pcexp c f fd Xc Plugging bound N L c 1N n fc X nc c pcexp c d fd Xc p X old LB c logZ old The term curly brackets contains potential parameters c uncoupled fashion Differentiating respect c gradient lower bound given LB c c N n fc X nc fc Xc exp c oldc d fd Xd p X old This gradient based optimisation procedure learn parameters c Intuitively parameters converge empirical average functions f match average functions respect samples drawn distribution line general condition maximum likeli hood optimal solution In general appear little advantage procedure general gradient approach section However special case functions sum c fc Xc zero gradient found analytically giving iterative scaling IS update c old c log N n fc X nc log fc Xc p Xc old The constraint features fc need non negative relaxed expense additional variational parameters exercise If junction tree formed exponential form Markov network limited tree width computational savings performing IPF cliques junction tree updating parameters clique IS This modified version constrained decomposable case See unified treatment propagation scaling junction trees Conditional random fields For input x output y CRF defined conditional distribution p y x Z x k k y x positive potentials k y x To learning straightforward potentials usually defined exp kfk y x fixed functions f y x parameters k In case distribution output conditioned input p y x Z x k exp kfk y x DRAFT April Maximum Likelihood Undirected models CRFs viewed simply Markov networks exponential form potentials section Equation equivalent equation parameters denoted variables x denoted y In CRF case inputs x simply effect determining feature fk y x For d dataset input outputs D xn yn n N training based conditional maximum likelihood requires maximisation L N n log p yn xn N n k kfk y n xn logZ xn In general closed form solution optimal exists needs determined numerically The methods ve discussed iterative scaling readily adapted optimisation problem practice gradient based techniques preferred For completeness describe gradient based training The gradient components L n fi y n xn fi y xn p y xn The terms fi y xn p y xn problematic tractability depends structure poten tials For multivariate y provided structure cliques defined subsets y singly connected computing average generally tractable More generally provided cliques resulting junction tree limited width exact marginals available An example given linear chain CRF section example Another quantity useful numerical optimisation Hessian components j L n fi y xn fj y xn fi y xn fj y xn averages respect p y xn This expression negated sum covariance elements negative semi definite Hence function L concave single global optimum In practice CRFs thousands millions parameters computing Newton update associated inverse Hessian prohibitively expensive In case alternative optimisation methods conjugate gradients preferable In practice regularisation terms added prevent overfitting section discussion regularisation Using term k c2k k positive regularisation constants c2k discourages weights large This term negative definite overall objective function remains concave Once trained CRF predicting output distribution novel input x The likely output y equivalently given y argmax y log p y x argmax y k kfk y x logZ x Since normalisation term independent y finding likely output equivalent y argmax y k kfk y x DRAFT April Maximum Likelihood Undirected models b Figure Training results linear chain CRF There training sequences subpanel In subpanel row corresponds input sequence x1 xt state represented different colour The middle row correct output sequence y1 yt state represented different colour Together input output sequences training data D The row contains likely output sequence given trained CRF arg maxy1 p y1 x1 D b Five additional test input sequences correct output sequence middle predicted output sequence Natural language processing In natural language processing application xt represent word yt corresponding linguistic tag noun verb etc A suitable form case constrain CRF form exp k kgk yt yt l lhl yt xt binary functions gk hl parameters k l The grammatical structure tag tag transitions encoded gk yt yt linguistic tag information hk yt xt importance determined corresponding parameters In case inference marginals ytyt x1 T straightforward factor graph corresponding inference problem linear chain Variants linear chain CRF heavily natural language processing including speech tagging machine translation input sequence x represents sentence English output sequence y corresponding translation French See example Example Linear chain CRF We consider CRF X input states Y output states form p y1 T x1 T T t e k kgk yt yt l lhl yt xt Here binary functions gk yt yt I yt ak I yt bk k ak bk simply index transitions consecutive outputs The binary functions hl yt xt I yt al I xt cl l al cl index translation input output There parameters total In fig plot training test results based small set data As model learns predict output training test data The training CRF obtained iterations gradient ascent learning rate See demoLinearCRF m DRAFT April Maximum Likelihood Undirected models Pseudo likelihood Consider MN variables x dim x D form p x Z c c Xc c For specially constrained c partition function Z intractable likelihood set d data intractable A surrogate use pseudo likelihood variable conditioned variables equivalent conditioning variable s neighbours MN L N n D log p xni xn The terms p xni xn usually straightforward work require finding normalisation univariate distribution In case gradient computed exactly learning parameters carried In general solution found correspond maximum likelihood solution However special cases Boltzmann machine forms consistent estimator Learning structure Learning structure Markov network based independence tests belief networks section A criterion finding MN set nodes X use fact edge exits x y conditioned nodes x y deemed independent This pairwise Markov property described section By checking x y X x y pair variables x y edge deletion approach principle reveals structure network For learning structure oracle method sound However practical difficulty case independencies determined data checking x y X x y requires principle enormous amounts data The reason conditioning selects parts dataset consistent conditioning In practice result small numbers remaining datapoints estimating independencies basis unreliable The Markov boundary criterion uses local Markov property section conditioned neighbours variable independent variables graph By starting variable x neighbourhood set progressively include neighbours testing inclusion renders remaining non neighbours independent x A difficultly doesn t correct Markov boundary including variable neighbourhood set deemed necessary To consider network corresponds linear chain x edge chain In case nearest neighbour x Markov boundary x However nearest neighbour currently set non nearest neighbour included strictly required To counter neighbourhood variables included neighbourhood x later removed deemed superfluous boundary In cases specific constraints imposed learning structures resulting triangulation bounded tree width whilst formally difficult approximate procedures available In terms network scoring methods undirected networks computing score hampered fact parameters clique coupled normalisation constant distribution This issue addressed hyper Markov priors Summary For discrete belief networks particularly convenient use Dirichlet parameter prior conjugate categorical distribution DRAFT April Code Provided assume local global parameter independence posterior belief network tables factorises Learning structure belief network complex The PC algorithm uses local independence tests decide variables linked A global alternative based network scoring method model likelihood network structure Dirichlet prior Learning maximum likelihood parameters decomposable Markov network straightforward achieved counting For non decomposable Markov networks closed form solution exists The maximum likelihood criterion equivalent ensuring clique marginals match empirical marginals The iterative proportional fitting algorithm technique set tables ensure marginals match For Markov networks parameterised feature functions iterative scaling maximum likelihood technique enables individual parameter updates Gradient based approaches straightforward popular conditional random fields Code condindepEmp m Bayes test Mutual Information empirical conditional independence condMI m Conditional Mutual Information condMIemp m Conditional Mutual Information Empirical distribution MIemp m Mutual Information Empirical distribution PC algorithm oracle This demo uses oracle determine x y z data determine empirical depen dence The oracle belief network For partial orientation unmarried collider rule implemented demoPCoracle m Demo PC algorithm oracle PCskeletonOracle m PC algorithm oracle PCorient m Orient skeleton Demo empirical conditional independence For half experiments data drawn distribution x y z true For half experiments data drawn random distribution x y z false We measure fraction experiments Bayes test correctly decides x y z We measure fraction experiments Mutual Information test correctly decides x y z based setting threshold equal median empirical conditional mutual information values A similar empirical threshold obtained Bayes factor strictly kosher pure Bayesian spirit principle set threshold zero The test based assumed chi squared distributed MI included comparison impractical small data cases demoCondIndepEmp m Demo empirical conditional independence based data Bayes Dirichlet structure learning It interesting compare result demoPCdata m demoBDscore m PCskeletonData m PC algorithm empirical conditional independence demoPCdata m Demo PC algorithm data BDscore m Bayes Dirichlet BD score node given parents learnBayesNet m Given ancestral order maximal parents learn network demoBDscore m Demo structure learning DRAFT April Exercises Fuse Drum Toner Paper Roller Burning Quality Wrinkled Mult Pages Paper Jam Figure Printer Nightmare belief network All variables binary The upper variables parents possible problems diagnoses lower variables consequences problems faults Exercises Exercise Printer Nightmare Cheapco honestly pain neck Not buy dodgy old laser printer StopPress use mercilessly try away substandard components materials Unfortunately StopPress contract maintain Cheapco s old warhorse end frequently sending mechanic repair printer They decide statistical model Cheapco s printer reasonable idea fault based information Cheapco s secretary tells phone In way StopPress hopes able send Cheapco junior repair mechanic having likely diagnosed fault phone Based manufacturer s information StopPress good idea dependencies printer likely directly affect printer components The belief network fig represents assumptions However specific way Cheapco abuse printer mystery exact probabilistic relationships faults problems idiosyncratic Cheapco StopPress following table faults column represents visit fuse assembly malfunction drum unit toner poor paper quality worn roller burning smell poor print quality wrinkled pages multiple pages fed paper jam The table contained printer mat Learn table entries basis maximum likeli hood Program belief network tables maximum likelihood tables BRMLtoolbox Compute probability fuse assembly malfunction given secretary complains burning smell paper jammed problems Repeat calculation Bayesian method flat Beta prior tables Continue use tables remainder question Given information secretary likely joint diagnosis diag nostic variables joint likely p Fuse Drum Toner Paper Roller evidence Use max absorption method associated junction tree Compute joint likely state distribution p Fuse Drum Toner Paper Roller burning smell paper jammed Explain compute efficiently max absorption method Exercise Consider data xn n N Show Gaussian distribution maximum likelihood estimator mean m N N n x n variance N N n x n m DRAFT April Exercises Exercise A training set consists dimensional examples classes The training examples class class Fit dimensional Gaussian Maximum Likelihood classes Also estimate class probabilities p1 p2 maximum likelihood What probability test point x belongs class Exercise For set N observations training data X x1 xN independently gathered observations log likelihood belief network generate X log p X N n K log p xni pa xni We define notation t p xi s pa xi t representing probability variable xi state s given parents variable xi vector states t Using Lagrangian L N n K log p xni pa xni K ti iti s ti Show maximum likelihood setting t js t j N n I xnj s I pa xnj tj N n s I xnj s I pa xnj tj Exercise Conditional Likelihood training Consider situation partition observable variables disjoint sets x y want find parameters maximize conditional likelihood CL N N n log p yn xn set training data xn yn n N All data assumed generated distribu tion p x y p y x p x unknown parameter In limit large d training data CL optimum Exercise Moment Matching One way set parameters distribution match moments distribution empirical moments This corresponds maximum likelihood Gaussian distribution example generally consistent maximum likelihood For data mean m variance s fit Beta distribution B x moment matching use m m m2 s s m m Does correspond maximum likelihood DRAFT April Exercises Exercise For d data xn n N generated Beta distribution B x b log likelihood given L b N n log xn b N n log xn N logB b B b Beta function Show derivatives L N n log xn N N b b L N n log xn N b N b x d log x dx digamma function suggest method learn parameters b Exercise Consider Boltzmann machine defined example Write pseudo likeli hood set d data v1 vN derive gradient respect wij j Exercise Show model likelihood equation written explicitly p D M k j ui vk j u vk j u vk j ui vk j Exercise Define set N consisting node belief networks node parents For given ancestral order restricted set written Na How belief networks Na What computational time find optimal member Na Bayesian Dirichlet score assuming computing BD score member Na takes second bearing mind decomposability BD score Estimate time required find optimal member N Exercise For Markov network p x y z Z x y y z derive iterative scaling algorithm learn unconstrained tables x y x y based set d data X Y Z Exercise In section considered maximum likelihood learning Markov network p X c c Xc parameters c potentials form c Xc exp cfc Xc constraint fc Xc Our interest drop positive constraint fc Xc By considering c cfc Xc c pc cfc Xc pc auxiliary variables pc c pc explain derive form iterative scaling training algorithm general fc parameter c updated separately Exercise Write MATLAB routine A ChowLiu X X D N data matrix containing multivariate datapoint column returns Chow Liu maximum likelihood tree X The tree structure returned sparse matrix A You find routine spantree m useful The file ChowLiuData mat contains data matrix variables Use routine find maximum likelihood Chow Liu tree draw picture resulting DAG edges oriented away variable Exercise Show graph N nodes N n 2n 2N N N 2N N valid DAGs DRAFT April CHAPTER Naive Bayes So far ve discussed methods generality touching use methods practical setting Here discuss simplest methods widely practice classify data This useful junction enables discuss issues parameter learning data constrained structure learning Naive Bayes Conditional Independence We shall discuss machine learning concepts detail chapter Here require intuitive concept classification means giving discrete label input For example wish classify input image classes male female Naive Bayes NB popular classification method aids discussion conditional independence overfitting Bayesian methods In NB form joint model D dimensional attribute input vector x corresponding class label c p x c p c D p xi c belief network depicted fig 1a Coupled suitable choice conditional distribution p xi c use Bayes rule form classifier novel input vector x p c x p x c p c p x p x c p c c p x c p c In practice common consider classes dom c The theory describe valid number classes c examples restricted binary class case Also attributes xi taken binary shall initially The extension attribute states continuous attributes straightforward Example EZsurvey org partitions radio station listeners groups young old They assume given knowledge customer young old sufficient determine customer like particular radio station independent likes dislikes stations p r1 r2 r3 r4 age p r1 age p r2 age p r3 age p r4 age variables r1 r2 r3 r4 states like dislike age variable value young old Thus information age customer determines individual Estimation Maximum Likelihood radio station preferences needing know To complete specification given customer young chance like Radio1 chance like Radio2 chance like Radio3 chance like Radio4 Similarly old listener chance like Radio1 chance like Radio2 chance like Radio3 chance like Radio4 They know listeners old Given model fact new customer likes Radio1 Radio3 dislikes Radio2 Radio4 probability new customer young This given p young r1 like r2 dislike r3 like r4 dislike p r1 like r2 dislike r3 like r4 dislike young p young age p r1 like r2 dislike r3 like r4 dislike age p age Using naive Bayes structure numerator given p r1 like young p r2 dislike young p r3 like young p r4 dislike young p young Plugging values obtain The denominator given value plus corresponding term evaluated assuming customer old Which gives p young r1 like r2 dislike r3 like r4 dislike Estimation Maximum Likelihood Learning table entries NB straightforward application general BN learning discussed section For fully observed dataset maximum likelihood learning table entries corresponds counting number occurrences training data This useful exercise reinforce concrete general theory section Binary attributes Consider dataset xn cn n N binary attributes xni D associated class label cn The number datapoints class c denoted n0 number class c denoted n1 For attribute classes need estimate values p xi c ci The c x1 x2 x3 cn xni n N c c D b Figure Naive Bayes classifier The central assumption given class c attributes xi independent b Assuming data d maximum likelihood learns optimal parameters c distribution p c parameters c class dependent attribute distributions p xi c DRAFT April Estimation Maximum Likelihood probability p xi c given normalisation requirement p xi c p xi c ci Based NB conditional independence assumption probability observing vector x compactly written1 p x c D p xi c D ci xi ci xi In expression xi term contributes factor c xi ci xi Together assumption training data d generated log likelihood attributes class labels L n log p xn cn n log p cn p xni cn n xni log cn xni log c n n0 log p c n1 log p c This written explicitly terms parameters L n I xni c n log 0i I x n c n log 0i I xni cn log 1i I xni c n log 1i n0 log p c n1 log p c We find maximum likelihood optimal ci differentiating w r t c equating zero giving ci p xi c n I x n c n c n I x n c n c I xni cn c number times xi class c number datapoints class c Similarly optimising equation respect p c gives p c number times class c occurs total number data points These results consistent general theory section maximum likelihood corresponds setting tables counting Classification boundary We classify novel input x class p c x p c x Using Bayes rule writing log expression equivalent log p x c log p c log p x log p x c log p c log p x From definition classifier equivalent normalisation constant log p x dropped sides log p x c log p c log p x c log p c 1This makes use general notation quantity raised power e x0 Unfortunately potential general mathematical notational confusion xy meaning possibly x raised power y alternatively y simply indexing set x variables This potential conflict hopefully arise resolved reference meaning symbols involved DRAFT April Estimation Maximum Likelihood Using binary encoding xi classify x class x log x log 1i log p c x log x log 0i log p c This decision rule expressed form classify x class iwix suitable choice weights wi constant exercise The interpretation w specifies hyperplane attribute space x classified lies positive hyperplane Example Are Scottish Consider following vector binary attributes shortbread lager whiskey porridge football A vector x T describe person likes shortbread like lager drinks whiskey eats porridge watched England play football Together vector x label nat describing nationality person dom nat scottish english fig We wish classify vector x T scottish english Using Bayes rule p scottish x p x scottish p scottish p x p x scottish p scottish p x scottish p scottish p x english p english By maximum likelihood prior class probability p scottish given fraction people database Scottish similarly p english given fraction people database English This gives p scottish p english For p x nat Naive Bayes assumption p x nat p x1 nat p x2 nat p x3 nat p x4 nat p x5 nat knowing Scottish don t need know calculate probability likes dislikes Based table fig maximum likelihood p x1 english p x1 scottish p x2 english p x2 scottish p x3 english p x3 scottish p x4 english p x4 scottish p x5 english p x5 scottish For x T p scottish x Since greater classify person Scottish Small data counts In example consider trying classify vector x T In training data Scottish people like shortbread This means particular x p x scottish extremely confident classification p scottish x This demonstrates difficulty maximum likelihood sparse data One way ameliorate smooth probabilities example adding small number frequency counts attribute This ensures DRAFT April Estimation Maximum Likelihood b Figure English tastes people attributes shortbread lager whiskey porridge football Each column represents tastes individual b Scottish tastes people zero probabilities model An alternative use Bayesian approach discourages extreme probabilities discussed section Potential pitfalls encoding In shelf packages implementing Naive Bayes binary attributes assumed In practice case non binary attributes occurs Consider following attribute age In survey person s age marked variable means person years old means person years old means person older One way transform variable binary representation use binary variables a1 a2 a3 representing respectively This called M coding binary variables active encoding M states By construction means variables a1 a2 a3 dependent example know a1 know a2 a3 Regardless class conditioning variables dependent contrary assumption Naive Bayes A correct approach use variables states explained section Multi state variables The extension method class variables c states straightforward We concentrate extending attribute variables having states For variable xi states dom xi S likelihood observing state xi s denoted p xi s c c s p xi s c The class conditional likelihood generating d data D xn cn n N N n p xn cn N n D S s C c c I xni s I c n c The effect indicators terms c survive attributes state s class c This gives class conditional log likelihood L N n D S s C c I xni s I c n c log c We optimize respect parameters Lagrange multiplier attributes classes c ensure normalisation This gives Lagrangian L N n D S s C c I xni s I c n c log c C c D ci S s c To find optimum function differentiate respect c equate zero Solving resulting equation obtain N n I xni s I c n c c ci DRAFT April Bayesian Naive Bayes cn xni c n N c c C D Figure Bayesian Naive Bayes factorised prior class con ditional attribute probabilities p xi s c For simplicity assume class probability c p c learned maximum likelihood distribution placed parameter Hence normalisation c p xi s c n I x n s I c n c s n I xn s I cn c The maximum likelihood setting parameter p xi s c equals relative number times attribute state s class c Text classification Consider set documents politics set sport Our interest method automatically classify new document pertaining sport politics We search sets documents find commonly occurring words including called stop words Each document represented dimensional vector representing number times words occurs document called bag words representation crude representation document discards word order A Naive Bayes model specifies distribution number occurrences p xi c xi count number times word appears documents type c One achieve multistate representation discussed section continuous xi represent relative frequency word document In case p xi c conveniently modelled example Beta distribution Despite simplicity naive Bayes classify novel documents surprisingly nat urally real practical methods work attributes choose wisely Intuitively potential justification conditional independence assumption know document politics good indication kinds words find document Because Naive Bayes reasonable classifier sense minimal storage fast training applied time storage critical applications automatically classifying webpages types spam filtering It forms simplest commonly basic machine learning classification routines Bayesian Naive Bayes As saw previous section naive Bayes powerful method classification overly zealous case small counts If single attribute counts class c irrespective attributes classifier x class c This happens prod uct remains To counter overconfidence effect use simple Bayesian method Given dataset D xn cn n N predict class c input x p c x D p x D c p x D c p c D DRAFT April Bayesian Naive Bayes For convenience simply set p c D maximum likelihood p c D N n I cn c However ve seen setting parameters p x D c maximum likelihood training yield confident predictions case sparse data A Bayesian approach addresses difficulty uses priors probabilities p xi s c c discourage extreme values The model depicted fig The prior Writing c i1 c S c vector probabilities c D c C global factorisation assumption section use prior p c p c We consider discrete xi states S In case p xi s c corresponds categorical distribution conjugate prior Dirichlet distribution Under factorised prior assumption define prior attribute class c p c Dirichlet c ui c ui c hyperparameter vector Dirichlet distribution table p xi c The posterior Consistent general Bayesian BN training result section parameter posterior factorises p c D p c D p c D p c n cn c p xni c By conjugacy posterior class c Dirichlet distribution p c D Dirichlet c u c vector u c components u c s uis c n cn c I xni s For Dirichlet hyperparameters ui c equation updates hyperparameter number times variable state s class c data A common default setting components u Classification The posterior class distribution novel input x given p c x D p c x D p c D p x D c p c D p x D c To compute p x D c use p x s D c c p x s c D c c p x s c p c D c c p c D DRAFT April Tree Augmented Naive Bayes c x1 x2 x3 x4 Figure Tree Augmented Naive TAN Bayes Each variable xi parent The maximum likelihood optimal TAN structure computed modified Chow Liu algorithm conditional mutual information MI xi xj c computed j A maximum weight spanning tree found turned directed graph orienting edges outwards chosen root node The table entries read usual maximum likelihood counting argument Using general identity sDirichlet u d Z u s s I s s d Z u Z u Z u normalisation constant distribution Dirichlet u u s s s s s obtain p c x D p c D Z u c Z u c u c u c I x s Example Bayesian Naive Bayes Repeating previous analysis Are Scottish data example probability uniform Dirichlet prior tables gives value probability Scottish compared value standard Naive Bayes assumption See demoNaiveBayes m Tree Augmented Naive Bayes A natural extension Naive Bayes relax assumption attributes independent given class p x c D p xi c The question arises structure choose p x c As saw section learning structure computationally infeasible small numbers attributes A practical algorithm requires specific form constraint structure In section saw learn single parent tree structured networks efficiently Below extend learning class dependent tree networks classification Learning tree augmented Naive Bayes networks For distribution p x1 xD c form tree structure single parent constraint fig readily find class conditional maximum likelihood solution computing Chow Liu tree class One adds links class node c variable learns class conditional probabilities c x read maximum likelihood usual counting argument Note generally result different Chow Liu tree class DRAFT April Exercises Practitioners typically constrain network structure classes The maximum likelihood objective TAN constraint corresponds maximising conditional mutual formation MI xi xj c KL p xi xj c p xi c p xj c p c exercise Once structure learned subsequently sets parameters maximum likelihood counting Techniques prevent overfitting discussed addressed Dirichlet priors simpler Naive Bayes structure One readily consider restrictive structures single parent Belief Networks However finding optimal BN structures generally computationally infeasible heuristics required limit search space Summary Naive Bayes simple class conditional generative model data form simple classifier Bayesian training parameters straightforward An extension standard naive Bayes model consider attributes single parent attribute addition class label Finding maximum likelihood optimal tree augmented structure straightforward corresponds maximum spanning tree problem weights given class conditional mutual information Code NaiveBayesTrain m Naive Bayes trained Maximum Likelihood NaiveBayesTest m Naive Bayes test NaiveBayesDirichletTrain m Naive Bayes trained Bayesian Dirichlet NaiveBayesDirichletTest m Naive Bayes testing Bayesian Dirichlet demoNaiveBayes m Demo Naive Bayes Exercises Exercise A local supermarket specializing breakfast cereals decides analyze buying patterns customers They small survey asking randomly chosen people age older younger years breakfast cereals Cornflakes Frosties Sugar Puffs Branflakes like Each respondent provides vector entries corresponding like dislike cereal Thus respondent like Cornflakes Frosties Branflakes Sugar Puffs The older years respondents provide following data The younger years old respondents responded A novel customer comes supermarket says likes Frosties Sugar Puffs Using naive Bayes trained maximum likelihood probability younger Exercise A psychologist small survey happiness Each respondent provides vector entries corresponding answer yes question respectively The question vector attributes x rich married healthy DRAFT April Exercises Thus response indicate respondent rich unmarried healthy In addition respondent gives value c content lifestyle c The fol lowing responses obtained people claimed content content Using Naive Bayes probability person rich married healthy content What probability person rich married content That know healthy Consider following vector attributes x1 customer younger x1 x2 customer years old x2 x3 customer older x3 x4 customer walks work x4 Each vector attributes associated class label rich poor Point potential difficulties previously described approach training naive Bayes Hence describe extend previous naive Bayes method deal dataset Exercise Whizzco decide text classifier To begin attempt classify documents sport politics They decide represent document row vector attributes describing presence absence words x goal football golf defence offence wicket office strategy Training data sport documents politics documents represented MATLAB matrix row represents attributes xP Politics xS Sport Using maximum likelihood naive Bayes classifier probability document x politics Exercise A Naive Bayes Classifier binary attributes xi parameterised 1i p xi class 0i p xi class p1 p class p0 p class Show decision classify datapoint x class holds wTx b w b state explicitly w b function p1 p0 Exercise This question concerns spam filtering Each email represented vector x x1 xD xi Each entry vector indicates particular symbol word appears email The symbols words money cash viagra etc DRAFT April Exercises example x2 word cash appears email The training dataset consists set vectors class label c c indicates email spam c spam Hence training set consists set pairs xn cn n N The naive Bayes model given p c x p c D p xi c Derive expressions parameters model terms training data maximum likelihood Assume data independent identically distributed p c1 cN x1 xN N n p cn xn Explicitly parameters p c p xi c p xi c D Given trained model p x c explain form classifier p c x If viagra appears spam training data discuss effect classification new email contains word viagra Explain counter effect Explain spammer try fool naive Bayes spam filter Exercise For distribution p x c approximation q x c p x c corresponds empirical distribution finding q x c minimises Kullback Leibler divergence KL p x c q x c corresponds maximum likelihood training q x c assuming d data Exercise Consider distribution p x c Tree Augmented approximation q x c q c q xi xpa c pa pa Show optimal q x c constrained solution q x c minimises KL p x c q x c plugged Kullback Leibler expression gives function parental structure KL p x c q x c log p xi xpa c p xpa c p xi c p xi xpa c const This shows single parent constraint tree q x c structure minimis ing Kullback Leibler divergence equivalent maximising sum conditional mutual information terms From exercise know corresponds maximum likelihood setting parental structure This achieved finding maximal weight spanning tree case Chow Liu tree DRAFT April Exercises DRAFT April CHAPTER Learning Hidden Variables In models variables directly observed latent hidden This occur data observed In chapter discuss methods learning presence missing information particular develop Expectation Maximisation algorithm related variants Hidden Variables Missing Data In practice data entries missing resulting incomplete information specify likelihood Obser vational variables split visible actually know state missing states nominally known missing particular datapoint Another scenario variables model observed called hidden latent variable models In case variables essential model description observed For example underlying physics model contain latent processes essential describe model directly measured Why hidden missing variables complicate proceedings In learning parameters models previously described chapter assumed complete information define variables joint model data p x Consider Asbestos Smoking Cancer network section Using multivariate variable x s c patient n complete record likelihood record p xn p sn cn p cn sn c p p sn s factorised terms table entry parameters We exploited property table entries learned considering local information maximum likelihood Bayesian frameworks Now consider case patients partial information available For example patient n incomplete record xn c s known patient cancer smoker exposure asbestos unknown Since use visible available information reasonable section assess parameters marginal likelihood p xn p sn cn p cn sn c p p sn s Hidden Variables Missing Data xvis xinv minv xvis xinv minv b Figure Missing random assumption The mechanism generates missing data depend parameter model value missing data b Missing completely random assumption The mechanism generating missing data completely independent model Note cases direction arrow xvis xinv irrelevant Using marginal likelihood result computational difficulties likelihood equation factorised product separate table parameters form fs s fa fc c In case maximisation likelihood complex parameters different tables coupled A similar complication holds Bayesian learning As saw section prior factorised CPT posterior factorised However missing variable introduces dependencies posterior parameter distribution making posterior complex In maximum likelihood Bayesian cases defined likelihood function table parameters posterior Note missing data parameter posterior non factorised For example cancer state unobserved cancer collider descendants conditional distribution simply sums left factor dependent s The missing random assumption Under circumstances valid use marginal likelihood assess parameters We partition variables x visible xvis invisible xinv set variables written x xvis xinv For visible variables observed state xvis v state invisible variables unknown To complete picture need model process informs data missing We use indicator minv denote state invisible variables unknown require model p minv xvis xinv Then datapoint contains visible invisible information p xvis v minv xinv p xvis v xinv minv xinv p minv xvis v xinv p xvis v xinv If assume mechanism generates invisible data independent parameter missing value xinv p minv xvis v xinv p minv xvis v p xvis v minv p minv xvis v xinv p xvis v xinv p minv xvis v p xvis v Only term p xvis v conveys information parameter model Therefore provided mechanism data missing depends visible states simply use marginal likelihood assess parameters This called missing random MAR assumption fig DRAFT April Hidden Variables Missing Data Example Not missing random EZsurvey org stop men street ask favourite colour All men favourite colour pink decline respond question colour men respond question Based data EZsurvey org produce histogram men s favourite colour based likelihood visible data confidently stating likes pink For simplicity assume colours blue green pink EZsurvey org attempts find histogram probabilities b g p b g p Each respondent produces visible response x dom x blue green pink m response Three men asked favourite colour giving data x1 x2 x3 blue missing green Based likelihood visible data log likelihood d data L b g p log b log g b g p Lagrange term ensures normalisation Maximising expression arrive b g p Hence EZsurvey org arrive erroneous conclusion men like pink reality men like pink don t want The unreasonable result EZsurvey org produce accounting correctly mechanism produces data In case data MAR data missing depends state missing variable The correct mechanism generates data including missing data p x1 blue p m2 p x3 green bpg b b g g p m2 p probability datapoint missing probability favourite colour pink Maximising likelihood arrive b g p expect On hand visible variable t denoting time day probability men respond question depends time t example probability having missing data high rush hour treat missing data missing random A stronger assumption MAR missing data mechanism completely independent model processes p minv xvis v xinv p minv called missing completely random This applies example latent variable models variable state missing independent Maximum likelihood Throughout remaining discussion assume missing data MAR missing completely random We partition variables visible variables v know states hidden variables h states observed For maximum likelihood learn model parameters optimising visible variables p v h p v h respect DRAFT April Expectation Maximisation Identifiability issues The marginal likelihood objective function depends parameters p v equivalent parameter solutions exist For example consider latent variable model distribution p x1 x2 x1 x2 variable x2 observed This means marginal likelihood depends entry p x1 x2 x1 x2 Given maximum likelihood solution find equivalent maximum likelihood solution provided exercise x2 x1 x2 x2 x1 x2 In cases inherent symmetry parameter space marginal likelihood For example consider network binary variables p c s p c s p p s We assume know table p s aim learn asbestos table p cancer tables p c s p c s p c s p c s denote parameter estimates We assume missing data states variable observed In case equivalent solution sense marginal likelihood given interchanging states p p tables p c s p c s p c s p c s p c s p c s p c s p c s A similar situation occurs general setting state variable consistently unobserved mixture models case point yielding inherent symmetry solution space A known characteristic maximum likelihood algorithms jostling occurs initial stages training symmetric solutions compete Expectation Maximisation The EM algorithm convenient general purpose iterative approach maximising likelihood missing data hidden variables It generally straightforward implement achieve large jumps parameter space particularly initial iterations Variational EM The key feature EM algorithm form alternative objective function parameter coupling effect discussed section removed meaning individual parameter updates achieved akin case fully observed data The way works replace marginal likelihood lower bound lower bound useful decoupled form We consider single variable pair v h v stands visible h hidden The model data p v h interest set maximising marginal likelihood p v To derive DRAFT April Expectation Maximisation Algorithm Expectation Maximisation Compute Maximum Likelihood value data hidden variables Input distribution p x dataset V Returns ML candidate t Iteration counter Choose initial setting parameters Initialisation converged likelihood converged t t n N Run datapoints qnt h n vn p hn vn t E step end t arg max N n log p hn vn qnt hn vn M step end return t The max likelihood parameter estimate bound marginal likelihood consider Kullback Leibler divergence non negative variational distribution q h v parametric model p h v KL q h v p h v log q h v log p h v q h v The term variational refers fact distribution parameter optimisation problem Using p h v p h v p v fact p v depend h KL q h v p h v log q h v q h v log p h v q h v log p v Rearranging obtain bound marginal likelihood1 log p v log q h v q h v Entropy log p h v q h v Energy The energy term called expected complete data log likelihood The bound potentially useful dependent energy term similar form fully observed case terms missing data log likelihood weighted prefactor Equation marginal likelihood bound single training example Under d assumption log likelihood training data V v1 vN sum individual log likelihoods log p V N n log p vn Summing training data obtain bound log marginal likelihood log p V L q N n log q hn vn q hn vn entropy N n log p hn vn q hn vn energy Note bound L q exact right hand equal log likelihood set q hn vn p hn vn n N The bound depends set variational distributions q Our aim try optimise bound w r t q push lower bound hopefully increase likelihood A simple iterative procedure optimise bound fix optimise w r t q fix q optimise bound w r t These known E M steps repeated convergence E step For fixed find distributions q hn vn n N maximise equation M step For fixed q hn vn n N find parameters maximise equation 1This analogous standard partition function bound statistical physics terminology energy entropy hails DRAFT April Expectation Maximisation Classical EM In variational E step fully optimal setting q hn vn p hn vn Since q fixed M step performing M step optimisation equivalent maximising energy term algorithm From onwards shall use term EM refer classical EM algorithm stated It important note EM algorithm generally guarantee find fully optimal maximum likelihood solution trapped local optima discussed example Example EM parameter model We consider model small plot fully evolution EM algorithm The model single visible variable v dom v R single state hidden variable h dom h We define model p v h p v h p h p v h e v h p h p h For observation v interest find parameter optimises likelihood p v h p v h p h e e The log likelihood plotted fig 2a optimum If state h given log likelihood single bump complex double bump case missing data To use EM approach find optimum need work energy log p v h q h v log p v h q h v log p h q h v v h q h v const There h states q distribution q h shorthand q h v normalisation requires q q Due normalisation fully parameterise q q The EM procedure iteratively optimises lower bound log p v L q q log q q log q h q h h const From initial starting EM algorithm finds q distribution optimises L q E step updates M step Depending initial solution found global local optimum likelihood fig The M step easy work analytically case new v h q h h2 q h Similarly E step sets qnew h p h v qnew h p v h p h p v e e e p v p v h p h p v h p h DRAFT April Expectation Maximisation lo g p v q h b q h c Figure The log likelihood model described example b Contours lower bound L q h For initial choice successive updates E vertical M horizontal steps plotted algorithm converging global optimum maximum likelihood setting c Starting EM algorithm converges local optimum Example Consider simple model p x1 x2 dom x1 dom x2 Assuming unconstrained distribution p x1 x2 x1 x2 aim learn data x1 x2 x3 The energy term classical EM log p x1 x2 log p x1 x2 p x2 x1 old log p x1 x2 p x1 x2 old Writing fully terms separate line gives energy log p x2 x1 old log p x2 x1 old log p x1 x2 old log p x1 x2 old log This expression resembles standard log likelihood fully observed data terms missing data weighted log parameters The parameters conveniently decoupled bound apart trivial normalisation constraint finding optimal parameters straightforward This achieved M step update gives p x2 x1 old p x2 x1 old p x1 x2 old p x1 x2 old p x2 x1 old oldx1 x2 E step etc The E M steps iterated till convergence The EM algorithm increases likelihood Whilst construction EM algorithm decrease lower bound likelihood important question log likelihood necessarily increased procedure DRAFT April Expectation Maximisation s c Figure A database containing information Smoker signifies individual smoker lung Cancer signifies individual lung Cancer Each row contains information individual individuals database We use new parameters previous parameters consecutive iterations Using q hn vn p hn vn function parameters lower bound single variable pair v h depends LB log p h v p h v log p h v p h v From definition lower bound equation log p v LB KL p h v p h v That Kullback Leibler divergence difference lower bound true likelihood We write log p v LB KL p h v p h v Hence log p v log p v LB LB KL p h v p h v The assertion true definition M step search higher value bound starting value The second assertion true non negative property Kullback Leibler divergence For single datapoint simply sum individual bound log p vn Hence reach important conclusion EM algorithm increases lower bound marginal likelihood marginal likelihood correctly EM decrease quantities Shared parameters tables It case models parameters shared components model The application EM shared parameter case essentially straightforward According energy term need identify terms shared parameter occurs The objective shared parameter sum energy terms containing shared parameter Application Belief networks Conceptually application EM training Belief Networks missing data straightforward The battle notational conceptual We begin development example intuition general case gleaned Example Consider network p c s p c s p p s set data states variable observed fig Our goal learn CPTs p c s p p s To apply EM algorithm case assume initial parameters 0a s c DRAFT April Expectation Maximisation The E step iteration t defines set distributions hidden variables hidden variable For notational convenience write qnt place q n t vn Then qn 1t p c s qn 2t p c s training examples n We M step The energy term iteration t E n log p cn sn log p log p sn qnt n log p cn sn qnt log p n qnt log p s n The final term log likelihood variable s p s appears explicitly term Hence usual maximum likelihood rule applies p s simply given relative number times s occurs database giving p s p s The contribution parameter p energy occurs terms n qnt log p qnt log p normalisation constraint log p n qnt log p n qnt Differentiating respect p solving zero derivative M step update p p n q n t n q n t n q n t N n qnt That standard maximum likelihood estimate real counts data formula replaced guessed values qnt q n t A similar story holds p c s Again need consider normalisation means p c s contributes The contribution term energy data indices n s n cn sn qnt log p c s n cn sn qnt log p c s Optimising respect p c s gives p c s n I c n I sn qnt n I cn I sn q n t n I cn I sn q n t For comparison setting complete data case p c s n I c n I sn I n I cn I sn I n I cn I sn I There intuitive relationship updates missing data case replace indicators assumed distributions q Iterating E M steps parameters converge local likelihood optimum DRAFT April Expectation Maximisation General case A belief network multivariate variable x takes general form p x p xi pa xi Some variables observed hidden We partition variable x visible hidden multivariate parts x v h Given d dataset V v1 vN interest learn tables p x maximise likelihood visible data V For datapoint index n mul tivariate variable xn vn hn decomposes visible hidden parts upper variable index typically datapoint lower index variable number From equation form energy term belief networks n log p xn qt hn vn n log p xni pa xni qt hn vn Here t indexes iteration count EM algorithm It useful define following notation qnt x qt h n vn v vn This means qnt x sets visible variables observed state defines conditional distribution unobserved variables We define mixture distribution qt x N N n qnt x The energy term left hand equation written compactly notation n log p xn qt hn vn N log p x qt x To consider right hand N log p x qt x N x log p x N n qt h n vn v vn n log p xn qt hn vn Using compact notation energy structure belief network decompose energy log p x qt x log p xi pa xi qt x log p xi pa xi qt xi pa xi qt pa xi This means maximising energy equivalent minimising log qt xi pa xi qt xi pa xi log p xi pa xi qt xi pa xi qt pa xi added constant term form Kullback Leibler divergence Since sum independent Kullback Leibler divergences optimally M step given setting pnew xi pa xi qt xi pa xi In practice storing qt x states variables x prohibitively expensive Fortunately M step requires distribution family variable xi requires local distributions qnt xi pa xi We dispense global qt x equivalently use pnew xi pa xi n q n t xi pa xi n q n t pa xi DRAFT April Expectation Maximisation Algorithm EM Belief Networks Input BN DAG dataset visible variables V Returns maximum likelihood estimate tables p xi pa xi K t Iteration counter Set pt xi pa xi initial values Initialisation p xi pa xi converged likelihood converged t t n N Run datapoints qnt x pt h n vn v vn E step end K Run variables pt xi pa xi N n q n t xi pa xi N n q n t pa xi M step end end return pt xi pa xi The max likelihood parameter estimate Using EM algorithm optimal setting E step use qt h n vn pold hn vn With notation EM algorithm compactly stated algorithm See EMbeliefnet m An illustration evolution log likelihood EM iterations given fig For readers comfortable KL based derivation describe classical approach based Lagrange multipliers specific case example Example Another belief network example Consider variable distribution discrete vari ables p x1 x2 x3 x4 x5 p x1 x2 p x2 x3 p x3 x4 p x4 x5 p x5 variables x2 x4 consistently hidden training data training data x1 x3 x5 present The distribution represented belief network x1 x2 x3 x4 x5 The M step given maximising energy According general form energy equation need consider variational distribution q h v hidden variables h x2 x4 condi tioned visible variables v x1 x3 x5 Using n datapoint index t EM iteration counter require variational distributions datapoint qt x n x n xn1 xn3 xn5 To notation compact drop iteration counter index write simply qn x2 x4 In case contributions energy form n log p xn1 x2 p x2 xn3 p xn3 x4 p x4 xn5 p xn5 qn x2 x4 written n log p xn1 x2 qn x2 x4 n log p x2 xn3 qn x2 x4 n log p xn3 x4 qn x2 x4 n log p x4 xn5 qn x2 x4 n log p xn5 DRAFT April Expectation Maximisation A useful property exploited term depends hidden variables family term represents Thus write n log p xn1 x2 qn x2 n log p x2 xn3 qn x2 n log p xn3 x4 qn x4 n log p x4 xn5 qn x4 n log p xn5 The final term set maximum likelihood Let consider difficult table p x1 x2 When table entry p x1 x2 j occur energy This happens xn1 state Since summation states variables x2 average term variable x2 state j Hence contribution energy terms form p x1 x2 j n I xn1 q n x2 j log p x1 x2 j indicator function I xn1 equals x n state zero To ensure normali sation table add Lagrange term n I xn1 q n x2 j log p x1 x2 j k p x1 k x2 j Differentiating respect p x1 x2 j equating zero n I xn1 qn x2 j p x1 x2 j p x1 x2 j n I xn1 q n x2 j Hence p x1 x2 j n I x n q n x2 j n k I x n k q n x2 j From E step qn x2 j p old x2 j xn1 xn3 xn5 This optimal distribution easy compute marginal family given evidential variables Hence M step update table pnew x1 x2 j n I x n p old x2 j xn1 xn3 xn5 n k I x n k p old x2 j xn1 xn3 xn5 If hidden data equation read pnew x1 x2 j n I xn1 I x n j All general EM case replace deterministic functions I xn2 missing variable equivalents pold x2 xn1 xn3 xn5 DRAFT April Extensions EM lo g li ke lih o o d Figure Evolution log likelihood versus iterations un der EM training procedure solving Printer Night mare missing data exercise Note rapid progress beginning convergence slow Convergence Convergence EM slow particularly number missing observations greater number visible observations In practice combines EM gradient based procedures improve convergence section Note log likelihood typically non convex function parameters This means multiple local optima solution found depends initialisation Application Markov networks Whilst examples belief networks apply EM application learning parameters Markov networks missing data For MN defined visible hidden variables separate parameters c clique c p v h Z c c h v c EM variational bound log p v H q c log c h v c q h logZ H p entropy function distribution H p log p x p x Whilst bound decouples clique parameters second term parameters coupled normalisation Z v h C c c h v c C Because directly optimise bound parameter parameter basis One approach use additional bound logZ iterative scaling section decouple clique parameters Z leave details exercise interested reader Extensions EM Partial M step It necessary find optimum energy term iteration As long finds parameter higher energy current parameter conditions required section hold likelihood decrease iteration Partial E step The E step requires find optimum log p V N n log q hn vn q hn vn N n log p hn vn q hn vn respect q hn vn The fully optimal setting q hn vn p hn vn DRAFT April Extensions EM For guaranteed increase likelihood iteration section required fully optimal setting q Unfortunately general guarantee partial E step partially optimsise lower bound respect q fixed increase likelihood Of course guaranteed increase lower bound likelihood likelihood We discuss partial E step scenarios Intractable energy The EM algorithm assumes calculate log p h v q h v However cases computationally carry averages respect fully optimal form q In case consider restricted class Q q distributions averages carried For example class averages computationally tractable factorised distributions q h v j q hj v popular class Gaussian q distri butions We find best distribution class Q numerical optimisation routine qopt argmin q Q KL q h p h v Alternatively assume certain structured form q distribution learn optimal factors distribution free form functional calculus This approach taken example section Viterbi training An extreme case partial E step restrict q hn vn delta function In case entropic term log q hn vn q hn vn constant zero discrete h optimal delta function q set q hn vn hn hn hn argmax h p h vn When energy average respect q trivial energy simply N n log p hn v n The corresponding bound log likelihood log p V H N n log p hn v n H entropy delta function zero discrete h As partial justification technique provided sufficient data hope likeli hood function parameter sharply peaked optimum value This means convergence approximation posterior p h v opt delta function reasonable update EM Viterbi training produce new approximately opt For highly suboptimal p h v far delta function Viterbi update reliable terms leading increase likelihood This suggests initialisation Viterbi training critical standard EM Note Viterbi training corresponds partial E step EM training restricted class q distribution guaranteed increase lower bound log likelihood likelihood This technique popular speech recognition community training HMMs section terminology Viterbi training arises DRAFT April A failure case EM Stochastic EM Another approximate q hn vn distribution popular use empirical distribution formed samples fully optimal distribution p hn vn That draws samples chapter discussion sampling hn1 h n L p h n vn forms q distribution q hn vn L L l hn hnl The energy proportional N n L l log p hnl v n Viterbi training energy computationally tractable restricted q class Provided samples p hn vn reliable stochastic training produce energy function average characteristics true energy classical EM algorithm This means solution obtained stochastic EM tend classical EM number samples increases A failure case EM Whilst EM algorithm useful cases work Consider likelihood form p v h p v h p h p v h v f h If attempt EM approach fail exercise To happens M step sets new argmax log p v h p h v old argmax log p v h p h v old fact model p h independent In case p v h v f h p h old v f h old p h optimising energy gives update new argmax log v f h p h v old Since p h old zero expect h v f h old energy term log f h old f h This effectively negative infinity old zero old Hence old optimal EM algorithm fails produce meaningful parameter update This situation occurs practice noted particular context Independent Component Analysis Whilst delta function output clearly extreme similar slowing parameter updates occur term p v h close deterministic One attempt heal behaviour deriving EM algorithm based distribution p v h v f h p h n v h 2For discrete variables Kronecker delta energy attains maximal value zero old In case continuous variables log Dirac delta function defined Considering delta function limit narrow width Gaussian small finite width energy largest old DRAFT April Variational Bayes n v h arbitrary distribution The original deterministic model corresponds p0 v h Defin ing p v h p v h p v p0 v n v EM algorithm p v decrease likelihood satisfies p v new p v old p0 v new p0 v old implies p0 v new p0 v old This means EM algorithm non deterministic case guaranteed increase likelihood deterministic model p0 v iteration convergence See application antifreeze technique learning Markov Decision Processes EM Variational Bayes Variational Bayes analogous EM helps deal hidden variables Bayesian method returns posterior distribution parameters single best given maximum likelihood To notation simple ll initially assume single datapoint observation v Our interest parameter posterior p v p v p h p v h p The VB approach assumes factorised approximation joint hidden parameter posterior fig p h v q h q The optimal settings factors q h q found minimising Kullback Leibler divergence p h v q h q discussed A bound marginal likelihood By minimising KL divergence KL q h q p h v log q h q h log q q log p h v q h q arrive bound log p v log q h q h log q q log p v h q h q Minimizing Kullback Leibler divergence respect q q h equivalent obtaining tightest lower bound log p v A simple coordinate wise procedure fix q solve q h vice versa analogous E M step EM algorithm E step qnew h argmin q h KL q h qold p h v M step qnew argmin q KL qnew h q p h v For set observations V hidden variables H procedure described algorithm For distributions q H q parameterised constrained best distributions minimal KL sense returned In general iteration VB guaranteed increase bound marginal likelihood marginal likelihood Like EM algorithm VB suffer local maxima issues This means converged solution dependent initialisation DRAFT April Variational Bayes Algorithm Variational Bayes t Iteration counter Choose initial distribution q0 Initialisation converged likelihood bound converged t t qt H arg minq H KL q H qt p H V E step qt arg minq KL qt H q p H V M step end return qt The posterior parameter approximation vn hn N hn N b Figure Generic form model hidden variables b A factorised posterior approximation Variational Bayes Unconstrained approximations For fixed q contribution KL divergence equation q h log q h q h log p v h q h q KL q h p h const p h Z exp log p v h q Z normalising constant Hence fixed q E step sets q h p q h exp log p v h q exp log p v h q Similarly fixed q h M step sets q exp log p v h q h p exp log p v h q h These E M step updates iterated convergence d Data Under d assumption obtain bound marginal likelihood dataset V v1 vN log p V n log q hn q hn log q q log p v n hn q hn q The bound holds q hn q tightest converged estimates VB procedure For d dataset straightforward loss generality assume q h1 hN n q hn Under arrive algorithm DRAFT April Variational Bayes Algorithm Variational Bayes d data t Iteration counter Choose initial distribution q0 Initialisation converged likelihood bound converged t t n N Run datapoints qnt h n exp log p vn hn qt E step end qt p exp n log p vn hn qnt hn M step end return qnt The posterior parameter approximation EM special case variational Bayes If wish find summary parameter posterior corresponding likely point use restricted q form q single optimal value parameter If plug assumption equation obtain bound log p v log q h q h log p v h q h const The M step given argmax log p v h q h log p For flat prior p const equivalent energy maximisation EM algorithm Using single optimal value VB E step update q hn qnt h p v h p h v standard E step EM Hence EM special case VB flat prior p const delta function approximation parameter posterior An example VB Asbestos Smoking Cancer network In section showed apply Bayesian methods train belief network giving rise posterior distribution parameter In previous discussion data fully observed Here wish revisit case assuming observations missing This complicates Bayesian analysis motivates approximate methods VB Let s reconsider Bayesian learning binary variable asbestos smoking cancer network described section p c s p c s p p s use factorised parameter prior p c p p s When data V d observed parameter posterior factorises However discussed section state asbestos observed parameter posterior longer factorises p s c V p p s p c p V s c p p s p c n p vn s c p p s p c n p sn s p cn sn c p DRAFT April Variational Bayes c s c s N c s N b Figure A model relation ship lung Cancer Asbestos expo sure Smoking factorised parame ter priors Variables c s observed variable consistently missing b A factorised parameter posterior approxi mation summation prevents factorisation product individual table parameters This means awkward represent posterior exactly posterior distributions parameter table This situation VB useful enables impose factorisations posterior In VB consider approximation posterior parameters latent variables q h q In case s c latent variables asbestos datapoint For example visible variables smoking cancer V sn cn n N The exact joint distribution variables p s c aN V p p s p c prior n p cn sn c p sn s p posterior In VB factorised assumption splitting parameters latent variables p a1 N V q q a1 N From general results equation ignoring terms independent exponent q a1 N exp n log p cn sn c q c n log p q From immediately approximation automatically factorises q a1 N n q Similarly equation q p exp n log p cn sn c p sn s p q p exp n log p cn sn c q n p sn s exp n log p q Since parameter prior factorised collect terms s c VB assumption automatically results factorised parameter posterior approximation Hence VB approximation form fig p s c aN V q q c q s n q All remains form E step q updates M step q updates described DRAFT April Variational Bayes M step q updates From q p n exp log p q log p q q n log q n log Hence exp log p q q q It convenient use Beta distribution prior p B 1a posterior approximation Beta distribution q B n q n q A similar calculation gives q s B s n I sn n I sn Finally table parental states c For example q c s B c n I sn q n I sn q These reminiscent standard Bayesian equations equation missing data counts replaced q s E step q updates From equation q exp log p cn sn c q c log p n q For example assume datapoint n s state c state q exp log c s q c s log q q exp log c s q c s log q These updates require Beta distribution averages log B log B straightforward compute exercise The complete VB procedure given iterating equations convergence DRAFT April Optimising Likelihood Gradient Methods Given converged factorised approximation computing marginal table p V straight forward approximation p V q q n q n n q n n q n The application VB learning tables arbitrarily structured BNs straightforward extension technique outlined Under factorised approximation q h q h q obtain simple updating equation analogous data case missing data replaced variational approximations Nevertheless variable missing parents number states average respect q distribution intractable constraints form approximation additional bounds required One readily extend case Dirichlet distributions multinomial variables exercise Indeed extension exponential family straightforward Optimising Likelihood Gradient Methods The EM algorithm typically works missing information small compared complete information In case EM exhibits approximately convergence Newton based gradient method However fraction missing information approaches unity EM converge slowly In case continuous parameters alternative compute gradient likelihood directly use standard continuous variable optimisation routine The gradient straightforward compute following identity Consider log likelihood L log p v The derivative written L p v p v p v h p v h At point derivative inside integral L p v h p v h h p h v log p v h log p v h p h v log f x f x f x The right hand average derivative log complete likelihood This closely related derivative energy term EM algorithm note average performed respect current distribution parameters old EM case Used way computing derivatives latent variable models relatively straightforward These derivatives standard optimisation routine conjugate gradients Undirected models Whilst equation represents general case possible easily compute required averages Consider undirected model contains hidden visible variables p v h Z exp v h For d data log likelihood visible variables assuming discrete v h L n log h exp vn h log h v exp v h DRAFT April Exercises gradient L n vn h p h vn clamped average v h p h v free average For Markov Network p intractable partition function Z computed efficiently averages w r t p computed exactly gradient particularly difficult estimate difference averages needs estimated Even getting sign gradient correct computationally difficult For reason learning general unstructured Markov networks particularly difficult unstructured Boltzmann machine hidden units particular case point Summary Provided data missing random safely learn parameters maximising likelihood observed data Variational Expectation Maximisation general purpose algorithm maximum likelihood learning missing information The classical EM algorithm special case V EM guarantees improvement non decrease likelihood iteration Bayesian learning case missing information potentially problematic posterior typically factored according prior assumptions In case approximations useful variational Bayes assumes factorisation parameters latent missing variables The gradient easily computed latent variable models optimisation routine This provides alternative training approach cases EM slow converge Code demoEMchestclinic m Demo EM learning Chest Clinic Tables In demo code original Chest Clinic network draw data samples network Our interest use EM algorithm estimate tables based data parts data missing random We assume know correct BN structure CPTs unknown We assume logic gate table known need learn EMbeliefnet m EM training Belief Network The code implements maximum likelihood learning BN tables based data possibly missing values Exercises Exercise Printer Nightmare continued Continuing BN given fig following table represents data gathered printer indicates entry missing Each column represents datapoint Use EM algorithm learn CPTs network DRAFT April Exercises fuse assembly malfunction drum unit toner poor paper quality worn roller burning smell poor print quality wrinkled pages multiple pages fed paper jam The table contained EMprinter mat states nan place BRMLtoolbox requires states numbered Given wrinkled pages burning smell poor print quality probability drum unit problem Exercise Consider following distribution discrete variables p x1 x2 x3 x4 x5 p x1 x2 x4 p x2 x3 p x3 x4 p x4 x5 p x5 variables x2 x4 consistently hidden training data training data x1 x3 x5 present Derive EM update table p x1 x2 x4 Exercise Consider simple variable BN p y x p y x p x y x binary variables dom x dom y You set training data yn xn n N cases xn missing We specifically interested learning table p x data A colleague suggests set p x simply looking datapoints x observed setting p x fraction observed x state Explain suggested procedure relates maximum likelihood EM Exercise Assume sequence v1 vT vt V generated Markov chain For single chain length T p v1 vT p v1 T t p vt vt For simplicity denote sequence visible variables v v1 vT For single Markov chain labelled h p v h p v1 h T t p vt vt h In total set H Markov chains h H The distribution visible variables p v H h p v h p h There set training sequences vn n N Assuming sequence vn inde pendently identically drawn Markov chain mixture model H components derive Expectation Maximisation algorithm training model The file sequences mat contains set fictitious bio sequence cell array sequences acup n t Thus sequences sequence GTCTCCTGCCCTCTCTGAAC consists timesteps There sequences total Your task cluster sequences clusters assum ing cluster modelled Markov chain State sequences belong assigning sequence vn state p h vn highest You wish use mixMarkov m DRAFT April Exercises Exercise Write general purpose routine VBbeliefnet pot x pars lines EMbeliefnet m performs Variational Bayes Dirichlet prior factorised parameter approximation As sume global local parameter independence prior approximation q section Exercise Consider layered Boltzmann Machine form p v h1 h2 h3 Z v h1 h1 h2 h2 h3 dim v dim h1 dim h2 dim h3 V x y exp V j Wijxiyj Aijxixj Bijyiyj All variables binary states parameters layer l l Wl Al Bl In terms fitting model visible data v1 vN layered model powerful fitting layered model factor h2 h3 present layer case If use restricted potential x y exp j Wijxiyj layered model powerful able fit visible data layered model Exercise The sigmoid Belief Network defined layered network p xL L l p xl xl vector variables binary components xl wl width layer l given wl In addition p xl xl wl p xl 1i x l p xl 1i x l wTi lx l x e x weight vector wi l describing interaction parental layer The layer p x L describes factorised distribution p xL1 p x L wL Draw Belief Network structure distribution For layer x0 computational complexity computing likelihood p x0 assuming layers equal width w Assuming fully factorised approximation equal width network p x1 xL x0 L l w q xli write energy term Variational EM procedure single data observation x0 discuss tractability computing energy Exercise Show find components b g p maximise equation DRAFT April Exercises Exercise A probability table p x1 x2 j j j j j learned maximal marginal likelihood x2 observed Show given maximal marginal likelihood solution marginal likelihood score DRAFT April Exercises DRAFT April CHAPTER Bayesian Model Selection So far ve Bayes rule inference parameter level Applied model level Bayes rule gives method evaluating competing models This provides alternative classical statistical hypothesis testing techniques Comparing Models Bayesian Way Given models M1 M2 parameters associated parameter priors p x M1 p x M1 p M1 p x M2 p x M2 p M2 compare performance models fitting set data D x1 xN The application Bayes rule models gives framework answering questions like form Bayesian hypothesis testing applied model level More generally given indexed set models M1 Mm associated prior beliefs appropriateness model p Mi interest model posterior probability p Mi D p D Mi p Mi p D p D m p D Mi p Mi Model Mi parameterised model likelihood given p D Mi p D Mi p Mi di In discrete parameter spaces integral replaced summation Note number parameters dim need model A point caution p Mi D refers probability relative set models specified M1 Mm This absolute probability model M fits To compute quantity require specify possible models Whilst interpreting posterior p Mi D requires care comparing competing model hypotheses Mi Mj straightforward requires Bayes factor p Mi D p Mj D Posterior Odds p D Mi p D Mj Bayes Factor p Mi p Mj Prior Odds Illustrations coin tossing b Figure Discrete prior p Mfair model fair coin A perfectly unbiased coin corresponds prior assume general form illustrate richer prior assumptions b Prior p Mbiased biased unfair coin In cases making explicit choices consider fair unfair require integration summation possible models We posterior odds posterior Bayes factor Illustrations coin tossing We ll consider illustrations testing coin biased The uses discrete parameter space mathematics simple In second use continuous parameter space In cases dataset D consists sequence x1 xN outcomes dom xn heads tails A discrete parameter space We consider competing models corresponding fair coin biased coin The bias coin probability coin land heads specified truly fair coin For simplicity assume dom For fair coin use distribution p Mfair fig 1a biased coin distribution p Mbiased fig 1b Note priors essentially encode subjective beliefs mean coin biased We free choose prior distributions wish A strength Bayesian framework spell mean case biased unbiased This subjective beauty framework disagree subjective choice prior free declare competing assumptions carry corresponding analysis adding possible models available For model M likelihood model generate data D contains NH heads NT tails given p D M p D M p M NH NT p M 1NH NT p M 9NH NT p M Assuming p Mfair p Mbiased posterior odds given ratio model likelihoods Example Discrete parameter space Heads Tails Using NH NT equation obtain p D Mfair p D Mbiased The posterior odds p Mfair D p Mbiased D indicating little choose models DRAFT April Illustrations coin tossing b Figure Probability density priors probability Head p For fair coin choose p Mfair B b For biased coin choose p Mbiased B B Note different vertical scales cases Heads Tails For case repeating calculation obtain p D Mfair p D Mbiased The posterior odds p Mfair D p Mbiased D indicating times belief biased model opposed fair model A continuous parameter space Here repeat calculation continuous parameter spaces As discrete case free choose prior wish consider simple priors integrations required straightforward Fair coin For fair coin uni modal prior appropriate We use Beta distribution p Mfair B b B b B b b convenience conjugate binomial distribution required integrations trivial For fair coin prior chose b shown fig 2a The likelihood given p D Mfair p NH NT B b b NH NT B b NH NT b B NH NT b B b Biased coin For biased coin use bimodal distribution formed convenience mixture Beta distributions p Mbiased B a1 b1 B a2 b2 shown fig 2b The model likelihood p D Mbiased given p D Mbiased p Mbiased NH NT B NH a1 NT b1 B a1 b1 B NH a2 NT b2 B a2 b2 DRAFT April Occam s Razor Bayesian Complexity Penalisation Figure The likelihood total dice score p t n n n dice Plotted horizontal axis total score t The vertical line marks comparison p t n different number die The complex models reach states lower likelihood normalisation t Assuming prior preference fair biased coin p M const repeating scenario discrete parameter case Example Continuous parameter space Heads Tails Here p D Mfair p D Mbiased The posterior odds p Mfair D p Mbiased D indicating little choose models Heads Tails Here p D Mfair p D Mbiased The posterior odds p Mfair D p Mbiased D indicating times belief biased model opposed fair model Occam s Razor Bayesian Complexity Penalisation We return dice scenario section There assumed dice scores s1 s2 known Only sum scores t s1 s2 known We computed posterior joint score distribution p s1 s2 t die We repeat calculation multiple dice twist don t know dice are1 sum scores That know n si told number dice involved n Assuming priori number n equally likely posterior distribution n From Bayes rule need compute posterior distribution models p n t p t n p n p t 1This description Occam s razor Taylan Cemgil DRAFT April Occam s Razor Bayesian Complexity Penalisation n Figure The posterior distribution p n t number die given observed summed score In likelihood term given p t n s1 sn p t s1 sn n s1 sn p t s1 sn p si s1 sn I t n si p si p si scores si By enumerating n states explicitly compute p t n displayed fig The important observation models explaining data complex n increases states accessible Assuming p n const posterior p n t plotted fig A posteriori plausible models n rest complex impossible Occam s Razor For model M parameters likelihood generating data given p D M p D M p M To simplify argument place flat priors parameter space p M V V volume number states discrete case parameter spaces Then p D M p D M V We approximate likelihood p D M thresholding value p D M L p D M p D M That likelihood appreciable bigger value L value Then p D M L V V V p D M One interpret model likelihood p D M approximately high likelihood value L multi plied fraction parameter volume likelihood high b Figure The likelihood p D Mcomplex higher maximum value simpler model likelihood drops quickly away regions high likelihood b The likelihood p D Msimple lower maximum value complex model likelihood changes quickly away regions high likelihood The corre sponding volume parameter space model fits higher simpler model DRAFT April Occam s Razor Bayesian Complexity Penalisation x y x y b x y c Figure Data wish fit regression model b The best simple model fit y ax maximum likelihood c The best complex model fit y ax cos bx maximum likelihood Consider models Msimple Mcomplex corresponding parameters Then flat parameter priors approximate p D Msimple L simple V simple Vsimple p D Mcomplex L complex V complex Vcomplex At point useful reflect constitutes complex model This characterised relative flexibility data generating process compared simple model Consequently means space complex model generate different datasets compared given dataset D This form parameter sensitivity meaning likelihood generating observed data D typically drop dramatically away regions parameter space model fits Hence simple model Msimple similar maximum likelihood complex model L simple L complex typically fraction parameter space likelihood appreciable smaller complex model simple model meaning p D Msimple p D Mcomplex fig If prior preference model p Msimple p Msimple Bayes factor given p Msimple D p Mcomplex D p D Msimple p D Mcomplex Bayes factor typically prefer simpler competing models similar maximum likelihood values This demonstrates Occam s razor effect Bayesian model inference penalises models complex As example effect consider regression problem fig consider following models clean underlying regression function Msimple y0 ax Mcomplex y0 ax cos bx To account noise observations y y0 N use mode p y x Msimple N y ax The likelihood collection Y independent observations set inputs X p Y X Msimple p Msimple N n p yn xn Msimple DRAFT April A continuous example curve fitting b b b Figure Likelihood plots problem fig The likelihood p Y X b Mcomplex b The likelihood p Y X Msimple plotted scale This displays characteristic complex model likelihood drops dramatically small distance parameter space point high likelihood Whilst maximum likelihood complex model higher simpler model volume parameter space complex model fits data smaller simpler model giving rise p Y X Msimple p Y X Mcomplex Similarly p y x b Mcomplex N y ax cos bx p Y X Mcomplex b p b Mcomplex N n p yn xn b Mcomplex Using discrete set values evenly spaced discrete values b evenly spaced compute corresponding likelihoods p Y X Msimple p Y X b Mcomplex For data maximum likelihoods max p Y X Msimple max b p Y X b Mcomplex complex model higher maximum likelihood However fig fraction parameter space complex model fits data relatively small Using flat prior parameter spaces models obtain p Y X Msimple p Y X Mcomplex 87e Whilst complex model higher maximum likelihood value factor roughly times likely correct model compared simpler model A continuous example curve fitting Consider additive set periodic functions y0 x w0 w1 cos x w2 cos 2x wK cos Kx This conveniently written vector form y0 x wT x DRAFT April Approximating Model Likelihood x K dimensional vector elements x cos x cos 2x cos Kx T vector w contains weights additive function We given set dataD xn yn n N drawn distribution yn clean y0 x n corrupted additive zero mean Gaussian noise variance yn y0 x n n n N n fig fig Assuming d data interested posterior probability number coefficients given observed data p K D p D K p K p D p K n p x n p D p y yN x1 xN K We assume priori preference number frequency components model p K const The likelihood term given integral p y1 yN x1 xN K w p w K N n p yn xn w K For p w K N w IK integrand Gaussian w straightforward evaluate integral section exercise log p y1 yN x1 xN K N log N n yn bTA 1b log det A K log A I N n xn T xn b N n yn xn Assuming sampled data model K components fig 9a Given data assuming know correct noise level prior precision task infer number components K generate data The posterior p K D plotted fig 9b sharply peaked K value generate data The clean posterior mean reconstructions yn0 D K plotted fig 9c Approximating Model Likelihood For model continuous parameter vector dim K data D model likelihood p D M p D M p M d For generic expression p D M p M exp f f particularly simple form quadratic example large K integral high dimensional exactly evaluated In order implement Bayesian model comparison methods practice typically need use form approximation model likelihood w K xn yn0 yn N Figure Belief Network representation Hierarchical Bayesian Model regression d data assumption Note intermediate nodes yn0 included highlight role clean underlying model Since p y w x y0 p y y0 p y0 w x y0 N y y0 y0 wTx N y wTx desired away intermediate node y0 place directly arrows w x n yn DRAFT April Approximating Model Likelihood b c Figure The data generated additive Gaussian noise K component model b The posterior p K D c The reconstruction data w T x w mean posterior vector optimal dimensional model p w D K Plotted continuous line reconstruction Plotted dots true underlying clean data Laplace s method A simple approximation given Laplace s method section finds optimum posterior fits Gaussian point based local curvature giving log p D M log p D M log p M log det 2H MAP solution argmax p D M p M H Hessian f log p D M p M evaluated For data D x1 xN d generated specialises p D M p M N n p xn M d In case Laplace s method computes optimum function f log p M N n log p xn M A partial justification Laplace s approximation number datapoints N increases posterior typically increasingly peaked single likely explanation data This means narrow width Gaussian tend good approximation large data limit Quantify ing approximation works typically difficult The Laplace method s popularity stems simplicity clearly cases posterior known strongly non Gaussian case Laplace s method caution Bayes information criterion BIC The Bayes information criterion simpler version Laplace s method replaces exact Hessian crude approximation For d data Hessian scales number training examples N somewhat severe approximation set H NIK K dim Continuing notation Laplace s method gives approximation log p D M log p D M log p M K log K logN DRAFT April Bayesian Hypothesis Testing Outcome Analysis For simple prior penalises length parameter vector p M N I reduces log p D M log p D M T K logN The Bayes Information Criterion approximates ignoring penalty term giving BIC log p D M K logN Typically BIC criterion specific prior specified case given maximum likelihood setting The BIC criterion approximate way compare models term K logN penalises model complexity In general Laplace approximation equation preferred BIC criterion correctly accounts uncertainty posterior parameter estimate Other techniques aim improve Laplace method discussed section section Bayesian Hypothesis Testing Outcome Analysis In outcome analysis wish analyse results experimental data We assume detailed model data generating mechanism asking generic questions results support basic hypothesis classifiers performing differently The techniques discuss general phrase terms classifier analysis concreteness The central question consider assess classifiers performing dif ferently For techniques based Bayesian classifiers principle direct way estimate suitability model M computing p M D p M p D M p D M We consider fortunate situation information presumed available test perfor mance classifiers To outline basic issue consider classifiers A B predict class test examples Classifier A makes errors correct classifications classifier B makes errors correct classifications Is classifier A better classifier B Our lack confidence pronouncing A better B results small number test examples On hand classifier A makes errors correct classifications whilst classifier B makes errors correct classifications intuitively confident classifier A better classifier B Perhaps practically relevant question machine learning perspective probability classifier A outperforms classifier B given available test information Whilst question addressed Bayesian procedure section focus simpler question classifier A B Outcome analysis Consider situation classifiers A B tested data example test set outcome pair oa n ob n n N N number test data points oa Q similarly ob That Q possible types outcomes occur For example binary classification typically cases dom o TruePositive FalsePositive TrueNegative FalseNegative If classifier predicts class c true false truth class t true false defined TruePositive c true t true FalsePositive c true t false TrueNegative c false t false FalseNegative c false t true We oa oa n n N outcomes classifier A similarly ob ob n n N classifier B To specific hypotheses wish test DRAFT April Bayesian Hypothesis Testing Outcome Analysis oa ob oa ob b P oa ob c Figure Hindep Corresponds comes classifiers independently gen erated b Hsame outcomes generated distribution c Hdep comes dependent Hindep oa ob different categorical distributions Hsame oa ob categorical distribution In cases use categorical models p oc q H cq unknown parameters c Hypothesis correspond parameters b classifiers hypothesis different parameters discuss In Bayesian framework want find likely model hypothesis responsible generating data For hypothesis H given p H oa ob p oa ob H p H p oa ob p H prior belief H correct hypothesis Note normalising constant p oa ob depend hypothesis For hypotheses independence trials assumption p oa ob H N n p oa n ob n H To progress need clarify meaning hypotheses Hindep model likelihood From Bayes rule write posterior hypothesis probability p Hindep oa ob p oa ob Hindep p oa ob p oa ob Hindep p Hindep p oa ob The outcome model classifier A specified continuous parameters giving p oa Hindep similarly use classifier B The finite data means uncertain parameter values joint term numerator p oa ob Hindep p oa ob Hindep p Hindep p Hindep dd p Hindep p oa Hindep p Hindep d p ob Hindep p Hindep d assumed p Hindep p Hindep p Hindep p oa ob Hindep p oa Hindep p ob Hindep See fig 10a depiction independence assumptions Note expect specific constraint models A B different However models assumed independent parameters sampled effectively infinite set continuous priori probability values randomly sampled zero consider Hindep equivalent Hdifferent Since dealing categorical distributions convenient use Dirichlet prior conjugate categorical distribution p Hindep Z u q uq q Z u Q q uq Q q uq DRAFT April Bayesian Hypothesis Testing Outcome Analysis The prior hyperparameter u controls strongly mass distribution pushed corners simplex fig Setting uq q corresponds uniform prior The likelihood observing oa given p oa Hindep p Hindep d q aq q Z u q uq q d Z u Z u vector components aq number times variable state q data Hence p oa ob Hindep p Hindep Z u Z u Z u b Z u Z u given equation Hsame model likelihood In Hsame hypothesis outcomes classifiers generated categorical distribution fig 10b Hence p oa ob Hsame p Hsame p oa Hsame p ob Hsame p Hsame d p Hsame Z u b Z u Bayes factor If assume prior preference hypothesis p Hindep p Hsame p Hindep oa ob p Hsame oa ob Z u Z u b Z u Z u b The higher ratio believe data generated different categorical distributions Example Two people classify expression image happy sad normal states respectively Each column data represents image classified people person row person second row Are people essentially agreement To help answer question perform Hindep versus Hsame test From data count vector person person Based flat prior categorical distribution assuming prior preference hypothesis posterior Bayes factor p persons classify differently p persons classify Z Z Z Z Z function given equation This strong evidence people classifying images differently Below discuss examples Hindep versus Hsame test As quantities need test vector counts data We assume kinds outcomes Q example dom o good bad ugly want test classifiers essentially producing outcome distributions different Throughout assume flat prior table entries u DRAFT April Bayesian Hypothesis Testing Outcome Analysis Example Hindep versus Hsame We outcome counts b The posterior Bayes factor equation strong evidence favour classifiers different Alternatively consider outcome counts b Then posterior Bayes factor equation weak evidence classifiers different As final example consider counts b This gives posterior Bayes factor equation strong evidence classifiers statistically In cases results consistent actual model fact generate count data Dependent outcome analysis Here consider case outcomes dependent For example case classifier A works classifier B work Our interest evaluate hypothesis Hdep outcomes classifiers dependent To assume categorical distribution joint states fig 10c p oa n ob n P Hdep Here P Q Q matrix probabilities P ij p oa ob j P ij probability A makes outcome B makes outcome j Then p o Hdep p o P Hdep dP p o P Hdep p P Hdep dP convenience write o oa ob Assuming Dirichlet prior P hyperparameters U p o Hdep p Hdep Z vec U Z vec U vec D vector formed concatenating rows matrix D Here count matrix ij equal number times joint outcome oa ob j occurred N datapoints We assume uniform prior U ij j Testing dependencies outcomes Hdep versus Hindep To test outcomes classifiers dependent Hdep hypothesis independent Hindep use assuming p Hindep p Hdep p Hindep o p Hdep o Z u Z u Z u b Z u Z vec U Z vec U Example Hdep versus Hindep DRAFT April Bayesian Hypothesis Testing Outcome Analysis Consider outcome count matrix b Then p Hindep o p Hdep o strong evidence classifiers perform independently Consider outcome count matrix b Then p Hindep o p Hdep o strong evidence classifiers perform dependently These results fact consistent way data generated case Is classifier A better B We return question began outcome analysis Given common scenario observ ing number binary errors classifier A test set number B classifier better This corresponds special case binary classes Q dom e correct incorrect Then probability classifier A generates correct label similarly b way judge compare hypotheses Ha b oa ob different binomial distributions corresponding probabilities b b Hsame oa ob binomial distribution Writing D oA oB assuming independent parameter priors p D Ha b b p oA p oB b p p b Using p oA correct incorrect p oB b b correct b b bincorrect Beta distribution priors p B u1 u2 p b B b u1 u2 readily Beta function B x y p D Ha b B u1 correct u2 incorrect B u1 b correct u2 b incorrect B u1 u2 b p b D Ha b DRAFT April Bayesian Hypothesis Testing Outcome Analysis p b D Ha b b B u1 acorrect u2 aincorrect B b u1 bcorrect u2 bincorrect The integral needs computed numerically betaXbiggerY m For Hsame hypothesis b p D Hsame B u1 u2 u1 correct b correct u2 aincorrect bincorrect B u1 correct b correct u2 incorrect b incorrect B u1 u2 The question A better B addressed computing p D Ha b p D Hsame B u1 correct u2 incorrect B u1 b correct u2 b incorrect B u1 u2 B u1 correct b correct u2 incorrect b incorrect p b D Ha b Examining quotient term related Hindep hypothesis p D Ha b p D Hsame p D Hindep p D Hsame p b D Ha b second term decreases Hindep versus Hsame Bayes factor This intuitive Ha b places constraints parameter space Hindep Example Classifier A makes errors correct classifications classifier B makes errors correct classifications Using flat prior u1 u2 gives p b oA oB Hindep betaXbiggerY The Bayes factor p D Ha b p D Hsame B B B B On hand classifier A makes errors correct classifications whilst classifier B makes errors correct classifications p b oA oB Hsame betaXbiggerY p D Ha b p D Hsame B B B B This demonstrates intuitive effect proportion correct incorrect classifications doesn t change scenarios confidence determining better classifier increases data See fig Summary Bayes rule enables evaluate models based fit data model likelihood There need explicitly penalise complex models Bayesian approach automatically incorporates Occam s razor effect integral posterior parameter distribution DRAFT April Exercises b Figure Two classifiers A B posterior distributions probability classify correctly uniform Beta prior For A correct incorrect labels B x solid curve B correct incorrect B y dashed curve Also plotted posterior assuming classifiers B x dot ted curve Whilst posterior overlap classifiers A B small overlap posterior assumption classifiers For reason signifi cannot evidence A better B b For A correct incorrect labels solid curve B x B correct incorrect B y dashed curve As data increases overlap distributions decreases certainty classifier better correspondingly increases Computing model likelihood complex task In continuous parameter models Laplace s method provides simple approximation BIC cruder version Laplace s approximation Assessing performance basis limited data achieved simple Bayesian hypothesis testing Code demoBayesErrorAnalysis m Demo Bayesian error analysis betaXbiggerY m p x y x B x b y B y c d Exercises Exercise Write program implement fair biased coin tossing model selection example section discrete domain Explain overcome potential numerical issues dealing large NH NT order Exercise You work Dodder s hedge fund manager wants model day returns yt based current day information xt The vector factors day xt captures essential aspects market He argues simple linear model yt K k wkxkt reasonable asks find weight vector w based historical information D xt yt t T In addition gives measure volatility 2t day DRAFT April Exercises Under assumption returns d Gaussian distributed p y1 T x1 T w T t p yt xt w T t N yt w Txt t explain set weight vector w maximum likelihood Your hedge fund manager convinced factors useless prediction wishes remove possible To decide use Bayesian model selection method use prior p w M N w I M 2K indexes model Each model uses subset factors By translating integer M binary vector representation model describes factors For example K models model yt w3x3 weight prior p w3 N w3 Similarly model yt w1x1 w2x2 w3x3 p w1 w2 w3 N w1 w2 w3 I3 You decide use flat prior p M const Draw hierarchical Bayesian network model explain find best model data Bayesian model selection suitably adapting equation Using data dodder mat perform Bayesian model selection K find factors x1 x6 likely explain data Exercise Here derive expression alternative form Starting p w N n p yn w xn K N w I n N yn wT xn K e wTw N e n y n wT xn Show expressed K N e n y n e wTAw bTw A I n xn T xn b n yn xn By completing square section derive Since yn n N linearly related w w Gaussian distributed joint vector y1 yN Gaussian distributed Using Gaussian propagation result derive alternative expression log p y1 yN x1 xN Exercise Similar example people classify images categories Each col umn table represents classifications image row class son middle person person You wish estimate p class image person assuming person image class drawn independently distribution DRAFT April Exercises Assuming prior preference hypotheses uniform prior counts compute p persons classify differently p persons classify Exercise Consider classifier makes R correct classifications W wrong classifications Is classifier better random guessing Let D represent fact R right W wrong answers Assume classifications d Show hypothesis data generated purely random likelihood p D Hrandom 5R W Define probability classifier makes error Then p D R W Now consider p D Hnon random p D p Show Beta prior p B b p D Hnon random B R W b B b B b Beta function Considering random non random hypotheses priori equally likely p Hrandom D 5R W 5R W B R W b B b For flat prior b compute probability correct incorrect classifications data purely random distribution according equation Repeat correct incorrect classifications Show standard deviation number errors random classifier R W relate computation Exercise Our interest discuss method learn direction edge belief network Consider distribution p x y My x p x y x y p y y parameters conditional probability tables For prior p p x y p y d data D xn yn n N likelihood data p D My x p x y p y n p xn yn x y p yn y For binary variables x y p y y y p x y x y y Beta distribution priors p y B y p y B y y y p p p DRAFT April Exercises p D My x given B x y x y B B x y x y B Now derive similar expression model edge direction reversed p D Mx y assuming values hyperparameters Using derive simple expression Bayes factor p D My x p D Mx y relate expression complexity tables DRAFT April Exercises DRAFT April Part III Machine Learning Introduction Part III Machine Learning fundamentally extracting value large datasets Often motivation ultimately produce algorithm mimic enhance human biological performance In III begin discussing basic concepts Machine Learning supervised unsupervised learning We discuss standard models Machine Learning related areas DRAFT April Machine Learning Supervised Learning Reinforce ment learning Dimension reduction Partial squares Canonical variates Fisher s linear disc Classification Discriminative Gaussian process Logistic regression Support vector machine Generative class conditional Naive Bayes Mixture models Factor analysis mixtures Nearest neighbour Mixture experts Regression Gaussian Process Parametric Model linear non linear Time series prediction Semi supervised Learning Unsupervised Learning Time series modelling Markov chain AR continuous CRF undirected Latent Markov chain HMM discrete LDS continuous SLDS mixed Dimension reduction Linear Factor analysis PCA PLSA NMF Non Linear Visualisation Latent variable models complete complete sparse basis Mixed membership Latent Dirichlet Allocation Clique decomp Anomaly detection Machine learning large field diagram denotes loose associations subjects In III discuss standard machine learning concepts including probabilistic variants classical algorithms DRAFT April CHAPTER Machine Learning Concepts Machine learning body research related automated large scale data analysis Historically field centred biologically inspired models long term goals community oriented producing models algorithms process information biological systems The field encompasses traditional areas statistics strong focus mathematical models prediction Machine learning central areas interest computer science related large scale information processing domains Styles Learning Broadly speaking main subfields machine learning supervised learning unsupervised learning In supervised learning focus accurate prediction unsupervised learning aim find compact descriptions data In cases interested methods generalise previously unseen data In sense distinguishes data train model data test performance trained model fig We discuss basic characteristics learning frameworks discussing supervised learning detail Supervised learning Consider database face images represented vector1 x Along image x output class y male female states image male female A database image class pairs available D xn yn n The task accurate predictor y x sex novel image x This example application hard program traditional manner formally specifying rule differentiates male female faces difficult An alternative example faces gender labels let machine automatically learn rule Definition Supervised Learning Given set data D xn yn n N task learn relationship input x output y given novel input x predicted output y accurate The pair x y D assumed generated unknown process generated D To specify explicitly accuracy means defines loss function L ypred ytrue conversely utility function U L In supervised learning interest describing y conditioned knowing x From probabilistic modelling perspective concerned primarily conditional distribution p y x D The term supervised indicates notional supervisor specifying output y input x available data D The output called label particularly discussing classification 1For m n face image elements Fmn form vector stacking entries matrix Styles Learning Train Test Figure In training evaluating model conceptually sources data The parameters model set basis train data If test data generated underlying process generated train data unbiased estimate generalisation performance obtained measuring test data performance trained model Importantly test performance adjust model parameters longer independent measure performance model Predicting tomorrow s stock price y T based past observations y y T form supervised learning We collection times prices D t y t t T time t input price y t output Example A father decides teach young son sports car Finding difficult explain words decides examples They stand motorway bridge car passes underneath father cries s sports car sports car passes After minutes father asks son s understood sports car The son says sure s easy An old red VW Beetle passes son shouts s sports car Dejected father asks Because sports cars red replies son This example scenario supervised learning Here father plays role supervisor son student learner It s indicative kinds problems encountered machine learning easy formally specify sports car knew wouldn t need process learning This example highlights issue difference performing training data performing novel test data The main interest supervised learning discover underlying rule generalise leading accurate prediction new inputs If insufficient train data scenario generalisation performance disappointing If output discrete number possible classes called classification problem In classification problems generally use c output If output continuous called regression problem For example based historical information demand sun cream supermarket asked predict demand month In cases possible discretise continuous output consider corresponding classification problem However cases impractical unnatural example output y high dimensional continuous valued vector Unsupervised learning Definition Unsupervised learning Given set data D xn n N unsupervised learning aim find plausible compact description data An objective quantify accuracy description In unsupervised learning special prediction variable probabilistic perspective interested modelling distribution p x The likelihood model generate data popular measure accuracy description Example A supermarket chain wishes discover different basic consumer buying behaviours based large database supermarket checkout data Items brought customer DRAFT April Styles Learning visit checkout represented sparse dimensional vector x contains ith element customer bought product Based million checkout vectors stores country D xn n supermarket chain wishes discover patterns buying behaviour In table column represents products bought customer customer records prod ucts shown A indicates customer bought item We wish find common patterns data buys diapers likely buy aspirin coffee tea milk beer diapers aspirin Example Clustering The table represents collection unlabelled dimensional points By simply eye balling data apparent clusters centred A reasonable compact description data clusters centred standard deviation x1 x2 Anomaly detection A baby processes mass initially confusing sensory data After baby begins understand environment sensory data environment familiar expected When strange face presents baby recognises familiar upset The baby learned representation environment distinguish expected unexpected example unsupervised learning Detecting anomalous events industrial processes plant monitoring engine monitoring unexpected buying behaviour patterns customers fall area anomaly detection This known novelty detection Online sequential learning In situations assumed data D given In online learning data arrives sequentially continually update model new data available Online learning occur supervised unsupervised context Interacting environment In certain situations agent able interact manner environment This interaction complicate enrich potential learning Query Active Learning Here agent ability request data environment For example predictor recognise confidently able predict certain regions space x requests training data region Active learning considered unsupervised context agent request information regions p x currently uninformative Reinforcement Learning In reinforcement learning agent inhabits environment actions Some actions eventually beneficial lead food example whilst DRAFT April Supervised Learning disastrous lead eaten example Based accumulated experience agent needs learn action given situation order maximise probability obtaining desired long term goal long term survival example Actions lead long term rewards need reinforced Reinforcement learning connections control theory Markov decision processes game theory Whilst discussed MDPs briefly mentioned environment learned based delayed rewards section discuss topic refer reader specialised texts example Semi supervised learning In machine learning common scenario small labelled large unlabelled data For example access images faces small number labelled instances known faces In semi supervised learning tries use unlabelled data better classifier based labelled data This common issue examples gathering unlabelled data cheap taking photographs example However typically labels assigned humans expensive Supervised Learning Supervised unsupervised learning mature fields wide range practical tools associated theoretical analyses Our aim brief introduction issues philosophies approaches We focus supervised learning classification particular Utility Loss Given new input x optimal prediction depends costly making error This quantified loss function conversely utility In forming decision function c x produce class label new input x don t know true class surrogate predictive distribution p c x If U ctrue cpred represents utility making decision cpred truth ctrue expected utility decision function U c x ctrue U ctrue c x p ctrue x optimal decision function c x maximises expected utility c x argmax c x U c x One consider equivalently loss L ctrue c x expected loss respect p c x termed risk The optimal decision function minimises risk respect Zero loss utility A count correct predictions measure prediction performance based zero utility conversely zero loss U ctrue c c ctrue c ctrue For class case expected utility equation given U c x p ctrue x c x p ctrue x c x Hence order highest expected utility decision function c x correspond selecting highest class probability p c x c x p c x p c x In case tie class selected random equal probability DRAFT April Supervised Learning General loss utility functions In general class problem U c x U ctrue c p ctrue x U ctrue c p ctrue x c x U ctrue c p ctrue x U ctrue c p ctrue x c x optimal decision function c x chooses class highest expected utility One readily generalise multiple class situations utility matrix elements Uij U c true cpred j j element matrix contains utility predicting class j true class Con versely think loss matrix entries Lij Uij In applications utility matrix highly non symmetric Consider medical scenario asked predict patient cancer dom c cancer benign If true class cancer predict benign terrible consequences patient On hand class benign predict cancer disastrous patient Such asymmetric utilities favour conservative decisions cancer case inclined decide sample cancerous benign predictive probability classes equal In solving optimal decision function c x equation assuming model p c x correct However practice typically don t know correct model underlying data dataset examples D xn cn n N domain knowledge We need form distribution p c x D ideally close true unknown joint data distribution ptrue c x Only decisions expected generalise examples outside train data Communities researchers machine learning form different strategies address lack knowledge ptrue c x Squared loss utilty In regression problems real valued prediction ypred truth ytrue common loss function squared loss L ytrue ypred ytrue ypred The decision framework follows replacing summation integration continuous variables Using empirical distribution A direct approach knowing correct model ptrue c x replace empirical distribution p c x D N N n c cn x xn That assume underlying distribution approximated placing equal mass points xn cn dataset Using gives empirical expected utility U c c x p c x D N n U cn c xn conversely empirical risk R N n L cn c xn DRAFT April Supervised Learning X C p x c L c c x p x c P c c x x Figure Empirical risk approach Given dataset X C model data p x c usually empirical distribution For classifier c x parameter learned minimising penalised empirical risk spect The penalty parameter set validation A novel input x assigned class c x given optimal Assuming loss minimal correct class predicted optimal decision c x input train set given c xn cn However new x contained D c x undefined In order define class novel input use parametric function c x For example class problem dom c linear decision function given c x Tx Tx If vector input x positive hyperplane defined vector bias assign class class We return geometric interpretation chapter The empirical risk function parameters R D N n L cn c xn The optimal parameters given minimising empirical risk respect opt argmin R D The decision new datapoint x given c x opt In empirical risk minimisation approach decision function c x flexible empirical risk goes However c x flexible little confidence c x perform novel input x The reason flexible decision function c x class label change small change x Such flexibility good means able find parameter setting train data fitted However constraining decision function known training points flexible c x change rapidly away train data leading poor generalisation To constrain complexity c x minimise penalised empirical risk R D R D P P function penalises complex functions c x The regularisation constant determines strength penalty typically set validation The empirical risk approach summarised fig For linear decision function reasonable penalise wildly changing classifications sense change input x small expect average minimal change class label The squared difference Tx inputs x1 x2 T x x x2 x1 By constraining length small limit ability classifier change class small change input space Assuming distance datapoints distributed according isotropic multivariate Gaussian zero mean covariance 2I average squared change T x 2T motivating choice Euclidean squared length parameter penalty term P T DRAFT April Supervised Learning Train Validate Test Figure Models trained train data based different regularisation parameters The optimal regularisation parameter determined empirical performance validation data An independent measure generalisation performance obtained separate test set Algorithm Setting regularisation parameters cross validation Choose set regularisation parameters A Choose set training validation set splits Ditrain Divalidate K A K ia argmin R Ditrain aP end L K K 1R Divalidate end opt argmin L Validation In penalised empirical risk minimisation need set regularisation constant This achieved evaluating performance learned classifier c x validation data Dvalidate different values choosing gave rise classifier best performance It s important validation data data model trained know optimal setting case zero little confidence generalisation ability Given dataset D split disjoint parts Dtrain Dvalidate size validation set usually chosen smaller train set fig For parameter finds minimal empirical risk parameter The optimal chosen gives rise model minimal validation risk Using optimal regularisation parameter practitioners retrain basis dataset D In cross validation dataset partitioned training validation sets multiple times validation results obtained partition Each partition produces different trainingDitrain validationDivalidate set optimal penalised empirical risk parameter ia associated unregularised validation performance R ia Divalidate The performance regularisation parameter taken average validation performances The best regularisation parameter given minimal average validation error algorithm More specifically K fold cross validation data D split K equal sized disjoint parts D1 DK Then Divalidate Di Ditrain D Divalidate This gives total K different training validation sets performance averaged fig In practice Train Validate Train TrainValidate TrainValidate Test Figure In cross validation dataset split train validation sets Depicted fold cross validation For range regularisation parameters optimal regularisation parameter found based empirical validation performance averaged different splits DRAFT April Supervised Learning b Figure The true function generated noisy data dashed line function learned data given solid line The unregularised fit training given Whilst training data fitted error validation examples denoted high b The regularised fit Whilst train error high validation error low fold cross validation popular leave cross validation validation sets consist single example Example Finding good regularisation parameter In fig fit function sin wx plotted data learning parameters w based minimising squared loss The unregularised solution fig 5a badly overfits data high validation error To encourage smoother solution regularisation term w2 The validation error based different values regularisation parameter computed gives lowest validation error The resulting fit data found retraining w validation optimal reasonable fig 5b Benefits empirical risk approach In limit large training data empirical distribution tends correct distri bution The discriminant function chosen basis minimal risk quantity ultimately interested The procedure conceptually straightforward Drawbacks empirical risk approach It extreme assume data follows empirical distribution particularly small amounts training data More reasonable assumptions p x account likely x arise train data If loss function changes discriminant function needs retrained Some problems require estimate confidence prediction Whilst heuristic ways evaluating confidence prediction inherent framework When penalty parameters performing cross validation discretised grid parameters infeasible During validation models trained subsequently discarded Bayesian decision approach An alternative empirical distribution fit model p c x train data D Given model decision function c x automatically determined maximal expected utility minimal risk respect model equation unknown p ctrue x replaced DRAFT April Supervised Learning X C p x c L c c p c x c p c x x Figure Bayesian decision approach A model p x c fitted data After learn ing optimal model parameters compute p c x For novel x distribution assumed truth p c x The prediction de cision given c minimises expected risk L c c p c x p c x See fig schematic depiction Bayesian decision approach There main approaches fitting p c x data D fig We parameterise joint distribution p c x p c x c x p x x discriminative approach p c x p x c x c p c c generative approach We ll consider approaches context trying system distinguish male female face The setup database face images image represented real valued vector xn n N label cn stating image male female Generative approach p x c p x c x c p c c For simplicity use maximum likelihood training parameters Assuming data D d log likelihood log p D n log p xn cn x c n log p cn c As dependence x c occurs term c occurs second This means learning optimal parameters equivalent isolating data male class fit ting model p x c male x male We similarly isolate female data fit separate model p x c female x female The class distribution p c c set according ratio males females set training data To classification new image x male female use Bayes rule p c male x p x c male x male p x c male x male p x c female x female cn xn c x c N cn xn c x x N b Figure Two generic strategies probabilistic classification Class dependent generative model x After learning parameters classification ob tained making x evidential inferring p c x b A discriminative classification method p c x DRAFT April Supervised Learning Based zero loss probability greater classify x male female For general loss function use probability decision process equation Advantages Prior information structure data naturally specified generative model p x c For example male faces expect heavier eyebrows squarer jaw etc Disadvantages The generative approach directly target classification model p c x goal generative training model p x c If data x complex finding suitable generative data model p x c difficult task Furthermore generative model separately trained class competition models explain x data On hand making model p c x simpler particularly decision boundary classes simple form data distribution class complex fig Discriminative approach p x c p c x c x p x x Assuming d data log likelihood log p D n log p cn xn c x n log p xn x The parameters isolated terms maximum likelihood training equivalent finding parameters c x best predict class c given training input x The parameters x modelling data occur second term setting treated separate unsupervised learning problem This approach consequently isolates modelling decision boundary modelling input distribution fig Classification new point x based p c x opt c x As generative case approach learns joint distribution p c x p c x p x decision process required equation Advantages The discriminative approach directly addresses finding accurate classifier p c x based modelling decision boundary opposed class conditional data distribution generative approach Whilst data class distributed complex way decision boundary relatively easy model Disadvantages Discriminative approaches usually trained black box classifiers little prior knowledge built describe data given class distributed Domain knowledge easily expressed generative framework Hybrid generative discriminative approaches One use generative description p x c building prior information use form joint distribution p x c discriminative model p c x formed Bayes rule Specifically use p c x p x c x c p c c c p x c x c p c c Subsequently parameters x c c hybrid model found maximising probability correct class A separate model learned p x x This approach appear leverage advantages discriminative generative frameworks readily incorporate domain knowledge generative model p x c x c train discriminative way This approach rarely taken practice resulting functional form likelihood depends complex manner parameters In case parameter separation c x c occurs previously case generative discriminative approaches DRAFT April Supervised Learning m m m m m m m f f f f f f f f f f x Figure Each point represents high dimensional vector associated class label male female The point x new point like predict male female In generative approach male model p x male generates data similar m points Similarly female model p x female generates points similar f points We use Bayes rule calculate probability p male x fitted models given text In discriminative approach directly model p male x cares points m f distributed describing boundary separate classes given line In case modelling distribution data required decisions related decision boundary point lies Features preprocessing It case discriminative training transforming raw input x form directly captures relevant label information greatly improve performance For example male female classification case building classifier directly terms elements face vector x difficult However features contain geometric information distance eyes width mouth etc finding classifier easier In practice data preprocessed remove noise centre image etc Learning lower dimensional representations semi supervised learning One way exploit large unlabelled training data improve classification find lower dimensional representation h data x Based mapping h c simpler learn mapping x c directly We use cn indicate class datapoint n missing We form likelihood visible data fig p C X H n p cn hn c h I cn p xn hn x h p h h set parameters example maximum likelihood opt argmax H p C X H Benefits Bayesian decision approach This conceptually clean approach tries ones best model environment generative discriminative approach independent subsequent decision process In case learning environment separated effect expected utility The decision c novel input x highly complex function x maximisation operation If p x c true model data approach optimal Drawbacks Bayesian decision approach If environment model p c x poor prediction c highly inaccurate modelling environment divorced prediction To avoid fully divorcing learning model p c x effect decisions practice includes regularisation terms environment model p c x set validation based empirical loss DRAFT April Bayes versus Empirical Decisions cn xnhn c h h x h N Figure A strategy semi supervised learning When cn miss ing term p cn hn absent The large training data helps model learn good lower dimension compressed representa tion h data x Fitting classification model p c h lower dimensional representation easier fitting model directly complex data class p c x Bayes versus Empirical Decisions The empirical risk Bayesian approaches extremes philosophical spectrum In empirical risk approach makes seemingly simplistic data generating assumption However decision function parameters set based task making decisions On hand Bayesian approach attempts learn meaningful p c x regard ultimate use larger decision process What objective criterion use learn p c x particularly interested classification low test risk The following example intended recapitulate generic Bayes empirical risk approaches ve considering Note ve previously written utilities suggesting example class labels theory applies generally example utilities u c d d decision necessarily form class label c Example The generic decision strategies Consider situation based patient information x need decision d operate The utility operating u c d depends patient cancer c For example u cancer operate u benign operate u cancer don t operate u benign don t operate We independent true assessments patient cancer giving rise set historical records D xn cn n N Faced new patient information x need decision operate In Bayesian decision approach model p c x D example discriminative model logistic regression section Using model decision given maximises expected utility d argmax d p cancer x D u cancer d p benign x D u benign d In approach learning model p c x D divorced ultimate use model decision making process An advantage approach viewpoint expected utility optimal provided model p c x D correct Unfortunately rarely case Given limited model resources sense focus ensuring prediction cancer correct significant effect utility However formally require corruption framework The alternative empirical utility approach recognises task stated translate patient information x operation decision d To parameterise d x f x learn maximising empirical utility u n u f xn cn For example x vector representing patient information parameter use linear decision function f x Tx d operate Tx d don t operate DRAFT April Exercises The advantage approach parameters decision directly related utility making decision However good model p c x wish use A disadvantage easily incorporate domain knowledge decision function Both approaches heavily practice preferred depends problem Whilst Bayesian approach appears formally optimal prone model mis specification A pragmatic alternative Bayesian approach fit parameterised distribution p c x data D penalises complexity fitted distribution setting validation risk This potential advantage allowing incorporate sensible prior information p c x whilst assessing competing models light actual predictive risk Similarly empirical risk approach modify extreme empirical distribution assumption plausible model p c x data Summary Supervised unsupervised learning main branches machine learning considered book The classical approaches supervised learning empirical risk minimisation Bayesian decision theory In empirical risk minimisation typically explicit model data focus end use predictor In Bayesian decision approach explicit model data end decision classification computed independently fitting model data A general introduction machine learning given An excellent reference Bayesian decision theory Approaches based empirical risk discussed Exercises Exercise Given distributions p x class N x p x class N x corresponding prior occurrence classes p1 p2 p1 p2 calculate decision boundary p class1 x explicitly function p1 p2 How solutions decision boundary reasonable Exercise Under zero loss Bayes decision rule chooses class k p class k x p class j x j k Imagine instead use randomized decision rule choosing class j probability q class j x Calculate error decision rule error minimized Bayes decision rule Exercise For novel input x predictive model class c given p c x p c x p c x The corresponding utility matrix U ctrue cpred elements In terms maximal expected utility best decision Exercise Consider datapoints generated different classes Class distribution p x c N x m1 class distribution p x c N x m2 The prior probabil ities class p c p c Show posterior probability p c x DRAFT April Exercises form p c x exp ax b determine b terms m1 m2 Exercise WowCo com new startup prediction company After years failures eventually find neural network trillion hidden units achieves zero test error learning problem posted internet week Each learning problem included train test set Proud achievement market product aggressively claim predicts perfectly known problems Discuss claims justify buying product Exercise For prediction model p y x true data generating distribution p x y define measure accuracy A x y p x y p y x average overlap true predicting distributions By defining p x y p x y p y x A considering KL q x y p x y distribution q x y logA log p y x q x y KL q x y p x y Consider set train data D xn yn n N define empirical distribution q x y N N n x xn y yn Show logA N N n log p yn xn KL q x y p x y This shows log prediction accuracy lower bounded training accuracy gap empirical distribution unknown true data generating mechanism According naive bound doesn t account possible overfitting best thing increase prediction accuracy increase training accuracy Kullback Leibler term indepen dent predictor As N increases empirical distribution tends true distribution Kullback Leibler term small justifying minimising train error Assuming train outputs drawn distribution p y x y f x determin istic logA N N n log p yn xn KL q x p x provided train data correctly predicted certainty accuracy related empirical true input distribution A exp KL q x p x Exercise You wish classifier variable c based kinds inputs x y You generative model p y c discriminative model p c x Explain combine model p c x y DRAFT April CHAPTER Nearest Neighbour Classification Often faced classification problem useful employ simple method produce baseline complex methods compared In chapter discuss simple nearest neighbour method The nearest neighbour methods extremely popular perform surprisingly We discuss methods related probabilistic mixture models Do As Your Neighbour Does Successful prediction typically relies smoothness data class label change small input space problem essentially random algorithm generalise In machine learning constructs appropriate measures smoothness problem hand hopes exploit obtain good generalisation Nearest neighbour methods useful starting point readily encode basic smoothness intuitions easy program In classification problem input vector x corresponding class label cn C Given dataset N train examples D xn cn n N novel x aim return correct class c x A simple effective strategy supervised learning problem stated novel x find nearest input train set use class nearest input algorithm For vectors x x representing different datapoints measure nearness dissimilarity function d x x A common dissimilarity squared Euclidean distance d x x x x T x x conveniently written x x Based squared Euclidean distance decision boundary determined perpendicular bisectors closest training points different training labels fig This partitions input space regions classified equally called Voronoi tessellation The nearest neighbour algorithm simple intuitive There issues How measure distance points Whilst Euclidean square distance popular appropriate A fundamental limitation Euclidean distance account data distributed For example length scales components vector x vary greatly largest length scale dominate squared distance potentially useful class specific information components x lost The Mahalanobis distance d x x x x T x x covariance matrix inputs classes overcome problems effectively rescales input vector components K Nearest Neighbours Figure In nearest neighbour classification new vector assigned label nearest vector training set Here classes training points given circles class The dots indicate class nearest training vector The decision boundary piecewise linear segment corresponding perpendicular bisector datapoints belonging different classes giving rise Voronoi tessellation input space Algorithm Nearest neighbour algorithm classify vector x given train data D xn cn n N Calculate dissimilarity test point x train points dn d x xn n N Find train point xn nearest x n argmin n d x xn Assign class label c x cn In case nearest neighbours different class labels numerous class chosen If single numerous class use K nearest neighbours The dataset needs stored classification novel point compared train points This partially addressed method called data editing datapoints little effect decision boundary removed training dataset Depending geometry training points finding nearest neighbour accelerated examining values components xi x turn Such axis aligned space split called KD tree reduce possible set candidate nearest neighbours training set novel x particularly low dimensions Each distance calculation expensive datapoints high dimensional Principal Com ponents Analysis chapter way address replaces x low dimensional projection p The Euclidean distance datapoints xa xb approximately given pa pb section This faster compute improve classification accuracy large scale characteristics data retained PCA projections It clear deal missing data incorporate prior beliefs domain knowledge K Nearest Neighbours If neighbour simply mistaken incorrect training class label particularly repre sentative example class situations typically result incorrect classification By including single nearest neighbour hope robust classifier smoother decision boundary swayed single neighbour opinions If assume Euclidean distance dissimilarity measure K Nearest Neighbour algorithm considers hypersphere centred test point x The radius hypersphere increased contains exactly K train inputs The class label c x given numerous class hypersphere fig DRAFT April K Nearest Neighbours Figure In K nearest neighbours centre hypersphere point wish classify central dot The inner circle corre sponds nearest neighbour square However near est neighbours find round class neighbours square class neighbour classify central point round class In case tie increase K tie broken Choosing K Whilst sense making K certainly little sense making K N N number training points For K large classifications simply assign novel x numerous class train data This suggests optimal intermediate setting K gives best generalisation performance This determined cross validation described section b c Figure Some train examples digit zero b seven c There train examples digit classes Example Handwritten Digit Example Consider classes handwritten digits zeros ones Each digit contains pixels The train data consists zeros ones subset plotted fig 3a b To test performance nearest neighbour method based Euclidean distance use independent test set containing digits The nearest neighbour method applied data correctly predicts class label test points The reason high success rate examples zeros ones sufficiently different easily distinguished A difficult task distinguish ones sevens We repeat experiment training examples ones training examples sevens fig 3b c Again new test examples containing ones sevens assess performance This time errors found nearest neighbour classification error rate class problem The test points nearest neighbour method makes errors plotted fig If use K nearest neighbours classification error reduces slight improvement As aside best machine learning methods classify real world digits classes error cent better performance average human DRAFT April A Probabilistic Interpretation Nearest Neighbours Figure versus classification ing NN method Top The test examples incorrectly clas sified Bottom nearest neighbours training set corresponding test point A Probabilistic Interpretation Nearest Neighbours Consider situation data classes class class We following mixture model data class places Gaussian datapoint p x c N0 n class N x xn 2I N0 D n class e x x n D dimension datapoint x N0 number train points class variance This Parzen estimator models data uniform weighted sum Gaussian distributions centred training points fig Similarly data class p x c N1 n class N x xn 2I N1 D n class e x x n To classify new datapoint x use Bayes rule p c x p x c p c p x c p c p x c p c The maximum likelihood setting p c N0 N0 N1 p c N1 N0 N1 An analogous expression equation holds p c x To class likely use ratio p c x p c x p x c p c p x c p c If ratio greater classify x Equation complicated function x However small numerator sum exponential terms dominated term datapoint xn0 class closest point x Similarly denominator dominated datapoint xn1 class closest x In case p c x p c x e x xn0 p c N0 e x xn1 p c N1 e x xn0 e x xn1 Taking limit certainty classify x class x closer xn0 xn1 The nearest single neighbour method recovered limiting case probabilistic generative model fig Figure A probabilistic interpretation nearest neighbours For class use mixture Gaussians model data class p x c placing training point isotropic Gaussian width The width Gaussian represented circle In limit novel point small black dot assigned class nearest neighbour For finite influence non nearest neighbours effect resulting soft version nearest neighbours DRAFT April Exercises The motivation K nearest neighbours produce classification robust unrepresen tative single nearest neighbours To ensure similar kind robustness probabilistic interpretation use finite value This smoothes extreme probabilities classification means points nearest effective contribution equation The exten sion classes straightforward requiring class conditional generative model class By richer generative model data Parzen estimator approach We examine cases detail later chapters particular chapter When nearest neighbour far away For novel input x far training points Nearest Neighbours soft probabilistic variant confidently classify x belonging class nearest training point This arguably opposite like classification tend prior probabilities class based number training data class A way avoid problem class include fictitious large variance mixture component mean data class For novel inputs close training data extra fictitious component appreciable effect However away high density regions training data additional fictitious component dominate larger variance components As distance x fictitious class point limit x far training data effect class information position x occurs See section example Summary Nearest neighbour methods general classification methods The NN method understood class conditional mixture Gaussians limit vanishingly small covariance mixture component model Code nearNeigh m K nearest neighbour majority m Find majority entry column matrix demoNearNeigh m K nearest neighbour Demo Exercises Exercise The file NNdata mat contains training test data handwritten digits Using leave cross validation find optimal K K nearest neighours use compute classification accuracy method test data Exercise Write routine SoftNearNeigh xtrain xtest trainlabels sigma implement soft nearest neighbours analogous nearNeigh m Here sigma variance equation The file NNdata mat contains training test data handwritten digits Using leave cross validation find optimal use compute classification accuracy method test data Hint numerical difficulty method To avoid consider logarithm numerically compute log ea eb large negative b See logsumexp m Exercise The editor YoMan mens magazine great idea Based success recent national poll test IQ decides Beauty Quotient BQ test She collects DRAFT April Exercises images male faces taking care sure images scaled roughly size lighting conditions She gives male face BQ score Severely Aesthetically Challenged Generously Aesthetically Gifted Thus real valued D dimensional image x associated value b range In total collects N images associated scores xn bn n N One morning bounces office tells good news task test male nation determine Beauty Quotient The idea explains man send online image face x YoMan immediately receive automatic BQ response b As step decide use K nearest neighbour method KNN assign BQ score b novel test image x Describe determine optimal number neighbours K use Your line manager pleased algorithm disappointed provide simple explanation Beauty present future version YoMan magazine To address decide model based linear regression That b wTx w parameter vector chosen minimise E w n bn wTxn After training finding suitable w YoMan explain readership simple way facial features important determining s BQ DRAFT April CHAPTER Unsupervised Linear Dimension Reduction High dimensional data prevalent machine learning related areas Indeed arises situation data dimensions data examples In cases seek lower dimensional representation data In chapter discuss standard methods improve prediction performance removing noise representation High Dimensional Spaces Low Dimensional Manifolds In machine learning problems data high dimensional images bag word descriptions gene expressions etc In cases expect training data densely populate space meaning large parts little known data For hand written digits chapter data dimensional binary valued pixels number possible images Nevertheless expect handful examples digit sufficient human understand recognise Digit like images occupy highly constrained volume dimensions expect small number degrees freedom required describe data reasonable accuracy Whilst data vectors high dimensional typically lie close lower dimensional manifold informally dimensional manifold corresponds warped sheet paper embedded high dimensional space meaning distribution data heavily constrained Here concentrate computationally efficient linear dimension reduction techniques high dimensional datapoint x projected lower dimensional vector y y Fx const The non square matrix F dimensions dim y dim x dim y dim x The methods chapter largely non probabilistic natural probabilistic interpretations For example PCA closely related Factor Analysis described chapter Principal Components Analysis If data lies close linear subspace fig accurately approximate data point vectors span linear subspace In cases aim discover low dimensional co ordinate system approximately represent data We express approximation datapoint xn xn c M j ynj b j x n Principal Components Analysis Figure In linear dimension reduction linear subspace fitted average squared dis tance datapoints red rings projec tions plane black dots minimal Here vector c constant defines point linear subspace bj basis vectors span linear subspace known principal component coefficients loadings Note authors define PCA constant c Collectively write B b1 bM The yni low dimensional coordinates data forming lower dimension yn datapoint n collectively write lower dimensional vectors Y y1 yN Equation expresses find reconstruction x n given lower dimensional representation yn components yni M For data space dimension dim x D hope accurately describe data small number M D co ordinates y To determine best lower dimensional representation convenient use squared distance error x reconstruction x E B Y c N n D xni x ni It straightforward optimal bias c given mean data n x n N exercise We assume data centred zero mean n x n set c zero concentrate finding optimal basis B Deriving optimal linear reconstruction To find best basis vectors B defining B j b j corresponding low dimensional coordinates Y wish minimize sum squared differences vector x reconstruction x E B Y N n D xni M j ynj b j trace X BY T X BY X x1 xN An important observation optimal solution B Y unique reconstruction error E B Y depends product BY Indeed loss generality constrain B orthonormal matrix To consider invertible transformation Q basis B B BQ orthonormal matrix B TB I Since Q invertible write BY B Y defining Y Q 1Y Since Y unconstrained Y unconstrained loss generality consider equation orthonormality constraint BTB I basis vectors mutually orthogonal unit length By differentiating equation respect ynk obtain orthonormality constraint DRAFT April Principal Components Analysis B ynk E B Y xni j ynj b j bki xni b k j ynj b j b k jk xni b k ynk The squared error E B Y zero derivative ynk bki x n written Y B TX We substitute solution equation write squared error function B Using X BY T X BY XTX XTBBTX XTBBTX XTB BTB I BTX The terms cancel Using trace ABC trace CAB obtain E B trace XXT I BBT Hence objective E B N trace S trace SBBT S sample covariance matrix data1 Since assumed data zero mean S N 1XX T N N n xn xn T More generally non zero mean data S N N n xn m xn m T m N N n xn To minimise equation constraint BTB I use set Lagrange multipliers L objective minimize trace SBBT trace L BTB I neglecting constant prefactor N trace S term Since constraint symmetric assume L symmetric Differentiating respect B equating zero obtain optimum SB BL We need find matrices B L satisfy equation One solution given L diagonal case form eigen equation columns B corresponding eigenvectors S In case trace SBBT trace L sum eigenvalues corresponding eigenvectors forming B For eigen solution N 1E B trace L trace S M const 1Here use unbiased sample covariance simply standard literature If replace sample covariance defined chapter change required replace N N effect form solutions found PCA DRAFT April Principal Components Analysis Figure Projection dimensional data dimensional PCA Plotted original dat apoints x larger rings reconstructions x small dots dimensional PCA The lines rep resent orthogonal projection original data point eigenvector The arrows eigenvectors scaled square root cor responding eigenvalues The data centred zero mean For high dimensional datapoint x low dimensional representation y given case distance possibly negative origin eigenvector direction cor responding orthogonal projection point Since wish minimise E B define basis eigenvectors largest corre sponding eigenvalues If order eigenvalues squared error given equation N 1E B trace S trace L D M D M Whilst solution eigen problem unique serves define solution subspace rotate scale B Y value squared loss exactly squares objective depends product BY The justification choosing non rotated eigen solution given additional requirement basis columns B correspond directions maximal variance explained section Maximum variance criterion To break invariance squares projection respect rotations rescaling need additional criterion One given searching single direction b variance data projected direction maximal possible projections This makes sense looking interesting directions data varies lot Using equation single vector b yn bix n The projection datapoint direction b bTxn unit length vector b Hence sum squared projections n bTxn bT n xn xn T b N bTSb Ignoring constants negative equation single basis vector b Since optimally b eigenvector Sb b squared projection N Hence optimal single b maximises projection variance given eigenvector corresponding largest eigenvalue S Under criterion optimal direction b orthonormal readily b given second largest eigenvector This explains despite squared loss equation invariant respect arbitrary rotation scaling basis vectors ones given eigen decomposition additional property correspond directions maximal variance These maximal variance directions found PCA called principal directions PCA algorithm The routine PCA presented algorithm In notation y Fx projection matrix F corresponds ET Similarly reconstruction equation coordinate yn corresponds DRAFT April Principal Components Analysis Algorithm Principal Components Analysis form M dimensional approximation dataset xn n N dim xn D Find D sample mean vector D D covariance matrix m N N n xn S N N n xn m xn m T Find eigenvectors e1 eD covariance matrix S sorted eigenvalue ei larger ej j Form matrix E e1 eM The lower dimensional representation data point xn given yn ET xn m The approximate reconstruction original datapoint xn xn m Eyn The total squared error training data approximation N n xn x n N D j M j M D eigenvalues discarded projection ETxn bi corresponds ei The PCA reconstructions orthogonal projections data subspace spanned eigenvectors corresponding M largest eigenvalues covariance matrix fig Example Reducing dimension digits We examples handwritten s image consists real values pixels fig Each image matrix stacked form dimensional vector giving dimensional data matrix X The covariance matrix data eigenvalue spectrum plotted fig plot largest eigenvalues Note components mean squared reconstruction error small indicating data lies close dimensional linear subspace The eigenvalues computed pca m The reconstructions different numbers eigenvectors plotted fig Note small number eigenvectors reconstruction closely resembles mean image Example Eigenfaces In fig present example images wish find lower dimensional representation Using PCA mean eigenfaces presented recon Figure Top row selection digit taken database ex amples Plotted beneath digit reconstruction eigen vectors Note reconstructions fewer eigenvectors express variability resemble mean digit DRAFT April Principal Components Analysis structions original data eigenfaces fig The PCA respresentation found SVD techniques described section PCA nearest neighbours classification In nearest neighbours classification chapter need compute distance datapoints For high dimensional data computing squared Euclidean distance vectors expensive sensitive noise It useful project data lower dimensional representation For example making classifier distinguish digit digit example form lower dimensional representation use robust representation data To ignore class label dataset training points Each training points xn projected lower dimensional PCA representation yn Subsequently distance calculations xa xb replaced ya yb To justify consider xa xb T xa xb Eya m Eyb m T Eya m Eyb m ya yb TETE ya yb ya yb T ya yb equality orthonormality eigenvectors ETE I Using principal components example number chosen nearest neighbour rule classify s s gave test set error examples compared standard method non projected data A plausible explanation improvement new PCA representation data robust interesting directions space retained low variance directions discarded Example Finding best PCA dimension There examples digit examples digit We use half data training half testing The training examples split training set examples separate validation set examples PCA reduce dimensionality inputs nearest neighbours classify validation examples Different reduced dimensions investigated based validation results selected optimal number PCA components retained fig The independent test error independent examples dimensions Comments PCA The intrinsic dimension data How dimensions linear subspace From equation reconstruction error proportional sum discarded eigenvalues If plot eigenvalue spectrum set eigenvalues ordered decreasing value hope large values small values If data lie close M dimensional linear subspace M large eigenvalues rest eigenvalue number e ig e n va lu e Figure The digits data consists examples digit image represented dimensional vector Plotted largest eigenvalues scaled largest eigenvalue sample covariance matrix DRAFT April High Dimensional Data Figure training images Each image consists greyscale pixels The train data scaled represented image components image sum The average value pixel images This subset images Olivetti Research Face Database b Figure SVD reconstruc tion images fig combination eigen images b The eigen images found ing SVD images fig taking mean eigenvec tors largest corresponding eigen value The images corresponding largest eigenvalues contained row row etc The root mean square reconstruction error small improvement PLSA fig small This gives indication number degrees freedom data intrinsic dimensionality Directions corresponding small eigenvalues interpreted noise Non linear dimension reduction In PCA presupposing data lies close linear subspace Is good description More generally expect data lie low dimensional non linear subspace Also data clustered examples handwritten s look similar form cluster separate s cluster Nevertheless linear dimension reduction computationally relatively straightforward common dimensionality reduction techniques High Dimensional Data The computational complexity computing eigen decomposition D D matrix O D3 You wondering possible perform PCA high dimensional data For example images pixels covariance matrix square matrix It appear significant computational challenge find eigen decomposition matrix directly In case vectors number non zero eigenvalues exceed One exploit fact reduce O D3 complexity naive approach described DRAFT April High Dimensional Data number eigenvalues n u m b e r o f e rr o rs Figure Finding optimal PCA dimension use clas sifying hand written digits nearest neighbours train ing examples validation error plotted examples Based validation error dimension reasonable Eigen decomposition N D First note zero mean data sample covariance matrix expressed S ij N N n xni x n j In matrix notation written S N 1XX T D N matrix X contains data vectors X x1 xN Since eigenvectors matrix M equal M scalar consider simply eigenvectors XXT Writing D N matrix eigenvectors E eigenvalues N N diagonal matrix eigen decomposition scaled covariance S satisfies XXTE E XTXXTE XTE XTXE E defined E XTE The final expression represents eigenvector equation XTX This matrix dimensions N N calculating eigen decomposition takes O N3 operations compared O D3 operations original high dimensional space We calculate eigenvectors E eigenvalues matrix easily Once found use fact eigenvalues S given diagonal entries eigenvectors E XE PCA Singular value decomposition An alternative eigen decomposition routine find PCA solution use Singular Value Decomposition SVD D N dimensional matrix X This given X UDVT UTU ID V TV IN D D N diagonal matrix positive singular values We assume decomposition ordered singular values upper left diagonal element D contains largest singular value The matrix XXT written XXT UDVTVDTUT UD UT D DDT D D diagonal matrix N squared singular values diagonal entries zero Since UD UT form eigen decomposition PCA solution equivalently given performing SVD decomposition X eigenvectors given U corresponding eigenvalues square singular values Equation shows PCA form matrix decomposition method X UDVT UMDMVTM UM DM VM correspond taking M singular values matrices DRAFT April Latent Semantic Analysis Figure Top Document data dictionary containing words documents Black indicates word present document The data consists distinct topics random background topic The topic contains sub topics differ usage words influenza flu Bottom The projections datapoint principal components Latent Semantic Analysis In document analysis literature PCA called Latent Semantic Analysis concerned analysing set N documents Each document represented vector xn xn1 x n D T word occurrences For example element xn1 count times word cat appears document n xn2 number occurrences dog etc This bag words formed choosing dictionary D words The vector element xni possibly normalised number occurrences word document n Typically D large order x sparse document contains small fraction available words dictionary Using n represent number times term occurs document n term frequency defined tfni n n An issue representation frequently occurring words dominate To counter measure unique term seeing documents contain term define inverse document frequency idfi log N number documents contain term An alternative term frequency representation term frequency inverse document frequency TF IDF representation given xni tf n idfi gives high weight terms appear document rarely documents Given set documents D aim LSA form lower dimensional representation document The document database represented called term document matrix X x1 xN dimension D N example fig entries typically defined term frequency TF IDF representation An interpretation PCA case principal directions define topics PCA arguably suboptimal document analysis expect presence latent topic contribute positive counts data A related version PCA decomposition constrained positive elements called PLSA discussed section 2More generally consider term counts terms single words sets words sub words DRAFT April Latent Semantic Analysis Example Latent Topic We small dictionary containing words influenza flu headache nose temperature bed cat dog rabbit pet The database contains large number articles discuss ailments articles talk effects influenza addition background documents specific ailments documents discuss pet related issues Some formal documents exclusively use term influenza tabloid documents use informal term flu Each document represented dimensional vector element vector set word occurs document The data represented fig The data generated artificial mechanism described demoLSI m The result PCA data represented fig plot eigenvectors scaled eigenvalue To aid interpretability use bias term c equation The eigenvector groups ailment words second pet words deals different usage terms influenza flu Note unlike clustering models section example PCA related methods PLSA datapoint principle constructed basis vectors document represent mixture different topics Rescaling In LSA common scale transformation projected vectors approximately unit covariance assuming centred data Using y N 1D 1M U T Mx covariance projections obtained N n yn yn T D 1M U T M n xn xn T XXT UMD M D M U T MUDD TUTUMD M I Given y approximate reconstruction x x N UMDMy The Euclidean distance points xa xb approximately d x x b N ya yb T DMU T MUMDM ya yb N ya yb T D2M ya yb It common ignore D2M term N factor consider measure dissimilarity projected space Euclidean distance y vectors Information retrieval Consider large collection documents web creating database D Our interest find similar document specified query document Using bag words style representation document n xn similarly query document x address task defining measure dissimilarity documents example d xn xm xn xm T xn xm One searches document minimises dissimilarity nopt argmin n d xn x DRAFT April PCA With Missing Data Figure Hinton diagram eigenvector matrix E eigen vector column scaled corresponding eigenvalue The dark shaded squares indicates positive light shaded squares negative values area square corresponds magnitude showing large eigenvalues Note overall sign eigenvector irrelevant The eigenvector corresponds topic words influenza flu headache nose temperature bed prevalent The second eigenvector denotes pet topic words The eigenvector shows negative correlation occurrence influenza flu The interpretation eigenvectors topics awkward basis eigenvectors definition orthogonal Contrast basis found PSLA components fig returns document xnopt result search query The squared difference documents written x x T x x xTx x T x 2xTx If commonly bag words representations scaled unit length x x xTx x Tx distance x x T x x x Tx equivalently consider cosine similarity s x x x Tx cos angle unit vectors x x fig A difficulty bag words representation representation zeros Hence differences noise real similarity query database document LSA helps alleviate problem somewhat lower dimensional representation y high dimensional x The y capture main variations data sensitive random uncorrelated noise Using dissimilarity defined terms lower dimensional y robust likely result retrieval useful documents Example Continuing Influenza example uploads query document uses term flu interested documents influenza However search query term flu contain word influenza retrieve documents Since component PCA LSA groups influenza terms use component representation y compare documents retrieve documents independent term flu influenza PCA With Missing Data When values data matrix X missing standard PCA algorithm described applied Unfortunately quick fix PCA solution xni missing complex DRAFT April PCA With Missing Data b Figure Two bag word vectors The Euclidean distance large b Normalised vectors The Euclidean distance related directly angle vectors In case documents relative frequency words dissimilarity number occurrences words different numerical procedures need invoked A simple approach case require squared reconstruction error small existing elements X That is3 E B Y N n D ni xni j ynj b j ni th entry nth vector available zero Differentiating respect ymk find optimal weights satisfy j mi y m j b j b k mi x m b k One substitutes expression squared error minimises error respect B orthonormality constraint An alternative iterative optimisation procedure follows First select random D M matrix B Then iterate convergence following steps Optimize Y fixed B E B Y N n D ni xni j ynj b j For fixed B E B Y quadratic function matrix Y optimised directly By differentiating equating zero obtains fixed point condition ni xni l ynl b l b ki Defining y n l yln M n kl b lib k n c n k ni x n b k matrix notation set linear systems c n M n y n n N One solve linear system yn Gaussian elimination4 It linear systems underdetermined occur observed values nth data column X components M In case use pseudo inverse provide minimal length solution Optimize B fixed Y One freezes Y considers function E B Y N n D ni xni j y nj b j 3For simplicity assume mean term included 4One avoid explicit matrix inversion operator MATLAB DRAFT April PCA With Missing Data Figure Top original data matrix X Black missing white present The data constructed set basis vectors Middle X missing data sparsity Bottom reconstruction found svdm m SVD missing data This problem essentially easy despite miss ing elements data constructed model SVD appropriate Such techniques application collaborative filtering recom mender systems wishes fill missing values matrix For fixed Y expression quadratic matrix B optimised linear algebra This corresponds solving set linear systems ith row B m F b m k n ni x n y n k F kj n ni y n j y n k Mathematically b F m In manner guaranteed iteratively decrease value squared error loss minimum reached This technique implemented svdm m fig Efficient techniques based updating solution new column X arrives time online updating available example Finding principal directions For missing data case basis B found technique based minimising squared reconstruction error necessarily satisfy maximal variance principal directions criterion columns B point eigen directions For given B Y approximate decomposition X BY return new orthonormal basis U performing SVD completed data BY USVT return orthonormal basis U An alternative explicitly transform solution B Forming SVD B B UDVT principal directions B given B BVD If D M matrix B non square M D matrix D non square non invertible To defined append D columns identity D D I am ID IK K th column identity matrix use D place D equation DRAFT April Matrix Decomposition Methods b Figure Under complete representation There basis vectors represent datapoints exactly b Over complete represen tation There basis vectors form unique representation datapoint terms linear combination basis vectors Collaborative filtering PCA missing data A database contains set vectors describing film ratings user database The entry xni vector x n specifies rating user n gives ith film The matrix X x1 xN contains ratings N users missing values single user given rating small selection possible D films In practical example D films N users For user n task predict reasonable values missing entries rating vector xn providing suggestion films like view Viewed missing data problem fit B Y svdm m Given B Y form reconstruction entries X X BY giving prediction missing values Matrix Decomposition Methods Given data matrix X column represents datapoint approximate matrix decomposition form X BY basis matrix B weight coordinate matrix Y Symbolically matrix decompositions form X Data D N B Basis D M Y Weights Components M N By considering SVD data matrix PCA class In section consider common matrix decomposition methods The form decomposition previously considered complete complete decompositions interest described Under complete decompositions When M D fewer basis vectors dimensions fig 12a The matrix B called tall thin In case matrix Y forms lower dimensional approximate representation data X PCA classic example Over complete decompositions For M D basis complete basis vectors dimensions fig 12b In cases additional constraints placed basis components For example require small number large number available basis vectors form representation given x Other popular constraints basis vectors sparse Such sparse representations common theoretical neurobiology issues energy efficiency rapidity processing robustness interest DRAFT April Matrix Decomposition Methods Below discuss popular constrained matrix factorisation methods particular including positivity constraints basis vectors components Probabilistic latent semantic analysis Consider objects x y dom x I dom y J dataset xn yn n N We count matrix elements Cij describes number times joint state x y j observed dataset We transform count matrix frequency matrix p elements p x y j Cij ij Cij Our interest find decomposition frequency matrix form fig 13a p x y j Xij k p x z k Bik p y j z k p z k Ykj p x y j quantities p distributions This form matrix decomposition positive basis B positive coordinates Y This interpretation discovering latent features topics z describe joint behaviour x y After training term p z rank basis vectors topics columns B terms importance desired Note PLSA sequential solver sense optimal basis vectors solution generally equivalent taking highest ranked according p z columns basis vector solution fig An EM style training algorithm In order find approximate decomposition need measure difference matrix elements pij approximation elements p ij Since elements bounded sum interpret p joint probability p approximation For probabilities useful measure discrepancy Kullback Leibler divergence KL p p log p p log p p Since p fixed minimising Kullback Leibler divergence respect approximation p equivalent maximising likelihood term log p p This x y p x y log p x y general notation Z implies summation states variables Z It s convenient derive EM style algorithm learn p x z p y z p z To consider KL q z x y p z x y z q z x y log q z x y z q z x y log p z x y Using p z x y p x y z p x y rearranging gives bound log p x y z q z x y log q z x y z q z x y log p z x y Plugging likelihood term bound x y p x y log p x y x y p x y z q z x y log q z x y x y p x y z q z x y log p x z log p y z log p z DRAFT April Matrix Decomposition Methods z x y zx y b Figure Joint PLSA b Conditional PLSA M step For fixed p x z p y z contribution bound p z x y p x y z q z x y log p z It straightforward optimal setting p z p z x y q z x y p x y equation constant KL x y q z x y p x y p z Similarly fixed p y z p z contribution bound p x z x y p x y z q z x y log p y z Therefore optimally p x z y p x y q z x y similarly p y z x p x y q z x y E step The optimal setting q distribution iteration q z x y p z x y fixed M step The likelihood equation guaranteed increase Kullback Leibler divergence equation decrease iterating E M steps method analogous EM pro cedure The procedure given algorithm demonstration demoPLSA m Generalisations simpler q distributions corresponding generalised EM procedures immediate based modifying derivation Example PLSA documents We repeated analysis toy document data fig PLSA As fig basis found interpretable intuitive Also projections basis vectors reasonable corresponds expect The limited expressibility basis model causes influenza flu terms placed basis vector basis vector clearly represents non overlapping topics Note requirement general topics non overlapping happens case example For richer basis vector model model distinguishes clearly similar topics differing single word DRAFT April Matrix Decomposition Methods Algorithm PLSA Given frequency matrix p x y j return decomposition k p x z k p y j z k p z k See plsa m Initialise p z p x z p y z Not Converged Set q z x y p z x y E step Set p x z y p x y q z x y M Steps Set p y z x p x y q z x y end Set p z x y p x y q z x y Algorithm Conditional PLSA Given frequency matrix p x y j return decomposition k p x z k p z k y j See plsaCond m Initialise p x z p z y Not Converged Set q z x y p z x y E step Set p x z y p x y q z x y M Steps Set p z y x p x y q z x y end Conditional PLSA In cases natural consider conditional frequency matrix p x y j seek approximate decomposition p x y j Xij k p x z k Bik p z k y j Ykj depicted fig 13b Deriving EM style algorithm straightforward exercise presented algorithm Example Discovering basis A set images given fig 15a These created defining base images fig 15b Each base image positive scaled sum pixels unity p x z k k x indexes pixels fig We summed images randomly chosen positive set weights constraint weights sum generate training image elements p x y j j indexes training image This repeated times form train set fig 15a The task given training set images reconstruct basis images formed We assume know correct number base images The results conditional PLSA task presented fig 15c SVD fig 15d In case PLSA finds correct natural basis corresponding way images generated The eigenbasis better terms mean squared reconstruction error training images case correspond constraints data generated Example Eigenfaces verus PLSA faces We return example rerun experiment seeking PLSA basis eigenbases In fig reconstruction original images corresponding PLSA basis The reconstruction error necessarily higher PLSA solution optimal mean squared error solution given PCA PLSA constraints form solution PCA However basis found PLSA arguably interpretable DRAFT April Matrix Decomposition Methods b c Figure PLSA document data fig Hinton diagram basis vectors b Hinton diagram basis vectors c The projections basis vectors case The solution satisfactory documents clearly considered similar ailments topics non specific background topic separate pet topic b c d Figure Training data consisting positive convex combination base images b The chosen base images training data derived c Basis learned conditional PLSA training data This virtually indistinguishable true basis d Eigenbasis called eigenfaces positive features positively summed produce image Because PLSA basis vectors tend sparse Extensions variations PLSA Latent Dirichlet Allocation An alternative viewpoint PLSA form generative model We phrase language Latent Dirichlet Allocation section models similar We describe generative process document n The document described distribution K latent topics n nk L k n k These topic distributions form parameters model Document n list words generate follows For word position w document draw topic zw word p zw k n nk Given topic word position generate word vnw word distribution topic k p vnw znw k k DRAFT April Matrix Decomposition Methods b Figure Conditional PLSA reconstruction images fig positive convex com bination positive base I am ages b The root mean square reconstruction error The base images tend lo calised corresponding eigen images fig 6b Here sees local structure foreheads chins etc We repeat process Wn word positions document This describes distribution p vn1 v n Wn zn1 z n Wn Wn w p vnw znw p znw n The marginal distribution observed data p vn1 v n Wn Wn w znw p vnw znw p znw n Since znw simply summation variable simplify p vn1 v n Wn Wn w k p vnw k p k n Consider specific word dictionary D words Each time word appears document obtain factor k p k p k n An equivalent expression likelihood p vn1 v n Wn D k p vnw k p k n fi n fi n frequency occurrence word document n Assuming generate document way likelihood collection documents product likelihoods document The parameters model word distributions topic k topic distributions document nk Maximising likelihood respect parameters equivalent maximising log likelihood N n D fi n log K k p k p k n p k k p k n nk This exactly form PLSA If normalise frequency counts p n f n n fi n PLSA factorisation p n k p k p k n efficient EM style algorithm available exercise The difference Latent Dirichlet Allocation p k n parameter topic distributions sampled Dirichlet distribution An alternative probabilistic interpretation PLSA Poisson processes DRAFT April Matrix Decomposition Methods Non negative matrix factorisation Non negative Matrix Factorisation NMF considers decomposition basis weight matrices non negative entries considered constrained Factor Analysis Closely related works generalisation PLSA requirement basis components sum unity In cases EM style training algorithms exist convergence slow We encounter similar models discussion Independent Component Analysis section Gradient based training EM style algorithms easy derive implement exhibit poor convergence Gradient based methods simultaneously optimize respect basis components developed require parameterisation ensures positivity solutions Array decompositions It straightforward extend PLSA decomposition multidimensional arrays For example p s t u v w p s t u v w p v w v w p s t u v p u w p v p w Such extensions require additional bookkeeping Applications PLSA NMF Modelling citations We collection research documents cite documents For example document cite documents etc Given list citations document identify key research papers communities cite Note question finding cited documents want identify documents communities find relevance community We use variable d D index documents c D index citations d c domain index research article If document d cites article c j set entry matrix Cij If citation Cij set zero We form distribution documents citations p d c j Cij ij Cij use PLSA decompose matrix citation topics example Example Modelling citations The Cora corpus contains archive computer science research papers From archive authors extracted papers machine learning category consisting documents citations Using distribution equation formed The documents additionally categorised hand topics Case based reasoning Genetic Algorithms Neural Networks Probabilistic methods Reinforcement Learning Rule Learning Theory In joint PLSA method fitted data dim z topics From trained model expression p c j z k defines authoritative paper j according community z k The method discovers intuitively meaningful topics presented table DRAFT April Kernel PCA factor Reinforcement Learning Learning predict methods temporal differences Sutton Neuronlike adaptive elements solve difficult learning control problems Barto et al Practical Issues Temporal Difference Learning Tesauro factor Rule Learning Explanation based generalization unifying view Mitchell et al Learning internal representations error propagation Rumelhart et al Explanation Based Learning An Alternative View DeJong et al factor Neural Networks Learning internal representations error propagation Rumelhart et al Neural networks bias variance dilemma Geman et al The Cascade Correlation learning architecture Fahlman et al factor Theory Classification Regression Trees Breiman et al Learnability Vapnik Chervonenkis dimension Blumer et al Learning Quickly Irrelevant Attributes Abound Littlestone factor Probabilistic Reasoning Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Pearl Maximum likelihood incomplete data em algorithm Dempster et al Local computations probabilities graphical structures Lauritzen et al factor Genetic Algorithms Genetic Algorithms Search Optimization Machine Learning Goldberg Adaptation Natural Artificial Systems Holland Genetic Programming On Programming Computers Means Natural Selection Koza factor Logic Efficient induction logic programs Muggleton et al Learning logical definitions relations Quinlan Inductive Logic Programming Techniques Applications Lavrac et al Table Highest ranked documents according p c z The factor topic labels manual assignments based similarity Cora topics Reproduced Modelling web Consider collection websites indexed If website j points website set Cij set Cij This gives directed graph website website links Since website discuss usually small number topics able explain link websites PLSA decomposition These algorithms proved useful internet search example determine latent topics websites identify authoritative websites See discussion Physical models Non negative decompositions arise naturally physical situations For example acoustics positive amounts energy combine linearly different signal sources form observed signal If consider kinds signals present acoustic signal piano singer NMF learn separate bases instrument reconstruct given signal bases This means potentially remove singer recording leaving piano This analogous reconstructing images fig 15a learned basis images example See related model acoustics Kernel PCA Kernel PCA non linear extension PCA designed discover non linear subspace Here briefly describe approach refer reader details In kernel PCA replace x feature vector x x Note use x interpretation approximate reconstruction Rather feature map takes vector x produces higher dimensional vector x For example map dimensional vector x x1 x2 T x x1 x2 x x x1x2 x T The idea perform PCA higher dimensional feature vectors subsequently mapping eigenvectors original space x The main challenge write explicitly computing DRAFT April Kernel PCA PCA potentially high dimensional feature vector space As reminder standard PCA zero mean data forms eigen decomposition sample matrix5 S N X X T For simplicity concentrate finding principal component e satisfies X X Te e corresponding eigenvalue writing N The dual representation obtained pre multiplying X T terms f X Te standard PCA eigen problem reduces solving X TX f f The feature eigenvector e recovered X f e We note matrix X TX elements X TX mn xm T xn recognise scalar product vectors This means matrix positive semidefinite equivalently use covariance function kernel section X TX mn k xm xn Kmn Then equation written Kf f One solves eigen equation find N dimensional principal dual feature vector f The projection feature x given y x Te x TX f More generally larger number components ith kernel PCA projection yi expressed terms kernel directly yi Ni N n k x xn f eigenvalue label f n th component ith eigenvector K The derivation implicitly assumed zero mean features x Even original data x zero mean non linear mapping features zero mean To correct modification required replace matrix K equation K mn k x m xn N N d k xd xn N N d k xm xd N2 N d d k xd xd 5We use normalisation N opposed N notational convenience practice little difference DRAFT April Canonical Correlation Analysis Finding reconstructions Through equation gives procedure finding KPCA projection y However cases like approximate reconstruction x lower dimensional y This straightforward mapping y x general highly non linear Here outline procedure achieving First find reconstruction x feature space vector x x yie yi n f ni x n Given x try find point x original data space maps x This found minimising E x x x Up negligable constants E x k x x yi n f ni k x n x One finds x minimising E x numerically The resulting procedure gives way form non linear extension PCA computationally relatively demanding results complex optimisation problem See details example applications Canonical Correlation Analysis Consider variables x y different views underlying object For example x rep resent segment video y corresponding audio Given collection xn yn n N interesting challenge identify parts audio video strongly correlated One expect example mouth region video strongly correlated audio One way achieve project x y dimension aTx bTy correlation projections maximal The unnormalised correlation projections aTx bTy n aTxnbTyn aT n xnynT b Defining Sxy N n xnynT similarly Syx Sxx Syy normalised correlation aTSxyb aTSxxa bTSyyb Since equation invariant respect recaling b consider equivalent objective E b aTSxyb subject aTSxxa b TSyyb To find optimal projections b constraints use Lagrangian L b b aTSxyb aTSxxa b bTSyyb DRAFT April Canonical Correlation Analysis training data true model b learned model c Figure Canonical Correlation Analysis Training data The panel contains X matrix dimensional points corresponding dimensional Y matrix b The data produced X Ah Y Bh A matrix B matrix The underlying latent h dimensional randomly chosen vector c Matrices A B learned CCA Note close true A B rescaling sign changes See demoCCA m obtain zero derivative criteria Sxyb aSxxa Syxa bSyyb Hence aTSxyb aa TSxxa b TSyxa bb TSyyb b Since aTSxyb b TSyxa b optimum If assume Syy invertible write b S 1yy Syxa use eliminate b equation giving SxyS yy Syxa 2Sxxa generalised eigen problem Assuming Sxx invertible equivalently write S 1xxSxyS yy Syxa 2a standard eigen problem albeit eigenvalue Once solved find b equation SVD formulation It straightforward find optimal projection x computing SVD S xx SxyS yy form UDVT extracting maximal singular vector u1 U column U Then optimally S xx u1 similarly b optimally S yy v1 v1 column V In way extension finding M multiple directions A a1 aM B b1 bM clear takes corresponding M singular values accordingly Doing maximises criterion trace ATSxyB trace ATSxxA trace BTSyyB DRAFT April Exercises This approach taken cca m fig demonstration One CCA corre sponds factor analysis block restriction form factor loadings section CCA related kernel extensions applied machine learning contexts example model correlation images text order improve image retrieval text queries Summary PCA classical linear dimension reduction method assumes data lies close linear subspace The PCA representation found eigen decomposition data covariance matrix alter natively SVD decomposition data matrix More generally PCA special case matrix decomposition method Other standard methods include PLSA non negative matrix factorisation considered constrained forms PCA posi tivity constraints Canonical correlation analysis attempts find low dimensional representation jointly models related data spaces CCA special case probabilistic factor analysis model PCA appears different research communities referred differently For example known Karhunen Loe ve decomposition proper orthogonal decomposition Another important use dimension reduction data visualisation Methods PCA context designed produce visually interpretable results See discussions work area Code pca m Principal components analysis demoLSI m Demo latent semantic indexing analysis svdm m Singular value decomposition missing data demoSVDmissing m Demo SVD missing data plsa m Probabilistic Latent Semantic Analysis plsaCond m Conditional probabilistic latent semantic analysis demoPLSA m Demo PLSA cca m Canonical correlation analysis CCA demoCCA m Demo canonical correlation analysis Exercises Exercise As described section wish optimal bias c equal sample mean data Explain considering bias c carrying derivation section obtain analogously equation E B c n xn c T I BBT xn c DRAFT April Exercises Show matrix M c cTMc Mc MTc Using derivative result optimal bias c satisfies I BBT n xn c determine optimal bias given mean data c N n x n N Exercise Consider dataset dimensions data lies circumference circle unit radius What effect PCA dataset attempt reduce dimensionality Suggest alternative dimensional representation data Exercise Consider vectors xa xb corresponding PCA approximations c M aie c M bie eigenvectors ei M mutually orthogonal unit length The eigenvector ei corresponding eigenvalue Approximate xa xb PCA representations data equal b Exercise Show solution CCA problem equation transformed form expressed equation claimed text Exercise Let S covariance matrix data The Mahalanobis distance xa xb defined xa xb T S xa xb Explain approximate distance M dimensional PCA Exercise PCA external inputs In applications suspect certain external variables v strong influence data x distributed For example x represents image know lighting condition v image large effect image It sense include known lighting condition forming lower dimensional representation image Note don t want form lower dimensional representation joint x v want form lower dimensional representation x bearing mind variability observed v We assume approximation xn j ynj b j k vnkc k coefficients yni N n N basis vectors b j j J ck k K determined The external inputs v1 vN given The sum squared error loss xn linear reconstruction equation E n xni j ynj b j k vnk c k Find parameters bj ck j J k K minimise E Exercise Consider following dimensional datapoints Perform Principal Components Analysis Calculating mean c data Calculating covariance matrix S n x n xn T ccT data DRAFT April Exercises Finding eigenvalues eigenvectors ei covariance matrix You find eigenvalues large data represented components Let e1 e2 eigenvectors largest eigenvalues Calculate dimensional representation datapoint eT1 x n c eT2 xn c n Calculate reconstruction datapoint c eT1 x n c e1 eT2 xn c e2 n Exercise Show missing data case transformed solution B given equation satisfies B TB I Exercise Consider conditional frequency matrix p x y j Following section derive EM style algorithm approximate decomposition matrix form p x y j k p x z k p z k y j k Z X j Y Exercise Consider probability matrix elements p n p n n p n wish approximate factoriation p n k p k p k n If find factors p requirement minimal KL p p obtain update equations p new k p k n p n p k n k p k p k n p new k n p k n p n p k n k p k p k n DRAFT April Exercises DRAFT April CHAPTER Supervised Linear Dimension Reduction PCA popular useful method However subsequently use projected data classifica tion problem making use class labels data potentially forming lower dimensional representations suboptimal terms separated different classes In chapter discuss classical methods reduce data dimension resulting data separated classes Supervised Linear Projections In chapter discussed dimension reduction unsupervised procedure In cases class information available ultimate interest reduce dimensionality improved classification makes sense use available class information forming projection We consider data different classes For class set N1 datapoints X1 x11 x N1 similarly class set N2 datapoints X2 x12 x N2 Our interest find linear projection y WTx dim W D L L D datapoints xi xj class distance projections yi yj small Conversely datapoints different classes distance projections large This useful classification purposes novel point x projection y WTx close class projected data expect x belong class In forming supervised pro jection class discriminative parts data retained procedure considered form supervised feature extraction Fisher s Linear Discriminant We restrict attention binary class data Also simplicity project data dimension The canonical variates algorithm section deals generalisations Fisher s Linear Discriminant b Figure The large crosses represent data class large circles class Their projections dimension represented small counterparts Fisher s Linear Discriminant Analysis Here little class overlap projections b Unsupervised dimension reduction Principal Components Analysis comparison There considerable class overlap projection In b dimensional projection distance line measured arbitrary chosen fixed point line Gaussian assumption We model data class Gaussian That p x1 N x1 m1 S1 p x2 N x2 m2 S2 m1 sample mean class data S1 sample covariance similarly class The projections points classes given yn1 w Txn1 y n w Txn2 Because projections linear projected distributions Gaussian p y1 N y1 w Tm1 w TS1w p y2 N y2 w Tm2 w TS2w We search projection w projected distributions minimal overlap This achieved projected Gaussian means maximally separated large However variances large large overlap classes A useful objective function represents fraction dataset class In terms projection w objective equation F w wT m1 m2 m1 m2 T w wT 1S1 2S2 w wTAw wTBw A m1 m2 m1 m2 T B 1S1 2S2 The optimal w found differentiating equation respect w This gives w wTAw wTBw wTBw wTBw Aw wTAw Bw DRAFT April Canonical Variates zero derivative requirement wTBw Aw wTAw Bw Multiplying inverse B B m1 m2 m1 m2 T w wTAw wTBw w Since m1 m2 T w scalar optimal projection explicitly given w B m1 m2 Although proportionality factor depends w constant objective function F w equation invariant rescaling w We w kB m1 m2 It common rescale w unit length wTw k m1 m2 T B m1 m2 An illustration method given fig demonstrates supervised dimension reduction produce lower dimensional representations suitable subsequent classification unsuper vised method PCA One arrive equation different starting objective By treating projection regression problem y wTx b outputs y defined y1 y2 classes class respectively suitably chosen y1 y2 solution squares criterion given equation This suggests way regularise LDA exercise Kernel extensions LDA possible example When naive method breaks The derivation relied existence inverse B In practice B invertible procedure requires modification A case B invertible fewer datapoints N1 N2 dimensions D A related problematic case elements input vectors vary For example hand written digits case pixels corner edges actually zero Let s pixel z The matrix B zero entry B z z z th row column B zero vector form wT wz wTBw This shows denominator Fisher s objective zero objective I shall defined We address issues section Canonical Variates Canonical Variates generalises Fisher s method projections dimension classes The projection point given y WTx W D L matrix Assuming data x class c Gaussian distributed p x N x mc Sc projections y Gaussian p y N y WTmc W TScW By analogy equation define following matrices DRAFT April Canonical Variates Between class Scatter Find mean m dataset mc mean class c Form A C c Nc mc m mc m T Nc number datapoints class c c C Within class Scatter Compute covariance matrix Sc data class c Define B C c NcSc Assuming B invertible section define Cholesky factor B B TB B A natural objective maximise Raleigh quotient F W trace WTB TAB 1W trace WTW If assume orthonormality constraint W equivalently require maximisation F W trace WTCW subject WTW I C D B TAB Since C symmetric positive semidefinite real eigen decomposition C EET diag D diagonal non negative entries containing eigenvalues sorted decreasing order ETE I Hence F W trace WTEETW By setting W e1 eL el l th eigenvector objective F W sum L eigenvalues This setting maximises objective function forming W columns E lower sum The procedure outlined algorithm Note A rank C C non zero eigenvalues corresponding directions q1 q2 Figure Each dimensional datapoint lies dimensional plane meaning matrix B rank invertible A solution given finding vectors q1 q2 span plane expressing Canonical Variates solution terms vectors DRAFT April Canonical Variates Algorithm Canonical Variates Compute class scatter matrices A equation B equation Compute Cholesky factor B B Compute L principal eigenvectors e1 eL B TAB Return W e1 eL projection matrix b Figure Canonical Variates projection examples handwritten digits o diamond There examples digit class Plotted projections dimensions b PCA projections comparison Dealing nullspace The derivation Canonical Variates Fisher s LDA requires invertibility matrix B However discussed section encounter situations B invertible A solution require W lies subspace spanned data contribution nullspace To concatenate training data classes large matrix X A basis X found example thin SVD technique returns orthonormal non square basis matrix Q We require solution W expressed basis fig W QW matrix W Substituting Canonical Variates objective equation obtain F W trace W T QTAQW trace W TQTBQW This form standard quotient equation replacing scatter A A QTAQ scatter B B QTBQ In case B guaranteed invertible B projected basis spans data One carry Canonical Variates section returns matrix W Transforming W given equation See CanonVar m DRAFT April Exercises Example Using canonical variates digit data We apply canonical variates project digit data dimensions fig There examples examples examples seven Thus overall examples lying pixels dimensional space The canonical variates projected data dimensions little class overlap fig 3a In comparison projections formed PCA discards class information displays high degree class overlap The different scales canonical variates PCA projections different constraints projection matrices W In PCA W unitary canonical variates WTBW I meaning W scale inverse square root largest eigenvalues class scatter matrix Since canonical variates objective independent linear scaling W rescaled arbitrary scalar prefactor W desired Summary Fisher s linear discriminant seeks scalar projection maximally different data classes Canonical variates generalises Fisher s method multiple classes multiple projected dimensions Fisher s method Canonical variates related standard eigen problems The applicability canonical variates depends assumption Gaussian good description data Clearly data multimodal single Gaussian model data class poor assumption This result projections large class overlap In principle conceptual difficulty complex distributions general criteria maximal Kullback Leibler divergence projected distributions However criteria typically result difficult optimisation problems Canonical variates popular simplicity lack local optima issues constructing projection Code CanonVar m Canonical Variates demoCanonVarDigits m Demo Canonical Variates Exercises Exercise What happens Fisher s Linear Discriminant datapoints dimensions Exercise Modify demoCanonVarDigits m project visualise digits data dimensions Exercise Consider N1 class datapoints xn1 n1 N1 N2 class datapoints xn2 n2 N2 We define linear predictor data y wTx b aim predict value y1 data class y2 data class A measure fit given E w b y1 y2 N1 n1 y1 wTxn1 b N2 n2 y2 wTxn2 b DRAFT April Exercises Show setting y1 N1 N2 N1 y2 N1 N2 N2 w minimises E corresponds Fisher s LDA solution Hint zero derivative conditions n1 y1 b wTxn1 n2 y2 b wTxn2 n1 y1 b wTxn1 xTn1 n2 y2 b wTxn2 xTn2 reduced single equation N m1 m2 NB N1N2 N m1 m2 m1 m2 T w B defined LDA text equation Note suggests way regularise LDA adding term wTw E w b y1 y2 This absorbed redefining equation B B I That increase covariance B additive I The optimal regularising constant set cross validation Exercise Consider digit data fives digit5 mat sevens digit7 mat Make training set consists examples digit class Use Canonical Variates project data dimensions compute Nearest Neighbour performance remaining digits Alternatively use PCA reduce data dimensions compare resulting Nearest Neighbour classification performance Visualise directions found Canonical Variates principal directions PCA Exercise Consider objective function form F w A w B w A w B w positive functions task maximise F w respect w It objective simple algebraic solution A w B w simple functions We consider alternative objective J w A w B w constant scalar Choose initial point wold random set old A wold B wold In case J wold old Now choose w J w old A w oldB w This certainly possible J wold old If find w J w old A w oldB w Show w F w F wold suggest iterative optimisation procedure objective functions form F w DRAFT April Exercises DRAFT April CHAPTER Linear Models In chapter discuss classical methods prediction based fitting simple linear models data These include standard methods linear logistic regression kernelised variants We discuss popular support vector machine related methods ensuring good generalisation performance Introduction Fitting A Straight Line Given training data xn yn n N scalar input xn scalar output yn linear regression fit y x bx To determine best parameters b use measure discrepancy observed outputs linear regression fit sum squared training error This called ordinary squares minimises average vertical projection points y fitted line fig 1a E b N n yn y xn N n yn bxn Our task find parameters b minimise E b Differentiating respect b obtain E b N n yn bxn b E b N n yn bxn xn Dividing N equating zero optimal parameters given solution linear equations y b x xy x b x2 notation f x y denote N N n f x n yn We readily solve equations determine b y b x b x2 yx x y b x b x2 x xy x y Linear Parameter Models Regression c h ir p s p e r s e c temperature F c h ir p s p e r s e c temperature F b Figure Data singbats number chirps second versus temperature Fahrenheit Straight line regression fit singbat data b PCA fit data In regression minimize residuals vertical distances datapoints line In PCA fit minimizes orthogonal projections line Both lines mean data Hence b xy x y x2 x found substituting value b equation In contrast ordinary squares regression PCA chapter minimises orthogonal projection y line known orthogonal squares example Example In fig plot number chirps c second singbats versus temperature t degrees Fahrenheit A biologist believes simple relation number chirps temperature form c bt needs determine parameters b For singbat data fit plotted fig 1a For comparison plot fit PCA fig 1b minimises sum squared orthogonal projections data line Linear Parameter Models Regression We generalise fitting linear functions vector inputs x For dataset xn yn n N linear parameter regression model LPM defined by1 y x wT x x vector valued function input vector x For example case straight line fit scalar input output equation x x T w b T We define train error sum squared differences observed outputs predictions linear model E w N n yn wTn n xn 1Note model linear parameter w necessarily linear x DRAFT April Linear Parameter Models Regression c h ir p s p e r s e c temperature F Figure Cubic polynomial fit singbat data We wish determine parameter vector w minimises E w In terms components w squared error E w N n yn wi n y n j wj n j Differentiating respect wk equating zero gives N n ynnk wi N n ni n k matrix notation N n ynn N n n n Tw These called normal equations solution w N n n n T N n ynn Although write solution matrix inversion practice finds numerical solution Gaussian elimination faster numerically stable Example A cubic polynomial fit A cubic polynomial given y x w1 w2x w3x w4x As LPM expressed x x x2 x3 T The ordinary squares solution form given equation The fitted cubic polynomial plotted fig See demoCubicPoly m Example Predicting return In fig fit LPM vector inputs x scalar output y The vector x represents factors believed affect stock price company stock price DRAFT April Linear Parameter Models Regression Figure Predicting stock return linear LPM The panels present inputs x1 x5 train days blue test days red The corresponding train output stock turn y day given panel The predictions y21 y25 predictions based yt iwixit w trained ordinary squares With regularisation term 01wTw OLS learned w De spite simplicity models application fi nance industry widespread significant investment finding factors x indicative future return See demoLPMhedge m return given scalar y A hedge fund manager believes returns linearly related factors yt wixit wishes fit parameters w order use model predict future stock returns This straightforward ordinary squares simply LPM linear function See fig example Such models form basis complex models finance example Vector outputs It straightforward generalise framework vector outputs y Using separate weight vector wi output component yi yi x w T x The mathematics follows similarly define train error output E wi n yni wTi n E w E wi Since training error decomposes individual terms output weights output trained separately In words problem decomposes set independent scalar output problems In case parameters w tied shared outputs training straight forward objective function remains linear parameters left exercise interested reader Regularisation For purposes interest find function best fits train data generalise To control complexity fitted function add extra regularising term R w train error penalise rapid changes output E w E w R w DRAFT April Linear Parameter Models Regression x b Figure A set fixed width radial basis functions exp x mi centres mi evenly spaced By taking linear combination functions form flexible function class b The training points validation points The solid line correct underlying function sin 10x corrupted small additive noise form train data The dashed line best predictor based validation set scalar adjust strength regularisation term For example regularising term added equation R w N n N n e xn xn y xn y xn The factor y xn y xn penalises large differences outputs corresponding inputs The factor exp xn xn effect weighting heavily terms input vectors xn xn close fixed length scale parameter Since y wT x expression written wTRw R N n N n e xn xn n n n n T The regularised train error E w N n yn wTn wTRw By differentiating regularised training error equating zero find optimal w given w n n n T R N n ynn In practice common use regulariser penalises sum squared length weights R w wTw w2i corresponds setting R I This known ridge regression Regularising pararameters determined validation set section DRAFT April Linear Parameter Models Regression Radial basis functions A popular LPM given non linear function x components x exp x mi These basis functions bump shaped centre bump given mi width An example given fig RBFs plotted different centres In LPM regression use linear combination bumps fit data One apply approach vector inputs For vector x centre m radial basis function depends distance x centre m giving bump input space fig Example Setting Consider fitting data fig 4b radial basis functions uniformly spread input space width parameter regularising term wTw The generalisation performance test data depends heavily width regularising parameter In order find reasonable values parameters use validation set For simplicity set regularisation parameter use validation set determine suitable In fig plot validation error E w function choosing lowest validation error The predictions optimal given fig 4b A curse dimensionality If data non trivial behaviour input region need cover region input space fairly densely bump type functions In case basis functions dimensional input space In dimensions wish cover dimension discretisation level need basis functions Similarly dimensions need functions To fit LPM require solving linear system variables This explosion number basis functions input dimension curse dimensionality A possible remedy basis functions broad covers high dimensional space However mean lack flexibility fitted function constrained smooth Another approach place basis functions centred training input points add basis functions randomly placed close training inputs The rationale come prediction likely novel x close training points need accurate predictions space A approach positions basis functions adaptive allowing moved space minimise error This approach neural network models An alternative reexpress problem fitting LPM reparameterising problem discussed va lid tio n e rr o r Figure The validation error function basis func tion width validation data fig 4b RBFs fig 4a Based validation error optimal setting basis function width parameter DRAFT April The Dual Representation Kernels x x x x b Figure The output RBF function exp x m1 Here m1 T b The combined output RBFs m1 m2 T The Dual Representation Kernels Consider set training data inputs X xn n N corresponding outputs yn n N For LPM form f x wTx interest find best fit parameters w We assume found optimal parameter w The nullspace X x orthogonal inputs X That x nullspace x T xn n N If consider vector w additional component nullspace w x T xn wT x n This means adding contribution w outside space spanned X effect predictions train data If training criterion depends LPM predicts train data need consider contributions w outside X That loss generality consider representation w N m amx m The parameters a1 aN called dual parameters We write output LPM directly terms dual parameters wTxn N m x m T xn More generally vector function x solution lie space spanned x1 xN w N n x n write wT xn N m x m T xn N m amK x m xn defined kernel function K xm xn xm T xn K m n In matrix form output LPM training input x wT xn Ka n Tkn kn nth column Gram matrix K By construction Gram matrix positive semidefinite kernel covariance function section DRAFT April Linear Parameter Models Classification Regression dual space For ordinary squares regression equation train error E N n yn aTkn Equation analogous standard regression equation interchanging w kn xn Similarly regularisation term expressed wTw N n m anam x n xm aTKa By direct analogy optimal solution N n kn kn T K N n ynkn We express solution conveniently writing N n K 1kn kn T I N n ynK 1kn Since kn nth column K K 1kn nth column identity matrix With little manipulation rewrite equation simply K I y y vector components formed training inputs y1 yN Using prediction new input x given y x kT K I y vector k components k m K x xm This dual space solution shows predictions expressed purely terms kernel K x x This means dispense defining vector functions x define kernel function directly This approach Gaussian Processes chapter enables use effectively large infinite dimensional vectors explicitly needing compute Note Gram matrix K dimension N N means computational complexity performing matrix inversion equation O N3 For moderate large N greater prohibitively expensive numerical approximations required This contrast computational complexity solving normal equations original weight space viewpoint O dim The dual parameterisation helps curse dimensionality complexity learning dual parameterisation scales cubically number training points cubically dimension vector Linear Parameter Models Classification In binary classification problem given train data D xn cn n N targets c Inspired LPM regression model assign probability novel input x belongs class p c x f xTw DRAFT April Linear Parameter Models Classification Figure The logistic sigmoid function x e x The parameter determines steepness sigmoid The blue line dashed red As logistic sigmoid tends Heaviside step func tion The dotted curve magenta error function probit erf x closely matches stan dard logistic sigmoid f x In statistics literature f x termed mean function inverse function f x link function2 Two popular choices function f x logit probit functions The logit given f x ex ex e x called logistic sigmoid written x fig The scaled version defined x x A closely related model probit regression uses place logistic sigmoid cumulative distribution standard normal distribution f x x e t2dt erf x standard error function erf x x e t dt The shape probit logistic functions similar rescaling fig We focus logit function Logistic regression Logistic regression corresponds model p c x b xTw b scalar w vector The decision boundary The decision boundary defined set x p c x p c x This given hyperplane b xTw On hyperplane b xTw inputs x classified s classified s The bias parameter b simply shifts decision boundary constant The orientation decision boundary determined w normal hyperplane fig To clarify geometric interpretation let x point decision boundary consider new point x x w w vector perpendicular w wTw Then b wTx b wT x w b wTx wTw b wTx Thus x decision boundary x plus vector perpendicular w In D dimensions space vectors perpendicular w occupy D dimensional hyperplane For example data dimensional decision boundary dimensional hyperplane line depicted fig 2These models generalised linear models class statistics literature includes regression model classification models special cases DRAFT April Linear Parameter Models Classification w Figure The decision boundary p c x solid line For dimensional data decision boundary line If train data class filled circles lie line class open circles data said linearly separable More generally w defines normal hyperplane data linearly separable data classes lies opposite sides hyperplane Definition Linear separability If train data class lies hyperplane class data said linearly separable For D dimensional data provided D training points linearly separable linearly independent To let cn xn class cn xn class For data linearly separable require wTxn b ncn n N n arbitrary positive constants The equations state input correct decision boundary If N D datapoints written matrix form Xw b d X square matrix nth column contains xn b b The vector d element d n ncn Provided X invertible solution w X d b The bias b set arbitrarily This shows provided xn linearly independent find hyperplane linearly separates data Provided data collinear occupying D dimensional subspace additional bias enables D arbitrarily labelled points linearly separated D dimensions An example dataset linearly separable given following training points class labels This data represents XOR function plotted fig This function linearly separable straight line inputs class class Classifying data linearly separable achieved non linear decision boundary It data non linearly separable original data space An alternative map data higher dimension non linear vector function creates set non linearly dependent high dimensional vectors separated high dimensional hyperplane We discuss section The perceptron We briefly describe perceptron important early model field AI example The perceptron deterministically assigns x class b wTx class That p c x b xTw Figure The XOR problem This linearly separable DRAFT April Linear Parameter Models Classification step function defined x x x If consider logistic regression model p c x b xTw limit perceptron like classifier p c x b xTw b xTw b xTw The difference probabilistic perceptron standard perceptron technical definition value step function The perceptron essentially viewed limiting case logistic regression Maximum likelihood training For class discriminative model model input distribution p x equivalently consider likelihood set output class variables C conditioned set training inputs X If assume data point drawn independently distribution generates data standard d assumption likelihood writing explicitly conditional dependence parameters b w p C b w X N n p cn xn b w p xn N n p c xn b w cn p c xn b w c n p xn fact cn For logistic regression gives log likelihood L w b N n cn log b wTxn cn log b wTxn Gradient ascent There closed form solution maximisation L w b needs carried numerically One simplest methods gradient ascent gradient given wL N n cn wTxn b xn Here use derivative relation logistic sigmoid d x dx x x The derivative respect bias dL db N n cn wTxn b The gradient ascent procedure corresponds updating weights bias wnew w wL bnew b dL db learning rate scalar chosen small ensure convergence3 The application rule lead gradual increase log likelihood 3In principle use different learning rate parameter DRAFT April Linear Parameter Models Classification b Figure The decision boundary p c x solid line confidence boundaries p c x p c x iterations batch gradient ascent Linearly separable data b Non linearly separable data Note confidence interval remains broad demoLogReg m Batch training Writing updates explicitly gives wnew w N n cn wTxn b xn bnew b N n cn wTxn b This called batch update parameters w b updated passing batch train data For linearly separable data weights infinite convergence Taking scalar product Equation w zero gradient requirement N n cn n wTxn n wTxn b For simplicity assume b For linearly separable data wTxn cn cn Then fact n cn n wTxn cn cn Each term cn n wTxn non negative zero gradient condition requires sum terms zero This happen terms zero implying cn n requiring sigmoid saturate weights infinite Online training In practice common update parameters training example pair xn cn considered wnew w N cn wTxn b xn bnew b N cn wTxn b An advantage online training dataset need stored performance current input required Provided data linearly separable online procedure converges provided large However data linearly separable online version converge opposing class labels continually pull weights way conflicting example form update For limiting case perceptron replacing x x linearly separable data online updating converges finite number steps converge non linearly separable data DRAFT April Linear Parameter Models Classification Figure Logistic regression classifying handwritten digits Displayed Hinton diagram learned weight vector w plotted image visual interpretation Light squares positive weights input x positive value component tend increase probability input classed Similarly inputs positive contributions dark regions tend increase probability classed digit Note elements input x positive zero Geometry error surface The Hessian log likelihood L w matrix elements4 Hij 2L wiwj n xni x n j n n This negative semidefinite z ij ziHijzj j n zix n zjx n j n n n zix n This means error surface concave upside bowl batch gradient ascent converges optimal solution provided learning rate small Example Classifying Handwritten Digits We apply logistic regression handwritten digits example ones sevens train data Using gradient ascent training suitably chosen stopping criterion number errors test points compared errors Nearest Neighbour methods See fig visualisation learned w Beyond order gradient ascent Since surface single optimum Newton update wnew wold H 1wold H Hessian matrix typically converge faster gradient ascent However large scale problems dim w inversion Hessian computationally demanding limited memory BFGS conjugate gradient methods practical alternatives section A Avoiding overconfident classification Provided data linearly separable weights continue increase classifications extreme This undesirable resulting classifications confident One way prevent early stopping limited number gradient updates performed An alternative method add penalty term objective function L w b L w b wTw The scalar constant encourages smaller values w remember wish maximise log likelihood An appropriate value determined validation data 4For simplicity ignore bias b This readily dealt extending x D dimensional vector x D component Then D dimensional w wT wD T w Tx wTx wD DRAFT April Linear Parameter Models Classification Figure Logistic regression p c x wT x ing quadratic function x x1 x2 x x x1x2 T iterations gradient ascent training performed learning rate Plotted datapoints classes cross circle equal probability con tours The decision boundary probability contour See demoLogRegNonLinear m Multiple classes For classes use softmax function p c x e wTi x bi C j e wTj x bj C number classes When C reduced logistic sigmoid model One likelihood case concave exercise Gradient based training methods applied training models straightforward extension class case The Kernel Trick Classification A drawback logistic regression described simplicity decision surface hyperplane Analogous regression case way achieve complex non linear decision boundaries map inputs x non linear way higher dimensional x use p c x wT x b Mapping higher dimensional space makes easier find separating hyperplane set points linearly independent linearly separated provided dimensions datapoints For maximum likelihood criterion use exactly algorithm replacing x x See fig demonstration quadratic function Since scalar product vectors plays role dual representation section assume weight expressed form w n m x n We subsequently find solution terms dual parameters n This potentially advantageous training points dimensions The classifier depends scalar product wT x n n x n T x write generally positive definite kernel K x x p c x n anK x x n For convenience write p c x aTk x N dimensional vector k x elements k x m K x x m Then exactly form original specification logistic regression function linear combination vectors Hence training algorithm maximise likelihood employed simply replacing xn k xn The details left interested reader follow closely treatment Gaussian Processes classification section DRAFT April Support Vector Machines Support Vector Machines Like kernel logistic regression SVMs form kernel linear classifier However SVM uses objective explicitly encourages good generalisation performance SVMs fit comfortably probabilistic framework describe briefly referring reader wealth excellent literature topic5 The description inspired largely Maximum margin linear classifier In SVM literature common use denote classes For hyperplane defined weight w bias b linear discriminant given wTx b class class For point x close decision boundary wTx b small change x lead change classification To classifier robust impose train data decision boundary separated data finite assuming instance data linearly separable wTx b class class Since w b rescaled arbitrary need fix scale break invariance It convenient set point x class closest decision boundary satisfies wTx b point x class closest decision boundary satisfies wTx b From vector algebra fig distance origin direction w point x given wTx wTw The margin hyperplanes classes difference distances direction w wT wTw x x wTw To set distance hyperplanes maximal need minimise length wTw Given xn corresponding label yn order classify training labels correctly maximise margin optimisation problem equivalent minimise wTw subject yn wTxn b n N This quadratic programming problem The factor convenience To account potentially mislabelled training points data linearly separable relax exact classification constraint use instead yn wTxn b n slack variables n Here n measures far xn correct margin fig For n datapoint xn correct decision boundary However n datapoint assigned opposite class training label Ideally want limit size violations n Here briefly describe standard approaches DRAFT April Support Vector Machines origin w Figure SVM classification data classes open circles filled circles The decision boundary wTx b solid line For linearly separa ble data maximum margin hyperplane equidis tant closest opposite class points These sup port vectors highlighted blue margin red The distance decision boundary origin b wTw distance gen eral point x origin direction w xTw wTw origin w n Figure Slack margin The term n measures far variable correct mar gin class If n point misclassified treated outlier Norm soft margin The norm soft margin objective minimise wTw C n n subject yn wTxn b n n N C controls number mislabellings train data The constant C needs determined empirically validation set The optimisation problem expressed formulated Lagrangian L w b wTw C n n n n yn wTxn b n n n minimised respect w b maximised respect For points xn correct decision boundary yn wTxn b n maximising L respect requires corresponding n set zero Only training points support vectors lying decision boundary non zero n Differentiating Lagrangian equating zero conditions wi L w b wi n nynxni b L w b n nyn n L w b Cn n From solution w given w n nynxn http www support vector net DRAFT April Support Vector Machines Since support vectors non zero n solution w typically depend small number training data Using conditions substituting original problem objective equivalent minimising L n n n m ynymnm xn T xm 2C n n subject n ynn n If define K xn xm xn T xm The optimisation problem maximize n n n m ynymnm K xn xm C n m subject n ynn n Optimising objective discussed section norm soft margin box constraint In norm soft margin version uses norm penalty C n n optimisation problem minimise wTw C n n subject yn wTxn b n n n N C empirically determined penalty factor controls number mislabellings train data To reformulate optimisation problem use Lagrangian L w b wTw C n n n n yn wTxn b n n rnn n n rn The variables rn introduced order non trivial solution n C Following similar argument norm case differentiating Lagrangian equating zero arrive optimisation problem maximize n n n m ynymnmK xn xm subject n ynn n C closely related norm problem box constraint n C Using kernels The final objectives depend inputs xn scalar product xn T xn If map x vector function x write K xn xm xn T xm This means use positive semidefinite kernel K non linear classifier See section DRAFT April Soft Zero One Loss Outlier Robustness Figure SVM training The solid red solid blue circles represent train data different classes The support vectors highlighted green For unfilled test points class assigned SVM given colour See demoSVM m Performing optimisation Both soft margin SVM optimisation problems quadratic programs exact computational cost scales O N3 Whilst solved general purpose routines specifically tailored routines exploit structure problem preferred practice Of particular practical interest chunking techniques optimise subset In limit updating components achieved analytically resulting Sequential Minimal Optimisation algorithm practical performance typically O N2 better A variant algorithm provided SVMtrain m See fig demonstration Once optimal solution N found decision function new point x n n y nK xn x b assign class assign class The optimal b determined maximum margin condition equations b min yn m m y mK xm xn max yn m m y mK xm xn Probabilistic interpretation Kernelised logistic regression characteristics SVM express large margin requirement Also sparse data usage SVM similar Relevance Vector Machine discuss section However probabilistic model MAP assignment matches exactly SVM hampered normalisation requirement probability distribution Whilst arguably fully satisfactory direct match SVM related probabilistic model achieved approximate matches obtained Soft Zero One Loss Outlier Robustness Both support vector machine logistic regression potentially misled outliers For SVM mislabelled datapoint far correct decision boundary require large slack However exactly large discouraged unlikely SVM admit solution For logistic regression probability generating mislabelled point far correct decision boundary exponentially small happen practice This means model trained maximum likelihood present solution In cases mislabelled points outliers potentially significant impact location decision boundary A robust technique deal outliers use zero loss mislabeled point contributes relatively small loss Soft variants obtained objective N n b w Txn cn wTw minimised respect w b For term tends zero loss The second term represents penalty length w prevents overfitting Kernel extensions DRAFT April Soft Zero One Loss Outlier Robustness Figure Soft zero loss decision boundary solid line versus logistic regression dotted line The number mis classified training points soft zero loss compared logistic regression The penalty soft loss For logistic regression penalty term The outliers significant impact decision boundary logistic regression whilst soft zero loss essentially gives outliers fits classifier remaining points See demoSoftLoss m soft zero loss straightforward Unfortunately objective non convex finding optimal w b computationally dif ficult A simple minded scheme fix components w perform numerical dimensional optimisation single parameter wi At step parameter wj cho sen procedure repeated convergence As usual set validation The practical difficulties minimising non convex high dimensional objective functions means approaches rarely practice A discussion practical attempts area given An illustration difference logistic regression soft zero loss given fig demonstrates logistic regression influenced mass data points zero loss attempts minimise number mis classifications whilst maintaining large margin Summary Fitting linear regression models based squares straightforward requires solution linear system In classification maximum likelihood criterion typically results simple concave function parameters simple gradient based learning methods suitable training The known historically important perceptron viewed limiting case logistic regression The kernel extensions linear models enable find non linear decision boundaries classification case These techniques closely related Gaussian Processes Historical note The perceptron long history artificial intelligence machine learning Rosenblatt discussed perceptron model human learning arguing distributive nature input output patterns stored weight vector closely related kind information storage believed present biological systems To deal non linear decision boundaries main thrust research ensuing neural network community use multilayered structures outputs ceptrons inputs perceptrons resulting potentially highly non linear discriminant functions This line research largely inspired analogies biological information processing layered structures prevalent Such multilayered artificial neural networks fascinating trained extremely fast forming decisions However reliably training systems highly complex task probabilistic generalisations priors placed parameters lead computational difficulties Whilst inspiring biological viewpoint alternative route kernel trick boost power linear classifier advantage ease training generalisation probabilistic variants More recently resurgence interest multilayer systems new heuristics aimed improving difficulties training example DRAFT April Exercises Code demoCubicPoly m Demo fitting cubic polynomial demoLogReg m Demo logistic regression LogReg m Logistic regression gradient ascent training demoLogRegNonLinear m Demo logistic regression non linear x SVMtrain m SVM training SMO algorithm demoSVM m SVM demo demoSoftLoss m softloss demo softloss m softloss function Exercises Exercise Give example dimensional dataset data linearly separable linearly independent Can find dataset linearly independent linearly separable Exercise Show Ordinary Orthogonal Least Squares regression fits data xn yn n N fitted lines point N n x n yn N Exercise Consider softmax function classifying input vector x c C classes p c x e wTc x C c e wT c x A set input class examples given D xn cn n N Relate model logistic regression C Write log likelihood L classes conditional inputs assuming data d Compute Hessian elements Hij 2L D wiwj w stacked vector w wT1 w T C T Hessian negative semidefinite zTHz z Hint point need use use result variance non negative Exercise Derive equation dual optimisation problem equation Exercise A datapoint x projected lower dimensional vector x x Mx M given fat short wide matrix For set data xn n N corresponding binary class labels yn logistic regression projected datapoints x n corresponds form constrained logistic regression original higher dimensional space x Explain reasonable use algorithm PCA reduce data dimensionality logistic regression DRAFT April Exercises Exercise The logistic sigmoid function defined x ex ex What inverse function x Exercise Given dataset D xn cn n N cn logistic regression uses model p c x wTx b Assuming data drawn independently identically derivative log likelihood L respect w wL N n cn wTxn b xn Exercise Consider dataset D xn cn n N cn x D dimen sional vector Show training data linearly separable hyperplane wTx b data separable hyperplane w Tx b w w b b scalar What consequence result maximum likelihood training logistic regression linearly separable data Exercise Consider dataset D xn cn n N cn x N dimen sional vector Hence N datapoints N dimensional space In text showed find hyperplane parameterised w b linearly separates data need datapoint xn wTxn b n n cn n cn Furthermore suggested algorithm find hyperplane Comment relation maximum likelihood training logistic regression algorithm suggested Exercise Given training data D xn cn n N cn x vector inputs discriminative model p c x b0 v1g wT1 x b1 v2g wT2 x b2 g x exp 5x2 x ex ex neural network single hidden layer hidden units Write log likelihood class conditioned inputs based usual d assump tion Calculate derivatives log likelihood function network parameters w1 w2 b1 b2 v0 v1 v2 Comment relationship model logistic regression Comment decision boundary model DRAFT April Exercises DRAFT April CHAPTER Bayesian Linear Models The previous chapter discussed use linear models classification regression In chapter discuss priors parameters resulting posterior distribution parameters This represents powerful extension enables specify principled way prior knowledge takes account data limited considerable uncertainty best parameter estimate Regression With Additive Gaussian Noise The linear models chapter trained maximum likelihood deal sue probabilistic perspective parameter estimates inherently uncertain lim ited available training data Regression refers inferring mapping basis observed data D xn yn n N xn yn represents input output pair We discuss scalar output case vector inputs x extension vector output case y straightforward We assume clean output generated model f x w parameters w function f unknown An observed output y generated addition noise clean model output y f x w If noise Gaussian distributed N model generates output y input x probability p y w x N y f x w exp y f x w Here interest modelling output distribution distribution inputs As parameters model inputs If assume data input output pair generated identically independently likelihood model generates data p D w N n p yn w xn p xn We use prior weight distribution p w quantify priori belief suitability parameter setting Writing D Dx Dy posterior weight distribution given p w D p D w p w p Dy w Dx p w Using Gaussian noise assumption convenience defining gives log p w D N n yn f xn w log p w N log const Regression With Additive Gaussian Noise w xn yn N Figure Belief Network representation Bayesian Model regression d data assumption The hyperparameter acts form regulariser controlling flexibility prior weights w The hyperparameter controls level noise observations Note similarity equation regularised training error equation In probabilistic framework identify choice sum squared error assumption additive Gaussian noise Similarly regularising term identified log p w Bayesian linear parameter models Linear parameter models discussed chapter form f x w B wii x wT x parameters wi called weights number basis functions dim w B Such models linear parameter dependence represent non linear input output mapping basis functions x non linear x Since output scales linearly w discourage extreme output values penalising large weight values A natural weight prior p w N w 1I B exp wTw precision inverse variance If large total squared length weight vector w encouraged small The model p D w specified fig Under Gaussian noise assumption posterior distribution log p w D N n yn wT xn wTw const represents hyperparameter set Parameters determine functions included hyperparameter set Completing square section weight posterior Gaussian distribution p w D N w m S covariance mean given S I N n xn T xn m S N n yn xn The mean prediction input x given f x f x w p w D dw mT x Similarly variance underlying estimated clean function var f x wT x p w D f x T x S x The output variance var f x depends input variables training outputs y Since additive noise uncorrelated model outputs predictive variance var y x var f x represents variance noisy output input x DRAFT April Regression With Additive Gaussian Noise b c Figure Along horizontal axis plot input x vertical axis output y The raw input output training data basis functions x b Prediction regularised training poorly chosen fixed hyperparameters c Prediction ML II optimised hyperparameters Also plotted standard error bars clean underlying function f x var f x Example In fig 2b mean prediction data fig 2a Gaussian basis functions x exp x ci width centres ci spread evenly dimensional input space We set hyperparameters hand The prediction severely overfits data result poor choice hyperparameter settings This resolved fig 2c ML II parameters described Determining hyperparameters ML II The hyperparameter posterior distribution p D p D p A simple summarisation posterior given MAP assignment takes single optimal setting argmax p D If prior belief hyperparameters weak p const equivalent maximises marginal likelihood p D p D w p w dw This approach setting hyperparameters called ML II Evidence Procedure In case Bayesian Linear Parameter models Gaussian additive noise computing marginal likelihood equation involves Gaussian integration A direct approach deriving expression marginal likelihood consider p D w p w exp N n yn wT xn wTw N B DRAFT April Regression With Additive Gaussian Noise By collating terms w completing square section represents Gaussian w additional factors After integrating Gaussian log p D N n yn dTS 1d log det S B log N log N log d n xn yn See exercise alternative expression Example We return example fig try find appropriate setting hyperparameters Using hyperparameters optimise expression gives results fig 2c plot mean predictions standard predictive error bars This demonstrates acceptable setting hyperparameters obtained maximising marginal likelihood Generally speaking provided number hyperparameters low compared number datapoints setting hyperparameters ML II risk overfitting Learning hyperparameters EM As described set hyperparameters maximising marginal likelihood equation A convenient computational procedure achieve interpret w latent variables apply EM algorithm section From equation energy term E log p D w p w p w D old According general EM procedure need maximise energy term For hyperparameter derivative energy given E log p D w p w p w D old For Bayesian LPM Gaussian weight noise distributions obtain E N N n yn wT xn p w old D N N n yn mT xn trace S N n xn T xn S m given equation Solving zero derivatives gives M step update new N N n yn mT xn trace SS S N N n xn T xn Similarly E B wTw p w old D B trace S mTm equating zero gives update new B trace S mTm An alternative fixed point procedure rapidly convergent EM given equation Closed form updates hyperparameters width basis functions generally available corresponding energy term needs optimised numerically DRAFT April Regression With Additive Gaussian Noise lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda lambda Figure Predictions RBF different widths For ML II optimal obtained running EM procedure convergence subsequently form predictions In panel dots represent training points x horizontal axis y vertical axis Mean predictions plotted predictive error bars standard deviation According ML II best model corresponds fig The smaller values overfit data giving rise rough functions The largest values underfit giving smooth functions See demoBayesLinReg m Hyperparameter optimisation gradient To maximise equation respect hyperparameters use general identity equation context log p D log p D w p w p w D Since likelihood independent log p D log p w p w D Using log p w wTw B log const x lo g m rg l l ik e lih o o d lambda Figure The log marginal likelihood log p D having found optimal values hyperparameters ML II These optimal values dependent Ac cording ML II best model corresponds From fig gives reasonable fit data DRAFT April Regression With Additive Gaussian Noise obtain log p D wTw B p w D Setting derivative zero optimal satisfies wTw p w D B One form fixed point equation new B wTw p w D equivalent EM update equation model For Gaussian posterior p w D N w m S wTw trace wwT w w T w w T trace S mTm new B trace S mTm One similarly find gradient associated fixed point update equivalent EM update model Gull MacKay fixed point iteration From equation wTw p w D B S mTm B alternative fixed point equation new B trace S mTm In practice update converges rapidly equation Similarly form alternative update new trace SS N N n y n mT xn Example Learning basis function widths In fig plot training data regression problem Bayesian LPM A set Radial Basis Functions RBFs x exp x ci ci spread evenly The hyperparameters learned ML II EM updating For fixed width present predictions time finding optimal width The optimal joint hyperparameter setting obtained described fig shows marginal log likelihood range widths The fit resulting jointly optimal hyperparameter reasonable DRAFT April Regression With Additive Gaussian Noise Validation likelihood The hyperparameters found ML II best explaining training data In principle different best prediction practice reasonable set hyperparameters validation techniques One method set hyperparameters minimal prediction error validation set Another common technique set hyperparameters likelihood validation set Xval Yval xmval ymval m M p Yval Xtrain Ytrain Xval w p Yval w p w Xtrain Ytrain obtain exercise log p Yval Dtrain Xval log det 2Cval yval valm T C 1val yval valm yval y1val y M val T covariance Cval valSTval 2IM design matrix explanatory variables Tval x1val x M val The optimal hyperparameters found maximising respect Prediction model averaging For fixed hyperparameters Bayesian LPM defines Gaussian posterior distribution weights w The distribution compute expected predictor variance predictor More generally place prior distribution hyperparameters obtain corresponding posterior p D p D p The mean function predictor given integrating posterior weights hyperpa rameters f x f x w p w D dwd f x w p w D dw p D d The term curly brackets mean predictor fixed hyperparameters Equation weights mean predictor posterior probability hyperparameter p D This general recipe combining model predictions model weighted posterior probability However computing integral hyperparameter posterior numerically challenging approximations usually required Provided hyperparameters determined data instead approximate hyperparameter integral finding MAP hyperparameters use f x f x w p w D dw Under flat prior p const equivalent mean predictor hyperparameters set ML II An alternative draw samples l l L posterior distribution p D form predictor averaging samples f x L L l f x w p w l D dw DRAFT April Regression With Additive Gaussian Noise Sparse linear models A common interest attempt explain data inputs possible More generally try find parsimonious explanation limited number features x From Bayesian LPM perspective require prior strongly encourages limited number weights wi active rest set zero Formally corresponds non Gaussian prior weights hampers use exact Bayesian methods Non Bayesian methods use penalty terms L1 lasso regularisation wi objective LPM remains concave l closely related Laplace priors Bayesian setting Here briefly discuss approximate methods sparse linear regression The relevance vector machine The relevance vector machine assumes small number components basis function vector relevant determining solution w For predictor f x w B wii x wT x case basis functions redundant sense linear combination basis functions reproduce training outputs insignificant loss accuracy To exploit effect seek parsimonious solution use refined prior encourages wi small p w p wi For computational convenience RVM typically chooses Gaussian p wi N wi exp w2i Sparsity achieved optimising hyperparameters large effectively forcing weight wi zero The modifications required description section replace S S diag N n xn T xn The marginal likelihood given log p D N n yn dTS 1d log det S B logi N log N log The EM update unchanged EM update newi S ii m A potential difficulty approach hyperparameters parameters finding optimal hyperparameters ML II problematic resulting overly aggressive pruning weights This ameliorated fully Bayesian approach Spike slab priors A natural alternative approach sparse linear regression use binary indicators si f x w B siwii x p w N wi DRAFT April Classification Algorithm Evidence Procedure Bayesian Logistic Regression Initialise w Not Converged Find optimal w iterating equation equation convergence E step Update according equation M Step end weights wi si contribute function One specify level sparsity choosing prior joint set p s1 sB encourages small number si rest zero This achieved product Bernoulli distributions p s B si si specifies prior level sparsity This equivalent original LPM f x w B wii x spike slab weight prior p w s siN wi si wi places spike si broader Gaussian slab N wi si These formulations equivalent result non Gaussian posterior weights approx imation methods required Popular approaches include Gibbs sampling chapter variational methods chapter Classification For logistic regression model p c w x B wii x maximum likelihood method returns single optimal w To deal inevitable uncertainty estimating w need determine posterior distribution To define prior weights p w As regression case convenient choice Gaussian p w N w 1I B B exp wTw inverse variance precision Given dataset input class labels D xn cn n N parameter posterior assuming model input distribution p w D p D w p w p D p D p w N n p cn xn w Unfortunately distribution standard form exactly inferring statistics mean formally computationally intractable DRAFT April Classification Hyperparameter optimisation Analogous regression case hyperparameters set maximising marginal likelihood p D p D w p w dw N n p cn xn w B exp wTw dw There approaches approximate integral discuss Laplace variational technique Common approaches form gradient differing statistics approximation posterior For reason derive generic hyperpa rameter update formulae apply approximations To find optimal search zero derivative log p D This equivalent linear regression case immediately obtain log p D wTw B p w D Setting derivative zero exact equation optimal satisfies wTw p w D B One form fixed point equation new B wTw p w D The averages expression computed exactly replaced averages respect approximation posterior q w D For Gaussian approximation posterior q w D N w m S wTw trace wwT w w T w w T trace S mTm new B trace S mTm In case Gull Mackay alternative fixed point equation new B S mTm The hyperparameter updates form regression model The mean m covariance S posterior regression classification cases different In classification case need approximate mean covariance discussed Laplace approximation The Laplace approximation section simple approximation fitting Gaussian locally probable point posterior The weight posterior logistic regression model given p w D exp E w E w wTw N n log wThn hn 2cn n DRAFT April Classification By approximating E w quadratic function w obtain Gaussian approximation q w D p w D To find minimum E w Differentiating obtain E w N n n hn n wThn It convenient use Newton method find optimum The Hessian matrix elements Hij wi wj E w given H I N n n n n n T J Note Hessian positive semidefinite exercise function E w convex bowl shaped finding minimum E w numerically unproblematic A Newton update wnew w H E Given converged w posterior approximation given q w D N w m S S H m w converged estimate minimum point E w H Hessian E w point Approximating marginal likelihood Using Laplace approximation marginal likelihood given p D w p D w p w w N n p cn xn w B e wTw w e E w For optimum value m w approximate marginal likelihood section log p D L w Tw n log w T hn log det I J B log Given approximation L marginal likelihood alternative strategy hyperparameter opti misation optimise L respect By differentiating L directly reader resulting updates fact equivalent general condition equation Laplace approximation posterior statistics Making predictions Ultimately interest classify novel situations averaging posterior weight uncertainty suming fixed suitable value p c x D p c x w p w D dw The B dimensional integrals w computed analytically numerical approximation quired Using Laplace approximation replace exact posterior p w D Laplace s method Gaussian approximation q w D N w m S p c x D p c x w q w D dw x Tw N w m S dw DRAFT April Classification Figure Bayesian logistic regression RBF functions x exp x mi placing cen tres mi subset training points The green points training data class red points training data class The contours represent probability class The op timal value found ML II set hand See demoBayesLogRegression m notational convenience write x x To compute predictions appear need carry integral B dimensions However term x Tw depends w scalar product x Tw require integral dimensional projection h x Tw exercise That p c x D h q h x D dh Since Laplace approximation w Gaussian distributed linear projection h q h x D N h x Tm x Tx Predictions numerically evaluating dimensional integral Gaussian distribution h equation Fast approximate methods computing discussed Approximating Gaussian average logistic sigmoid Predictions Gaussian posterior approximation require computation I x N x Gaussian quadrature obvious numerical candidate An alternative replace logistic sigmoid suitably transformed erf function reason Gaussian average erf function erf function Using single erf approximation is1 x erf x These functions agree A reasonable criterion derivatives agree x locally slope origin globally similar shape Using derivative requires A accurate approximation obtained taking convex combination scaled erf functions logsigapp m 1Note definition erf function taken consistent MATLAB erf x x e t dt Other authors define cumulative density function standard Gaussian x e t2dt DRAFT April Classification Variational Gaussian approximation An alternative Laplace s method use called variational method Since marginal likelihood key quantity example hyperparameter selection useful lower bound log marginal likelihood Like EM find best hyperparameters maximising lower bound likelihood To notation reasonably simple drop conditioning hyperparameters attempt find lower bound log marginal likelihood p D One approach obtaining bound based Kullback Leibler divergence KL q w p w D Since p w D p D w p w p D obtain log p D log p D w p w q w log q w q w holds distribution q w The angled brackets q denote expectation respect q Using explicit form logistic regression model right hand given by2 BKL N n log snw Tx n q w KL q w p w To form tractable lower bound need choose class distributions q w expres sion evaluated example Gaussian q w N w m S For Gaussian prior p w N w KL q w p w straightforward For numerical convenience parameterise covariance approximation Cholesky decomposition S CTC upper triangular C This gives 2KL q w p w logCii log det trace CTC mT 1m B B dim x In computing bound problematic remaining terms In log snw Tx n N w m S We define activation snwTx n Since w Gaussian distributed activation p N n n n snx nm n x nC Cx n In log N n 2n log n z n z z denotes expectation respect standard normal distribution z N z In way In computed standard dimensional numerical integration method Gaussian quadrature Since bound numerically accessible parameters m C approximating Gaussian proceed find optimal parameters direct numerical maximisation lower bound The gradient bound respect m BKL m 1m N n snx n n z n z Similarly BKL C C T C C N n x nx T n n z n z n z understanding upper triangular matrix expression taken One bound equation concave mean covariance These gradients general purpose optimisation routine find best m C approximation parameters 2The generalisation x follows simply replacing x x DRAFT April Classification Figure The logistic sigmoid x exp x red Gaussian lower bound blue dashed curve operating point Local variational approximation An alternative KL bounding method use called local method bounds term integrand To lower bound p D n snw Tx n p w dw bound logistic sigmoid function fig x x x2 Hence log snw Tx n log n snw Tx n n n wTx n 2n Using write p D dwN w n e log n snw Tx n n n wTx n 2n For fixed n n N right hand analytically integrated w resulting bound log p D log det S det m TS 1m N n log n n n n A N n n x nx T n b N n snx n S A m A 1b The bound equation maximised respect variational parameters n At convergence Gaussian N w m S approximation posterior Relation KL variational procedure As alternative numerical integration KL procedure described section instead bound problematic term log snw Tx n q w equation As dis cuss section enables relate KL local bounding procedures showing unconstrained Cholesky parameterisation C KL approach results provably tighter lower bound local approach As illustration approximations dimensional data D plot contours posterior approximations fig demonstrates compact nature local approximation The Laplace approximation yield bound plotted comparison DRAFT April Classification p w D JJ b Laplace c VG d Figure Posterior approximations dimensional Bayesian logistic regression posterior based N datapoints True posterior b The Gaussian Jaakola Jordan local approximation c The Laplace Gaussian approximation d The variational Gaussian approximation KL approach Relevance vector machine classification It straightforward apply sparse linear model methods section classification For example adopting RVM prior classification encourage individual weights small p w p wi p wi N wi The alterations previous ML II approach replace equation equation E iwi n n hni H diag J These Newton update formula The update equation s given newi m2i Sii Similarly Gull MacKay update given newi iSii m2i Running procedure typically finds s tend infinity corresponding weights pruned system The remaining weights typically correspond basis functions RBF case centres mass clusters datapoints class fig Contrast situation SVMs retained datapoints tend decision boundaries The number training points retained RVM tends small smaller number retained SVM framework Whilst RVM support large margins robust classifier retain advantages probabilistic framework A potential critique RVM coupled ML II procedure learning overly aggressive terms pruning Indeed verify running demoBayesLogRegRVM m common find instance problem exists set training data classified perfectly ML II set zero training data longer classified perfectly An alternative apply spike slab prior technique suffers overly aggressive pruning requires sophisticated approximation methods Multi class case We briefly note multi class case treated softmax function m class coding scheme The class probabilities p c m y e ym m e ym DRAFT April Classification b Figure Classification RVM RBF e x m placing basis function subset training data points The green points training data class red points training data class The contours represent probability class Training points b The training points weighted relevance value n Nearly points value small effectively vanish See demoBayesLogRegRVM m automatically enforces constraint m p c m Naively appear C classes cost Laplace approximation scales O C3N3 However careful implementa tion cost reduced O CN3 analogous cost savings possible Gaussian Process classification model Summary A simple extension linear regression achieved Gaussian prior parameters Coupled assumption additive Gaussian noise output posterior distribution Gaussian predictions computed readily In case classification closed form Bayesian solution obtained simple Gaussian priors parameters approximations required The parameter posterior behaved simple unimodal approximations adequate Code demoBayesLinReg m Demo Bayesian linear Regression BayesLinReg m Bayesian linear regression demoBayesLogRegRVM m Demo Bayesian logistic regression RVM BayesLogRegressionRVM m Bayesian logistic regression RVM avsigmaGauss m Approximation Gaussian average logistic sigmoid logsigapp m Approximation logistic sigmoid mixture erfs DRAFT April Exercises Exercises Exercise This exercise concerns Bayesian regression Show f wTx p w N w p f x Gaussian distributed Furthermore find mean covariance Gaussian Consider target point t f N What p f t x Exercise A Bayesian Linear Parameter regression model given yn wT xn n In vector notation y y1 yN T written y w T x1 xN zero mean Gaussian distributed vector covariance 1I An expression marginal likelihood dataset given equation We aim find compact expression likelihood given hyperparameters p y1 yN x1 xN Since yn linearly related w p w N w 1I y Gaussian distributed mean y w covariance matrix C yyT y y T w w T Show covariance matrix expressed C I T Hence log marginal likelihood written log p y1 yN x1 xN log det 2C yTC 1y Exercise Using exercise basis derive expression log likelihood validation set Exercise Consider function E w defined equation Show Hessian matrix elements Hij wi wj E w I N n n n n n T ij Show Hessian positive definite Exercise Show function f f xTw p w dw f h p h dh p h distribution scalar xTw The significance high dimensional integral form reduced dimensional integral distribution field h DRAFT April Exercises Exercise This exercise concerns Bayesian Logistic Regression Our interest derive op timal regularisation parameter based Laplace approximation section marginal log likelihood given log p D L w Tw n log w T hn log det I J B log The Laplace procedure finds optimal w minimises wTw n log wThn equation depend setting Formally finding optimises L use total derivative formula dL d L L wi wi However evaluated w w L w This means order compute derivative respect need consider terms explicit dependence Equating derivative zero log det M trace M M optimal satisfies fixed point equation new N w Tw trace I J DRAFT April CHAPTER Gaussian Processes In Bayesian linear parameter models saw relevant quantities related scalar product data vectors In Gaussian Processes use motivate prediction method necessarily correspond parametric model data Such models flexible Bayesian predictors Non Parametric Prediction Gaussian Processes flexible Bayesian models fit probabilistic modelling framework In developing GPs useful step information need form predictor Given set training data D xn yn n N X Y xn input datapoint n yn corresponding output continuous variable regression case discrete variable classification case aim prediction y new input x In discriminative framework model inputs x assumed outputs modelled conditioned inputs Given joint model p y1 yN y x1 xN x p Y y X x subsequently use conditioning form predictor p y x D In previous chapters ve use d assumption datapoint independently sampled generating distribution In context appear suggest assumption p y1 yN y x1 xN x p y X x n p yn X x However clearly little use predictive conditional simply p y D x p y X x meaning predictions use training outputs For non trivial predictor need specify joint non factorised distribution outputs From parametric non parametric If revisit d assumptions parametric models parameter model input output distribution p y x For parametric model predictions formed p y x D p y x D p y Y x X p y Y x X p x X Non Parametric Prediction y1 yN y x1 xN x y1 yN y x1 xN x b Figure A parametric model predic tion assuming d data b The form model integrating parameters Our non parametric model structure Under assumption given data d obtain p y x D p y x p n p yn xn p y x p D p D p n p yn xn After integrating parameters joint data distribution given p y Y x X p y x p n p yn xn general factorise individual datapoint terms fig The idea non parametric approach specify form dependencies p y Y x X reference explicit parametric model One route non parametric model start parametric model integrate parameters In order tractable use simple linear parameter predictor Gaussian parameter prior For regression leads closed form expressions classification case require numerical approximation From Bayesian linear models Gaussian processes To develop GP briefly revisit Bayesian linear parameter model section For parameters w basis functions x output given assuming zero output noise y wii x If stack y1 yN vector y write predictors y w x1 xN T design matrix Assuming Gaussian weight prior p w N w w joint output p y x w y w p w Gaussian distributed mean y w p w covariance yyT wwT p w T w T w w T DRAFT April Non Parametric Prediction From w absorbed Cholesky decomposition In words loss generality assume w I Hence integrating weights Bayesian linear regression model induces Gaussian distribution set outputs y p y x N y K covariance matrix K depends training inputs K n n x n T xn n n N Since matrix K formed scalar product vectors construction positive semidefinite section After integrating weights thing model directly depends covariance matrix K In Gaussian process directly specify joint output covariance K function inputs specifying linear model parameters w Specifically need define n n element covariance matrix inputs xn xn This achieved covariance function k xn xn K n n k x n xn The matrix K formed covariance function k called Gram matrix The required form function k xn xn special applied create elements matrix K produce positive definite matrix We discuss create covariance functions section One explicit straightforward construction form covariance function scalar product basis vector xn xn For finite dimensional known finite dimensional Gaussian Process Given covariance function find corresponding basis vector representation GP relate parametric Bayesian LPM However commonly covariance functions basis functions corresponds infinite dimensional vectors It cases advantages GP framework particularly evident able compute efficiently corresponding infinite dimensional parametric model A prior functions The nature machine learning applications knowledge true underlying mechanism data generation process limited Instead relies generic smoothness assump tions example wish inputs x x close corresponding outputs y y similar Many generic techniques machine learning viewed different char acterisations smoothness An advantage GP framework respect mathematical smoothness properties functions understood giving confidence procedure For given covariance matrix K equation specifies distribution functions1 following sense specify set input points x x1 xN N N covariance matrix K Then draw vector y y1 yN Gaussian defined equation We plot sampled function finite set points xn yn n N What kind function GP correspond In fig 2a sample functions drawn Squared Exponential covariance function defined points uniformly spaced Each sampled function looks reasonably smooth Conversely Ornstein Uhlenbeck covariance function sampled functions fig 2c look locally rough These smoothness properties related form covariance function discussed section Consider scalar inputs xi xj corresponding sampled outputs yi yj For covariance func tion large k xi xj expect yi yj similar highly correlated Conversely covariance function low k xi xj expect yi yj effectively independent In general expect correlation yi yj decrease apart xi xj 1The term function potentially confusing explicit functional form input output mapping For finite set inputs x1 xN values function given outputs points y1 yN 2For periodic functions high correlation inputs far apart DRAFT April Gaussian Process Prediction The zero mean assumption implies draw large number functions mean functions given point x tends zero Similarly points x x compute sample covariance corresponding y y sampled functions tend covariance function value k x x The zero mean assumption easily relaxed defining mean function m x p y x N y m K In practical situations typically deals detrended data mean trends removed For reason development GPs machine learning literature zero mean case Gaussian Process Prediction For dataset D x y novel input x zero mean GP makes Gaussian model joint outputs y1 yN y given joint inputs x1 xN x For convenience write p y y x x N y y 0N K 0N N dimensional zero vector The covariance matrix K block matrix elements K Kx x Kx x Kx x Kx x Kx x covariance matrix training inputs x x1 xN Kx x n n k x n xn n n N The N vector Kx x elements Kx x n k x n x n N Kx x transpose vector The scalar covariance given Kx x k x x The predictive distribution p y x x y obtained Gaussian conditioning result giving Gaussian distribution p y x D N y Kx xK x xy Kx x Kx xK 1x xKx x GP regression exact method issues local minima Furthermore GPs attractive automatically model uncertainty predictions However computational complexity making prediction O N3 requirement performing matrix inversion solving corresponding linear system Gaussian elimination This prohibitively expensive large datasets large body research efficient approximations exists A discussion techniques scope book reader referred Regression noisy training outputs To prevent overfitting noisy data useful assume training output yn result clean Gaussian process fn corrupted independent additive Gaussian noise yn fn n n N n DRAFT April Gaussian Process Prediction In case interest predict clean signal f novel input x Then distribution p y f x x zero mean Gaussian block covariance matrix Kx x 2I Kx x Kx x Kx x Kx x replaced Kx x 2I forming prediction equation This follows yn fn n assumed independence noise clean signal fm n independence noise components m n m n ymyn fm m fn n fmfn k xm xn fm n fm n fn m fn m m n 2m n Using m obtain form equation Example Training data dimensional input x dimensional output y plotted fig 2b d mean regression function fit based different covariance functions Note smoothness prior translates smoothness prediction The smoothness function space prior consequence choice covariance function Naively partially understand behaviour covariance function origin section An intuitive way think GP regression sample infinite number functions prior filtered fit data likelihood term leaving posterior distribution functions See demoGPreg m The marginal likelihood hyperparameter learning For set N dimensional training inputs represented N dimensional vector y covariance matrix K defined inputs x x1 xN log marginal likelihood log p y x yTK 1y log det 2K One learn free hyper parameters covariance function maximising marginal likelihood For example squared exponential covariance function parameters v0 k x x v0 exp x x The parameter equation specifies appropriate length scale inputs v0 variance function The dependence marginal likelihood parameters typically complex closed form expression maximum likelihood optimum exists case resorts numerical optimisation techniques conjugate gradients Vector inputs For regression vector inputs scalar outputs need define covariance function vectors k x x Using multiplicative property covariance functions definition simple way define k x x k xi x DRAFT April Covariance Functions b c d Figure The input space split evenly points x1 x1000 Three samples GP prior Squared Exponential SE covariance function The covariance matrix K defined SE kernel samples drawn ing mvrandn zeros K b Prediction based training points Plotted posterior predicted function based SE covariance The central line mean prediction standard errors bars The log marginal likelihood c Three samples Ornstein Uhlenbeck GP prior d Posterior prediction OU covariance The log marginal likelihood meaning SE covariance heavily supported data rougher OU covariance For example squared exponential covariance function gives k x x e x x correlated forms possible exercise We generalise param eters k x x v0 exp D l l xl x l xl lth component x v0 D parameters The l equation allow different length scale input dimension learned numerically maximising marginal likelihood For irrelevant inputs corresponding l small model ignore lth input dimension Covariance Functions Covariance functions k x x special define elements positive definite matrix These functions referred kernels particulary machine learning literature DRAFT April Covariance Functions Definition Covariance function Given collection points x1 xM covariance function k xi xj defines elements M M matrix C j k x xj C positive semidefinite Making new covariance functions old The following rules exercise generate new covariance functions existing covariance functions k1 k2 Definition Sum k x x k1 x x k2 x x Definition Product k x x k1 x x k2 x x Definition Product Spaces For z x y k z z k1 x x k2 y y k z z k1 x x k2 y y Definition Vertical Rescaling k x x x k1 x x x function x Definition Warping Embedding k x x k1 u x u x mapping x u x mapping u x arbitrary dimension A small collection covariance functions commonly machine learning given We refer reader popular covariance functions Stationary covariance functions Definition Stationary Kernel A kernel k x x stationary kernel depends separation x x That k x x k x x DRAFT April Covariance Functions For stationary covariance function write k d d x x This means functions drawn GP average functions depend distance inputs absolute position input In words functions average translation invariant For isotropic covariance functions covariance defined function distance d Such covariance functions construction rotationally invariant Definition Squared Exponential k d exp d The Squared Exponential common covariance functions There ways covariance function An elementary technique consider exp xn xn T xn xn exp xn exp xn exp xn T xn The factors right form kernel form xn xn In final term k1 x n xn xn T xn linear kernel Taking exponential writing power series expansion expo nential exp k1 x n xn ki1 x n xn expressed series integer powers k1 positive coefficients By product sum rules kernel We use fact equation product kernels kernel Definition Exponential k d exp d When squared exponential covariance function When Ornstein Uhlenbeck covariance function Definition Mate rn k d d K d K modified Bessel function Definition Rational Quadratic k d d Definition Periodic For dimensional x x stationary isotropic covariance function obtained mapping x dimensional vector u x cos x sin x SE covariance exp u x u x k x x exp sin2 x x See fig 4a DRAFT April Analysis Covariance Functions b Figure Plots Gamma Exponential covariance e x versus x The case corresponds SE covariance function The drop covariance rapid function separation x small suggesting functions corresponding smaller locally rough possess relatively higher long range correlation b As zoomed origin For SE case derivative covariance function zero OU covariance order contribution drop covariance suggesting locally OU sampled functions rougher SE functions Non stationary covariance functions Definition Linear k x x xTx Definition Neural Network k x x arcsin 2xTx 2xTx 2x Tx The functions defined covariance origin To shift use embedding x x effect bias origin To change scale bias non bias contributions use additional parameters x b x The NN covariance function derived limiting case neural network infinite hidden units making use exact integral results See fig 4b Definition Gibbs k x x ri x ri x r2i x r x exp xi x r2i x r x functions ri x Analysis Covariance Functions Smoothness functions We examine local smoothness translation invariant kernel k x x k x x For dimensional points x x separated small x x covariance outputs y DRAFT April Analysis Covariance Functions b Figure Samples GP prior x points uniformly placed Samples periodic covariance function exp sin2 x x b Samples Neural Network covariance function bias b y Taylor expansion k x x k dk dx x O change covariance local level dominated derivative covariance function For SE covariance k x e x dk dx 2xe x2 zero x This means SE covariance function order change covariance zero higher order terms contribute For Ornstein Uhlenbeck covariance k x e x right derivative origin lim k k lim e result obtained L Ho pital s rule Hence OU covariance function order negative change covariance local level decrease covariance rapid SE covariance fig Since low covariance implies low dependence Gaussian distributions locally functions generated OU process rough smooth SE case A formal treatment stationary case obtained examining eigenvalue frequency plot covariance function spectral density section For rough functions density eigenvalues high frequency components higher smooth functions Mercer kernels Consider function k x x x T x B s s x s x x vector component functions x x B x Then set points x xP construct matrix K elements K ij k x xj B s s x s x j We claim matrix K constructed positive semidefinite valid covariance matrix Recalling matrix positive semidefinite non zero vector z zTKz Using definition DRAFT April Analysis Covariance Functions K zTKz P j ziKijzj B s P zis x s P j s x j zj s B s 2s Hence function form equation covariance function We generalise Mercer kernel complex functions x k x x x T x represents complex conjugate Then matrix K formed inputs xi P positive semidefinite real vector z zTKz B s P zis x s P j s x j zj s B s s use general result complex variable xx x A generalisation write k x x f s x s x s ds real f s scalar complex functions x s Then replacing summations integration assuming interchange sum components z integral s obtain zTKz f s P zi x s s P j xj s zj s ds f s s ds Fourier analysis stationary kernels For function g x Fourier transform g s use inverse Fourier transform write g x g s e ixsds For stationary kernel k x Fourier transform k s write k x x k s e x x sds k s e ixseix sds form equation Fourier transform k s identified f s x s e isx Hence provided Fourier transform k s positive translation invariant kernel k x x covariance function Bochner s Theorem asserts converse translation invariant covariance function Fourier representation Application squared exponential kernel For translation invariant squared exponential kernel k x e x2 Fourier transform k s e x2 isxdx e s2 e x 2dx 2e s2 Hence Fourier transform SE kernel Gaussian Since positive SE kernel covariance function DRAFT April Gaussian Processes Classification c1 cN c y1 yN y x1 xN x Figure GP classification The GP induces Gaussian distribution latent activations y1 y N y given observed values c1 cN The classification new input x given correlation induced training points latent activation y Gaussian Processes Classification Adapting GP framework classification requires replacing Gaussian regression term p y x corresponding classification term p c x discrete label c To use GP define latent continuous space y mapped class probability p c x p c y x p y x dy p c y p y x dy Given training data inputs X x1 xN corresponding class labels C c1 cN novel input x p c x C X p c y p y X C dy p y X C p y C X p y Y C X x dY p C Y p y Y X x dY N n p cn yn class mapping p y1 yN y x1 xN x Gaussian Process dy1 dyN The graphical structure joint class latent y distribution depicted fig The posterior marginal p y X C marginal Gaussian Process multiplied set non Gaussian maps latent activations class probabilities We reformulate prediction problem conveniently follows p y Y x X C p y Y C x X p y Y x X p Y C X p Y C X N n p cn yn p y1 yN x1 xN In equation term p y Y x X contain class label information simply conditional Gaussian The advantage description form approximation p Y C X reuse approximation prediction different x needing rerun approximation Binary classification For binary class case use convention c We need specify p c y real valued activation y A convenient choice logistic transfer function3 x e x 3We refer sigmoid function More strictly sigmoid function refers s shaped function Greek s DRAFT April Gaussian Processes Classification Then p c y 2c y valid distribution x x ensuring sum class states A difficulty non linear class mapping term makes computation posterior distribution equation difficult integrals y1 yN carried analytically There ap proximate techniques apply case including variational methods analogous described section Below describe straightforward Laplace method leaving sophisticated methods reading Laplace s approximation In Laplace method section approximate non Gaussian distribution Gaussian4 q Y C X p Y C X q Y C X From equation approximate predictions formed joint Gaussian p y Y x X C p y Y x X q Y C X We marginalise Gaussian find Gaussian distribution y use form prediction p c y For compactness define class label vector outputs c c1 cN T y y1 yN T notationally drop present conditioning inputs x Also convenience define y1 yN T Finding mode The Laplace approximation section corresponds second order expansion mode distribution Our task find maximum p y c p y c exp y y cTy N n log eyn yTK 1x xy log det Kx x N log The maximum needs found numerically convenient use Newton method ynew y Differentiating equation respect y obtain gradient Hessian c K 1x xy K 1x x D noise matrix given D diag N N 4Some authors use term Laplace approximation solely approximating integral Here use term refer Gaussian approximation non Gaussian distribution DRAFT April Gaussian Processes Classification Using expressions Newton update gives ynew y K 1x x D c K 1x xy To avoid unnecessary inversions rewrite form ynew Kx x I DKx x Dy c For initial guess y repeatedly apply equation convergence This guaranteed converge Hessian negative definite unique maximum Making predictions Given converged solution y found Gaussian approximation q y X x C N y y K 1x x D We Gaussians p y y q y X x C equation Predictions p y x X C p y x X y q y X x C dy conditioning section p y y x X N y Kx xK x xy Kx x Kx xK 1x xKx x We write linear system y Kx xK x xy N Kx x Kx xK 1x xKx x Using equation equation averag ing y noise obtain y x X C Kx xK 1x xy Kx x c y Similarly variance latent prediction var y x X C Kx xK 1x x K 1x x D K 1x xKx x Kx x Kx xK 1x xKx x Kx x Kx x Kx x D Kx x line obtained Matrix Inversion Lemma definition A The class prediction new input x given p c x X C y N y y var y In order calculate Gaussian integral logistic sigmoid function use approximation sigmoid function based error function erf x section avsigmaGauss m Example An example binary classification given fig dimensional input training data binary class labels plotted class probability predictions range input points In cases covariance function form 2exp xi xj 001ij The square exponential covariance produces smoother class prediction Ornstein Uhlenbeck covariance function See demoGPclass1D m demoGPclass m DRAFT April Gaussian Processes Classification b Figure Gaussian Process classification The x axis inputs class y axis Green points training points class red class The dots predictions p c x points x ranging x axis Square exponential covariance b OU covariance See demoGPclass1D m Marginal likelihood The marginal likelihood given p C X Y p C Y p Y X Under Laplace approximation marginal likelihood approximated p C X y exp y exp y y T A y y A Integrating y gives log p C X log q C X log q C X y log det 2A y log det K 1x x D N log cTy N n log exp y n y TK 1x xy log det I Kx xD y converged iterate equation One simplify conver gence K 1x xy c y Hyperparameter optimisation The approximate marginal likelihood assess hyperparameters kernel A little care required computing derivatives approximate marginal likelihood optimum y depends We use total derivative formula d d log q C X log q C X y log q C X d d y log q C X yTK 1x xy log det I Kx xD DRAFT April Gaussian Processes Classification evaluated standard results derivative matrix determinant inverse Since derivative zero y noting D depends explicitly y y log q C X y log det I Kx xD The implicit derivative obtained fact convergence y Kx x c y d d y I Kx xD Kx x c These results substituted equation find explicit expression derivative See exercise Multiple classes The extension preceding framework multiple classes essentially straightforward achieved softmax function p c m y e ym m e ym automatically enforces constraint m p c m Naively appear C classes cost implementing Laplace approximation multiclass case scales O C3N3 However careful implementation cost O CN3 refer reader details Summary Gaussian Processes powerful regression models mathematically understood The computational complexity prediction cubic number datapoints This prohibitive large datasets approximate implementations required Gaussian Process classification analytically intractable approximations required The posterior log concave simple unimodal approximation schemes provide satisfactory results Many classical models statistics physics related Gaussian Processes For example view linear dynamical systems chapter specially constrained GPs Gaussian Processes heavily developed machine learning community recent years finding efficient approximations regression classification remains active research topic We direct interested reader discussion Code GPreg m Gaussian Process Regression demoGPreg m Demo GP regression covfnGE m Gamma Exponential Covariance function GPclass m Gaussian Process Classification demoGPclass m Demo Gaussian Process Classification DRAFT April Exercises Exercises Exercise The Gram matrix K covariance function k x x positive semidefinite expressible Kij l uilujl suitable uil The Gram matrix sum covariance functions k x x k1 x x k2 x x form K K1 K2 suitable Gram matrices K1 K2 Show k x x covariance function Consider element wise Hadamard product positive semidefinite matrices That K ij K ijK ij Using K1ij l uilujl K ij m vimvjm K positive semidefinite product covariance functions k x x k1 x x k2 x x covariance function Exercise Show sample covariance matrix elements Sij N n x n x n j N x ix j x N n x n N positive semidefinite Exercise Show k x x exp sin x x covariance function Exercise Consider function f xi xj exp xi xj dimensional inputs xi Show f xi xj exp x2i exp xixj exp x2j By Taylor expanding central term exp xi xj kernel find explicit repre sentation kernel f xi xj scalar product infinite dimensional vectors Exercise Show covariance function k1 x x k x x f k1 x x covariance function polynomial f x positive coefficients Show exp k1 x x tan k1 x x covariance functions Exercise For covariance function k1 x x f x x T x x k2 x x f x x T A x x valid covariance function positive definite symmetric matrix A DRAFT April Exercises Exercise Show derivative Laplace approximation marginal likelihood equation given d d log q C X yTK 1x xK x xK x xy trace L 1K x xD MiiD ii L 1K x x c K x x Kx x L I Kx xD M L 1Kx x D diag y1 D11 y2 D22 Hint You need use general derivative results x A A x A A x log det A trace A x A Exercise String kernel Let x x strings characters s x number times substring s appears string x Then k x x s wss x s x string kernel covariance function provided weight substring ws positive Given collection strings politics collection sport explain form GP classifier string kernel Explain weights ws adjusted improve fit classifier data explicit formula derivative respect ws log marginal likelihood Laplace approximation Exercise Vector regression Consider predicting vector output y given training data X Y xn yn n n To GP predictor p y x X Y need Gaussian model p y1 yN y x1 xn x A GP requires specification covariance c ymi y n j xn xm components outputs different input vectors Show dimension independence assumption c ymi y n j xn xm ci ymi yni xn xm j ci y m y n xn xm covariance function ith dimension separate GP predictors constructed independently output dimension Exercise Consider Markov update linear dynamical system section xt Axt t t A given matrix t zero mean Gaussian noise covariance tj t 2i jt t Also p x1 N x1 Show x1 xt Gaussian distributed Show covariance matrix x1 xt elements xt x T t At At T min t t At At T explain linear dynamical system constrained Gaussian Process Consider yt Bxt t t zero mean Gaussian noise covariance t j t 2i jt t The vectors uncor related vectors Show sequence vectors y1 yt Gaussian Process suitably defined covariance function DRAFT April CHAPTER Mixture Models Mixture models assume data essentially clustered component mixture repre senting cluster In chapter view mixture models viewpoint learning missing data discuss classical algorithms EM training Gaussian mixture models We discuss powerful models allow possibility object member cluster These models applications areas document modelling Density Estimation Using Mixtures A mixture model set component models combined produce richer model p v H h p v h p h The variable v visible observable discrete variable h dom h H indexes component model p v h weight p h The variable v discrete continuous Mixture models natural application clustering data h indexes cluster This interpretation gained considering generate sample datapoint v model equation First sample cluster h p h draw visible state v p v h For set d data v1 vN mixture model form fig p v1 vN h1 hn N n p vn hn p hn observation likelihood given p v1 vN N n hn p vn hn p hn Finding likely assignment datapoints clusters achieved inference argmax h1 hN p h1 hN v1 vN thanks factorised form distribution equivalent computing arg maxhn p h n vn datapoint Expectation Maximisation Mixture Models hn vn h v h N Figure A mixture model graphical representation DAG single hidden node h indexes mixture component given setting h generate observation v p v h Independence N observations means model replicated plate The parameters assumed common datapoints In applications location clusters priori unknown parameters model need learned locate clusters Explicitly writing dependence parameters model single datapoint v corresponding cluster index h p v h p v h v h p h h The optimal parameters v h h mixture model commonly set maximum likelihood opt argmax p v1 vN argmax n p vn Numerically achieved optimisation procedure gradient based approaches Al ternatively treating component indices latent variables apply EM algorithm described following section classical models produces simple update formulae Example The data fig naturally clusters modelled mixture dimensional Gaussians Gaussian describing clusters Here clear visual interpretation meaning cluster mixture model placing datapoints cluster likely generated model component A priori don t know location clusters need find parameters Gaussian mean covariance cluster mh Ch h This achieved maximum likelihood Expectation Maximisation Mixture Models Our task find parameters maximise likelihood observations v1 vN p v1 vN N n h p vn h p h By treating index h missing variable mixture models trained EM algorithm section There sets parameters v h component model p v h v h h mixture weights p h h According general approach d data section M step Figure Two dimensional data displays clusters In case Gaussian mixture model 2N x m1 C1 2N x m2 C2 fit data suitable means m1 m2 covariances C1 C2 To human eye identifying clusters easy task interested automatic methods cluster potentially high dimensional data cluster solutions far obvious DRAFT April Expectation Maximisation Mixture Models need consider energy term E N n log p vn h pold h vn N n log p vn h v h pold h vn N n log p h h pold h vn maximise respect parameters v h h h H The E step results update pnew h vn p vn h oldv h p h old h For initial parameters updates E M steps convergence This general approach training mixture models Below flesh updates work specific models Unconstrained discrete tables Here consider training simple belief network p v h v h p h h dom v V dom h H tables unconstrained This special case general framework discussed section instructive EM algorithm derived specific case M step p h If constraint placed p h h write parameters simply p h understanding p h h p h Isolating dependence equation p h obtain N n log p h pold h vn h log p h N n pold h vn We wish maximise equation respect p h constraint hp h There different ways perform constrained optimisation One approach use Lagrange multipliers exercise Another arguably elegant approach use techniques described section based similarity Kullback Leibler divergence First define distribution p h N n p old h vn h N n p old h vn N N n pold h vn Then maximising equation equivalent maximising log p h p h log p h p h log p h p h KL p p log p h p h equation related log p h p h constant factor N By subtracting independent term log p h p h equation obtain negative Kullback Leibler divergence KL p p This means optimal p h distribution minimises Kullback Leibler divergence Optimally p h p h M step given pnew h N N n pold h vn DRAFT April Expectation Maximisation Mixture Models hn vni h vi h n N D Figure Mixture product Bernoulli distributions In Bayesian Treatment parameter prior In text simply set parameters maximum likelihood M step p v h The dependence equation p v h N n log p vn h v h pold h vn v N n H h I vn v pold h vn log p v h If distributions p v h v h constrained apply similar Kullback Leibler method section p h As alternative approach describe Lagrange method works case We need ensure p v h distribution mixture states h H This achieved set Lagrange multipliers giving Lagrangian L V v N n H h I vn v pold h vn log p v h H h h V v p v h Differentiating respect p v h j equating zero L p v h j N n I vn pold h j vn p v h j h j Solving p v h j h j N n I vn pold h j vn Using normalisation requirement v p v h j shows h j numerator equation summed v Hence M step update given pnew v h j N n I v n pold h j vn V N n I vn pold h j vn E step According general EM procedure section optimally set pnew h vn pold h vn pnew h vn p old vn h pold h h p old vn h pold h Equations repeated convergence guarantee likelihood equation decrease The initialisation tables mixture probabilities severely affect quality solution found likelihood local optima If random initialisations recommended record converged value likelihood parameters higher likelihood The solution highest likelihood preferred DRAFT April Expectation Maximisation Mixture Models Mixture product Bernoulli distributions As example mixture model practical clustering example describe simple mixture model cluster binary vectors v v1 vD T vi The mixture Bernoulli products1 model given p v H h p h D p vi h term p vi h Bernoulli distribution The model depicted fig parameters p h p vi h dom h H One way understand model imagine drawing samples In case datapoint n draw cluster index h H p h Then D draw state vi p vi h EM training To train model maximum likelihood convenient use EM algorithm usual derived writing energy n log p vn h pold h vn n log p vni h pold h vn n log p h pold h vn performing maximisation table entries From general results section know M step equivalent maximum likelihood variables observed replacing unobserved variables h conditional distribution p h v Using write immediately M step pnew vi h j n I v n p old h j vn n I v n p old h j vn n I v n p old h j vn pnew h j n p old h j vn h n p old h vn E step pnew h j vn pold h j D pold vni h j Equations iterated convergence If attribute missing datapoint n needs sum states corresponding vni The effect performing summation model simply remove corresponding factor p vni h algorithm exercise Initialisation The EM algorithm sensitive initial conditions Consider following initialisation p vi h j p h set arbitrarily This means iteration pold h j vn p h j The subsequent M step updates pnew h pold h pnew vi h j pnew vi h j j j This means parameters p v h immediately independent h model numerically trapped symmetric solution It makes sense initialise parameters non symmetric fashion The model readily extendable output class left exercise interested reader 1This similar Naive Bayes classifier class labels hidden DRAFT April Expectation Maximisation Mixture Models Figure Top Data questionnaire responses people asked questions yes white black answers Gray denotes absence response missing data This training data generated component product Bernouilli model Missing data simulated randomly removing values dataset Middle correct h values sampled data A denotes hn denotes hn Bottom The estimated p hni vn values Example Questionnaire A company sends questionnaire containing set D yes questions set customers The binary responses customer stored vector v v1 vD T In total N customers send questionnaires v1 vN company wishes perform analysis find kinds customers The company assumes H essential types customer profile responses defined customer type Data questionnaire containing questions respondents presented fig The data large number missing values We assume H kinds respondents attempt assign respondent clusters Running EM algorithm data random initial values tables produces results fig Based assigning datapoint vn cluster maximal posterior probability hn arg maxh p h vn given trained model p v h p h model assigns data correct cluster known simulated case See MIXprodBern m Example Handwritten digits We collection handwritten digits wish cluster groups fig Each digit dimensional binary vector Using mixture Bernoulli products trained iterations EM random initialisation clusters presented fig As method captures natural clusters data example kinds slightly slanted kinds etc b Figure EM learning mixture Bernoulli products True p h left learned p h right h b True p v h left learned p v h right v Each column pair corre sponds p vi h right column p vi h left column The learned probabilities reasonably close true values DRAFT April The Gaussian Mixture Model Figure Top selection handwritten digits training set Bottom trained cluster outputs p vi h h mixtures See demoMixBernoulliDigits m The Gaussian Mixture Model We turn attention modelling continuous vector observations x x plays role visible variable v previously Gaussians particularly convenient continuous mixture components constitute bumps probability mass aiding intuitive interpretation model As reminder D dimensional Gaussian distribution continuous variable x p x m S det 2S exp x m T S x m m mean S covariance matrix A mixture Gaussians p x H p x mi Si p p mixture weight component For set data X x1 xN usual d assumption log likelihood log p X N n log H p det 2Si exp xn mi T S 1i x n mi parameters mi Si p H The optimal parameters set maxi mum likelihood bearing mind constraint Si symmetric positive definite matrices addition p p Gradient based optimisation approaches feasible parame terisation Si e g Cholesky decomposition p e g softmax enforce constraints An alternative EM approach case particularly convenient automatically provides parameter updates ensure constraints EM algorithm From general approach section M step need consider energy Using fact component index plays role latent variable energy given N n log p xn pold xn N n log p xn p pold xn Plugging definition Gaussian components N n H pold xn xn mi T S 1i x n mi log det 2Si log p The M step requires maximisation respect mi Si p DRAFT April The Gaussian Mixture Model M step optimal mi Maximising equation respect mi equivalent minimising N n H pold xn xn mi T S 1i x n mi Differentiating respect mi equating zero N n pold xn S 1i x n mi Hence optimally mi N n p old xn xn N n p old xn By defining membership distribution pold n p old xn N n p old xn quantifies membership datapoints cluster write equation compactly update mnewi N n pold n xn Intuitively updates mean cluster average datapoints weighted mem bership cluster M step optimal Si Optimising equation respect Si equivalent minimising N n ni TS 1i n log det S 1i pold xn ni xn mi To aid matrix calculus isolate dependency Si trace S 1i N n pold xn ni ni T log det S 1i N n pold xn Differentiating respect S 1i equating zero obtain N n pold xn ni ni T Si N n pold xn Using membership distribution pold n resulting update given Snewi N n pold n xn mi xn mi T As mean essentially softly filters datapoints belong cluster takes covariance This update ensures Si symmetric positive semidefinite A special case constrain covariances Si diagonal update exercise Si N n pold n diag xn mi xn mi T DRAFT April The Gaussian Mixture Model Algorithm EM training GMM Initialise centres mi covariances Si weights p p H Likelihood converged termination criterion reached n N H p xn p exp xn mi T S 1i xn mi det Si responsibility end Normalise p xn p xn p n p xn membership end Normalise p n n p n H mi N n p n xn M step means Si N n p n xn mi xn mi T M step covariances p N N n p xn M step weights end L N n log H p det 2Si exp xn mi T S 1i xn mi Log likelihood end diag M means forming new matrix matrix M zero entries diagonal entries M A extreme case isotropic Gaussians Si I The reader optimal update 2i case given taking average diagonal entries diagonally constrained covariance update 2i D N n pold n xn mi M step optimal mixture coefficients If constraint placed weights update follows general formula given equation pnew N N n pold xn E step From general theory section E step given p xn p xn p Explicitly given responsibility p xn p exp xn mi T S 1i xn mi det Si p exp xn mi T S 1i xn mi det Si The equations iterated sequence convergence Note means new means update covariance algorithm The performance EM Gaussian mixtures strongly dependent initialisation discuss In addition constraints covariance matrix required order find sensible solutions DRAFT April The Gaussian Mixture Model iteration b iterations c iterations d iterations Figure Training mixture isotropic Gaussians If start large variances Gaussians iteration Gaussians centred close mean data b The Gaussians begin separate c One Gaussians appropriate parts data d The final con verged solution The Gaussians constrained variances greater set See demoGMMem m Practical issues Infinite troubles A difficulty arises maximum likelihood fit Gaussian mixture model Consider placing component p x mi Si mean mi set datapoints mi xn The contribution Gaussian datapoint xn p xn mi Si det 2Si e xn xn TS 1i x n xn det 2Si In limit width covariance goes zero eigenvalues Si tend zero proba bility density infinite This means obtain maximum likelihood solution placing zero width Gaussians selection datapoints resulting infinite likelihood This clearly undesirable arises case maximum likelihood solution constrain param eters sensible way Note related EM algorithm property maximum likelihood method All computational methods aim fit unconstrained mixtures Gaussians maximum likelihood succeed finding reasonable solutions merely getting trapped favourable local maxima A remedy include additional constraint width Gaussians ensuring small One approach monitor eigenvalues covariance matrix update result new eigenvalue smaller desired threshold update rejected In GMMem m use similar approach constrain determinant product eigenvalues covariances greater desired specified minimum value One view formal failure maximum likelihood case Gaussian mixtures result inappropriate prior maximum likelihood equivalent MAP flat prior placed matrix Si This unreasonable matrices required positive definite non vanishing width A Bayesian solution problem possible placing prior covariance matrices The natural prior case Wishart Distribution Gamma distribution case diagonal covariance Initialisation A useful initialisation strategy set covariances diagonal large variances This gives components chance sense data lies An illustration performance algorithm given fig Symmetry breaking If covariances initialised large values EM algorithm appears little progress beginning component jostles try explain data Eventually Gaussian DRAFT April The Gaussian Mixture Model b c Figure A Gaussian mixture model H components There component purple large variance small weight little effect distribution close components appreciable mass As away additional component gains influence b The GMM probability density function c Plotted log scale influence Gaussian far origin clearer component breaks away takes responsibility explaining data vicinity fig The origin initial jostling inherent symmetry solution makes difference likelihood relabel components called The symmetries severely handicap EM fitting large number component models mixture number permutations increases dramatically number components A heuristic begin small number components symmetry breaking problematic Once local broken solution found models included mixture initialised close currently found solutions In way hierarchical scheme envisaged Another popular method initialisation center means found K means algorithm section requires heuristic initialisation Classification Gaussian mixture models We use GMMs class conditional generative model order powerful classifier Consider data drawn classes c We fit GMM p x c X1 data X1 class GMM p x c X2 data X2 class This gives rise class conditional GMMs p x c Xc H p c N x mci Sci For novel point x posterior class probability p c x X p x c Xc p c p c prior class probability The maximum likelihood setting p c proportional number training points class c Overconfident classification Consider testpoint x long way training data classes For point proba bility class models generated data low Nevertheless probability exponentially higher Gaussians drop exponentially quickly different rates meaning posterior probability confidently close class component closest x This unfortunate property end confidently predicting class novel data similar ve seen We prefer opposite effect novel data far training data classification confidence drops classes equally likely A remedy situation include additional component Gaussian mixture class broad We collect input data classes dataset X let m mean data S covariance Then model class c data include additional DRAFT April The Gaussian Mixture Model b Figure Class conditional GMM train ing classification Data different classes We fit GMM components data class The magenta diamond test point far training data wish classify b Upper subpanel class probabil ities p c n training points 41st point test point This shows test point confidently placed class The lower subpanel class probabilities including ad ditional large variance Gaussian term The class label test point maxi mally uncertain See demoGMMclass m Gaussian dropping notational dependency X p x c H p ciN x mci Sci p cH 1N x m S p ci pci H H small positive value inflates covariance demoGMMclass m The effect additional component training likelihood negligible small weight large variance compared components fig However away region H components appreciable mass additional component gains influence higher variance If include additional component GMM class c influence additional component class dominating far influence components For point far training data likelihood roughly equal class region additional broad component dominates class equal measure The posterior distribution tend prior class probability p c mitigating deleterious effect single GMM dominating testpoint far training data Example The data fig 9a cluster structure class Based fitting GMM classes test point diamond far training data confidently classified belonging class This undesired effect prefer points far training data classified certainty By including additional large variance Gaussian component class little effect class probabilities training data desired effect making class probability test point maximally uncertain fig 9b The Parzen estimator The Parzen density estimator formed placing bump mass x xn datapoint p x N N n x xn A popular choice D dimensional x x xn N x xn 2ID DRAFT April The Gaussian Mixture Model Algorithm K means Initialise centres mi K converged For centre find xn nearest Euclidean sense centre Call set points Ni Let Ni number datapoints set Ni Update means mnewi Ni n Ni xn end giving mixture Gaussians p x N N n D exp x xn There training required Parzen estimator positions N datapoints need storing Whilst Parzen technique reasonable cheap way form density estimator enable form simpler description data In particular perform clustering lower number clusters assumed underly data generating process This contrast GMMs trained maximum likelihood fixed number H N components K Means Consider mixture K isotropic Gaussians covariance constrained equal 2I mixture weights pi pi p x K piN x mi 2I Whilst EM algorithm breaks Gaussian component allowed set mi equal datapoint constraining components variance algorithm defined limit The reader exercise case membership distribution equation deterministic p n mi closest x n In limit EM update mean mi given taking average points closest mi This limiting constrained GMM reduces called K means algorithm algorithm Despite simplicity K means algorithm converges quickly gives reasonable clustering pro vided centres initialised sensibly See fig K means simple form data compression Rather sending datapoint xn sends instead index centre associated This called vector quantisation form lossy compression To improve quality information transmitted approximation difference xn corresponding closest mean m improve reconstruction compressed datapoint Bayesian mixture models Bayesian extensions include placing priors parameters model mixture component mixture weights In cases rise intractable integral marginal likelihood Methods approximate integral include sampling techniques See approximate variational treatment focussed Bayesian Gaussian mixture models DRAFT April Mixture Experts b Figure datapoints clustered K means com ponents The means given red crosses b Evolution mean square distance nearest centre iterations algo rithm The means initialised close overall mean data See demoKmeans m Semi supervised learning In cases know mixture component certain datapoints belong For example given collection images wish cluster cluster labels subset images Given information want fit mixture model specified number components H parameters We write vm h m m M M known datapoints corresponding components vn hn n N remaining datapoints components hn unknown We aim maximise likelihood p v1 M v N h1 M m p vm hm n hn p vn hn p hn If lump datapoints essentially equivalent standard unsupervised case expect h fixed known states The effect EM algorithm terms p h vm labelled datapoints delta functions p h vm h hm known state resulting minor modification standard algorithm exercise Mixture Experts The mixture experts model extension input dependent mixture weights For output y discrete class continuous regression variable input x general form fig p y x W U H h p y x wh p h x U Here h indexes mixture component Each expert h parameters wh W w1 wH corresponding gating parameters uh U u1 uH Unlike standard mixture model compo nent distribution p h x U dependent input x This called gating distribution conventionally taken softmax form p h x U e uT h x h e uT h x The idea set H predictive models experts p y x wh different parameter wh h H How suitable model h predicting output input x determined alignment input x weight vector uh In way input x softly assigned appropriate experts Learning parameters Maximum likelihood training achieved form EM We derive EM algorithm mixture experts model merely pointing direction derivation continue For single datapoint x EM energy term log p y x wh p h x U p h x Wold Uold DRAFT April Indicator Models xn hny n W U n N Figure Mixture experts model The prediction output yn real continuous given input xn averaged individual experts p yn xn whn The expert hn selected gating mechanism probability p hn xn U experts responsible predicting output xn input space The parameters W U learned maximum likelihood marginalising hidden expert indices h1 hN For regression simple choice p y x wh N y xTwh binary classification p y x wh xTwh In cases computing derivatives energy respect parameters W straightforward EM algorithm readily available An alternative EM compute gradient likelihood directly standard approach discussed section A Bayesian treatment consider p y W U h x p y x wh p h x u p W p U conventional assume p W h p wh p U h p uh The integrals required calculate marginal likelihood generally intractable approximations required See variational treatment regression variational treatment classification An extension Bayesian model selection number experts estimated considered Indicator Models Indicator models generalise previous mixture models allowing general prior distribution cluster assignments For consistency literature use indicator z opposed hidden variable h play role A clustering model parameters component models joint indicator prior p z1 N takes form fig 12a p v1 N z1 N p v1 N z1 N p z1 N Since zn indicate cluster membership p v1 N z1 N p z1 N N n p vn zn Below discuss role different indicator priors p z1 N clustering Joint indicator approach factorised prior Assuming prior independence indicators p z1 N N n p zn zn K DRAFT April Indicator Models z1 v1 zN vN z1 v1 zN vN b zn vn N c Figure A generic mixture model data v1 N Each zn indicates cluster datapoint set parameters zn k selects parameter k datapoint vn b For potentially large number clusters way control complexity constrain joint indicator distribution c Plate notation b obtain equation p v1 N z1 N N n p vn zn p zn N n zn p vn zn p zn recovers standard mixture model equation As discuss sophisticated joint indicator priors explicitly control complexity indicator assignments open path essentially infinite dimensional models Polya prior For large number available clusters mixture components K factorised joint indicator distribution potentially lead overfitting resulting little meaningful clustering One way control effective number components parameter regulates complexity fig 12b c p z1 N n p zn p p z categorical distribution p zn k k This means example k low unlikely select cluster k datapoint n A convenient choice p Dirichlet distribution conjugate categorical distribution p Dirichlet K k K k The integral equation performed analytically Polya distribution p z1 N N K k Nk K K Nk n I zn k The number unique clusters given U k I Nk The distribution likely cluster numbers controlled parameter The scaling K equation ensures sensible limit K fig limit models known Dirichlet process mixture models This approach means need explicitly constrain number possible components K number active components U remains limited large K Clustering achieved considering argmax z1 N p z1 N v1 N In practice common consider argmax zn p zn v1 N DRAFT April Mixed Membership Models b c Figure The distribution number unique clusters U indicators sampled Polya distribution equation N datapoints K b K c K Even number available clusters K larger number datapoints number clusters U remains constrained See demoPolya m Unfortunately posterior inference p zn v1 N class models formally computationally tractable approximate inference techniques required A detailed discussion techniques scope book refer reader deterministic variational approach discussion sampling approaches Mixed Membership Models Unlike standard mixture models object assumed generated single cluster mixed membership models object member group Latent Dirichlet Allocation discussed example mixed membership model number models developed recent years Latent Dirichlet allocation Latent Dirichlet Allocation considers datapoint belong single cluster A typical application identify topic clusters collection documents A single document contains sequence words example v cat sat mat If word available dictionary D words assigned unique state dog tree cat represent nth document vector word indices vn vn1 v n Wn vni D Wn number words n th document The number words Wn document vary overall dictionary came fixed The aim find common topics documents assuming document potentially contain topic It useful think underlying generative model words including latent topics later integrate For document n distribution topics n K k n k gives latent description document terms topic membership For example document n discusses issues related wildlife conservation topic distribution high mass latent animals environment topics Note topics latent animal given post hoc based kinds words latent topic generate k As section control complexity use Dirichlet prior limit number topics active particular document p n Dirichlet n vector length number topics DRAFT April Mixed Membership Models n znw vnw Wn N Figure Latent Dirichlet Allocation For document n sample distribution topics n Then word position w Wn document sample topic znw topic distribution Given topic sample word word distribution topic The parameters model word distributions topic parameters topic distribution We sample probability distribution histogram n represents topics likely occur document Then word position document sample topic subsequently word distribution words topic For document n wth word position document vnw use znw K indicate K possible topics word belongs For topic k categorical distribution words D dictionary p vnw znw k k For example animal topic high probability emit animal like words etc A generative model sampling document vn Wn word positions given fig Choose n Dirichlet n For word position vnw w Wn Choose topic znw p znw n b Choose word vnw p vnw znw Training LDA model corresponds learning parameters relates number topics describes distribution words topic Unfortunately finding requisite marginals learning posterior formally computationally intractable Efficient approximate ference class models topic research interest variational sampling approaches recently developed There close similarities LDA PLSA section describe document terms distribution latent topics LDA probabilistic model issues setting hyperparameters addressed maximum likelihood PLSA hand essentially matrix decomposition technique PCA Issues hyperparameters setting PLSA addressed validation data Whilst PLSA description training data LDA generative data model principle synthesise new documents Example An illustration use LDA given fig The documents taken TREC Associated Press corpus containing newswire articles unique terms After removing standard list stop words frequent words etc dominate statistics EM algorithm variational approximate inference find Dirichlet conditional categorical parameters topic LDA model The words resulting categorical distributions k illustrated fig 15a These distributions capture underlying topics corpus An example document corpus presented words coloured probable latent topic correspond DRAFT April Mixed Membership Models Arts Budgets Children Education new million children school film tax women students program people schools music budget child education movie billion years teachers play federal families high musical year work public best spending parents teacher actor new says bennett state family manigat york plan welfare namphy opera money men state theater programs percent president actress government care elementary love congress life haiti The William Randolph Hearst Foundation million Lin coln Center Metropolitan Opera Co New York Philharmonic Juilliard School Our board felt real opportunity mark future performing arts grants act bit important traditional areas support health medical research education social services Hearst Foundation President Randolph A Hearst said Monday announcing grants Lincoln Centers share new building house young artists provide new public facilities The Metropolitan Opera Co New York Philharmonic receive The Juilliard School music performing arts taught The Hearst Foun dation leading supporter Lincoln Center Consolidated Corporate Fund usual annual donation b Figure A subset latent topics discovered LDA high probability words associated topic Each column represents topic topic arts assigned hand viewing likely words corresponding topic b A document training data words coloured according likely latent topic This demonstrates mixed membership nature model assigning datapoint document case clusters topics Reproduced b Figure The social network set individuals repre sented undirected graph Here individual belongs group b By contrast graph partitioning breaks graph roughly equally sized disjoint partitions node member single partition minimal number edges partitions Graph based representations data Mixed membership models variety contexts distinguished form data available Here focus analysing representation interactions collection objects particular data processed information interest characterised interaction matrix For graph based representations data objects similar neighbours graph representing data objects In field social networks example individual represented node graph link nodes individuals friends Given graph wish identify communities closely linked friends Interpreted social network fig 16a individual member work group poker group These groups individuals disjoint Discovering groupings contrasts graph partitioning node assigned set subgraphs fig 16b typical criterion subgraph roughly size connections subgraphs Another example nodes graph represent products link nodes j indicates customers buy product frequently buy product j The aim decompose graph groups corresponding products commonly co bought customers A growing area application graph based representations bioinformatics nodes represent genes link representing genes similar activity profiles The task identify groups similarly behaving genes Dyadic data Consider kinds objects example films customers Each film indexed f F user u U The interaction user u film f described element matrix Muf representing rating user gives film A dyadic dataset consists matrix aim decompose matrix explain ratings finding types films types user DRAFT April Mixed Membership Models b Figure Graphical representation dyadic data There documents words A link represents particular word document pair occurs dataset b A latent decomposition topics A topic corresponds collection words document collection topics The open nodes indicate latent variables Another example consider collection documents summarised interaction matrix Mwd word w appears document d zero This matrix represented bipartite graph fig 17a The lower nodes represent documents upper nodes words link word occurs document One seeks assignments documents groups latent topics succinctly explain link structure bipartite graph small number latent nodes schematically depicted fig 17b One view form matrix factorisation PLSA section Mwd t UwtV T td t indexes topics feature matrices U V control word topic mapping topic document mapping This differs latent Dirichlet allocation probabilistic interpretation generating topic word conditional chosen topic Here interaction document topic matrix V word topic matrix U non probabilistic In real valued data modelled p M U W V N M UWVT 2I U V assumed binary real valued W topic interaction matrix In viewpoint learning consists inferring U W V given dyadic observation matrix M Assuming factorised priors posterior matrices p U W V M p M U W V p U p W p V A convenient choice Gaussian prior distribution W feature matrices U V sampled Beta Bernoulli priors The resulting posterior distribution formally computationally intractable addressed sampling approximation Monadic data In monadic data type object interaction objects represented square interaction matrix For example matrix elements Aij proteins j bind A depiction interaction matrix given graph edge represents interaction example fig In following section discuss particular mixed membership model highlight potential applications The method based clique decompositions graphs require short digression clique based graph representations Figure The minimal clique cover DRAFT April Mixed Membership Models b Figure Bipartite representations decomposi tions fig Shaded nodes represent observed vari ables open nodes latent variables Incidence matrix representation b Minimal clique decomposition Cliques adjacency matrices monadic binary data A symmetric adjacency matrix elements Aij indicating link nodes j For graph fig adjacency matrix A include self connections diagonal Given A aim find simpler description reveals underlying cluster structure fig Given undirected graph fig incidence matrix Finc alternative description adjacency structure Given V nodes graph construct Finc follows For link j graph form column matrix Finc zero entries th jth row The column ordering arbitrary For example graph fig incidence matrix Finc The incidence matrix property adjacency structure original graph given outer product incidence matrix The diagonal entries contain degree number links node For example gives FincF T inc A H FincF T inc Here H element wise Heaviside step function H M ij Mij A useful viewpoint incidence matrix identifies cliques graph term clique non maximal sense There cliques fig column Finc specifies elements clique Graphically depict incidence decomposition bipartite graph fig 19a open nodes represent cliques The incidence matrix generalised describe larger cliques Consider following matrix decomposition fig outer product F FFT The interpretation F represents decomposition cliques As incidence matrix column represents clique rows containing express elements clique defined column This decomposition represented bipartite graph fig 19b For graph fig Finc F satisfy A H FFT H FincF T inc DRAFT April Mixed Membership Models x Figure The function x e x As increases sigmoid function tends step function One view equation form matrix factorisation binary square symmetric matrix A non square binary matrices For clustering purposes decomposition F preferred incidence decomposition F decomposes graph smaller number larger cliques A formal specification problem finding minimum number maximal fully connected subsets computationally hard problem MIN CLIQUE COVER Definition Clique matrix Given adjacency matrix A ij j V Aii clique matrix F elements Fic V c C A H FFT Diagonal elements FFT ii express number cliques columns node occurs Off diagonal elements FFT ij contain number cliques columns nodes j jointly inhabit Whilst finding clique decomposition F easy use incidence matrix example finding clique decomposition minimal number columns e solving MIN CLIQUE COVER NP Hard In cliques required maximal definition cliques non maximal A generative model adjacency matrices Given adjacency matrix A prior clique matrices F interest posterior p F A p A F p F We concentrate generative term p A F To find connected clusters relax constraint decomposition form perfect cliques original graph view absence links statistical fluctuations away perfect clique Given V C matrix F desire higher overlap rows2 fi fj greater probability link j This achieved example p Aij F fif T j x e x controls steepness function fig The shift equation ensures approximates step function argument integer Under equation fi fj position fif T j p Aij F high Absent links contribute p Aij F p Aij F The parameter controls strictly FFT matches A large little flexibility allowed cliques identified For small subsets cliques small number missing links clustered The setting user problem dependent Assuming element adjacency matrix sampled independently generating process joint probability observing A neglecting diagonal elements p A F j fif T j Aij fif T j Aij 2We use lower indices fi denote th row F DRAFT April Mixed Membership Models b c Figure Adjacency ma trix Political Books black b Clique matrix non zero en tries c Adjacency reconstruction approximate clique matrix cliques fig demoCliqueDecomp m The ultimate quantity interest posterior distribution clique structure equation specify prior p F clique matrices Clique matrix prior p F Since interested clustering ideally want place nodes graph possible cluster This means wish bias contributions adjacency matrix A occur small number columns F To achieve reparameterise F F 1f Cmaxf Cmax c play role indicators f c column c F Cmax assumed maximal number clusters Ideally like find F low number indicators Cmax state To achieve define prior distribution binary hypercube Cmax p c c c To encourage small number cs use Beta prior p suitable parameters ensure This gives rise Beta Bernoulli distribution p p p B N b Cmax N B b B b Beta function N Cmax c c number indicators state To encour age small number components active set b The distribution vertices binary hypercube Cmax bias vertices close ori gin Through equation prior induces prior F The resulting distribution p F A p F p formally intractable addressed variational technique Clique matrices play natural role parameterisation positive definite matrices constraint specified zeros matrix exercise Example Political Books Clustering The data consists books US politics sold online bookseller Amazon The adjacency matrix element Aij fig 21a represents frequent co purchasing books j Valdis Krebs Additionally books labelled liberal neutral conservative according judgement politically astute reader The interest assign books clusters A clusters correspond way ascribed political leanings book Note information minimal known clustering algorithm books co bought matrix A information content title books exploited algorithm With initial Cmax cliques Beta parameters b steepness probable posterior marginal solution contains cliques fig 21b giving perfect reconstruction adjacency A For comparison incidence matrix cliques However clique matrix large provide compact interpretation data clusters books To cluster data aggressively fix Cmax run algorithm This results approximate clique decomposition A H FFT plotted fig 21c The resulting DRAFT April Mixed Membership Models Y e rs f o r R e ve n g e B u sh v s t h e B e ltw y C h rl ie W ils o n s W r L o si n g B L d e n S le e p g W ith t h e D e vi l T h e M n W h o W rn e d A m e ri W h y A m e ri S le p t G h o st W rs A N tio n l P rt y N o M o B u sh C o u n tr y D e lic tio n o f D u ty L e g cy O ff w ith T h e ir H e d s P e rs e cu tio n R u m sf e ld s W r B kd o w n B e tr ya l S h u t U p n d S g M e n t T o B e T h e R ig h t M n T e n M u te s fr o m N o rm l H I shall ry s S ch e m e T h e F n ch B e tr ya l o f A m e ri T le s fr o m t h e L e ft C o st H tin g A m e ri T h e T h ir d T e rr o ri st E n d g m e S p S te rs A ll th e S h h s M e n D n g e ro u s D I am p lo m cy T h e P ri ce o f L o ya lty H o u se o f B u sh H o u se o f S u d T h e D e th o f R ig h t n d W ro n g U se fu l I d io ts T h e O R e I shall y F ct o r L e t F e d o m R g T h o se W h o T sp ss B ia s S la n d e r T h e S va g e N tio n D e liv e r U s fr o m E vi l G iv e M e B k T h e E n e m y W ith T h e R e l A m e ri W h o s L o o ki n g O u t fo r Y o u T h e O ff ic ia l H n d b o o k V st R ig h t W g C o n sp ir cy P o w e r P la ys A rr o g n ce T h e P e rf e ct W ife T h e B u sh e s T h g s W o rt h F ig h tin g F o r S u rp ri se S e cu ri ty th e A m e ri n E xp e ri e n ce A lli e s W h y C o u ra g e M tt e rs H o lly w o o d I n te rr u p te d F ig h tin g B ck W e W I shall P va il T h e F ith o f G e o rg e W B u sh R e o f th e V u lc n s D o w n si ze T h S tu p id W h ite M e n R u sh L I am b u g h I s B ig F t Id io t T h e B e st D e m o cr cy M o n e y C n B u y T h e C u ltu o f F e r A m e ri U n b o u n d T h e C h o ic e T h e G t U n ra ve lin g R o g u e N tio n S o ft P o w e r C o lo ss u s T h e S o rr o w s o f E m p ir e A g st A ll E n e m ie s A m e ri n D yn st y B ig L ie s T h e L ie s o f G e o rg e W B u sh W o rs e T h n W te rg te P la n o f A tt ck B u sh t W r T h e N e w P e rl H rb o r B u sh w o m e n T h e B u b b le o f A m e ri n S u p m cy L iv g H ry T h e P o lit ic s o f T ru th F n tic s n d F o o ls B u sh w h ck e d D rm g I ra q L ie s n d t h e L yi n g L ia rs W h o T e ll T h e m M o ve O n s W ys t o L o ve Y o u r C o u n tr y T h e B u yi n g o f th e P si d e n t P e rf e ct ly L e g l H e g e m o n y o r S u rv iv l T h e E xc e p tio n t o t h e R u le rs F e th ke rs H d E n o u g h It s S til l t h e E co n o m y S tu p id W e r e R ig h t T h e y W ro n g W h t L ib e ra l M e d ia T h e C lin n W rs W e p o n s o f M ss D e ce p tio n D u d e W h e s M y C o u n tr y T h ie ve s H ig h P la ce s S h ru b B u ck U p S u ck U p T h e F u tu o f F e d o m E m p ir e Figure Political Books dimensional clique matrix broken groups politically astute reader A black square indicates q fic Liberal books red Conservative books green Neutral books yellow By inspection cliques largely correspond conservative books approximate clique matrix plotted fig demonstrates individual books present cluster Interestingly clusters found basis adjacency matrix correspondence ascribed political leanings book cliques correspond largely conservative books Most books belong single clique cluster suggesting single topic books consistent assumption mixed membership model Summary Mixture models discrete latent variable models trained maximum likelihood A classical approach training use EM algorithm gradient based approaches possible Standard mixture models assume priori object datapoint member single cluster In mixed membership models object priori belong single cluster Models latent Dirichlet allocation interesting application example text modelling including automatic discovery latent topics Mixed membership models considered monadic dyadic data The literature mixture modelling extensive good overview entrance literature contained Code MIXprodBern m EM training mixture product Bernoulli distributions demoMixBernoulli m Demo mixture product Bernoulli distributions GMMem m EM training mixture Gaussians GMMloglik m GMM log likelihood demoGMMem m Demo EM mixture Gaussians DRAFT April Exercises demoGMMclass m Demo GMM classification Kmeans m K means demoKmeans m Demo K means demoPolya m Demo number active clusters Polya distribution dirrnd m Dirichlet random distribution generator cliquedecomp m Clique matrix decomposition cliquedecomp c Clique matrix decomposition C code DemoCliqueDecomp m Demo clique matrix decomposition Exercises Exercise Consider mixture factorised models vector observations v p v h p h p vi h For assumed d data vn n N observation components missing example component fifth datapoint v53 unknown Show maximum likelihood training observed data corresponds ignoring components vni missing Exercise Derive optimal EM update fitting mixture Gaussians constraint covariances diagonal Exercise Consider mixture K isotropic Gaussians covariance Si 2I In limit EM algorithm tends K means clustering algorithm Exercise Consider term N n log p h pold h vn We wish optimise respect distribution p h This achieved defining Lagrangian L N n log p h pold h vn h p h By differentiating Lagrangian respect p h normalisation constraint h p h optimally p h N N n pold h vn Exercise We showed fitting unconstrained mixture Gaussians maximum likelihood problematic placing Gaussians datapoint letting covariance determinant zero obtain infinite likelihood In contrast fitting single Gaussian N x d data x1 x2 xN maximum likelihood optimum non zero determinant optimal likelihood remains finite Exercise Modify GMMem m suitably deal semi supervised scenario mixture component h observations v known DRAFT April Exercises Exercise You wish parameterise covariance matrices S constraint specified elements zero The constraints specified matrix A elements Aij Sij Aij Consider clique matrix Z A H ZZT matrix S Z Z T Z ij Zij ij Zij parameters Show S positive semidefinite parameterises covariance matrices zero constraints specified A DRAFT April CHAPTER Latent Linear Models In chapter discuss simple continuous latent variable models The Factor Analysis model classical statistical model essentially probabilistic version PCA Such models form simple low dimensional generative models data As example consider application face recognition By extending model use non Gaussian priors latent variables independent dimensions underlying data discovered rise radically different low dimensional representations data afforded PCA Factor Analysis In chapter discussed Principal Components Analysis forms lower dimensional representations data based assuming data lies close linear subspace Here describe related prob abilistic model extensions Bayesian methods envisaged Any probabilistic model component larger complex model mixture model enabling natural generalisations We use v describe real data vector emphasise visible observable quantity The dataset given set vectors V v1 vN dim v D Our interest find lower dimensional probabilistic description data If data lies close H dimensional linear subspace accurately approximate datapoint low H dimensional coordinate system In general datapoints lie exactly linear subspace model discrepancy Gaussian noise Mathematically FA model generates observation v according fig v Fh c noise Gaussian distributed zero mean covariance N The constant bias c sets origin coordinate system1 The D H factor loading matrix F plays similar role basis matrix PCA Similarly hidden coordinates h plays role components section The difference PCA Factor Analysis choice 1Depending application useful force origin zero aid interpretation factors results minor modification framework Factor Analysis h1 h2 h3 v1 v2 v3 v4 v5 Figure Factor Analysis The visible vector variable v related vector hidden variable h linear mapping independent additive Gaussian noise visible variable The prior hidden variable taken isotropic Gaussian independent components Probabilistic PCA 2I Factor Analysis diag D Factor analysis differs probabilistic PCA richer description subspace noise A probabilistic description From equation equation given h data Gaussian distributed mean Fh c covariance p v h N v Fh c exp v Fh c T v Fh c To complete model need specify hidden distribution p h A convenient choice Gaussian p h N h I exp hTh Under prior coordinates h preferentially concentrated values close If sample h vector p h draw value v p v h sampled v vectors produce saucer pancake points v space fig Using correlated Gaussian prior p h N h H effect flexibility model H absorbed F exercise Since v linearly related h equation h Gaussian v Gaussian distributed The mean covariance computed propagation result p v p v h p h dh N v c FFT What says model data Gaussian centred c covariance matrix constrained form FFT The number free parameters factor analysis D H Compared unconstrained covariance v D D free parameters choosing H D significantly fewer parameters estimate factor analysis unconstrained covariance case Invariance likelihood factor rotation Since matrix F appears final model p v FFT likelihood unchanged rotate F FR RRT I FR FR T FRRTFT FFT The solution space F unique arbitrarily rotate matrix F produce equally likely model data Some care required interpreting entries F Varimax provides interpretable F suitable rotation matrix R The aim produce rotated F column small number large values Finding suitable rotation results non linear optimisation problem needs solved numerically See details DRAFT April Factor Analysis Maximum Likelihood b Figure Factor Analysis points generated model latent dimensional points hn sampled N h I These transformed point dimensional plane xn0 c Fh n The covariance x0 degenerate covariance matrix FF T b For point xn0 plane random noise vector drawn N added plane vector form sample xn plotted red The distribution points forms pancake space Points underneath plane shown Finding optimal bias For set data V usual d assumption log likelihood log p V F c N n log p vn N n vn c T 1D v n c N log det 2D D FFT Differentiating equation respect c equating zero arrive maximum likelihood optimal setting bias c mean data c N N n vn v We use setting With setting log likelihood equation written log p V F N trace 1D S log det 2D S sample covariance matrix S N N n v v v v T Factor Analysis Maximum Likelihood We specialise assumption diag D We consider methods learning factor loadings F eigen approach2 section EM approach section The eigen approach common statistics software packages whilst EM approach common machine learning 2The presentation follows closely DRAFT April Factor Analysis Maximum Likelihood Eigen approach likelihood optimisation As noise matrix given find optimal factor matrix F solving eigen problem If unknown starting guess find optimal F use reestimate noise iterating step process convergence This section necessarily technical skipped reading Optimal F fixed To find maximum likelihood setting F differentiate log likelihood equation respect F equate zero This gives trace 1D FD D S trace 1D FD Using F D F FF T F FF T FF F T stationary point given 1D F D S D F The optimal F satisfies F S 1D F Using definition D equation rewrite D F exercise 1D F 1F I FT 1F Plugging zero derivative condition equation rearranged F I FT 1F S 1F Using reparameterisations F F S S equation written isotropic form F I F TF S F We assume transformed factor matrix F thin SVD decomposition F UHLW T dim UH D H dim L H H dim W H H UTHUH IH W TW IH L diag l1 lH singular values F Plugging assumption equation obtain UHLW T IH WL 2WT S UHLW T gives UH IH L S UH L diag l21 l H Equation eigen equation UH Intuitively s clear need find eigen decomposition S set columns UH eigenvectors corresponding largest eigenvalues This derived formally DRAFT April Factor Analysis Maximum Likelihood Determining appropriate eigenvalues We relate form solution eigen decomposition S S UUT U UH Ur Ur arbitrary additional columns chosen complete UH form orthogonal U U TU UUT I Using diag D equation stipulates l li H Given solution F solution F found equation To determine optimal write log likelihood terms follows Using new parameterisation D F F T I S S trace 1D S trace F F T ID S The log likelihood equation new parameterisation N log p V F trace ID F F T S log det ID F F T log det Using l equation write ID F F T ID UHL 2UTH Udiag H U T inverse matrix given Udiag H UT trace ID F F T S H H Similarly log det ID F F T H log Using write log likelihood function eigenvalues fixed N log p V F H log H D H log det To maximise likelihood need minimise right hand Since log place largest H eigenvalues log term A solution fixed F UH H IH R H diag H H largest eigenvalues S UH matrix corresponding eigenvectors R arbitrary orthogonal matrix DRAFT April Factor Analysis Maximum Likelihood Algorithm Factor analysis training SVD N D dimensional datapoints v1 vN H latent number factors required Initialise diagonal noise Find mean v data v1 vN Find variance 2i component data v v N Compute centred matrix X v1 v xN v Likelihood converged termination criterion reached Form scaled data matrix X X N Perform SVD X U W T set Set UH H columns U set H contain H diagonal entries F UH H IH factor update L N H log H D H log det log likelihood diag diag FFT noise update end SVD based approach Rather finding eigen decomposition S avoid forming covariance matrix considering thin SVD decomposition X N X centred data matrix X v1 v xN v Given thin decomposition X UH W T obtain eigenvalues ii When matrix X large store memory online SVD methods available Finding optimal The zero derivative log likelihood equation respect occurs diag S FFT F given equation There closed form solution equations A simple iterative scheme guess values diagonal entries find optimal F equation Subsequently updated new diag S FFT We update F equation equation convergence algorithm Alternative schemes updating noise matrix improve convergence considerably For example updating single component rest fixed achieved closed form expression Expectation maximisation An alternative way way train factor analysis popular machine learning use EM sec tion We assume bias c optimally set data mean v DRAFT April Factor Analysis Maximum Likelihood M step As usual need consider energy neglecting constants E F N n dn Fh T dn Fh q h vn N log det dn vn v The optimal variational distribution q h vn determined E step Maximising E F respect F gives F AH A N n dn h Tq h vn H N n hhT q h vn Finally N n diag dn Fh dn Fh T q h vn diag N n dn dn T 2FAT FHFT Note new F update covariance E step The updates depend statistics h q h vn hhT q h vn Using EM optimal choice E step q h vn p vn h p h N h mn mn h q h vn I FT 1F FT 1dn I FT 1F Using results express statistics equation H N n mn mn T Equations iterated till convergence As EM algorithm likelihood equation diagonal constraint increases iteration Convergence EM technique slower eigen approach section Provided reasonable initialisation performance training algorithms similar A useful initialisation use PCA set F principal directions Mixtures FA An advantage probabilistic models components complex models mixtures FA Training achieved EM gradient based approaches Bayesian extensions clearly interest whilst formally intractable addressed approximate methods example DRAFT April Interlude Modelling Faces f1 f2 F G G x x x x x origin Figure Latent Identity Model The mean represents mean faces The subspace F represents direc tions variation different faces f1 Fh1 mean face individual similarly f2 Fh2 The subspace G denotes directions variability individual face caused pose lighting etc This vari ability assumed person A particular mean face given mean face person plus pose illumination variation example x f1 Gw12 A sample face given mean face x ij plus Gaussian noise N ij Interlude Modelling Faces Factor Analysis widespread application statistics machine learning As inventive application FA highlighting probabilistic nature model describe face modelling technique heart latent linear model Consider gallery face images X xij I j J vector xij represents j th image ith person As latent linear model faces consider xij Fhi Gwij ij Here F dim F D F model variability people G dim G D G models variability related pose illumination etc different images person The contribution fi Fhi accounts variability different people constant individual For fixed contribution Gwij ij ij N ij accounts variability images person explaining images person look identical See fig graphical representation As probabilistic linear latent variable model image xij p xij hi wij N xij Fhi Gwij p hi N hi I p wij N wij I The parameters F G For collection images assuming d data p X w h I J j p xij hi wij p wij p hi xij wij hi J I Figure The jth image ith person xij modelled linear latent model parameters DRAFT April Interlude Modelling Faces Mean b Variance c d e f g h Figure Latent Identity Model face images Each image represented vector comes RGB colour coding There I individuals database J images person Mean data b Per pixel standard deviation black low white high c d e Three directions individual subspace F f g h Three samples model h fixed drawing randomly w individual subspace G Reproduced graphical model depicted fig The task learning maximise likelihood p X w h p X w h This model constrained version Factor Analysis stacked vectors single individual I x11 x12 x1J F G F G F G h1 w11 w12 w1J 1J The generalisation multiple individuals I straightforward The model trained constrained form eigen method EM described Example images trained model presented fig Recognition Using constrained factor analysis models faces extend application classifica tion briefly describe In closed set face recognition new probe face x matched person n gallery training faces In model Mn nth gallery face forced share latent identity variable hn test face indicating faces belong person fig Assuming single exemplar person J p x1 xI x Mn p xn x I n p xi Bayes rule gives posterior class assignment p Mn x1 xI x p x1 xI x Mn p Mn 3This analogous Bayesian outcome analysis section hypotheses assume errors generated different model DRAFT April Probabilistic Principal Components Analysis x1 x2 x h1 h2 w1 w2 w M1 x1 x2 x h1 h2 w1 w2 w b M2 Figure Face recognition model de picted single exemplar son J In model M1 test image probe x assumed person albeit different pose illumination b For model M2 test image assumed son One calculates p x1 x2 x M1 p x1 x2 x M2 uses Bayes rule infer person test image x likely belongs For uniform prior term p Mn constant neglected All marginal quantities straightforward derive simply marginals Gaussian In practice best results obtained individual subspace dimension F individual subspace dimension G equal This model performance competitive state art A benefit probabilistic model extension mixtures model essentially straightforward boosts performance Related models open set face recognition problem probe face belong individuals database Probabilistic Principal Components Analysis PPCA corresponds Factor Analysis restriction 2ID Plugging assumption eigen solution equation gives F UH H IH R eigenvalues diagonal entries H corresponding eigenvectors columns UH largest eigenvalues 2S Since eigenvalues 2S S simply scaled eigenvectors unchanged equivalently write F UH H 2IH R R arbitrary orthogonal matrix RTR I UH H eigenvectors correspond ing eigenvalues sample covariance S Classical PCA section recovered limit Note correspondence PCA needs set R I points F principal directions Optimal A particular convenience PPCA optimal noise found immediately We order eigenvalues S D In equation expression log likelihood given eigenvalues 2S On replacing write explicit expression log likelihood terms eigenvalues sample covariance S L N D log H log D H D H log H By differentiating L equating zero maximum likelihood optimal setting D H D j H j In summary PPCA obtained taking H principal eigenvalues corresponding eigenvectors sample covariance matrix S setting variance equation The single shot training nature PPCA makes attractive algorithm gives useful initialisation Factor Analysis DRAFT April Canonical Correlation Analysis Factor Analysis FA FA FA FA FA PCA PCA PCA PCA PCA Figure For hidden unit model plotted results training PPCA FA examples hand written digit seven The row contains Factor Analysis factors row largest eigenvectors PPCA plotted Example A Comparison FA PPCA We trained PPCA FA model handwritten digits number From database images fitted PPCA FA iterations EM case random initialisation hidden units The learned factors models fig To feeling models data drew samples model given fig 8a Compared PPCA FA individual noise observation pixel enables cleaner representation regions zero sample variance Canonical Correlation Analysis Factor Analysis We outline CCA discussed section related constrained form FA As brief minder CCA considers spaces X Y example X represent audio sequence person speaking Y corresponding video sequence face person speaking The streams data dependent expect parts mouth region correlated speech signal The aim CCA find low dimensional representation explains correlation X Y spaces A model achieves similar effect CCA use latent factor h underlie data X Y spaces fig That p x y p x h p y h p h dh p x h N x ha x p y h N y hb y p h N h We express equation form Factor Analysis writing x y b h x y x N x x y N y y By stacked vectors z x y f b Factor Analysis b PPCA Figure sam ples learned FA model Note noise variance de pends pixel zero pixels boundary image b samples learned PPCA model noise variance pixel DRAFT April Independent Components Analysis h x y Figure Canonical Correlation Analysis CCA corresponds latent variable model common latent variable generates observed x y variables This formed constrained Factor Analysis integrating latent variable h obtain p z N z ffT x y This clearly simply factor analysis model case single latent factor We learn best b maximising likelihood set data From FA results equation optimal f given S sample covariance matrix f fT 1f S 1f f S 1f optimally f given principal eigenvector S By imposing x xI y yI equation expressed coupled equations 2x Sxxa 2y Sxyb b 2x Syxa 2y Syyb Eliminating b arbitrary proportionality constant I 2x Sxx 2x y Sxy I 2y Syy Syxa In limit 2x y tends zero derivative condition equation CCA seen fact limiting form FA thorough correspondence By viewing CCA manner extensions single latent dimension H clear exercise addition benefits probabilistic interpretation As ve indicated CCA corresponds training form FA maximising joint likelihood p x y w u If x represents input y output interested finding good predictive representation In case training based maximising conditional p y x w u corresponds special case technique called Partial Least Squares example This correspondence left exercise interested reader Independent Components Analysis Independent Components Analysis ICA seeks representation data v coordinate system h components hi independent Such independent coordinate systems arguably form natural representation data In ICA common assume observations linearly related latent variables h For technical reasons convenient practical choice use4 v Ah A square mixing matrix likelihood observation v p v A p v h A p hi dh v Ah p hi dh det A p A 1v 4This treatment follows presented DRAFT April Independent Components Analysis Figure Latent data sampled prior p xi exp xi mixing matrix A shown green cre ate observed dimensional vectors y Ax The red lines mixing matrix estimated ica m based obser vations For comparison PCA produces blue dashed com ponents Note components scaled improve visualisation As expected PCA finds orthogonal directions maximal variation ICA correctly estimates di rections components independently generated See demoICA m For given set data V v1 vN prior p h aim find A For d data log likelihood conveniently written terms B A L B N log det B n log p Bvn Note Gaussian prior p h exp h2 log likelihood L B N log det B n vn T BTBvn const invariant respect orthogonal rotation B RB RTR I This means Gaussian prior p h estimate uniquely mixing matrix To break rotational invariance need use non Gaussian prior Assuming non Gaussian prior p h taking derivative w r t Bab obtain Bab L B NAba n Bv v n b x d dx log p x p x d dx p x A simple gradient ascent learning rule B Bnew B B T N n Bvn vn T An alternative natural gradient algorithm approximates Newton update given multi plying gradient BTB right update Bnew B I N n Bvn Bvn T B Here learning rate code ica m nominally set The performance algorithm relatively insensitive choice prior function x In ica m use tanh function An example given fig data clearly underlying X shape axes X orthogonal In case ICA PCA representations different fig A natural extension consider noise outputs exercise EM algorithm readily available However limit low output noise EM formally fails effect related general discussion section DRAFT April Exercises A popular alternative estimation method FastICA5 related iterative maximum likelihood optimisation procedure ICA motivated alternative directions including information theory We refer reader depth discussion ICA related extensions Summary Factor Analysis classical probabilistic method finding low dimensional representations data There closed form solution general iterative procedures typically find maximum likelihood parameters Canonical Correlation Analysis special case Factor Analysis Under assumption non Gaussian latent variable priors able discover independent directions data Code FA m Factor Analysis demoFA m Demo Factor Analysis ica m Independent Components Analysis demoIca m Demo ICA Exercises Exercise Factor Analysis scaling Assume H factor model holds x Now consider transformation y Cx C non singular square diagonal matrix Show Factor Analysis scale invariant e H factor model holds y factor loadings appropriately scaled How specific factors scaled Exercise For constrained Factor Analysis model x A B h N diag n h N h I derive maximum likelihood EM algorithm matrices A B assuming datapoints x1 xN d Exercise An apparent extension FA analysis consider correlated prior p h N h H Show provided constraints placed factor loading matrix F correlated prior p h equivalent model original uncorrelated FA model Exercise Using Woodbury identity definition D equation rewrite 1D F 1D F 1F I FT 1F 5See www cis hut fi projects ica fastica DRAFT April Exercises Exercise For log likelihood function L N D log H log D H D H log H Show L maximal D H D j H j Exercise Consider ICA model y represents outputs x latent components p y x W j p yj x W p xi W w1 wJ p yj x W N yj w T j x For model derive EM algorithm set d data y1 yN required statistics M step x p x yn W xxT p x yn W Show non Gaussian prior p xi posterior p x y W non factorised non Gaussian generally intractable normalisation constant com puted efficiently Show limit EM algorithm fails DRAFT April Exercises DRAFT April CHAPTER Latent Ability Models In chapter discuss application latent variable models ascertaining ability players games These models applicable variety contexts exam performance football match prediction online gaming analysis The Rasch Model Consider exam student s answers question q correctly xqs incorrectly xqs For set N students Q questions performance students given Q N binary matrix X Based data wish evaluate ability student One approach define ability fraction questions student s answered correctly A subtle analysis accept questions difficult student answered difficult questions awarded highly student answered number easy questions A priori know difficult questions needs estimated based X To account inherent differences question difficulty model probability student s gets question q correct based student s latent ability s latent difficulty question q A simple generative model response fig p xqs s q x e x Under model higher latent ability latent difficulty question likely student answer question correctly Maximum likelihood training We use maximum likelihood find best parameters Making d assumption likelihood data X model p X S s Q q s q xqs s q xqs The log likelihood L log p X q s xqs log s q xqs log s q derivatives L s Q q xqs s q L q S s xqs s q The Rasch Model xqs s q S Q Figure The Rasch model analysing questions Each element binary matrix X xqs student s gets question q correct generated latent ability student s latent difficulty question q A simple way learn parameters use gradient ascent demoRasch m extensions Newton methods straightforward The generalisation responses xqs achieved softmax style function More generally Rasch model falls item response theory subject dealing analysis questionnaires Missing data Assuming data missing random missing data treated computing likelihood observed elements X In rasch m missing data assumed coded nan likelihood gradients straightforward compute based summing terms containing non nan entries Example We display example use Rasch model fig estimat ing latent abilities students based set questions Based number questions student answered correctly best students ranked Alternatively ranking students according latent ability gives This differs slightly case number correct ranking Rasch model takes account fact students answered difficult questions correctly For example student answered difficult questions correctly Bayesian Rasch models The Rasch model potentially overfit data especially small data For case natural extension use Bayesian technique placing independent priors ability question difficulty posterior ability question difficulty given p X p X p p Natural priors discourage large values parameters Gaussians p s N s p q N q hyperparameters learned maximising p X Even Gaussian priors posterior distribution p X standard form approximations required In case posterior log concave approximation methods based variational Laplace techniques potentially adequate chapter alternatively use sampling approximations chapter DRAFT April Competition Models question st u d e n t question e st I am te d d iff ic u lty b student fr ct io n o f q u e st io n s co rr e ct c student e st I am te d b ili ty d Figure Rasch model The data correct answers white incorrect answers black b The estimated latent difficulty question c The fraction questions student answered correctly d The estimated latent ability Competition Models Bradley Terry Luce model The Bradley Terry Luce model assesses ability players based matches Here describe games win lose outcomes arise leaving aside complicating possibility draws For win lose scenario BTL model straightforward modification Rasch model latent ability player latent ability j player j probability beats j given p iB j j iB j stands player beats player j Based matrix games data X xnij iB j game n likelihood model given p X n ij j x n ij ij j Mij Mij n x n ij number times player beat player j Training maximum likelihood Bayesian technique proceed Rasch model These models called pairwise comparison models pioneered Thurstone s applied models wide range data DRAFT April Competition Models competitor co m p e tit o r true ability e st I am te d b ili ty b Figure BTL model The data M Mij number times competitor beat competitor j b The true versus estimated ability competitors unlabelled Even data sparse reasonable estimate latent ability competitor found Example An example application BTL model given fig matrix M containing number times competitor beat competitor j given The matrix entries M drawn BTL model based true abilities Using M maximum likelihood estimate latent abilities close agreement true abilities Elo ranking model The Elo system chess ranking closely related BTL model added complication possibility draws In addition Elo system takes account measure variability performance For given ability actual performance player game given N The variance fixed players takes account intrinsic variability performance More formally Elo model modifies BTL model p X p X p p N 2I p X given equation replacing Glicko TrueSkill Glicko TrueSkill essentially Bayesian versions Elo model refinement latent ability modelled single number Gaussian distribution p N This capture fact player consistently reasonable high low erratic genius high large The parameters model S set S players The interaction model p X win lose Elo model equation The likelihood model given parameters p X p X p DRAFT April Exercises This integral formally intractable numerical approximations required In context expectation propagation section proven useful technique The TrueSkillsystem example assess abilities players online gaming taking account abilities teams individuals tournaments A temporal extension recently reevaluate change ability chess players time Summary The Rasch model simple model latent student ability question difficulty In principle better able assess student performance simply counting number correct questions implicitly takes account fact questions difficult Related models assess underlying ability players games Code rasch m Rasch model training demoRasch m Demo Rasch model Exercises Exercise Bucking Bronco bronco mat contains information bucking bronco competition There competitors bucking broncos A competitor j attempts stay bucking bronco minute If competitor succeeds entry Xij Each competitor gets ride bucking broncos missing data coded nan Having viewed amateurs Desperate Dan enters competition bribes organisers letting avoid having ride difficult broncos Based Rasch model difficult broncos order difficult Exercise BTL training Show log likelihood Bradley Terry Luce model given L ij Mij log j Mij number times player beats player j set games Compute gradient L Compute Hessian BTL model verify negative semidefinite Exercise La Reine Program simple gradient ascent routine learn latent abilities competitors based series win lose outcomes In modified form Swiss cow fighting set cows compete pushing submission At end competition cow deemed la reine Based data BTL mat Xij contains number times cow beat cow j fit BTL model return ranked list best fighting cows la reine DRAFT April Exercises Exercise An extension BTL model consider additional factors describe state competitors play For example set S football teams set matrices X1 XN Xnij team beat team j match n In addition match n team vector binary factors fnh h H describes team For example team Madchester United factor fn1 Bozo playing match n It suggested ability team game n measured ni di H h wh n h Here di default latent ability team assumed constant games We set factors match Using definition latent ability BTL model interest find weights W abilities d best describe ability team given set historical plays Xn Fn n N Write likelihood BTL model function set team weights W d Compute gradient log likelihood model Explain model assess importance Bozo s contribution Madchester United s ability Given learned W d knowledge Madchester United team play Chelski team tomorrow explain given list factors f Chelski includes issues playing team select best Madchester United team maximise probability winning game DRAFT April Part IV Dynamical Models Introduction Part IV Natural organisms inhabit dynamical environment arguably large natural intelligence modelling causal relations consequences ac tions In sense modelling temporal data fundamental interest In artificial environment instances predicting future interest particularly areas finance tracking moving objects In Part IV discuss classical models timeseries represent temporal data predictions future Many models known different branches science Physics Engineering heavily areas speech recognition financial prediction control We discuss sophisticated models chapter skipped reading As allusion fact natural organisms inhabit temporal world ad dress chapter basic models information processing achieved distributed systems DRAFT April Dynamic models Markov models Complete Cont OLDS AR Discrete Markov chain Latent DBN factorised structure discrete latent HMM cont latent state space models LDS switching models eg SLDS Non Markov models Gaussian process Some dynamical graphical models timeseries In Part IV cover inference learning standard models DRAFT April CHAPTER Discrete State Markov Models Timeseries require specialised models number variables large typically increases new datapoints arrive In chapter discuss models process generating observed data fundamentally discrete These models rise classical models interesting applications fields finance speech processing website ranking Markov Models Timeseries datasets constituent datapoints naturally ordered This order corresponds underlying single physical dimension typically time single dimension The time series models consider probability models collection random variables v1 vT individual variables vt indexed discrete time t A probabilistic time series model requires specification joint distribution p v1 vT For case observed data vt discrete joint probability table p v1 vT exponentially entries We expect independently specify exponentially entries need simplified models entries parameterised lower dimensional manner Such simplifications heart timeseries modelling discuss classical models following sections Definition Time Series Notation xa b xa xa xb xa b xa b For timeseries data v1 vT need model p v1 T For consistency causal nature time natural consider cascade decomposition p v1 T T t p vt v1 t convention p vt v1 t p v1 t It useful assume influence immediate past relevant remote past Markov models limited number previous observations required predict future fig Definition Markov chain A Markov chain defined discrete continuous variables v1 T following conditional independence assumption holds p vt v1 vt p vt vt L vt Markov Models v1 v2 v3 v4 v1 v2 v3 v4 b Figure First order Markov chain b Second order Markov chain L order Markov chain vt t For order Markov chain p v1 T p v1 p v2 v1 p v3 v2 p vT vT For stationary Markov chain transitions p vt s vt s f s s time independent Otherwise chain non stationary p vt s vt s f s s t For discrete state order time independent Markov chain visualise transition p vt vt state transition diagram fig Figure A state transition diagram state Markov chain Note state transition diagram graphical model simply displays non zero entries transition matrix p j The absence link j indicates p j Equilibrium stationary distribution Markov chain For transition p xt xt interesting know marginal p xt evolves time For discrete state system given p xt j p xt xt j Mij p xt j For initial distribution p x1 recursively defines marginal future timepoints The marginal p xt interpretation frequency visit state time t given started sample p x1 subsequently repeatedly drew samples transition p x x That drawing state x1 x1 p x1 draw samples x2 xt Markov chain drawing sample p x2 x1 x1 p x3 x2 x2 etc As repeatedly sample new state chain marginal distribution time t represented vector pt p xt initial distribution p1 pt Mpt M t 1p1 If t p independent initial distribution p1 p called equilibrium distribution chain See exercise example Markov chain equilibrium distribution The called stationary distribution Markov chain defined condition p j p xt xt j p j In matrix notation written vector equation p Mp stationary distribution proportional eigenvector unit eigenvalue transition matrix Note stationary distribution See exercise DRAFT April Markov Models h v1 v2 v3 v4 Figure Mixture order Markov chains The discrete hidden variable dom h H indexes Markov chain t p vt vt h Such models useful simple sequence clustering tools Example PageRank Despite apparent simplicity Markov chains interesting use information retrieval search engines Define matrix Aij website j hyperlink website From define Markov transition matrix elements Mij Aij Ai j The equilibrium distribution Markov Chain interpretation If follow links random jumping website website equilibrium distribution component p relative number times visit website This natural interpretation importance website website isolated web visited infrequently random hopping website linked visited frequently A crude search engine works follows For website list words associated website collected After websites inverse list websites contain word w When user searches word w list websites contain word returned ranked according importance site defined equilibrium distribution Fitting Markov models Given sequence v1 T fitting stationary order Markov chain maximum likelihood corresponds setting transitions counting number observed order transitions sequence p v v j T t I vt vt j To convenience write p v v j j likelihood assuming v1 known p v2 T v1 T t vt vt T t j I vt vt j j Taking logs adding Lagrange constraint normalisation L T t j I vt vt j log j j j j Differentiating respect j equating zero immediately arrive intuitive setting equa tion For set timeseries vn1 Tn n N transition given counting transitions time datapoints The maximum likelihood setting initial timestep distribution p v1 n I v n DRAFT April Markov Models Bayesian fitting For simplicity assume factorised prior transition p j p j A convenient choice conditional transition Dirichlet distribution hyperparameters uj p j Dirichlet j uj conjugate categorical transition giving p v1 T p v1 T p t j I vt vt j j uij j j Dirichlet j u j u j uij T t I vt j vt number j transitions dataset Mixture Markov models Given set sequences V vn1 T n N cluster To notation cluttered assume sequences length T extension differing lengths straightforward One simple approach fit mixture Markov models Assuming data d p V n p v n T define mixture model single sequence v1 T Here assume component model order Markov p v1 T H h p h p v1 T h H h p h T t p vt vt h The graphical model depicted fig Clustering achieved finding maximum likelihood parameters p h p vt vt h subsequently assigning clusters according p h vn1 T EM algorithm The EM algorithm section particularly convenient finding maximum likelihood solution case M step performed simply Under d data assumption log likelihood log p V N n log H h p h T t p vnt vnt h For M step task maximise energy E N n log p vn1 T h pold h vn T N n log p h pold h vn T T t log p vt vt h pold h vn T The contribution energy parameter p h N n log p h pold h vn T By defining p old h N n pold h vn1 T view maximising equivalent minimising KL p old h p h optimal choice M step set pnew p old pnew h N n pold h vn1 T DRAFT April Markov Models For comfortable argument direct maximisation including Lagrange term ensure normalisation p h derive result Similarly M step p vt vt h pnew vt vt j h k N n pold h k vn1 T T t I vnt I vnt j The initial term p v1 h updated pnew v1 h k N n pold h k vn1 T I vn1 Finally E step sets pold h vn1 T p h p vn1 T h p h T t p vnt vnt h Given initialisation EM algorithm iterates convergence For long sequences explicitly computing product terms lead numerical underflow issues In practice best work logs log pold h vn1 T log p h T t log p vnt vnt h const In way large constants common h removed distribution computed accurately See mixMarkov m Example Gene Clustering Consider fictitious gene sequences presented arbitrarily chosen order Each sequence consists symbols set A C G T The task try cluster sequences groups based assumption gene sequences cluster follow stationary Markov chain CATAGGCATTCTATGTGCTG CCAGTTACGGACGCCGAAAG TGGAACCTTAAAAAAAAAAA GTCTCCTGCCCTCTCTGAAC GTGCCTGGACCTGAAAAGCC CGGCCGCGCCTCCGGGAACG AAAGTGCTCTGAAAACTCAC ACATGAACTACATAGTATAA GTTGGTCAGCACACGGACTG CCTCCCCTCCCCTTTCCTGC CACTACGGCTACCTGGGCAA CGGTCCGTCCGAGGCACTC TAAGTGTCCTCTGCTCCTAA CACCATCACCCTTGCTAAGG AAAGAACTCCCCTCCCTGCC CAAATGCCTCACGCGTCTCA GCCAAGCAGGGTCTCAACTT CATGGACTGCTCCACAAAGG AAAAAAACGAAAAACCTAAG GCGTAAAAAAAGTCCTGGGT A simple approach assume sequences generated component H mixture Markov models train model maximum likelihood The likelihood local optima procedure needs run times solution highest likelihood chosen One assign sequences examining p h vn1 T If posterior probability greater assign cluster cluster Using procedure find following clusters CATAGGCATTCTATGTGCTG TGGAACCTTAAAAAAAAAAA CCAGTTACGGACGCCGAAAG GTCTCCTGCCCTCTCTGAAC CGGCCGCGCCTCCGGGAACG GTGCCTGGACCTGAAAAGCC ACATGAACTACATAGTATAA AAAGTGCTCTGAAAACTCAC GTTGGTCAGCACACGGACTG CCTCCCCTCCCCTTTCCTGC CACTACGGCTACCTGGGCAA TAAGTGTCCTCTGCTCCTAA CGGTCCGTCCGAGGCACTCG AAAGAACTCCCCTCCCTGCC CACCATCACCCTTGCTAAGG AAAAAAACGAAAAACCTAAG CAAATGCCTCACGCGTCTCA GCGTAAAAAAAGTCCTGGGT GCCAAGCAGGGTCTCAACTT CATGGACTGCTCCACAAAGG sequences column assigned cluster sequences second column cluster In case data fact generated component Markov mixture posterior assignment agreement known clusters See demoMixMarkov m DRAFT April Hidden Markov Models v1 v2 v3 v4 h1 h2 h3 h4 Figure A order hidden Markov model hidden variables dom ht H t T The visible variables vt discrete con tinuous Hidden Markov Models The Hidden Markov Model HMM defines Markov chain hidden latent variables h1 T The observed visible variables dependent hidden variables emission p vt ht This defines joint distribution p h1 T v1 T p v1 h1 p h1 T t p vt ht p ht ht graphical model depicted fig For stationary HMM transition p ht ht emission p vt ht distributions constant time The use HMM widespread subset applications HMMs given section Definition Transition Distribution For stationary HMM transition distribution p ht ht defined H H transition matrix Ai p ht ht initial distribution ai p h1 Definition Emission Distribution For stationary HMM emission distribution p vt ht discrete states vt V define V H emission matrix Bi j p vt ht j For continuous outputs ht selects H possible output distributions p vt ht ht H In engineering machine learning communities term HMM typically refers case discrete variables ht convention adopt In statistics term HMM refers model independence structure equation regardless form variables ht example The classical inference problems The common inference problems HMMs summarised Filtering Inferring present p ht v1 t Prediction Inferring future p ht v1 s t s Smoothing Inferring past p ht v1 u t u Likelihood p v1 T Most likely Hidden path Viterbi alignment argmax h1 T p h1 T v1 T The likely hidden path problem termed Viterbi alignment engineering speech recognition literature All classical inference problems computationally straightforward distribution singly connected standard inference method adopted problems The factor graph junction trees order HMM given fig In cases suitable setting factors clique potentials filtering corresponds passing messages left right upwards DRAFT April Hidden Markov Models smoothing corresponds valid schedule message passing absorption forwards backwards edges It straightforward derive appropriate recursions directly This instructive useful constructing compact numerically stable algorithms The algorithms derive exploit conditional independence statements belief network structure analogous procedures hold models conditional independencies example continuous states replacing summation integration Filtering p ht v1 t Filtering refers ascertaining distribution latent variable ht given information present v1 t To compute joint marginal p ht v1 t conditional marginal p ht v1 t subsequently obtained normalisation A recursion p ht v1 t obtained considering p ht v1 t ht p ht ht v1 t vt ht p vt v1 t ht ht p ht v1 t ht p v1 t ht ht p vt ht p ht ht p ht v1 t The cancellations follow conditional independence assumptions model Hence define ht p ht v1 t equation gives recursion ht p vt ht corrector ht p ht ht ht predictor t h1 p h1 v1 p v1 h1 p h1 This recursion interpretation filtered distribution ht propagated forwards dynamics timestep reveal new prior distribution time t This distribution modulated observation vt incorporating new evidence filtered distribution referred predictor corrector method Since smaller recursion involves multiplication terms s small To avoid numerical problems advisable work log ht HMMforward m Normalisation gives filtered posterior p ht v1 t ht If require filtered posterior free rescale s wish In case alternative working log messages work normalised messages ht ht h1 h2 h3 h4 v1 v2 v3 v4 v1 h1 h1 h2 h2 h3 h3 h4 v2 h2 v3 h3 v4 h4 h1 h2 h3 h2 h3 h4 b Figure Factor graph order HMM fig b Junction tree fig DRAFT April Hidden Markov Models We write equation directly recursion filtered distribution p ht v1 t ht p vt ht p ht ht p ht v1 t t Intuitively term p ht v1 t effect removing nodes graph time t replacing influence modified prior distribution ht One interpret p vt ht p ht ht likelihood giving rise joint posterior p ht ht v1 t Bayesian updating At timestep previous posterior new prior Parallel smoothing p ht v1 T There main approaches computing p ht v1 T Perhaps common HMM literature parallel method equivalent message passing factor graphs In separates smoothed posterior contributions past future p ht v1 T p ht v1 t vt T p ht v1 t past p vt T ht v1 t future ht ht The cancellation occurs fact ht d separates past future The term ht obtained forward recursion The ht term obtained backward recursion The forward backward recursions independent run parallel results combined obtain smoothed posterior The recursion p vt T ht ht p vt vt T ht ht ht p vt vt T ht ht p vt T ht ht ht p vt ht p vt T ht ht p ht ht Defining ht p vt T ht equation gives recursion ht ht p vt ht p ht ht ht t T hT As forward pass working log space recommended avoid numerical difficulties If desires posterior distributions perform local normalisation stage ht ht relative magnitude components importance The smoothed posterior given p ht v1 T ht ht ht ht ht ht Together recursions called Forward Backward algorithm Correction smoothing An alternative parallel method form recursion directly smoothed posterior This achieved recognising conditioning present makes future redundant p ht v1 T ht p ht ht v1 T ht p ht ht v1 t vt T p ht v1 T DRAFT April Hidden Markov Models This gives recursion ht p ht v1 T ht ht p ht ht v1 t ht hT hT The term p ht ht v1 t computed filtered results p ht v1 t p ht ht v1 t p ht ht v1 t p ht v1 t p ht ht p ht v1 t p ht v1 t term p ht v1 t ht p ht ht p ht v1 t found normalisation This form dynamics reversal reversing direction hidden hidden arrow HMM This procedure termed Rauch Tung Striebel smoother1 sequential need complete recursions recursion begin This called correction smoother corrects filtered result Interestingly filtering carried evidential states v1 T needed subsequent recursion The recursions related ht ht ht Computing pairwise marginal p ht ht v1 T To implement EM algorithm learning section require terms p ht ht v1 T These obtained message passing factor graph junction tree pairwise marginals contained cliques fig 4b Alternatively explicit recursion follows p ht ht v1 T p v1 t vt vt T ht ht p vt T v1 t vt ht ht p v1 t vt ht ht p vt T ht p vt v1 t ht ht p v1 t ht ht p vt T ht p vt ht p ht v1 t ht p v1 t ht Rearranging p ht ht v1 T ht p vt ht p ht ht ht See HMMsmooth m The likelihood p v1 T The likelihood sequence observations computed p v1 T hT p hT v1 T hT hT An alternative computation found making use decomposition p v1 T T t p vt v1 t Each factor computed p vt v1 t ht p vt ht v1 t ht p vt ht v1 t p ht v1 t ht p vt ht ht p ht ht v1 t p ht v1 t 1It common use terminology continuous variable case adopt discrete variable case DRAFT April Hidden Markov Models final term p ht v1 t filtered result In approaches likelihood output sequence requires forward computation filtering If required compute likelihood equation p v1 T ht ht ht valid t T Sampling p h1 T v1 T Sometimes wishes sample joint trajectories h1 T posterior p h1 T v1 T A general purpose method described section applied case noting conditioned observations v1 T write latent distribution Markov network This means posterior distribution p h1 T v1 T simple linear Markov chain reexpress distribution p h1 T v1 T p h1 h2 v1 T p hT hT v1 T p hT v1 T equation p ht ht v1 T p ht ht v1 T ht p ht ht Sampling begins drawing state hT p hT v1 T Given state draws p hT t hT v1 T equation This procedure known forward filtering backward sampling needs run filtering compute required time reversed transitions p ht ht v1 T Most likely joint state The likely path h1 T p h1 T v1 T likely state fixed v1 T p h1 T v1 T t p vt ht p ht ht The likely path found max product version factor graph max absorption junction tree Alternatively explicit derivation obtained considering max hT T t p vt ht p ht ht T t p vt ht p ht ht max hT p vT hT p hT hT hT The message hT conveys information end chain penultimate timestep We continue manner defining recursion ht max ht p vt ht p ht ht ht t T hT This means effect maximising h2 hT compressed message h1 likely state h given h argmax h1 p v1 h1 p h1 h1 Once computed backtracking gives h t argmax ht p vt ht p ht h t ht This special case max product algorithm called Viterbi algorithm Similarly use N max product algorithm section obtain N likely hidden paths DRAFT April Hidden Markov Models Creaks b Bumps Figure Localising burglar The latent variable ht denotes positions defined grid ground floor house A representation probability floor creak positions p vcreak h Light squares represent probability dark square b A representation probability p vbump h burglar bump po sitions Creaks Bumps b Filtering c Smoothing d Viterbi e True Burglar position Figure Localising burglar time time steps Each panel represents visible information vt vcreakt v bump t vcreakt means creak floorboard vcreakt v bump t meaning bumped state There panels time t The left half panel represents v1t right half v t The lighter shade represents occurrence creak bump darker shade absence b The filtered distribution p ht v1 t representing think burglar c The smoothed distribution p ht v1 represents distribution burglar s position given know past future observations d The likely Viterbi burglar path arg maxh1 p h1 v1 e The actual path burglar Prediction The step ahead predictive distribution given p vt v1 t ht ht p vt ht p ht ht p ht v1 t Example A localisation example You asleep upstairs house awoken noises downstairs You realise burglar ground floor attempt understand listening movements You mentally partition ground floor grid For grid position know probability position floorboard creak fig 6a Similarly know position probability bump dark fig 6b The floorboard creaking bumping objects occur independently In addition assume burglar grid square forwards backwards left right single timestep Based series bump bump creak creak information fig 7a try figure based knowledge ground floor burglar We represent scenario HMM h denotes grid square The visible variable composite form v vcreak vbump vcreak vbump states To use DRAFT April Hidden Markov Models v1 v2 v3 v4 h1 h2 h3 h4 m1 m2 m3 Figure A model robot self localisation At time robot makes intended movement mt As generative model knowing intended ment mt current grid position ht robot idea time step sensor reading vt expect Based sensor information v1 T intended movements m1 T task infer distribution robot locations p h1 T m1 T v1 T standard code form new visible variable v states p v h p vcreak h p vbump h Based past information belief burglar represented filtered distribution p ht v1 t fig In beginning filtered distribution significant mass states don t sufficient information establish burglar As time goes gather information filtered distribution concentrated small number states After burglar left T police arrive try piece burglar went based sequence creaks bumps provide At time t information burglar represented smoothed distribution p ht v1 The smoothed distribution generally concentrated states filtered distribution information past future help establish burglar s position time The police s single best guess trajectory burglar took provided likely joint hidden state arg maxh1 p h1 v1 See demoHMMburglar m Self localisation kidnapped robots A robot internal grid based map environment location h H knows likely sensor readings expect location The robot kidnapped placed environment The robot starts gathering sensor information Based readings v1 t intended movements m1 t robot attempts figure location comparing actual sensor readings internal map expected sensor readings location Due wheel slippage floor intended action robot forwards successful Given information robot like infer p ht v1 t m1 t This problem differs burglar scenario robot knowledge intended movements makes This information One view extra visible information natural think additional input information A model scenario fig p v1 T m1 T h1 T T t p vt ht p ht ht mt p mt The visible variables v1 T known intended movements m1 T The model expresses movements selected robot random decision making terms We assume robot knowledge conditional distributions defining model knows map environment state transition emission probabilities If interest localising robot inputs m known model fact form time dependent HMM p v1 T h1 T T t p vt ht p ht ht t time dependent transition p ht ht t defined intended movement mt Any inference task required follows standard stationary HMM algorithms albeit replacing time independent DRAFT April Hidden Markov Models b c Figure Filtering smoothing robot tracking HMM S A realisation HMM example described text The dots indicate true latent locations robot whilst open circles indicate noisy measured locations b The squares indicate filtering distribution timestep t p ht v1 t This probability proportional grey level black corresponding white Note posterior timesteps multimodal true position accurately estimated c The squares indicate smoothing distribution timestep t p t v1 T Note t T estimate position retrospectively uncertainty significantly lower compared filtered estimates transitions p ht ht known time dependent transitions In self localisation mapping SLAM robot know map environment This corre sponds having learn transition emission distributions fly explores environment Example Robot localisation Consider following toy tracking problem Taylan Cemgil A robot moving circular corridor time occupies S possible locations indicated At timestep t robot stays probability moves point counter clockwise direction probability This conveniently represented S S matrix A elements Aji p ht j ht For example S A At timestep t robot sensors measure position obtaining correct location prob ability w uniformly random location probability w For example S B w w A typical realisation y1 T process defined HMM S T w depicted fig 9a We interested inferring true locations robot noisy measured locations At time t true location inferred filtered posterior p ht v1 t fig 9b uses measurements t smoothed posterior p ht v1 T fig 9c uses past future observations generally accurate DRAFT April Learning HMMs Natural language models A simple generative model language obtained letter letter transitions called bigram In example use HMM clean mis typings Example Stubby fingers A stubby fingers typist tendency hit correct key single neighbouring key For simplicity assume keys lower case lower case z space bar To model use emission distribution Bij p v h j j depicted fig A database letter letter frequencies yields transition matrix Aij p h h j English For simplicity assume p h1 uniform Also assume intended key press results single press Given typed sequence kezrninh likely word corresponds By listing likely hidden sequences N max product algorithm discarding standard English dictionary likely word intended learning See demoHMMbigram m b c d e f g h j k l m n o p q r s t u v w x y z b c d e f g h j k l m n o p q r s t u v w x y z b c d e f g h j k l m n o p q r s t u v w x y z b c d e f g h j k l m n o p q r s t u v w x y z b Figure The letter letter transition matrix English p h h j b The letter emission matrix typist stubby fingers key neighbour keyboard likely hit Learning HMMs Given set data V v1 vN N sequences sequence vn vn1 Tn length Tn seek HMM transition matrix A emission matrix B initial vector likely generated V We d assumption sequence independently generated assume know number hidden states H For simplicity concentrate case discrete visible variables assuming know number states V Whilst implementing EM gradient based maximum likelihood straightforward HMM likelihood local optima care needs taken initialisation EM algorithm The application EM HMM model called Baum Welch algorithm follows general strategy outlined section The EM algorithm convenient case leads closed form expressions M step DRAFT April Learning HMMs M step Assuming d data M step given maximising energy N n log p vn1 vn2 vnTn hn1 hn2 hnTn pold hn vn respect parameters A B hn denotes h1 Tn Using form HMM obtain N n log p h1 pold h1 vn Tn t log p ht ht pold ht ht vn Tn t log p vnt ht pold ht vn compactness drop sequence index h variables To avoid potential confusion write pnew h1 denote new table entry probability initial hidden variable state Optimising equation respect p h1 enforcing p h1 distribution obtain anewi pnew h1 N N n pold h1 vn average number times respect pold hidden variable state Similarly M step transition Anewi pnew ht ht N n Tn t pold ht ht vn number times transition hidden state hidden state occurs averaged times assumed stationarity training sequences Normalising obtain Anewi N n Tn t p old ht ht vn N n Tn t p old ht ht vn Finally M step update emission Bnewj pnew vt j ht N n Tn t I vnt j p old ht vn expected number times observation state j hidden state The proportionality constant determined normalisation requirement E step In computing M step quantities pold h1 vn pold ht ht vn pold ht vn obtained smoothed inference techniques described section Equations repeated convergence See HMMem m demoHMMlearn m Parameter initialisation The EM algorithm converges local maximum likelihood general guarantee algorithm find global maximum How best initialise parameters thorny issue suitable initialisation emission distribution critical success A practical strategy initialise emission p v h based fitting simpler non temporal mixture model h p v h p h data DRAFT April Learning HMMs Continuous observations For continuous vector observation vt dim vt D require model p vt ht mapping discrete state ht distribution outputs Using continuous output change standard inference message passing equations inference carried essentially arbitrarily complex emission distributions Indeed filtering smoothing Viterbi inference normalisation Z emission p v h v h Z required For learning emission normalisation constant required dependent parameters model Mixture emission To richer emission model particularly continuous observations approach use mixture p vt ht kt p vt kt ht p kt ht kt discrete summation variable For learning useful consider kt additional latent variables EM algorithm carries straightforward way Using notation q shorthand pold E step sets q kt hnt p vn hnt kt p kt ht The energy given E n T t log q kt hnt q kt hnt log p v n t kt hnt q kt hnt log p kt h n t q kt hnt q hnt v n T The contribution emission component p v v h h k k n T t q kt k hnt h q hnt h vn1 T log p vnt h h k k For fixed q kt k hnt h needs numerically optimise expression respect emission parameters Similarly contribution energy bound mixture weights given log p k k h h n T t q kt k hnt h q hnt h vn1 T M step update mixture weights p k k h h n T t q kt k hnt h q hnt h vn1 T In case EM algorithm composed emission EM loop transitions q hnt h vn1 T fixed emissions p v h k learned updating q kt k hnt h The transition EM loop fixes emission distribution p v h learns best transition p ht ht The HMM GMM A common continuous observation mixture emission model component Gaussian p vt kt ht N vt kt ht kt ht kt ht indexes K H mean vectors covariance matrices EM updates means covariances straightforward derive equation exercise These models common tracking applications speech recognition usually constraint covariances diagonal DRAFT April Related Models v1 v2 v3 v4 h1 h2 h3 h4 c1 c2 c3 c4 Figure An explicit duration HMM The counter variables ct deterministically count zero When reach h transition allowed new value duration ct sampled Discriminative training HMMs supervised learning sequences That sequence vn1 T cor responding class label cn For example associate particular composer c C sequence v1 T wish model predict composer novel music sequence A generative approach HMMs classification train separate HMM class p v1 T c subsequently use Bayes rule form classification novel sequence v T p c v T p v T c p c C c p v T c p c If data noisy difficult model generative approach work expressive power model model complex data focussing decision boundary In applications speech recognition improvements performance reported models trained discriminative way In discriminative training example defines new single discriminative model formed C HMMs p c v1 T p v1 T c p c C c p v1 T c p c maximises likelihood set observed classes corresponding observations v1 T For single data pair cn vn1 T log likelihood log p cn vn1 T log p vn1 T cn generative likelihood log p cn log C c p vn1 T c p c The term represents generative likelihood term term accounting dis crimination Whilst deriving EM style updates hampered discriminative terms computing gradient straightforward technique described section Related Models Explicit duration model For HMM self transition p ht ht probability latent dynamics stays state timesteps decays exponentially time In practice like constrain dynamics remain state minimum number timesteps specified duration distribution A way enforce use latent counter variable ct beginning initialised duration sampled duration distribution pdur ct maximal duration Dmax Then timestep counter decrements reaches new duration sampled p ct ct ct ct ct pdur ct ct The state ht transition ct p ht ht ct ht ht ct ptran ht ht ct DRAFT April Related Models Including counter variable c defines joint latent distribution p c1 T h1 T ensures h remains desired minimal number timesteps fig Since dim ct ht DmaxH naively computa tional complexity inference model scales O TH2D2max However runs forward backward recursions deterministic nature transitions means reduced O TH2Dmax exercise The hidden semi Markov model generalises explicit duration model new duration ct sampled model emits distribution p vt t ct ht defined segment ct observations Input Output HMM The IOHMM HMM additional input variables x1 T fig Each input continuous discrete modulates transitions p v1 T h1 T x1 T t p vt ht xt p ht ht xt The IOHMM conditional predictor outputs vt represent prediction time t In case continuous inputs discrete outputs tables p vt ht xt p ht ht xt usually parameterised non linear function example p vt y ht h xt x w exp wTh yh y x h y x vector function input x Inference follows similar manner standard HMM Defining ht p ht v1 t x1 t forward pass given ht ht p ht ht v1 t vt x1 t ht p vt v1 t x1 t ht ht p ht v1 t x1 t ht p v1 t ht x1 t p vt xt ht ht p ht ht xt ht The backward pass p ht x1 T v1 T ht p ht ht x1 t xt T v1 T ht p ht ht x1 t v1 t p ht x1 T v1 T need p ht ht x1 t v1 t p ht ht x1 t v1 t p ht x1 t v1 t p ht ht xt p ht x1 t v1 t ht p ht ht xt p ht x1 t v1 t The likelihood found hT hT v1 v2 v3 v4 h1 h2 h3 h4 x1 x2 x3 x4 Figure A order input output hidden Markov model The input x output v nodes shaded emphasise states known dur ing training During testing inputs known outputs predicted DRAFT April Related Models x y1 y2 y3 Figure Linear chain CRF Since input x observed distribu tion linear chain factor graph The inference pairwise marginals p yt yt x straightforward message passing Direction bias Consider predicting output distribution p vt x1 T given past future input information x1 T Because hidden states unobserved p vt x1 T p vt x1 t Thus IOHMM prediction uses past information discards future contextual information This direction bias considered problematic particularly natural language modelling motivates use undirected models conditional random fields Linear chain CRFs Linear chain Conditional Random Fields CRFs extension unstructured CRFs briefly discussed section application modelling distribution set outputs y1 T given input vector x For example x represent sentence English y1 T represent translation French Note vector x dimension T A order linear chain CRF form p y1 T x Z x T t t yt yt x free parameters potentials In practice common use potentials form exp K k kfk t yt yt x fk t yt yt x features section Given set input output sequence pairs xn yn1 T n N assuming sequences equal length T simplicity learn parameters maximum likelihood Under standard d data assumption log likelihood L t n k kfk y n t y n t x n n logZ xn The reader readily check log likelihood concave objective function local optima The gradient given L n t fi y n t y n t x n fi yt yt xn p yt yt xn Learning requires inference marginal terms p yt yt x Since equation cor responds linear chain factor graph fig inference pairwise marginals straightforward message passing This achieved standard factor graph message passing deriving explicit algorithm exercise Given gradient use standard numeri cal optimisation routine learn parameters In applications particularly natural language processing dimension K vector features f1 fK hundreds thousands This means storage Hessian feasible Newton based training limited memory methods conjugate gradient techniques typically preferred After training use model find likely output sequence novel input x This straightforward y T argmax y1 T t t yt yt x corresponds simple linear chain max product inference yields required result exercise See example small example DRAFT April Applications x1 t x2 t x3 t x1 t x2 t x3 t Figure A Dynamic Bayesian Network Possible transitions variables time slice shown v1 t h1 t h2 t v2 t v1 t h1 t h2 t v2 t Figure A Coupled HMM For example HMM model speech lower cor responding video sequence The upper hidden units correspond phonemes lower mouth positions model captures expected coupling mouth positions phonemes Dynamic Bayesian networks A DBN defined belief network replicated time For multivariate xt dim xt D DBN defines joint model p x1 xT T t D p xi t x t x t x t denotes set variables time t xi t The form p xi t x t x t chosen overall distribution remains acyclic At time step t set variables xi t D observed In order DBN variable xi t parental variables taken set variables previous time slice xt present time slice In applications model temporally homogeneous fully describe distribution terms time slice model fig The generalisation higher order models straightforward A coupled HMM special DBN model coupled streams information example video audio fig Applications Object tracking HMMs track moving objects based understanding dynamics object encoded transition distribution understanding object known position observed encoded emission distribution Given observed sequence hidden position inferred The burglar example case point HMMs applied tracking contexts including tracking people videos musical pitch Automatic speech recognition Many speech recognition systems use HMMs Roughly speaking raw scalar speech signal x1 T translated stream continuous acoustic vectors v1 T time t vt represents frequencies present speech signal small window time t These acoustic vectors typically formed taking discrete Fourier transform speech signal small window time t additional transformations mimic human auditory processing Alternatively related forms linear coding observed acoustic waveform The corresponding discrete latent state ht represents phoneme basic unit human speech standard English Training data painstakingly constructed human linguist DRAFT April Applications determines phoneme ht time t different observed sequences x1 T Given acoustic vector vt associated phoneme ht use maximum likelihood fit mixture usually isotropic Gaussians p vt ht vt This forms emission distribution HMM Using database labelled phonemes phoneme transition p ht ht learned simple count ing forms transition distribution HMM Note case hidden variable h observation v known training training HMM straightforward boils training emission transition distributions independently observed acoustic vectors associated phonemes For new sequence acoustic vectors v1 T use HMM infer likely phoneme sequence time arg maxh1 T p h1 T v1 T takes account way phonemes generate acoustic vectors prior language constraints phoneme phoneme transitions The fact people speak different speeds addressed time warping latent phoneme remains state number timesteps If HMM model single word natural constrain hidden state sequence forwards set available phonemes e revisit state possibly current state In case structure transition matrices corresponds DAG state transition diagram suitable labelling states represented triangular left right transition matrix Bioinformatics In field Bioinformatics HMMs widely applied modelling genetic sequences Multiple sequence alignment forms constrained HMMs particularly successful Other applications involve gene finding protein family modelling Part speech tagging Consider sentence word linguistically tagged hospitality_NN is_BEZ an_AT excellent_JJ virtue_NN _ but_CC not_XNOT when_WRB the_ATI guests_NNS have_HV to_TO sleep_VB in_IN rows_NNS in_IN the_ATI cellar_NN _ The linguistic tags denoted end word example NN singular common noun tag ATI article tag etc Given training set tagged sentences task tag novel sentence One approach use ht tag vt word fit HMM data For training data tags words observed maximum likelihood training transition emission distributions achieved simple counting Given new sequence words likely tag sequence inferred Viterbi algorithm More recent speech taggers tend use conditional random fields input sequence x1 T sentence output sequence y1 T tag sequence One possible parameterisation linear chain CRF use potential form yt yt yt x section factor encodes grammatical structure language second priori likely tag yt Summary Timeseries require simplifying assumptions Markov assumption makes feasible specify model process A discrete state Markov chain generalisation deterministic finite state transitions stochastic tran sitions states Mixtures Markov models extensions thereof simple timeseries clustering models DRAFT April Exercises Hidden Markov models popular timeseries models latent process discrete state Markov chain The observations discrete continuous classical inference tasks filtering smoothing computing joint likely latent sequence computationally straightforward Dynamic Bayes nets essentially structured HMMs conditional independencies encoded transition emission distributions HMMs widespread application variety tracking scenarios speech recognition genetic sequence analysis Undirected linear chain structures conditional random field popular areas natural language processing translating input sequence structured output sequence Code demoMixMarkov m Demo mixture Markov models mixMarkov m Mixture Markov models demoHMMinference m Demo HMM inference HMMforward m Forward recursion HMMbackward m Forward recursion HMMgamma m RTS correction recursion HMMsmooth m Single pairwise smoothing HMMviterbi m Most likely state Viterbi algorithm demoHMMburglar m Demo burglar localisation demoHMMbigram m Demo stubby fingers typing HMMem m EM algorithm HMM Baum Welch demoHMMlearn m Demo EM algorithm HMM Baum Welch demoLinearCRF m Demo learning linear chain CRF linearCRFpotential m Linear CRF potential linearCRFgrad m Linear CRF gradient linearCRFloglik m Linear CRF log likelihood Exercises Exercise A stochastic matrix Mij non negative entries iMij Consider eigenvalue eigenvector e jMijej ei By summing provided ei equal Exercise Consider Markov chain transition matrix M Show Markov chain equilibrium distribution state stationary distribution chain Exercise Consider HMM states M output symbols left right state transition matrix A DRAFT April Exercises Aij p ht ht j emission matrix Bij p vt ht j B initial state probability vector T Given observed symbol sequence v1 Compute p v1 Compute p h1 v1 Find probable hidden state sequence arg maxh1 p h1 v1 Exercise This exercise follows example Given long character string rgenmonleunosbpnntje vrancg typed stubby fingers likely correct English sentence intended In list decoded sequences value log p h1 v1 sequence You need modify demoHMMbigram m suitably Exercise Show HMM transition matrix A emission matrix B initialised uniformly constant values EM algorithm fails update parameters meaningfully Exercise Consider problem finding likely joint output sequence v1 T HMM That v T argmax v1 T p v1 T p h1 T v1 T T t p vt ht p ht ht Explain local message passing algorithm general found problem discuss computational complexity finding exact solution Explain adapt expectation maximisation algorithm form recursive algorithm finding approximate v T Explain approach guarantees improved solution iteration Additionally explain algorithm implemented local message passing Exercise Explain train HMM EM expectation maximisation constrained transition matrix In particular explain learn transition matrix triangular structure Exercise Using correspondence A C G T define transition matrix p produces sequences form A C G T A C G T A C G T A C G T Now define new transition matrix pnew p ones Define transition matrix q produces sequences form T G C A T G C A T G C A T G C A Now define new transition matrix qnew q ones Assume probability initial state Markov chain p h1 constant states A C G T DRAFT April Exercises What probability Markov chain pnew generated sequence S given S A A G T A C T T A C C T A C G C Similarly probability S generated qnew Does sense S higher likelihood pnew compared qnew Using function randgen m generate sequences length Markov chain defined pnew Similarly generate sequences length Markov chain defined qnew Concatenate sequences cell array v v contains sequence v sequence Use MixMarkov m learn Maximum Likelihood parameters generated sequences Assume H kinds Markov chain The result returned phgv indicates posterior probability sequence assignment Do agree solution found Take sequence S defined equation Define emission distribution output states p v h j j j Using emission distribution transition given pnew defined equation adapt demoHMMinferenceSimple m suitably find likely hidden sequence h p generated observed sequence S Repeat computation transition qnew h q Which hidden sequence h p h q preferred Justify answer Exercise Derive algorithm find likely joint state argmax h1 T T t t ht ht arbitrarily defined potentials t ht ht First consider max h1 T T t t ht ht Show maximisation hT pushed inside product result maximisation interpreted message T T hT Derive recursion t t ht max ht t ht ht t t ht Explain recursion enables computation argmax h1 T t t ht ht Explain likely state h1 computed efficiently compute remaining optimal states h2 hT Exercise Derive algorithm compute pairwise marginals p ht ht joint distribution p h1 T T t t ht ht arbitrarily defined potentials t ht ht DRAFT April Exercises First consider h1 hT T t t ht ht Show summation h1 pushed inside product result summation interpreted message h2 h1 h1 h2 Show summation variables h1 t accomplished recursion t t ht ht t ht ht t t ht Similarly push summation hT inside product define T T hT hT T hT hT Show summation variables hT t accomplished recursion t t ht ht t ht ht t t ht Show p ht ht t t ht ht ht t t ht Exercise A second order HMM defined p h1 T v1 T p h1 p v1 h1 p h2 h1 p v2 h2 T t p ht ht ht p vt ht Following similar approach order HMM derive explicitly message passing algorithm compute likely joint state argmax h1 T p h1 T v1 T Exercise Derive algorithm efficiently compute gradient log likelihood HMM assume unconstrained discrete transition emission matrices Exercise Consider HMM defined hidden variables H h1 hT observations V v1 vT p V H p h1 p v1 h1 T t p ht ht p vt ht Show posterior p H V Markov chain p H V p h1 T t p ht ht p ht ht p h1 suitably defined distributions DRAFT April Exercises Exercise For training HMM Gaussian mixture emission HMM GMM model sec tion derive following EM update formulae means covariances newk h N n T t k h t n v n t newk h N n T t k h t n vnt k h vnt k h T k h t n q kt k hnt h q hnt h vn1 T n t q kt k hnt h q hnt h vn1 T Exercise Consider HMM duration model defined equation equation emission distribution p vt ht Our interest derive recursion filtered distribution t ht ct p ht ct v1 t Show t ht ct p vt ht ht ct p ht ht ct p ct ct t ht ct Using derive t ht ct p vt ht ht p ht ht c p ct ct t ht ct ht p ht ht c Dmax ct p c ct t ht ct Show right hand written ht p ht ht ct c p ct c ct t ht I c Dmax ht p ht ht c t ht c Show recursion given t h p vt ht h pdur ht ptran h ht t ht I Dmax p vt ht h ht ptran ht ht t ht c t h c p vt ht h pdur c t h I c Dmax t h c Explain computational complexity filtered inference duration model O TH2Dmax Derive efficient smoothing algorithm duration model DRAFT April Exercises Exercise Fuzzy String Search Consider searching pattern s1 s2 sL longer text v1 v2 vT For example seek find pattern ACGTAA occurs text AAACGTAATAT We model naively HMM L states The idea define generating mechanism places pattern s text possibly multiple times We defining latent state represents emitting pattern position pattern For notation define state ht mean t starting position pattern We define p ht ht p ht L ht denote latent variable h remains non starting point probability jumps starting point pattern s probability The remaining latent state transition defined p ht ht ht ht decrements latent variable deterministically This counter position pattern Given position pattern emission given p vt ht In deterministic case p vt ht vt sht L generally define distribution p vt ht account possible corruption Show inference filtering smoothing Viterbi computed O TL operations deterministic perfect matching fuzzy matching case Note perfect matching case reader referred classical approaches Aho Corasick Boyer Moore algo rithms generally faster For text st find smoothed probability pt pattern starts position t sequence based assuming p vt ht uniform probability mismatch states Assume ht uniform emission distribution Exercise As noted section numerical issue standard recursion ht p vt ht ht p ht ht ht messages typically exponentially small time resulting numerical underflow Whilst addressed computing log messages require special treatment cases messages zero values An alternative log messages renormalise messages stage resulting messages represent filtered distribution p ht v1 t A drawback normalising messages unable directly compute likelihood term p v1 T ht hT However way define normalised messages retain information required compute log likelihood Define local normalisation term zt ht p vt ht p ht ht ht normalised message ht zt p vt ht p ht ht ht z1 h1 p v1 h1 p h1 h1 p v1 h1 h h1 z1 Show ht ht t z log p v1 T T t log zt DRAFT April Exercises DRAFT April CHAPTER Continuous state Markov Models Many physical systems understood continuous variable models undergoing transition de pends state system previous time In chapter discuss classical models area highly specialised retain tractability inference These models found wide variety fields finance signal processing engineering Observed Linear Dynamical Systems In practical timeseries applications data naturally continuous particularly models physical environment In contrast discrete state Markov models chapter parametric continuous state distributions automatically closed operations products marginalisation To practical algorithms inference learning carried efficiently heavily restricted form continuous transition p vt vt A simple powerful class transitions linear dynamical systems A deterministic observed linear dynamical system1 OLDS defines temporal evolution vector vt according discrete time update equation vt Atvt At transition matrix time t For case At invariant t process called stationary time invariant assume explicitly stated A motivation studying OLDSs equations describe physical world written OLDS OLDSs interesting simple prediction models vt describes state environment time t Avt predicts environment time t As models widespread application branches science engineering physics economics The OLDS equation deterministic specify v1 future values v2 v3 defined For dim v V dimensional vector evolution described assuming A diagonalisable vt A t 1v1 P t 1P 1v1 diag V diagonal eigenvalue matrix P corresponding eigenvector matrix A If large t vt explode On hand t tend zero For stable systems require eigenvalues magnitude greater unit eigenvalues contribute long term Note eigenvalues complex 1We use terminology observed LDS differentiate general LDS state space model In texts term LDS applied models discussion chapter Observed Linear Dynamical Systems corresponds rotational behaviour exercise More generally consider additive noise v define stochastic OLDS defined Definition Observed Linear Dynamical System vt Atvt t t noise vector sampled Gaussian distribution N t t t This equivalent order Markov model transition p vt vt N vt Atvt t t At t initial distribution p v1 N v1 For t parameters time independent t At A t process called time invariant Stationary distribution noise Consider dimensional linear system independent additive noise vt avt t t N t v If start state v1 t recursively sample according vt avt t distribution vt Since transitions linear noise Gaussian vt Gaussian Assuming represent distribution vt Gaussian mean t variance t vt N vt t t t vt vt t t v2t avt t a2 v2t 2a vt t 2t 2t a22t 2v vt N vt 22t v Infinite time limit Does distribution vt t tend steady fixed distribution If variance increases indefinitely t mean value For assuming finite variance infinite time case equation stationary distribution satisfies v 2v a2 Similarly mean given Hence mean tends zero variance remains finite Even magnitude vt decreased factor iteration additive noise average boosts magnitude remains steady long run More generally system updating vector vt according non zero additive noise vt Avt t existence steady state require eigenvalues A DRAFT April Auto Regressive Models Figure Fitting order AR model training points The x axis represents time y axis value timeseries The dots represent observations y1 The solid line indicates mean predictions y t t dashed lines y t See demoARtrain m Auto Regressive Models A scalar time invariant auto regressive model defined vt L l alvt l t t N t a1 aL T called AR coefficients called innovation noise The model predicts future based linear combination previous L observations As belief network AR model written Lth order Markov model p v1 T T t p vt vt vt L vi p vt vt vt L N vt L l alvt l Introducing vector L previous observations v t vt vt vt L T write compactly p vt vt vt L N vt Tv t AR models heavily financial time series prediction example able capture simple trends data Another common application area speech processing dimensional speech signal partitioned windows length T L AR coefficients best able describe signal window found These AR coefficients form compressed representation signal subsequently transmitted window original signal The signal approximately reconstructed based AR coefficients This known linear predictive vocoder Note interesting property AR representation signal scale signal constant vt L l alvt l t provided scale noise suitably AR coefficients represent signal AR coefficients degree amplitude invariant DRAFT April Auto Regressive Models Training AR model Maximum likelihood training AR coefficients straightforward based log p v1 T T t log p vt v t T t vt v Tt 1a T log Differentiating w r t equating zero arrive t vt v Tt 1a v t optimally t v t 1v T t t vtv t These equations solved Gaussian elimination The linear system Toeplitz form efficiently solved Levinson Durbin method Similarly optimally T T t vt v Tt 1a Above assume negative timesteps available order notation simple If times window learn coefficients available minor adjustment required start summations t L Given trained future predictions vt v T t Example Fitting trend We illustrate simple example AR models estimate trends underlying timeseries data A order AR model fit set observations shown fig maximum likelihood A prediction mean y t recursively generated y t ai y t t yt t As solid line fig predicted means time t capture underlying trend data Whilst example simple AR models powerful model complex signal behaviour AR model OLDS We write equation OLDS vt vt vt L a1 a2 aL vt vt vt L t vector notation v t Av t t t N t define block matrices A a1 L aL I L L L L In representation component vector updated according standard AR model remaining components copies previous values DRAFT April Auto Regressive Models v1 v2 v3 v4 a1 a2 a3 a4 Figure A time varying AR model latent LDS Since observations known model time varying latent LDS smoothed inference determines time varying AR coefficients Time varying AR model An alternative maximum likelihood view learning AR coefficients problem inference latent LDS model discussed detail section If latent AR coefficients term vt v T t 1at t t N t viewed emission distribution latent LDS hidden variable time dependent emission matrix given v Tt By placing simple latent transition t t N aI encourage AR coefficients change slowly time This defines model fig p v1 T a1 T t p vt v t p Our interest conditional p a1 T v1 T compute posteriori likely sequence AR coefficients Standard smoothing algorithms applied yield time varying AR coefficients demoARlds m Definition Discrete Fourier Transform For sequence x0 N DFT f0 N defined fk N n xne 2i N kn k N fk complex representation frequency k present sequence x0 N The power component k defined absolute length complex fk Definition Spectrogram Given timeseries x1 T spectrogram time t representation frequencies present window localised t For window computes Discrete Fourier Transform obtain vector log power frequency The window moved usually step forward DFT recomputed Note taking logarithm small values original signal translate visibly appreciable values spectrogram Example Nightingale In fig 3a plot raw acoustic recording second fragment nightingale song freesound org sample The spectrogram plotted gives indication frequencies present signal function time The nightingale song complicated locally repetitive A crude way find segments repeat cluster time slices spectrogram In fig 3c results fitting Gaussian mixture model section components repetition components locally time An alternative representation signal given time varying AR coefficients section plotted fig 3d A GMM clustering components fig 3e case produces somewhat clearer depiction different phases nightingale singing afforded spectrogram DRAFT April Auto Regressive Models b c d e Figure The raw recording seconds nightingale song additional background birdsong b Spectrogram Hz c Clustering results panel b component Gaussian mixture model Plotted time vertically distribution cluster indices darker component responsible component spectrogram slice timepoint d The time varying AR coefficients learned 2v 2h ARlds m e Clustering results panel d Gaussian mixture model components The AR components group roughly according different song regimes Time varying variance AR models In standard AR model equation variance assumed fixed time For applications particular finance undesirable volatility change dramatically time A simple extension AR model write vt L l alvt l t t N t t v t L l alvt l 2t Q vt v t The motivation equation represents estimate variance noise based weighted sum squared discrepancies mean prediction v actual observation v previous Q timesteps This called AutoRegressive Conditional Heteroskedasticity ARCH model A extension generalised ARCH GARCH model 2t Q vt v t P t DRAFT April Latent Linear Dynamical Systems v1 v2 vt t v1 v2 vt t b Figure A order L Q ARCH model observations dependent previous observation variance dependent previous observations deterministic manner b An L Q P GARCH model observations dependent previous observation variance dependent previous observations previous variances deterministic manner These special cases general deterministic latent variable models section One interpret deterministic latent variable models section albeit extension higher order Markov process In sense learning parameters maximum likelihood straightforward derivatives log likelihood computed deterministic propagation described section Indeed based insight range possible non linear volatility models come view Latent Linear Dynamical Systems The Latent LDS defines stochastic linear dynamical system latent hidden space sequence vectors h1 T Each observation vt linear function latent vector ht This model called linear Gaussian state space model2 The model considered form LDS joint variables xt vt ht parts vector xt missing For reason refer model Linear Dynamical System latent prefix These models powerful models timeseries use widespread Their latent nature means use latent variable ht track explain observation t The formal definition model given Definition Latent linear dynamical system ht Atht h t h t N ht h t h t transition model vt Btht v t v t N vt v t vt emission model ht v t noise vectors At called transition matrix Bt emission matrix The terms h t v t hidden output bias respectively The transition emission models define order Markov model p h1 T v1 T p h1 p v1 h1 T t p ht ht p vt ht transitions emissions given Gaussian distributions p ht ht N ht Atht h t h t p h1 N h1 p vt ht N vt Btht v t vt 2These models called Kalman Filters We avoid terminology word filter refers specific kind inference runs risk confusing filtering algorithm model DRAFT April Inference v1 v2 v3 v4 h1 h2 h3 h4 Figure A latent LDS Both hidden visible variables Gaussian distributed Figure A single phasor plotted damped dimensional rotation ht Rht damping factor By taking projection y axis phasor generates damped sinusoid The model represented belief network fig extension higher orders intuitive One include external input ot time add Cot mean hidden variable Dot mean observation Explicit expressions transition emission distributions given time invariant case At A Bt B ht h vt v zero biases v t h t Each hidden variable multidimensional Gaussian distributed vector ht transition p ht ht 2h exp ht Aht T 1h ht Aht states ht mean Aht covariance h Similarly p vt ht 2v exp vt Bht T 1v vt Bht describes output vt mean Bht covariance v Example Consider dynamical system defined dimensional vectors ht ht Rht R cos sin sin cos R rotates vector ht angle timestep Under LDS trace points circle h1 ht time By taking scalar projection ht example vt ht Tht elements vt t T describe sinusoid time fig By block diagonal R blkdiag R1 Rm taking scalar projection extended m dimensional ht vector construct representation signal terms m sinusoidal components Hence LDSs represent potentially highly complex period behaviour Inference Given observation sequence v1 T consider filtering smoothing HMM section For HMM deriving message passing recursions inde pendence structure encoded belief network Since LDS independence structure HMM use independence assumptions deriving updates LDS However implementing need deal issue continuous hidden variables discrete states The fact distributions Gaussian means deal continuous DRAFT April Inference messages exactly In translating HMM message passing equations replace summation integration For example filtering recursion p ht v1 t ht p vt ht p ht ht p ht v1 t t Since product Gaussians Gaussian integral Gaussian Gaussian resulting p ht v1 t Gaussian This closure property Gaussians means represent p ht v1 t N ht ft Ft mean ft covariance Ft The effect equation equivalent updating mean ft covariance Ft mean ft covariance Ft p ht v1 t Our task find explicit algebraic formulae updates Numerical stability Translating message passing inference techniques developed HMM LDS largely straightforward Indeed simply run standard sum product algorithm albeit continuous variables demoSumprodGaussCanonLDS m In long timeseries numerical instabilities build result grossly inaccurate results depending transition emission distribution parameters method implementing message updates For reason specialised routines developed numerically stable certain parameter regimes For HMM section discussed alternative methods smoothing parallel approach sequential approach The recursion suitable emission transition covariance entries large recursion preferable standard case small covariance values Analytical shortcuts In deriving inference recursions need frequently multiply integrate Gaussians Whilst prin ciple straightforward algebraically tedious possible useful appeal known shortcuts For example exploit general result linear transform Gaussian random variable Gaussian random variable Similarly convenient use conditioning formulae dynamics reversal intuition These results stated section derive useful purposes This explain equations filtering derived Consider linear transformation Gaussian random variable y Mx N x N x x x x assumed generated independent processes To find distribution p y approach write formally p y N y Mx N x x x dx carry integral completing square However Gaussian variable linear transformation Gaussian shortcut find mean covariance transformed variable It is mean given y M x Mx To find covariance p y consider displacement variable x mean write x x x The covariance definition x xT For y displacement y M x DRAFT April Inference Algorithm LDS Forward Pass Compute filtered posteriors p ht v1 t N ft Ft LDS parameters t A B h v h v t The log likelihood L log p v1 T returned f1 F1 p1 LDSFORWARD v1 L log p1 t T ft Ft pt LDSFORWARD ft Ft vt t L L log pt end function ldsforward f F v h Af h v Bh v Mean p ht vt v1 t hh AFAT h vv BhhBT v vh Bhh Covariance p ht vt v1 t f h Tvh 1vv v v F hh Tvh 1vv vh Find p ht v1 t conditioning p exp v v T 1vv v v det 2vv Compute p vt v1 t return f F p end function So covariance y yT M x M x T M x xT MT M x T xT MT T Since noises x assumed independent xT y MxM T Filtering We represent filtered distribution Gaussian mean ft covariance Ft p ht v1 t N ht ft Ft This called moment representation Our task find recursion ft Ft terms ft Ft A convenient approach find joint distribution p ht vt v1 t condition vt find distribution p ht v1 t The term p ht vt v1 t Gaussian statistics found relations vt Bht v t ht Aht h t Using assuming time invariance zero biases readily find ht h T t v1 t AFt 1A T h vt h T t v1 t B AFt 1A T h vt v T t v1 t B AFt 1A T h BT v vt v1 t BA ht v1 t ht v1 t A ht v1 t In moment representation forward messages ht v1 t ft ht h T t v1 t Ft Then conditioning3 p ht vt v1 t mean ft ht v1 t ht v T t v1 t vt v T t v1 t vt vt v1 t 3p x y Gaussian mean x xy yy y y covariance xx xy 1yy yx DRAFT April Inference covariance Ft ht h T t v1 t ht v T t v1 t vt v T t v1 t vt h T t v1 t Writing explicitly mean covariance ft Aft PB T BPBT v vt BAft Ft P PBT BPBT v BP P AFt 1AT h The recursion initialised f0 F0 The filtering procedure presented algorithm single update LDSforwardUpdate m One write covariance update Ft I KB P define Kalman gain matrix K PBT V BPB T Symmetrising updates A potential numerical issue covariance update difference positive definite matrices If numerical errors Ft numerically positive definite sym metric Using Woodbury identity definition A equation written compactly Ft P BT 1v B Whilst positive semidefinite numerically expensive involves matrix inversions An alternative use definition K write KvK T I KB PBTKT Hence arrive Joseph s symmetrized update Ft I KB P I KB T KvKT The right hand addition positive definite matrices resulting update covariance numerically stable A similar method backward pass An alternative avoid covariance matrices directly use square root parameter deriving updates instead Prediction From dynamics definition distribution future visible variable given propagating filtered distribution forwards timestep written time independent case p vt v1 t ht ht p vt ht p ht ht p ht v1 t N vt B Aft h v B AFtA T h BT v DRAFT April Inference Smoothing Rauch Tung Striebel correction method The smoothed posterior p ht v1 T necessarily Gaussian conditional marginal larger Gaussian By representing posterior Gaussian mean gt covariance Gt p ht v1 T N ht gt Gt form recursion gt Gt follows p ht v1 T ht p ht ht v1 T ht p ht v1 T ht p ht v1 T ht p ht v1 t ht p ht v1 T The term p ht v1 t ht found conditioning joint distribution p ht ht v1 t p ht ht v1 t p ht v1 t obtained usual manner finding mean covariance The term p ht v1 t known Gaussian filtering mean ft covariance Ft Hence joint distribution p ht ht v1 t means ht v1 t ft ht v1 t Aft covariance elements ht h T t v1 t Ft ht h T t v1 t FtA T ht h T t v1 t AFtA T h To find p ht v1 t ht use conditioned Gaussian results result However turn useful use system reversal result section interprets p ht v1 t ht equivalent linear system going backwards time ht Atht mt t At ht h T t v1 t ht h T t v1 t mt ht v1 t ht h T t v1 t ht h T t v1 t ht v1 t t N t t t ht h T t v1 t ht h T t v1 t ht h T t v1 t ht h T t v1 t Using dynamics reversal equation assuming ht Gaussian distributed straightforward work statistics p ht v1 T The mean given gt ht v1 T At ht v1 T mt Atgt mt covariance Gt ht h T t v1 T At ht h T t v1 T ATt t AtGt ATt t This procedure Rauch Tung Striebel Kalman smoother This called correction method takes filtered estimate p ht v1 t corrects form smoothed estimate p ht v1 T The procedure outlined algorithm detailed LDSbackwardUpdate m See LDSsmooth m DRAFT April Inference Algorithm LDS Backward Pass Compute smoothed posteriors p ht v1 T This requires filtered results algorithm GT FT gT fT t T gt Gt LDSBACKWARD gt Gt ft Ft t end function ldsbackward g G f F h Af h h h AFAT h h h AF Statistics p ht ht v1 t F Th h 1h h h h A Th h 1h h m f Ah Dynamics Reversal p ht ht v1 t g Ag m G AG AT Backward propagation return g G end function Sampling trajectory p h1 T v1 T One use equation draw sample trajectory h1 T posterior p h1 T v1 T We start drawing sample hT Gaussian posterior p hT v1 T N hT fT FT Given value write hT AT 1hT mT T By drawing sample zero mean Gaussian N T T value hT We continue way reversing backwards time build sample trajectory hT hT hT h1 This equivalent forward filtering backward sampling approach described section The cross moment An advantage dynamics reversal interpretation given cross moment required learning immediately obtained ht h T t v1 T AtGt hth T t v1 T AtGt gtg T t The likelihood For discrete HMM section showed likelihood computed directly filtered messages hT hT We apply technique message distributions passing conditional distributions p ht v1 t joint distributions p ht v1 t However help hand compute likelihood decomposition p v1 T T t p vt v1 t p vt v1 t N vt t t B BBT t t BAft t B AFt 1A T h BT v t The log likelihood given log p v1 T T t vt t T 1t vt t log det 2t DRAFT April Inference Most likely state Since mode Gaussian equal mean difference probable joint posterior state argmax h1 T p h1 T v1 T set probable marginal states ht argmax ht p ht v1 T t T Hence likely hidden state sequence equivalent smoothed mean sequence Time independence Riccati equations Both filtered Ft smoothed Gt covariance recursions independent observations v1 T depending parameters model This general characteristic linear Gaussian systems Typically covariance recursions converge quickly values constant dynamics appreciable differences close boundaries t t T In practice drops time dependence covariances approximates single time independent covariance This approximation dramatically reduces storage requirements The converged filtered F satisfies recursion F AFAT h AFAT h BT B AFAT h BT v B AFAT h form algebraic Riccati equation A technique solve equations initialise covariance F With new F found right hand subsequently recursively updated Alternatively Woodbury identity converged covariance satisfies F AFAT h BTv 1B form numerically convenient forming iterative solver F requires matrix inversions Example Newtonian Trajectory Analysis A toy rocket unknown mass initial velocity launched air In addition constant accelerations rocket s propulsion system unknown It known Newton s laws apply instrument noisy measurements horizontal distance x t vertical height y t rocket time t Based noisy measurements task infer position rocket time Although appropriately considered continuous time dynamics translate discrete time approximation Newton s law states d2 dt2 x fx t m d2 dt2 y fy t m m mass object fx t fy t horizontal vertical forces respectively As stand equations form directly usable LDS framework A naive approach reparameterise time use variable t t t t integer unit time The dynamics x t x t x t y t y t y t y t dy dt We write update equation x y x t x t fx t m y t y t fx t m DRAFT April Learning Linear Dynamical Systems x y Figure Estimate trajectory Newtonian ballistic object based noisy observations small cir cles All time labels known omitted plot The x points true positions ob ject crosses estimated smoothed mean positions xt yt v1 T object plotted time steps See demoLDStracking m These sets coupled discrete time difference equations approximate Newton s law equation For simplicity relabel ax t fx t m t ay t fy t m t These accelerations unknown assumed change slowly time ax t ax t x ay t ay t y x y small noise terms The initial distributions accelerations assumed vague zero mean Gaussian large variance We describe model defining ht x t x t y t y t ax t ay t T hidden variable giving rise H dimensional LDS transition emission matrices A B We use large covariance state little known latent state initial values Based noisy observations vt Bht t attempt infer unknown trajectory smoothing A demonstration given fig Despite significant observation noise object trajectory accurately inferred Learning Linear Dynamical Systems Whilst applications particularly underlying known physical processes parameters LDS known machine learning tasks need learn parameters LDS based v1 T For simplicity assume know dimensionality H LDS Identifiability issues An interesting question uniquely identify learn parameters LDS There trivial redundancies solution obtained permuting hidden variables arbitrarily flipping signs To potentially equivalent solutions consider following LDS vt Bht v t ht Aht h t We attempt transform original system new form produce exactly outputs v1 T For invertible matrix R consider Rht RAR 1Rht R h t DRAFT April Learning Linear Dynamical Systems representable new latent dynamics h t A h t h t A RAR h t Rht ht Rht In addition reexpress outputs function transformed h vt BR 1Rht v t B h t v t Hence provided place constraints A B h exists infinite space equivalent solutions A RA R B BR H RhR T likelihood value This means interpreting learned parameters needs care EM algorithm For simplicity assume single sequence v1 T wish fit LDS maximum likelihood Since LDS contains latent variables approach use EM algorithm As usual M step EM algorithm requires maximise energy log p v1 T h1 T pold h1 T v1 T respect parameters A B v h Thanks form LDS energy decomposes log p h1 pold h1 v1 T T t log p ht ht pold ht ht v1 T T t log p vt ht pold ht v1 T It straightforward derive M step parameters given angled brackets denote expectation respect smoothed posterior pold h1 T v1 T new h1 new h1h T h1 h1 T Anew T t ht 1h T t T t hth T t Bnew T t vt ht T T t hth T t newv T T t vtv T t vt ht T BnewT Bnew ht vTt Bnew hth T t BnewT newh T T t ht 1h T t Anew hth T t ht 1h T t AnewT Anew hth T t AnewT The simplified newv T t vtv T t vt ht T BnewT Similarly newh T T t ht 1h T t Anew hth T t The statistics required include smoothed means covariances cross moments The extension learning multiple timeseries straightforward energy simply summed individual DRAFT April Learning Linear Dynamical Systems sequences The performance EM algorithm LDS depends heavily initialisation If remove hidden hidden links model closely related Factor Analysis LDS considered temporal extension Factor Analysis One initialisation technique learn B matrix Factor Analysis treating observations temporally independent Note whilst LDS model identifiable described section M step EM algorithm unique This apparent contradiction resolved considers EM algorithm conditional method updating depending previous parameters ultimately initialisation It initialisation breaks invariance Subspace Methods An alternative maximum likelihood training use subspace method The chief benefit techniques avoid convergence difficulties EM To motivate subspace techniques consider deterministic LDS ht Aht vt Bht Under assumption vt Bht BAht generally vt BA t 1h1 This means H dimensional system underlies visible information points Ath1 lie H dimensional subspace projected form observation This suggests form subspace identification technique enable learn A B Given set observation vectors v1 vt consider block Hankel matrix formed stacking L consecutive observation vectors For example T L M v1 v2 v3 v4v2 v3 v4 v5 v3 v4 v5 v6 If v generated noise free LDS write M Bh1 Bh2 Bh3 Bh4BAh1 BAh2 BAh3 BAh4 BA2h1 BA 2h2 BA 2h3 BA 2h4 BBA BA2 h1 h2 h3 h4 We find SVD M M U S V T W W termed extended observability matrix The matrix S contain singular values dimension hidden variables H remaining singular values From equation means emission matrix B contained U V H The estimated hidden variables contained submatrix W1 H T L h1 h2 h3 h4 W1 H T L Based relation ht Aht find best squares estimate A minimising T t ht Aht optimal solution A h2 h3 ht h1 h2 hT DRAFT April Switching Auto Regressive Models v1 v2 v3 v4 s1 s2 s3 s4 Figure A order switching AR model In terms inference conditioned v1 T HMM denotes pseudo inverse LDSsubspace m Estimates covariance matrices obtained residual errors fitting block Hankel matrix extended observability matrix Whilst derivation formally holds noise free case apply case non zero noise hope gain estimate A B correct mean In addition forming solution right subspace method forms potentially useful way initialise EM algorithm Structured LDSs Many physical equations local time space For example weather models atmosphere partitioned cells hi t containing pressure location The equations describing pressure updates depend pressure current cell small number neighbouring cells previous time t If use linear model measure aspects cells time weather describable LDS highly structured sparse transition matrix A In practice weather models non linear local linear approximations employed A similar situation arises brain imaging voxels local cubes activity depend neighbours previous timestep Another application structured LDSs temporal independent component analysis This defined discovery set independent latent dynamical processes data projected observation If independent dynamical process described LDS gives rise structured LDS block diagonal transition matrix A Such models extract inde pendent components prior knowledge likely underlying frequencies temporal compoments Bayesian LDSs The extension placing priors transition emission parameters LDS leads general computational difficulties computing likelihood For example prior A likelihood p v1 T A p v1 T A p A difficult evaluate dependence likelihood matrix A complicated function Approximate treatments case scope book briefly note sampling methods popular context addition deterministic variational approximations Switching Auto Regressive Models Whilst linear dynamical models considered far chapter powerful inherent restrictions For example model abrupt changes observation Here describe extension AR model We consider set S different AR models associated coefficients s s S allow model select AR models time For time series scalar values v1 T L th order switching AR model written vt v T t 1a st t t N t st set AR coefficients s s s S The discrete switch variables Markov transition p s1 T t p st st model fig p v1 T s1 T t p vt vt vt L st p st st DRAFT April Switching Auto Regressive Models sample switches learned switches Figure Learning Switching AR model The upper plot shows train data The colour indicates AR models active time Whilst information plotted assumed unknown learning algo rithm coefficients s We assume order L num ber switches S known In plot time se ries training colour points according likely smoothed AR model timestep See demoSARlearn m Inference Given observed sequence v1 T parameters inference straightforward form HMM To apparent write p v1 T s1 T t p vt st p st st p vt st p vt vt vt L st N vt v T t 1a st st Note emission distribution p vt st time dependent The filtering recursion st st p vt st p st st st Smoothing achieved standard recursions modified use time dependent emissions demoSARinference m With high frequency data unlikely change switch variable reasonable time t A simple constraint account use modified transition p st st p st st mod t Tskip st st Maximum likelihood learning EM To fit set AR coefficients innovation variances s s s S maximum likelihood training set data v1 T use EM algorithm M step Up negligible constants energy given E t log p vt v t st pold st v1 T t log p st st pold st st need maximise respect parameters Using definition emission isolating dependency 2E t st vt v Tt 1a st log st pold st v1 T const DRAFT April Switching Auto Regressive Models On differentiating respect s equating zero optimal s satisfies linear equation t pold st s v1 T vtv t s t pold st s v1 T v t 1v T t s s solved Gaussian elimination Similarly updates maximise energy respect s t p old s t s v1 T t pold st s v1 T vt v Tt 1a s The update p st st follows standard EM HMM rule equation SARlearn m Here don t include update prior p s1 insufficient information start sequence assume p s1 flat E step The M step requires smoothed statistics pold st s v1 T pold st s st s v1 T obtained HMM inference Example Learning switching AR model In fig train data generated Switching AR model know ground truth model generated parts data Based train data assuming labels st unknown Switching AR model fitted EM In case problem straightforward good estimate obtained sets AR parameters switches time Example Modelling parts speech In fig segment speech signal corresponding spoken digit shown We model data switching AR model S states Each available AR models responsible modelling dynamics basic subunit speech The model trained example clean spoken digit sequences S states left right transition matrix An additional complexity wish use model clean noisy speech signal A simple model noise Gaussian additive original speech signal vt forming new noisy observation v t Based sequence noisy v T wish infer switch states clean signal v1 T Unfortunately task longer form SAR continuous discrete latent states Formally model falls class switching linear dynamical systems described following chapter The results complex model denoising complex signals achieved prelude advanced material following chapter Summary Continuous observations modelled autoregressive models Observed linear dynamical systems vector versions autoregressive models Latent continuous dynamical processes model physical systems In order computationally tractable needs restrict transition emission distributions The latent linear dynamical system restriction linear Gaussian transitions emissions The latent linear dynamical system powerful timeseries model widespread application tracking signal representation DRAFT April Code v v v v v1 v2 v3 v4 s1 s2 s3 s4 Figure A latent switching second order AR model Here st indicates set available AR models active time t The square nodes emphasise discrete variables The clean AR signal vt observed corrupted additive noise form noisy observations v t In terms inference conditioned v T expressed Switching LDS chapter b Signal reconstruction latent switching AR model Top noisy signal v T reconstructed clean signal v1 T The dashed lines numbers likely state segmentation arg maxs1 T p s1 T v T states left right Code In Linear Dynamical System code simplest form recursions given No attempt ensure numerical stability LDSforwardUpdate m LDS forward LDSbackwardUpdate m LDS backward LDSsmooth m Linear dynamical system filtering smoothing LDSforward m Alternative LDS forward algorithm SLDS chapter LDSbackward m Alternative LDS backward algorithm SLDS chapter demoSumprodGaussCanonLDS m Sum product algorithm smoothed inference demoLDStracking m Demo tracking Newtonian system LDSsubspace m Subspace Learning Hankel matrix method demoLDSsubspace m Demo subspace learning method Autoregressive models Note code autoregressive vector entry AR coefficient e reverse order presented text ARtrain m Learn AR coefficients Gaussian Elimination demoARtrain m Demo fitting AR model data ARlds m Learn time varying AR coefficients LDS demoARlds m Demo learning AR coefficients LDS demoSARinference m Demo inference switching autoregressive model SARlearn m Learning SAR EM demoSARlearn m Demo SAR learning HMMforwardSAR m Switching autoregressive HMM forward pass HMMbackwardSAR m Switching autoregressive HMM backward pass DRAFT April Exercises Exercises Exercise Consider dimensional linear model ht Rht R cos sin sin cos R rotation matrix rotates vector ht angle timestep Explain eigenvalues rotation matrix general imaginary Explain model sinusoid rotating angular velocity dimensional latent LDS By writing xt yt R11 R12 R21 R22 xt yt eliminate yt write equation xt terms xt xt Explain model sinusoid AR model Explain relationship second order differential equation x x describes Harmonic Oscillator second order difference equation approximates differential equation Is possible find difference equation exactly matches solution differential equation particular points Exercise Show square anti symmetric matrix M M MT matrix exponential MATLAB expm A exp M orthogonal ATA I Explain construct random orthogonal matrices control angles complex eigenvalues Exercise Run demoLDStracking m tracks ballistic object Linear Dynamical system example Modify demoLDStracking m addition x y positions x speed observed Compare contrast accuracy tracking extra information Exercise nightsong mat contains small stereo segment nightingale song sampled Hertz Plot original waveform plot x Plot spectrogram y myspecgram x imagesc log abs y The routine demoGMMem m demonstrates fitting mixture Gaussians data The mixture assign ment probabilities contained phgn Write routine cluster data v log abs y Gaussian components explain segment series x different regions Examine demoARlds m fits autoregressive coefficients interpretation Linear Dy namical System Adapt routine demoARlds m learn AR coefficients data x You certainly need subsample data x example taking 4th datapoint With learned AR coefficients use smoothed results fit Gaussian mixture components Compare contrast results obtained Gaussian mixture model fit spectrogram DRAFT April Exercises Exercise Consider supervised learning problem linear model scalar output yt based vector input xt yt w T t xt y t y t zero mean Gaussian noise variance y Train data D xt yt t T available For time invariant weight vector wt w explain find single weight vector w noise variance 2y maximum likelihood Extend model include transition wt wt w t wt zero mean Gaussian noise given covariance w w1 zero mean Explain cast finding wt D smoothing related Linear Dynamical System Write routine W LinPredAR X Y SigmaW SigmaY takes input data matrix X x1 xT column contains input vector Y y1 yT T SigmaW additive weight noise SigmaY assumed known time invariant output noise The returned W contains smoothed mean weights Exercise This exercise relates forming style smoothing approach described sec tion applied LDS Note derivations section hold continuous vari ables simply replacing summation integration By virtue fact smoothed posterior LDS ht p ht v1 T Gaussian relation ht ht ht explain message LDS represented form ht ztexp hTt Ztht h T t zt Zt necessarily rank matrix Based recursion ht ht p vt ht p ht ht ht derive recursion ignoring prefactor zt Lt Q t B T t R t Bt Zt Zt A T t Q 1t Q 1t L 1t Q 1t At zt A T t Q t L t BTt R t zt initialisation ZT zT The notation Qt ht covariance matrix transition distribution p ht ht Rt vt covariance emission p vt ht Show posterior covariance mean given F 1t Zt F 1t Zt F 1t ft zt Ft ft filtered mean covariance Note parallel smoothing recursion appropriate case small covariance explicit appearance inverse covariances numerical stability issues arise However possible reexpress recursion explicit reference inverse noise covariances DRAFT April Exercises DRAFT April CHAPTER Switching Linear Dynamical Systems Hidden Markov models assume underlying process discrete linear dynamical systems underlying process continuous However scenarios underlying system jump continuous regime In chapter discuss class models situation Unfortunately technical demands class models somewhat involved previous chapters models correspondingly powerful Introduction Complex timeseries described globally single Linear Dynamical System divided segments modelled potentially different LDS Such models handle situations underlying model jumps parameter setting For example single LDS represent normal flows chemical plant When break pipeline occurs dynamics system changes set linear flow equations This scenario modelled set linear systems different parameters The discrete latent variable time st normal pipe broken indicates LDSs appropriate current time This called Switching LDS disciplines econometrics machine learning The Switching LDS At time t switch variable st S describes set LDSs The continuous observation visible variable vt dim vt V linearly related continuous hidden variable ht dim ht H vt B st ht v st v st N v st v st v st Here st describes set emission matrices B B S active time t The observation noise v st drawn Gaussian mean v st covariance v st The transition dynamics continuous hidden state ht linear ht A st ht h st h st N h st h st h st switch variable st selects single transition matrix available set A A S The Gaussian transition noise h st depends switch variable The dynamics st Markovian transition p st st For general augmented aSLDS model switch st dependent Gaussian Sum Filtering s1 h1 v1 s2 h2 v2 s3 h3 v3 s4 h4 v4 Figure The independence structure aSLDS Square nodes st denote discrete switch variables ht continuous latent hidden variables vt continuous ob served visible variables The discrete state st determines Linear Dynamical system finite set Linear Dynamical systems operational time t In SLDS links h s normally considered previous st ht The model defines joint distribution fig p v1 T h1 T s1 T T t p vt ht st p ht ht st p st ht st p vt ht st N vt v st B st ht v st p ht ht st N ht h st A st ht h st At time t p s1 h0 s0 denotes initial switch distribution p s1 p h1 h0 s1 denotes initial Gaussians p h1 s1 N h1 s1 s1 The SLDS thought marriage hidden Markov model Linear Dynamical System The SLDS called Jump Markov model process switching Kalman Filter Switching Linear Gaussian State Space model Conditional Linear Gaussian Model Exact inference computationally intractable Both exact filtered smoothed inference SLDS intractable scaling exponentially time As informal explanation consider filtered posterior inference analogy equation forward pass p st ht v1 t st ht p st ht st ht vt p st ht v1 t At timestep p s1 h1 v1 p h1 s1 v1 p s1 v1 indexed set Gaussians At timestep summation states s1 p s2 h2 v1 indexed set S Gaussians similarly timestep S2 general gives rise St Gaussians time t Even small t number components required exactly represent filtered distribution computationally intractable Analogously smoothing intractable The origin intractability SLDS differs struc tural intractability ve previously encountered In SLDS terms cluster variables x1 T xt st ht visible variables v1 T graph distribution singly connected From purely graph theoretic viewpoint envisage little difficulty carrying inference Indeed saw derivation filtering algorithm straightforward graph singly connected However numerical implementation algorithm intractable description messages requires exponentially increasing number terms In order deal intractability approximation schemes introduced Here focus techniques approximate switch conditional posteriors limited mixture Gaussians Since exact posterior distributions mixtures Gaussians albeit exponentially large number components aim drop low weight components resulting approximation accurately represents posterior Gaussian Sum Filtering Equation describes exact filtering recursion generates exponentially increasing number components time In general influence ancient observations relevant DRAFT April Gaussian Sum Filtering recent observations This suggests effective time limited cor responding limited number components Gaussian mixture suffice accurately represent filtered posterior Our aim form recursion p st ht v1 t based Gaussian mixture approxi mation p ht st v1 t Given approximation filtered distribution p st ht v1 t q st ht v1 t exact recursion equation approximated q st ht v1 t st ht p st ht st ht vt q st ht v1 t This approximation filtered posterior timestep contain S times components previous timestep prevent exponential explosion mixture components need subsequently collapse mixture q st ht v1 t suitable way It useful break filtered approximation equation continuous discrete parts q ht st v1 t q ht st v1 t q st v1 t derive separate filtered update formulae described Continuous filtering The exact representation p ht st v1 t mixture O St components To retain computational feasibility approximate limited I component mixture q ht st v1 t I q ht st v1 t q st v1 t q ht st v1 t Gaussian parameterised mean f st covariance F st Strictly speaking use notation ft st time t set means indexed st drop dependencies notation An important remark techniques approximate p ht st v1 t single Gaussian state st Naturally gives rise mixture Gaussians p ht v1 t st p ht st v1 t p st v1 t However making single Gaussian approximation p ht st v1 t representation posterior poor Our aim maintain accurate approximation p ht st v1 t mixture Gaussians To find recursion approximating distribution assume know filtered approxi mation q ht st v1 t propagate forwards exact dynamics To consider relation q ht st v1 t st q ht st st v1 t st q ht st st v1 t q st st v1 t Wherever possible substitute exact dynamics evaluate factors The usefulness decomposing update way new filtered approximation form Gaussian mixture q ht st st v1 t Gaussian q st st v1 t weights mixing proportions components We describe compute terms explicitly Equa tion produces new Gaussian mixture I S components collapse I components end computation Evaluating q ht st st v1 t We aim find filtering recursion q ht st st v1 t Since conditional switch states components corresponds single LDS forward step evaluated considering joint distribution q ht vt st st v1 t ht p ht vt ht st st v1 t q ht st st v1 t DRAFT April Gaussian Sum Filtering subsequently conditioning vt In use exact dynamics possible To ease burden notation derive h t v t t The exact forward dynamics given ht A st ht h st vt B st ht v st Given mixture component index q ht v1 t st N ht f st F st propagate Gaussian exact dynamics equation Then q ht vt st st v1 t Gaussian covariance mean elements hh A st F st A T st h st vv B st hhB T st v st vh B st hh T hv v B st A st f st h A st f st These results obtained integrating forward dynamics equations ht result To find q ht st st v1 t condition q ht vt st st v1 t vt standard Gaussian conditioning formulae result obtain q ht st st v1 t N ht h v h v h v h hv vv vt v h v hh hv 1vv vh quantities required defined equation Evaluating mixture weights q st st v1 t Up normalisation constant mixture weight equation found q st st v1 t q vt st st v1 t q st st v1 t q st v1 t q st v1 t The factor equation q vt st st v1 t Gaussian mean v covariance vv given equation The factors q st v1 t q st v1 t given previous filtered iteration Finally q st st v1 t found q st st v1 t p st ht st q ht st v1 t augmented SLDS p st st standard SLDS In aSLDS term equation generally need computed numerically A sim ple approximation evaluate equation mean value distribution q ht st v1 t To covariance information account alternative draw samples Gaussian q ht st v1 t approximate average p st ht st sampling Note equate Gaussian Sum filtering augmented SLDS sequential sampling procedure Particle Filtering section The sampling exact convergence issues arise Closing recursion We position calculate equation For setting variable st mixture I S Gaussians To prevent number components increasing exponentially time numerically collapse q ht st v1 t I Gaussians form q ht st v1 t I q ht st v1 t q st v1 t The numerical collapse generates new Gaussian components corresponding mixture weights Any method choice supplied collapse mixture smaller mixture A straightforward approach repeatedly merge low weight components explained section In way new mixture coefficients q st v1 t I defined This completes description form recursion continuous filtered posterior approximation q ht st v1 t equation DRAFT April Gaussian Sum Filtering t 1t Figure Gaussian Sum Filtering The leftmost column depicts previous Gaussian mixture approximation q ht v1 t states S red blue mixture components I mixture weight represented area oval There S different linear systems components mixture new filtered state colour arrow indicating dynamic system After time step mixture component branches S components joint approximation q ht st v1 t contains S2I components middle column To representation computationally tractable mixture Gaussians state st collapsed I components This means coloured set Gaussians needs approximated smaller I component mixture Gaussians There ways achieve A naive computationally efficient approach simply ignore lowest weight components depicted right column mix2mix m Discrete filtering A recursion switch variable distribution equation q st v1 t st q st st vt v1 t The r h s equation proportional st q vt st st v1 t q st st v1 t q st v1 t q st v1 t terms computed recursion q ht st v1 t We quantities required compute Gaussian Sum approximation filtering forward pass A schematic representation Gaussian Sum filtering given fig pseudo code presented algorithm See SLDSforward m The likelihood p v1 T The likelihood p v1 T found p v1 T T t p vt v1 t p vt v1 t st st q vt st st v1 t q st st v1 t q st v1 t q st v1 t In expression terms computed forming recursion filtered posterior q ht st v1 t Collapsing Gaussians A central filtering recursion collapse mixture Gaussians smaller number Gaussians That given mixture N Gaussians p x N piN x wish collapse smaller K N mixture Gaussians We describe simple method advantage computational efficiency disadvantage spatial information DRAFT April Gaussian Sum Smoothing Algorithm aSLDS Forward Pass Approximate filtered posterior p st v1 t t p ht st v1 t wt st N ht ft st Ft st Also return approximate log likelihood L log p v1 T It number components Gaussian mixture approximation We require I1 I2 S It S It s A s B s h s v s h s v s The routine LDSFORWARD found algorithm s1 S f1 s1 F1 s1 p LDSFORWARD v1 s1 p s1 p end t T st S It s S x y s x y s p LDSFORWARD ft s Ft s vt st p st s p st ht st s p ht st s v1 t p st s wt s p st s t s p end Collapse It S mixture Gaussians defined x y x y weights p s st p st s Gaussian It components p ht st v1 t It p st v1 t p ht st v1 t This defines new means ft st covariances Ft st mixture weights wt st p st v1 t Compute t st s p st s end normalise t L L log st s p st s end mixture First describe collapse mixture single Gaussian This achieved finding mean covariance mixture distribution These pii pi T T To collapse mixture K component mixture retain K Gaussians largest mixture weights The remaining N K Gaussians simply merged single Gaussian method Alternative heuristics recursively merging Gaussians lowest mixture weights reasonable More sophisticated methods retain spatial information clearly potentially useful The method presented suitable approach considers removing Gaussians spatially similar low weight components retaining sense diversity possible solu tions In applications thousands timesteps speed factor determining method collapsing Gaussians preferred Relation methods Gaussian Sum Filtering considered form analytical particle filtering section instead point distributions delta functions propagated Gaussians propagated The collapse operation smaller number Gaussians analogous resampling Particle Filtering Since Gaussian expressive delta function Gaussian Sum filter generally improved approximation technique point particles See numerical comparison Gaussian Sum Smoothing Approximating smoothed posterior p ht st v1 T involved filtering requires additional approximations For reason smoothing prone failure assumptions need satisfied approximations hold The route assume Gaussian Sum DRAFT April Gaussian Sum Smoothing filtered approximation carried approximate backward pass analogous section By analogy RTS smoothing recursion equation exact backward pass SLDS reads p ht st v1 T st ht p ht st ht st v1 t p ht st v1 T p ht st v1 T p st v1 T p ht st v1 T composed discrete continuous com ponents smoothed posterior time step The recursion runs backwards time beginning initialisation p hT sT v1 T set filtered result time t T filtered smoothed posteriors coincide Apart fact number mixture components increase step computing integral ht equation problematic conditional distribution term non Gaussian ht For reason useful derive approximate recursion beginning exact relation p ht st v1 T st p st v1 T p ht st st v1 T p st st v1 T expressed directly terms SLDS dynamics p ht st v1 T st p st v1 T p ht ht st st v1 t vt T p ht st st v1 T p st ht st v1 T p ht st v1 T In forming recursion assume access distribution p ht st v1 T future timestep However require distribution p ht st st v1 T directly known needs inferred computationally challenging task In Expectation Correction EC approach assumes approximation fig p ht st st v1 T p ht st v1 T resulting approximate recursion smoothed posterior p ht st v1 T st p st v1 T p ht ht st st v1 t ht p st ht st v1 T ht ht represents averaging respect distribution p ht st v1 T In carrying approximate recursion end mixture Gaussians grows timestep To avoid exponential explosion problem use finite mixture approximation q ht st v1 T p ht st v1 T q ht st v1 T q ht st v1 T q st v1 T plug approximate recursion From equation recursion approximation given q ht st v1 T st q st v1 T q ht ht st st v1 t q ht st v1 T q ht st st v1 T q st ht st v1 t q ht st v1 T q st st v1 T As filtering possible replace approximate terms exact counterparts parame terise posterior q ht st v1 T q ht st v1 T q st v1 T To reduce notational burden outline method case single component approximation forward backward passes The extension mixture approximate p ht st v1 T conceptually straightforward deferred section In single Gaussian case assume Gaussian approximation available q ht st v1 T N ht g st G st DRAFT April Gaussian Sum Smoothing st ht vt st ht vt st ht vt st ht vt Figure The EC backpass approximates p ht st st v1 T p ht st v1 T The moti vation st influences ht indirectly ht However ht likely heavily fluenced v1 t knowing state st likely secondary importance The green shaded node variable wish find pos terior The values blue shaded nodes known red shaded node indicates known variable assumed unknown forming ap proximation Continuous smoothing For given st st RTS style recursion smoothed continuous distribution obtained equation giving q ht st st v1 T ht p ht ht st st v1 t q ht st v1 T To compute equation perform single update LDS backward recursion sec tion Discrete smoothing The second average equation corresponds recursion discrete variable given q st ht st v1 t q ht st v1 T q st st v1 T The average q st ht st v1 t respect q ht st v1 T achieved closed form A simple approach approximate average evaluation mean1 q st ht st 1v1 t q ht st v1 T q st ht st v1 t ht ht st v1 T ht st v1 T mean ht respect q ht st v1 T Replacing ht mean gives approximation q st ht st v1 t q ht st v1 T Z e zTt st st st st v1 t zt st st det st st v1 t q st st v1 t zt st st ht st v1 T ht st st v1 t Z ensures normalisation st st st v1 t filtered covariance ht given st st observations v1 t taken hh equation Approximations covariance information account considered simple fast method suffice practice Collapsing mixture From section section terms equation compute approximation equation Due summation st equation number mixture components multiplied S iteration To prevent exponential explosion components mixture equation collapsed single Gaussian q ht st v1 T q ht st v1 T q st v1 T The collapse mixture discussed section 1In general approximation form f x f x DRAFT April Gaussian Sum Smoothing Algorithm aSLDS EC Backward Pass Approximates p st v1 T p ht st v1 T Jt jt ut jt st N gt jt st Gt jt st mixture Gaussians JT IT Jt S It Jt This routine needs results algorithm The routine LDSBACKWARD found algorithm GT FT gT fT uT wT t T s S s S It j Jt s j s LDSBACKWARD gt j s Gt j s ft s Ft s s p st jt st v1 T p st s ht st s jt j v1 t p ht st s jt j v1 T p s j s v1 T p st s v1 T ut j s p st jt st v1 T end st S Collapse mixture defined weights p st s jt j st v1T p st j s v1 T means st jt st covariances st jt st mix ture Jt components This defines new means gt jt st covariances Gt jt st mixture weights ut jt st p st v1 T j s p st j s v1 T end end Using mixtures smoothing The extension mixture case straightforward based representation p ht st v1 T J jt q jt st v1 T q ht st jt v1 T Analogously case single component q ht st v1 T jt st p st v1 T p jt st v1 T q ht jt st st v1 T q st ht jt st v1 t q ht jt st v1 T The average line equation tackled techniques outlined single Gaussian case To approximate q ht jt st st v1 T consider marginal joint distribution q ht ht st jt st v1 T q ht ht st jt st v1 t q ht st jt st v1 T As case single mixture problematic term q ht st jt st v1 T Analogously equation assumption q ht st jt st v1 T q ht jt st v1 T meaning information current switch state st ignored We form p ht st v1 T jt st p jt st st v1 T p ht st jt st v1 T This mixture collapsed smaller mixture method choice p ht st v1 T jt q jt st v1 T q ht jt st v1 T The resulting procedure sketched algorithm including mixtures forward backward passes DRAFT April Gaussian Sum Smoothing Relation methods A classical smoothing approximation SLDS generalised pseudo Bayes GPB In GPB starts exact recursion p st v1 T st p st st v1 T st p st st v1 T p st v1 T The quantity p st st v1 T difficult obtain GPB makes approximation p st st v1 T p st st v1 t Plugging equation p st v1 T st p st st v1 t p st v1 T st p st st p st v1 t st p st st p st v1 t p st v1 T The recursion initialised approximate filtered p sT v1 T Computing smoothed recursion switch states GPB equivalent running RTS backward pass hidden Markov model independently backward recursion continuous variables The information GPB method uses form smoothed distribution p st v1 T filtered distribution p st v1 t Markov switch transition p st st This approximation drops information future information passed continuous variables taken account In contrast GPB EC Gaussian smoothing technique preserves future information passing continuous variables GPB forms approximation p ht st v1 T recursion q st st v1 T replaced q st st v1 t In SLDSbackward m choose use EC GBP Example Traffic Flow A illustration modelling inference SLDS consider simple network traffic flow fig Here junctions b c d traffic flows roads direction indicated Traffic flows junction goes different routes d Flow junction match flow junction noise There traffic light switches junctions b depending state route traffic differently roads Using denote clean noise free flow model flows switching linear system t d t b t b d t b c t c d t t t I sa t I sa t t I sa t I sa t b t I sb t b t I sb t I sb t b c t By identifying flows time t dimensional vector hidden variable ht write flow equations ht A st ht h t set suitably defined matrices A s indexed switch variable s sa sb takes states We additionally include noise terms model cars parking de parking single timestep The covariance h diagonal larger variance inflow point model total volume traffic entering system vary Noisy measurements flow network taken v1 t t v t DRAFT April Gaussian Sum Smoothing b cd Figure A representation traffic flow junctions b c d traffic lights b If sa d b carry flow respectively If sa flow goes d sa flow goes b For sb flow b split equally b d b c For sb flow b goes b c Figure Time evolution traffic flow measured points network Sensors measure total flow network upper panel t total flow network lower panel d t d t b d t c d t The total inflow undergoes random walk Note flow measured d momentarily drop zero traffic routed b c consecutive time steps noisy measurement total flow system d v2 t d t b d t c d t v t The observation model represented vt Bht v t constant projection matrix B The switch variables follow simple Markov transition p st st biases switches remain state preference jumping state See demoSLDStraffic m details Given system prior initialises flow draw samples model forward ancestral sampling form observations v1 fig Using observations known model structure attempt infer latent switch variables traffic flows Gaussian Sum filtering smoothing EC method mixture components switch state fig We note naive HMM approximation based discretising continuous flow bins contain million states Even modest size problems naive approximation based discretisation impractical Example Following price trend The following simple model price trend stock assumes price tends continue going reverses direction h1 t h1 t h2 t h st h2 t I st h2 t h2 st vt h1 t v st h1 represents clean price h2 direction There single observation variable time clean price plus small noise There switch states dom st When st model functions normally direction equal previous direction plus small noise h2 st When st direction sampled Gaussian large variance The transition p st st set normal dynamics likely st likely normal dynamics timestep Full details SLDSpricemodel mat In fig plot samples model smoothed inference switch distribution showing analyse series infer points likely change stock price direction See exercise DRAFT April Reset Models b c Figure Given observations fig infer flows switch states latent variables The correct latent flows time switch variable state generate data The colours corresponds flows corresponding coloured edges nodes fig b Filtered flows based I Gaussian Sum forward pass approximation Plotted components vector ht v1 t posterior distribution sa sb traffic light states p sat v1 t p sbt v1 t plotted c Smoothed flows ht v1 T corresponding smoothed switch states p st v1 T Gaussian Sum smoothing approximation EC J Figure The panel time series prices The prices tend going infrequent changes direction Based fitting simple SLDS model capture kind behaviour probability significant change price direction given panel based smoothed distribution p st v1 T Reset Models Reset models special switching models switch state isolates present past resetting position latent dynamics known changepoint models Whilst models general helpful consider specific model consider SLDS changepoint model states We use state st denote LDS continues standard dynamics With st continuous dynamics reset prior p ht ht st p0 ht ht st p1 ht st p0 ht ht N ht Aht p1 ht N ht Similarly write p vt ht st p0 vt ht st p1 vt ht st For simplicity assume switch dynamics order Markov transition p st st fig Under model dynamics follows standard LDS st ht reset value drawn DRAFT April Reset Models s1 h1 v1 s2 h2 v2 s3 h3 v3 s4 h4 v4 Figure The independence structure reset model Square nodes st denote binary reset variables ht continuous state The ht continuous variables vt continuous observations If dynamics resets st dependence continuous ht past cut Gaussian distribution independent past Such models interest prediction time series following trend suddenly changes past forgotten Whilst like big change model model computationally tractable exact filtered inference scaling O T compared O T2T general state SLDS To consider filtering recursion ht st ht st p vt ht st p ht ht st p st st ht st We consider cases ht st ht st p0 vt ht p0 ht ht p st st ht st ht st p1 vt ht p1 ht ht st p st st ht st p1 vt ht p1 ht st p st st st Equation shows p ht st v1 t mixture model ht contains single component proportional p1 vt ht p1 ht If use information equation ht st ht p0 vt ht p0 ht ht p st st ht st ht p0 vt ht p0 ht ht p st st ht st Assuming ht st mixture distribution withK components ht st mixture K components In general ht st contain T components ht st single component As opposed SLDS case number components grows linearly time opposed exponentially This means computational effort perform exact filtering scales O T Smoothing achieved approach exercise Despite reduction complexity SLDS long timeseries T filtering smoothing reset models computationally expensive See approximations based retaining limited number mixture components reducing complexity linear T Run length formalism One describe reset models run length formalism defining time t latent variable rt describes length current segment If change run length variable reset zero increased p rt rt Pcp rt Pcp rt rt Pcp probability reset changepoint The joint distribution given p v1 T r1 T t p rt rt p vt v1 t rt p vt v1 t rt p vt vt rt t DRAFT April Reset Models understanding rt p vt vt rt t p vt The graphical model distribution awkward draw number links depends run length rt Predictions p vt v1 t rt p vt vt rt t p rt v1 t filtered run length p rt v1 t given forward recursion p rt v1 t rt p rt rt v1 t vt rt p rt vt rt v1 t p rt v1 t rt p vt rt rt v1 t p rt rt v1 t p rt v1 t rt p rt rt p vt vt rt t p rt v1 t shows filtered inference scales O T A Poisson reset model The changepoint structure limited conditionally Gaussian cases To illustrate consider following model2 At time t observe count yt assume Poisson distributed unknown positive intensity h The intensity constant certain unknown times t jumps new value The indicator variable ct denotes time t changepoint Mathematically model p h0 G h0 a0 b0 p ct BE ct p ht ht ct I ct ht ht I ct G ht b p vt ht PO vt ht The symbols G BE PO denote Gamma Bernoulli Poisson distributions respectively G h b exp log h bh log log b BE c exp c log c log PO v h exp v log h h log v Given observed counts v1 T task find posterior probability change associated intensity levels region consecutive changepoints Plugging definitions generic updates equation equation ht ct Gamma potential gt ct mixture Gamma potentials Gamma potential defined h elG h b triple b l For corrector update step need calculate product Poisson term observation model p vt ht PO vt ht A useful property Poisson distribution given observation latent variable Gamma distributed PO v h v log h h log v v log h h log v G h v Hence update equation requires multiplication Gamma potentials A nice property Gamma density product Gamma densities Gamma potential a1 b1 l1 a2 b2 l2 a1 a2 b1 b2 l1 l2 g a1 b1 a2 b2 2This example Taylan Cemgil DRAFT April Reset Models g a1 b1 a2 b2 log a1 a2 a1 a2 log b1 b2 a1 log b1 b1 b2 a2 log b2 b1 b2 The recursions reset model closed space mixture Gamma potentials additional Gamma potential mixture timestep A similar approach form smoothing recursions exercise Example Coal mining disasters We illustrate Poisson reset model coal mining disaster dataset The data set consists number deadly coal mining disasters England year time span years It widely agreed statistical literature change intensity expected value number disasters occurs year new health safety regulations introduced In fig marginals p ht y1 T filtering density Note constraining number changepoints principle allow number The smoothed density suggests sharp decrease t o f cc id e n ts fil te d te n si ty sm o o th e d te n si ty year Figure Estimation change points Top coal mining disaster dataset Middle Filtered esti mate marginal intensity p ht v1 t Bottom smoothed estimate p ht v1 T Here darker color means higher probability Reset HMM LDS The reset model defined equations useful applications limited single dynamical model considered An extension consider set available dynamical models indexed st S reset cuts dependency continuous variable past p ht ht st ct p0 ht ht st ct p1 ht st ct The states st follow Markovian dynamics p st st ct fig A reset occurs state st changes reset occurs p ct st st I st st The computational complexity filtering model O S2T understood analogy reset recursions equations replacing ht ht st To consider filtering recursion cases ht st ct ht st ct p0 vt ht st p0 ht ht st p st st ct p ct st st ht ct ht st ct ht st ct p1 vt ht st p1 ht st p st st ct p ct st st ht st ct p1 vt ht st p1 ht st st ct p ct st st p st st ct st ct DRAFT April Exercises c1 c2 c3 c4 s1 h1 v1 s2 h2 v2 s3 h3 v3 s4 h4 v4 Figure The independence structure reset HMM LDS model Square nodes ct denote reset vari ables ht continuous latent variables vt continuous observations The discrete state st S determines Linear Dynamical system finite set Linear Dynamical systems operational time t From equation ht st ct contains single component proportional p1 vt ht st p1 ht st This exactly analogous standard reset model need index set messages st message taking O S steps compute The computational effort perform exact filtering scales O S2T Summary The switching linear dynamical system marriage discrete state HMM continuous latent state linear dynamical system It able model discrete jumps underlying continuous process finding application large variety domains finance speech processing The classical inference problems SLDS formally intractable representing messages requires exponential space Many approximation methods developed SLDS chapter described robust deterministic method based mixture Gaussians representation Reset models continuous variable forgets past reset Unlike SLDS models amenable exact inference Extensions include reset HMM allows set discrete continuous states special discrete state resets continuous states Code SLDSforward m SLDS forward SLDSbackward m SLDS backward Expectation Correction mix2mix m Collapse mixture Gaussians smaller mixture Gaussians SLDSmargGauss m Marginalise SLDS Gaussian mixture logeps m Logarithm offset deal log demoSLDStraffic m Demo traffic flow switching linear dynamical system Exercises Exercise Consider setup described example SLDS model given SLDSpricemodel mat following notation demoSLDStraffic m Given data vector v task fit prediction model data To approximate filtered distribution p ht st v1 t mixture I components The prediction mean price day v pred t h1 t h2 t p ht v1 t DRAFT April Exercises Figure Data intermittent mean reverting process See exercise p ht v1 t st p ht st v1 t Compute mean prediction error mean_abs_pred_error mean abs vpred v Compute mean naive prediction error mean_abs_pred_error_naive mean abs v v corresponds saying tomorrow s price today s Hint find SLDSmargGauss m interest Exercise The data fig observed prices intermittent mean reverting process contained meanrev mat There states S There true latent price pt observed price vt plotted When s true underlying price reverts mean m rate r Otherwise true price follows random walk pt r pt m m pt st pt p t st p t N pt st N pt st The observed price vt related unknown price pt vt N vt pt It known time st state time t time t state s1 equally likely Also t p1 N p1 m Based information Gaussian Sum filtering I components use SLDSforward m probability time t dynamics following random walk p s280 v1 Repeat computation smoothing p s280 v1 based Expectation Correction I J components Exercise We derive smoothing recursion based approach reset LDS described section Smoothing achieved p ht st v1 T p ht st v1 t ht st p vt T ht st ht st From formal recursion ht st st ht p vt ht st p ht ht st p st st ht st Show rhs equation written p st st ht p0 vt ht p0 ht ht ht st ht st p st st ht p1 vt ht p1 ht ht st st DRAFT April Exercises Writing ht st ht st st derive recursions ht st p st st ht p0 vt ht p0 ht ht ht st st st p st st ht p1 vt ht p1 ht ht st st The contribution simply scalar complexity representation fixed According recursion include additional component representation timestep backwards time Thus number components represent ht st O T t time T define hT st This means term p ht st v1 T ht st ht st contain O t T t components To form complete smoothing pass time takes O T time DRAFT April CHAPTER Distributed Computation In chapter discuss probabilistic perspective models loosely based crude standing neural systems biology This fascinating area patterns stored recalled reference standard tools learning probabilistic models We discuss general model enables consider non linear latent continuous dynamics whilst retaining computational tractability Introduction How natural organisms process information fascinating subject grand challenges science Whilst subject early stages loosely speaking generic properties systems believed possess patterns stored set neurons recall patterns robust noise neural activity essentially binary information processing distributed highly modular In chapter discuss classical toy models developed test bed analysing properties Stochastic Hopfield Networks Hopfield networks models biological memory pattern represented activity set V interconnected neurons The term network refers set neurons fig belief network representation distribution neural states unrolled time fig At time t neuron fires vi t quiescent vi t firing depending states neurons preceding time t Explicitly neuron fires depending potential ai t bi V j wijvj t wij characterizes efficacy neuron j transmits binary signal neuron The bias bi relates neuron s predisposition firing Writing state network time t v t v1 t vV t T probability neuron fires time t modelled p vi t v t ai t x e x controls level stochastic behaviour neuron The probability quiescent state given normalization p vi t v t p vi t v t ai t Learning Sequences v1 v2 v3 v4 v4 Figure A depiction Hopfield network neurons The connectivity neurons described weight matrix elements wij The graph represents snapshot state neurons time t simultaneously update functions network previous time t These rules compactly written p vi t v t vi t ai t follows directly x x In limit neuron updates deterministically vi t sgn ai t In synchronous Hopfield network neurons update independently simultaneously rep resent temporal evolution neurons dynamic belief network fig p v t v t V p vi t v t Given description neurons update wish use network interesting things example store pattern sequences recall cue The patterns stored weights biases following section address learn suitable settings based simple local learning rules Learning Sequences A single sequence Given sequence network states V v v T like network store sequence later recalled cue That network initialized correct starting state training sequence v t remainder training sequence t reproduced deterministic dynamics equation error Two classical approaches learning temporal sequence Hebb Pseudo Inverse rules In standard Hebb PI cases biases bi usually set zero Standard Hebb rule The standard Hebb rule sets weights according to1 wij V T t vi t vj t 1Donald Hebb neurobiologist actually stated Let assume persistence repetition reverberatory activity trace tends induce lasting cellular changes add stability When axon cell A near excite cell B repeatedly persistently takes firing growth process metabolic change takes place cells A s efficiency cells firing B increased This statement misinterpreted mean weights exclusively correlation form equation discussion This severely limit performance introduce adverse storage artifacts including local minima DRAFT April Learning Sequences v1 t v2 t v3 t v4 t v1 t v2 t v3 t v4 t Figure A dynamic belief network representation Hop field Network The network operates simultaneously gener ating new set neuron states previous set Equa tion defines Markov transition matrix modelling transition probability v t v t furthermore imposes constraint neurons conditionally independent given previous state network The Hebb rule motivated mathematically considering j wijvj t V T vi j vj vj t V vi t j v2j t V T t vi j vj vj t vi t V T t vi j vj vj t If patterns uncorrelated interference term V T t vi j vj vj t relatively small To note uniform randomly drawn patterns mean zero patterns randomly The variance given V T t j k vi vi vj vj t vk vk t For j k terms independent contribute zero average Therefore V T j vi vi vj vj v2j t When terms independent zero mean contribute zero Hence V t j v2i v j v j t T V Provided number neurons V significantly larger length sequence T average size interference small In case term vi t equation dominates meaning sign j wijvj t vi t correct pattern sequence recalled A careful analysis scope shows Hebb rule capable storing random uncorrelated temporal sequence length 269V time steps However Hebb rule performs poorly case correlated patterns interference patterns significant Pseudo inverse rule The PI rule finds matrix W ij wij solves linear equations j wijvj t vi t t T DRAFT April Learning Sequences Under condition sgn j wijvj t sgn vi t vi t patterns correctly recalled In matrix notation require WV V V vi t t T V vi t t T For T V problem determined multiple solutions exist One solution given pseudo inverse W V VTV VT The Pseudo Inverse PI rule store sequence V linearly independent patterns Whilst attractive compared standard Hebb rule terms ability store longer correlated sequences rule suffers small basins attraction temporally correlated patterns fig The maximum likelihood Hebb rule An alternative classical algorithms view problem pattern storage DBN equation First need clarify mean store Given initialize network state v t wish remaining sequence generated high probability That wish adjust network parameters probability p v T v T v v maximal2 Furthermore hope sequence recalled high probability initialized correct state states close Hamming distance correct initial state v Due Markov nature dynamics conditional likelihood p v T v T v v T t p v t v t This product transitions given states given states Since transition probabilities known equation equation conditional likelihood easily evaluated The sequence log conditional likelihood L w b log T t p v t v t T t log p v t v t T t V log vi t ai t Our task find weights w biases b maximise L w b There closed form solution parameters need determined numerically Nevertheless corresponds straightforward computational problem log likelihood convex function To compute Hessian neglecting b expositional clarity d2L dwijdwkl T t vi t vj t t t vk t vl t ik defined t vi t ai t 2Static patterns considered framework set patterns map DRAFT April Learning Sequences Training Sequence time n e u ro n n u m b e r Max Likelihood Hebb Pseudo Inverse Figure Leftmost panel The highly correlated train ing sequence desire store The panels temporal evolution network initialization correct starting state corrupted noise During recall deterministic updates The Max imum Likelihood rule trained batch epochs See demoHopfield m It straightforward Hessian negative semidefinite exercise likelihood single global maximum To increase likelihood sequence use simple method gradient ascent3 wnewij wij dL dwij bnewi bi dL dbi dL dwij T t t vi t vj t dL dbi T t t vi t The learning rate chosen empirically sufficiently small ensure convergence The maximum likelihood learning rule equation seen modified Hebb learning rule basic Hebb rule given t As learning progresses factors t typically tend values close learning rule seen asymptotically equivalent making update case disagreement ai t vi t different signs This batch training procedure readily converted online process update occurs immediately presentation consecutive patterns Storage capacity ML Hebb rule The ML Hebb rule capable storing sequence V linearly independent patterns To form input output training set neuron v t vi t t T Each neuron associated weight vector wi wij j V forms logistic regressor limit perceptron For perfect recall patterns need vectors constituting pattern sequence linearly separable This case patterns linearly independent regardless outputs vi t t T section Relation perceptron rule In limit activation large ai t vi t ai vi t ai Provided activation desired output sign update neuron In limit equation called perceptron rule For activation close decision boundary small change lead different sign neural firing To guard common include stability criterion t vi t ai M vi t ai M 3Naturally use sophisticated methods Newton method conjugate gradients In theoretical neurobiology emphasis gradient style updates deemed biologically plausible DRAFT April Learning Sequences flip probability fr ct io n c o rr e ct sequence length Max Likelihood noise trained Max Likelihood perceptron M perceptron M hebb pseudo inverse Figure The fraction neurons correct fi nal state network T neuron Hop field network trained store length sequence patterns After initialization correct initial state t Hopfield network updated determinis tically randomly chosen percentage neu rons flipped updating The correlated sequence length T produced flipping prob ability previous state network A fraction correct value indicates perfect recall final state value indicates formance better random guessing final state For maximum likelihood epochs training During recall deterministic updates The results presented averages simulations resulting standard errors order symbol sizes M empirically chosen positive threshold Example Storing correlated sequence In fig consider storage temporal sequence length T neurons learning rules Hebb Maximum Likelihood Pseudo Inverse The sequence highly correlated represents difficult learning task The biases bi set zero facilitate comparison The initial state training sequence corrupted noise presented trained networks desire remaining training sequence recalled initial noisy state Whilst Hebb rule operating feasible limit uncorrelated patterns strong correlations training sequence rise poor results The PI rule capable storing sequence length robust perturbations correct initial state The Maximum Likelihood rule performs small training Stochastic interpretation By straightforward manipulations weight update rule equation written dL dwij T t vi t vi t p vi t ai t vj t A stochastic online learning rule wij t vi t v t vj t v t sampled state probability ai t Provided learning rate small stochastic updating approximate learning rule Example Recalling sequences perpetual noise We consider neurons storing T length sequence zero biases compare performance Maximum Likelihood learning rule Hebb Pseudo Inverse Perceptron rule The training sequences produced starting random initial state v choosing random percent neurons flip chosen neurons flipped probability giving random training sequence high degree DRAFT April Learning Sequences b Figure Original T binary video sequence set neurons b The reconstructions beginning noise perturbed initial state Every odd time reconstruction randomly perturbed Despite high level noise basin attraction pattern sequence broad patterns immediately fall close pattern sequence single timestep temporal correlation After training network initialized noise corrupted version correct initial state v t training sequence The dynamics run number steps length training sequence The fraction bits recalled final state training sequence final state v T measured fig At stage dynamics state network corrupted noise flipping neuron state specified flip probability The standard Hebb rule performs relatively poorly particularly small flip rates whilst methods perform relatively robust small flip rates As flip rate increases pseudo inverse rule unstable especially longer temporal sequence places demands network The perceptron rule perform maximum likelihood rule performance critically dependent appropriate choice threshold M The results M Perceptron training poor small flip rates An advantage maximum likelihood rule performs need fine tuning parameters A similar example larger network given fig consists highly correlated sequences The weights learned maximum likelihood procedure For short sequences basin attraction large video sequence stored robustly Multiple sequences We address learning set sequences Vn n N If assume sequences independent log likelihood set sequences sum individual sequences The gradient given dL dwij N n T t ni t v n t v n j t dL dbi N n T t ni t v n t ni t vni t ani t ani t bi j wijv n j t The log likelihood remains convex sum convex functions standard gradient based learning algorithms successfully DRAFT April Tractable Continuous Latent Variable Models Boolean networks The Hopfield network particular parameterisation table p vi t v t However constrained parameters considered In fully unconstrained case neuron associated 2V parental states However specify exponentially large number states impractical interesting restriction consider neuron K parents table contains 2K entries Learning table parameters maximum likelihood straightforward log likelihood convex function table entries Hence given sequence set sequences readily find parameters maximise sequence reconstruction probability The maximum likelihood method produces large basins attraction associated stochastic dynamical system Such models interest Artificial Life Random Boolean networks emergent macroscopic behaviour appears local update rules Such systems study robustness chemical gene regulatory networks Sequence disambiguation A limitation time independent order networks defined visible variables Hopfield network observation transition p vt vt v time joint state v encountered This means sequence contains subsequence b c recalled high probability joint state transitions different states different times Whilst attempt resolve sequence disambiguation problem higher order Markov model account longer temporal context time dependent model lose biological plausibility Using latent variables alternative way sequence disambiguation In Hopfield model recall capacity increased latent variables making sequencing joint latent visible space linearly independent visible variable sequence In section discuss general method extends dynamic belief networks defined visible variables Hopfield network include non linearly updating latent variables Tractable Continuous Latent Variable Models A dynamic belief network hidden latent variables h visible variables observations v takes form p v T h T p v p h v T t p v t v t h t p h t v t v t h t As saw chapter provided hidden variables discrete inference models straight forward However physical systems natural assume continuous h t In chapter saw tractable continuous h t model given linear Gaussian transitions emissions LDS Whilst useful represent non linear changes latent process LDS The Switching LDS chapter able model non linear continuous dynamics switching saw leads computational difficulties For computational reasons limited purely discrete h limitation discrete transitions purely continuous h forced use simple linear dynamics Is way continuous state non linear dynamics posterior inference remains tractable The answer yes provided assume hidden transitions deterministic When conditioned visible variables renders hidden unit distribution trivial This allows consideration rich non linear dynamics hidden space Note models limited distributed computation context chapter example ARCH GARCH models chapter special cases Deterministic latent variables Consider Belief Network defined sequence visible variables v T To enrich model include additional continuous latent variables h T follow non linear Markov transition To retain tractability inference constrain latent dynamics deterministic described p h t v t v t h t h t f v t v t h t h DRAFT April Tractable Continuous Latent Variable Models v v v t h h h t h h h t b v v v t c Figure A order dynamic belief network deterministic hidden transitions represented diamonds hidden node certainly single state determined parents b Conditioning visible variables forms directed chain hidden space deterministic Hidden unit inference achieved forward propagation c Integrating hidden variables gives cascade style directed visible graph v t depends v t The possibly non linear function f parameterises conditional probability table Whilst restric tion deterministic transitions appears severe model retains attractive features The marginal p v T non Markovian coupling variables sequence fig 6c whilst hidden unit inference p h T v T deterministic See fig The adjustable parameters hidden visible distributions represented h v respectively For learning log likelihood single training sequence v T L log p v v T t log p v t v t h t v hidden unit values calculated recursively h t f v t v t h t h To maximise log likelihood gradient techniques need derivatives respect model parameters These calculated recursively follows dL dv v log p v v T t v log p v t v t h t v dL dh T t h t log p v t v t h t v dh t dh dh t dh f t h f t h t dh t dh use shorthand f t f v t v t h t h Hence derivatives calculated deterministic forward propagation The case training multiple independently generated sequences straightforward extension obtained summing individual sequences Whilst deterministic latent variable models general context chapter interesting apply simple neurobiological models enriching Hopfield model powerful internal dynamics DRAFT April Tractable Continuous Latent Variable Models b Figure The training sequence consists random set vectors V T time steps b The reconstruction H hidden units The initial state v t recalled sequence set correct initial training value albeit values flipped Note method capable sequence disambiguation sense transitions form b c recalled An augmented Hopfield network To deterministic latent variable model explicit consider case continuous vector hidden variables h t discrete binary vector visible variables components vi t In particular restrict attention Hopfield model augmented latent variables simple linear dynamics exercise non linear extension h t Ah t Bv t deterministic latent transition p v t v t h t V vi t t t Ch t Dv t This model generalises recurrent stochastic heteroassociative Hopfield network include deterministic hidden units dependent previous network states The parameters model A B C D For gradient based training require derivatives respect parameters The derivative log likelihood generic parameter d d L t d d t t vi t t vi t This gives d dA t j Cij d dA hj t d dB t j Cij d dB hj t d dC t ih t d dD t iv t d dA hi t t j Aij d dA hj t ih t d dB hi t t j Aij d dB hj t iv t t hi t hi t If assume h given fixed value compute derivatives recursively forward propagation Gradient based training augmented Hopfield network straightforward implement This model extends power original Hopfield model capable resolving ambiguous transitions sequences b c example In terms dynamic system learned network attractor training sequence stable point demonstrates models capable learning attractor networks powerful hidden units DRAFT April Neural Models Example Sequence disambiguation The sequence fig 7a contains repeated patterns reliably recalled order model containing visible variables To deal consider Hopfield network visible units additional hidden units deterministic linear latent dynamics The model trained gradient ascent maximise likelihood binary sequence fig 7a See demoHopfieldLatent m As shown fig 7b learned network capable recalling sequence correctly initialised incorrect state having difficulty fact sequence transitions ambiguous Neural Models The tractable deterministic latent variable model introduced section presents opportunity extend models Hopfield network include biologically realistic processes losing computational tractability First discuss general framework learning class neural models special case deterministic latent variable models generalisation spike response model theoretical neurobiology Stochastically spiking neurons We assume neuron fires depending membrane potential ai t p vi t v t h t p vi t ai t To specific p vi t ai t ai t Here define quiescent state vi t p vi t ai t 2vi t ai t The use sigmoid function x fundamental chosen merely analytical convenience The log likelihood sequence visible states V v v T L T t V log 2vi t ai t gradient dL dwij T t vi t ai t dai t dwij fact vi Here wij parameters membrane potential We equation common following models membrane potential ai t described increasing sophistication Hopfield membrane potential As step Hopfield maximum likelihood training rule described section recovered special case framework The Hopfield membrane potential ai t V j wijvj t bi wij characterizes efficacy information transmission neuron j neuron bi bias Applying maximum likelihood framework model learn temporal sequence V adjustment DRAFT April Neural Models n e u ro n n u m b e r Original t Reconstruction t x values t Hebb Reconstruction t Figure Learning depression U t Despite apparent complexity dynamics learning appropriate neu ral connection weights straightforward max imum likelihood The reconstruction stan dard Hebb rule contrast poor parameters wij bi fixed simplicity obtain batch learning rule dai dwij vj t equation wnewij wij dL dwij dL dwij T t vi t ai t vj t learning rate chosen empirically sufficiently small ensure convergence Equa tion matches equation uses encoding Dynamic synapses In realistic synaptic models neurotransmitter generation depends finite rate cell subcomponent production quantity vesicles released affected history firing Loosely speaking neuron fires releases chemical substance local reservoir If neuron fires times short succession ability continue firing weakens reservoir release chemical depleted This phenomenon modelled depression mechanism reduces membrane potential ai t wijxj t vj t depression factors xj t A simple dynamics depression factors xj t xj t t xj t Uxj t vj t t U represent time scales recovery times spiking effect parameters respectively Note depression factor dynamics exactly form deterministic hidden variables It straightforward include dynamic synapses principled way maximum likelihood learning framework For Hopfield potential learning dynamics simply given equations dai t dwij xj t vj t Example Learning depression In fig demonstrate learning random temporal se quence timesteps neurons dynamic depressive synapses After learning wij trained network initialised state training sequence The remaining states sequence correctly recalled forward sampling learned model The corresponding generated factors xi t plotted reconstruction characteristic drop x neuron fires grad ual recovery For comparison plot results dynamics having set wij temporal Hebb rule equation The poor performance correlation based Hebb rule demonstrates necessity general tailor learning rule dynamical system attempts control DRAFT April Neural Models Leaky integrate fire models Leaky integrate fire models step biological realism membrane potential increments receives excitatory stimulus wij decrements receives inhibitory stimulus wij After firing membrane potential reset low value firing threshold steadily increases resting level example A model incorporates effects ai t ai t j wijvj t rest vi t vi t fired Since vi neuron fires time t potential reset fired time t Similarly synaptic input potential equilibrates rest time constant log Despite increase complexity membrane potential Hopfield case deriving appropriate learning dynamics new system straightforward hidden variables membrane potentials update deterministic fashion The membrane potential derivatives dai t dwij vi t dai t dwij vj t By initialising derivative dai t dwij equations define order recursion gradient adapt wij usual manner w new ij wij dL dwij We apply synaptic dynamics case replacing term vj t equation xj t vj t Although detailed discussion properties neuronal responses networks trained way scope interesting consequence learning rule equation spike time dependent learning window qualitative agreement experimental observation real biological systems Summary Classical models Hopfield network trained learn temporal sequences maximum likelihood This results robust storage mechanism patterns large basin attraction We form tractable non linear continuous latent systems provided latent dynamics deterministic These deterministic latent variable models powerful inference straightforward We demonstrated complex models neurobiology considered deterministic latent variable framework learning rules derived straightforward manner It important learning rule derived particular neural dynamics mind undesirable effects pattern storage occur Code demoHopfield m Demo Hopfield sequence learning HebbML m Gradient ascent training set sequences maximum likelihood HopfieldHiddenNL m Hopfield network additional non linear latent variables demoHopfieldLatent m Demo Hopfield net deterministic latent variables HopfieldHiddenLikNL m Hopfield network hidden variables sequence likelihood DRAFT April Exercises Exercises Exercise Consider large V stochastic Hopfield network section store single temporal sequence v T length T V In case weight matrix elements wij computationally difficult store Explain justify assumption wij T t ui t vi t vj t ui t dual parameters derive maximum likelihood update rule dual parameters ui t Exercise A Hopfield network store raw uncompressed binary video sequence Each image sequence contains binary pixels At rate frames second hours video neurons store Exercise Derive update equation Exercise Show Hessian equation negative semidefinite That j k l xijxkl d2L dwijdwkl x Exercise For augmented Hopfield network section latent dynamics hi t j Aijhj t Bijvj t derive derivative recursions described section Exercise The storage static pattern v considered equivalent requirement temporal dynamics probability p v t v v t v high Based estimate numerically static patterns neuron stochastic Hopfield network robustly recover elements pattern flipped DRAFT April Part V Approximate Inference Introduction Part V In Part I discussed inference showed certain models com putationally tractable However models interest perform inference exactly approximations required In Part V discuss approximate inference methods beginning sampling based approaches These popular known branches mathematical sciences having origins chemistry physics We discuss alternative deterministic approximate inference methods cases remarkably accurate performance It important bear mind single algorithm going best inference tasks For reason attempt explain assumptions techniques select appropriate technique problem hand DRAFT April Graphical model deterministic approx Laplace cont variables variational bounds KL q p factorised naive MF structured convex bounds loopy message passing EP messages intract Sampling approx exact sampling eg cestral MCMC auxilliary variable methods Hybrid MCMC Slice sampling Metropolis Hastings Gibbs sampling structured Gibbs samplingImportance sampling Approximate inference methods loose associations The leaf nodes denote specific methods Part V discusses methods application models interest machine learning DRAFT April CHAPTER Sampling In cases exact results obtained general purpose approach draw samples distribution In chapter discuss classical exact sampling methods typically limited small number variables models high degree structure When longer applied discuss approximate sampling methods including Markov chain Monte Carlo Introduction Sampling concerns drawing realisations samples X x1 xL variable x distribution p x For discrete variable x limit large number samples fraction samples state x tends p x x That lim L L L l I xl x p x x In continuous case consider region R probability samples occupy R tends integral p x R Given finite set samples approximate expectations f x p x L L l f xl f X The subscript f X emphasises approximation dependent set samples drawn This sample approximation holds discrete continuous variables A sampling procedure produces realisations set X considered generating distri bution p X Provided marginals sampling distribution equal marginals target distribution p xl p xl average approximation f X respect draws sample set X f X p X L L l f xl p xl f x p x Hence mean sample approximation exact mean f provided marginals p X correspond required marginals p x p X f X unbiased estimator f x p x Note holds individual samples x1 xL dependent p X factorise l p x l Introduction For sampling method important issue variance sample estimate If low small number samples required sample mean close true mean assuming unbiased Defining f X f X f X p X f x f x f x p x variance approximation assuming p xl p xl l f X p X L2 l l f xl f xl p xl xl L2 L f x p x l l f xl f xl p xl xl Provided samples independent p X L l p xl p x p x p xl xl p xl p xl The second term equation composed f xl f xl zero f x Hence f X p X L f x p x variance approximation scales inversely number samples In principle provided samples independently drawn p x small number samples required accurately estimate expectation Importantly result independent dimension x However critical difficulty actually generating independent samples p x Drawing samples high dimensional distributions generally difficult guarantees exist ensure practical timeframe samples produced independent Whilst dependent sampling scheme unbiased variance resulting estimate high large number samples required expectations approximated accurately There different sampling algorithms work principle working practice distribution satisfies particular properties example Markov chain Monte Carlo methods produce independent samples large number samples necessary produce satisfactory approximation Before develop schemes multivariate distributions consider univariate case Univariate sampling In following assume random number generator exists able produce value uniformly random unit interval We use uniform sampler draw samples non uniform distributions Discrete case Consider dimensional discrete distribution p x dom x p x x x x This represents partitioning unit interval interval labelled state state state fig If drop point random uniformly interval chance land interval chance interval similarly interval This defines valid DRAFT April Introduction Figure A representation discrete distribu tion equation The unit interval partitioned parts lengths equal sampling procedure discrete dimensional distributions draw uniform distribution identify partition unit interval lies This easily found cumulant algorithm Sampling discrete univariate distribution straightforward computing cumulant takes O K steps K state discrete variable In example c0 c1 c2 c3 We draw sample uniformly u Then sampled state state interval c1 c2 Since samples independent number samples required accurately represent marginal reasonably small fig Continuous case Intuitively generalisation discrete continuous case clear First calculate cumulant density function C y y p x dx Then sample u uniformly obtain corresponding sample x solving C x u x C u Formally sampling continuous univariate variable straightforward provided compute integral corresponding probability density function For special distributions avoid explicit use cumulant p x alternative procedures based coordinate transformations For Gaussian example avoid use cumulant exercise Rejection sampling Consider efficient sampling procedure distribution q x Can use help sample distribution p x We assume p x known normalisation constant Z p x p x Z One way sample p x use binary auxiliary variable y define q x y q x q y x q x y q x q y x We use term q y x advantage set q y x p x q x q x y p x sampling q x y gives procedure sampling p x To achieve assume find positive M q y x p x Mq x x Algorithm Sampling univariate discrete distribution p K states Label K states K associated probabilities pi Calculate cumulant ci j pj set c0 Draw value u uniformly random unit interval Find ci u ci Return state sample p DRAFT April Introduction Algorithm Rejection sampling draw L independent samples p x p x Z Given p x q x find M p x q x M x l L repeat Draw candidate sample xcand q x Let p xcand Mq xcand Draw value u uniformly u xl xcand Accept candidate end To sample y given x draw value u uniformly If value q y x set y set y To draw sample p x draw candidate xcand q x y q y xcand If y xcand independent sample p x sample algorithm Note samples drawn proportionally p x proportionality constant common x equivalent drawing p x The expected rate accept sample q y x q y x q x Z M increase acceptance rate seek minimal M subject p x Mq x In cases q x free parameters q x parameter adjusted minimise M If set q x p x M Z q y x sampling efficient In general q y x sampling ideally efficient In high dimensions vector x q y x rejection sampling extremely inefficient To consider simple scenario distributions p x q x factorised p x D p xi q x D q xi Then q y x D p xi Miq xi D q y xi O D typical value q y xi dimension Hence probability accepting x decrease exponentially number dimensions x Rejection sampling potentially useful method drawing independent samples low dimensions likely impractical higher dimensions Multivariate sampling One way generalise dimensional univariate discrete case higher dimensional multivariate distribution p x1 xn translate equivalent dimensional distribution This achieved enumerating possible joint states x1 xn giving unique integer total number states constructing univariate distribution probability p This trans forms multivariate distribution equivalent univariate distribution sampling achieved In general course procedure impractical number states grows exponentially number variables x1 xn b Figure Histograms samples state distribution p x sam ples b samples As number samples increases relative frequency samples tends distribution p x DRAFT April Introduction An alternative exact approach holds discrete continuous variables capitalise relation p x1 x2 p x2 x1 p x1 We sample joint distribution p x1 x2 sampling state x1 dimensional p x1 x1 clamped state sampling state x2 dimensional p x2 x1 It clear generalise variables cascade decomposition p x1 xn p xn xn x1 p xn xn x1 p x2 x1 p x1 However order apply technique need know conditionals p xi xi x1 Unless explicitly given need compute joint distribution p x1 xn Computing conditionals general require summation exponential number states small n generally impractical For belief networks construction conditionals specified technique practical discuss section Drawing samples multivariate distribution general complex task seeks exploit structural properties distribution computationally feasible A common approach seek transform distribution product lower dimensional distributions general change variables method continuous variables definition required A classic example sampling multivariate Gaussian reduced sampling set univariate Gaussians suitable coordinate transformation discussed example For remaining chapter discuss methods appropriate drawing multivariate distributions obvious transformation exists reduce problem univariate form Note exist transformations result approximately independent variables useful preprocessing step Example Sampling multivariate Gaussian Our interest draw sample mul tivariate Gaussian p x N x m S For general covariance matrix S p x factorise product univariate distributions However consider transformation y C x m C chosen CCT S Since linear transformation y Gaussian distributed mean y C x m p x C x p x m C m m Since mean y zero covariance given yyT p x C x m x m T p x C T C 1SC T C 1CCTC T I Hence p y N y I N yi A sample y obtained independently drawing sample univariate zero mean unit variance Gaussians Given sample y sample x obtained x Cy m Drawing samples univariate Gaussian studied topic popular method Box Muller technique exercise DRAFT April Ancestral Sampling x1 x2 x3 x4 x5 x6 Figure An ancestral Belief Network evidential variables To sample distribution draw sample variable variables order Ancestral Sampling Belief networks general form p x p xi pa xi assume conditional distributions p xi pa xi specified Provided variables evidential sample distribution straightforward manner For convenience rename variable indices parent variables come children ancestral ordering example fig p x1 x6 p x1 p x2 p x3 x1 x2 p x4 x3 p x5 x3 p x6 x4 x5 One sample nodes parents x1 x2 Given values sample x3 x4 x5 finally x6 Despite presence loops graph forward sampling procedure straightforward This procedure holds discrete continuous variables If attempted carry exact marginal inference scheme complex multiply connected graph moralisation triangulation steps result large cliques exact inference intractable However regardless loop structure ancestral sampling remains straightforward Ancestral forward sampling case perfect sampling termed exact sampling sample independently drawn required distribution This contrast Markov Chain Monte Carlo methods sections dependent samples drawn p x limit large number iterations Dealing evidence How sample distribution subset variables xE clamped evidential states Writing x xE x E formally wish sample p x E xE p x E xE p xE If evidential variable xi parents simply set variable state continue forward sampling For example compute sample p x1 x3 x4 x5 x6 x2 defined equa tion simply clamps x2 evidential state continues forward sampling The reason straightforward conditioning x2 merely defines new distribution subset variables belief network representation distribution immediately known remains ancestral On hand consider sampling p x1 x2 x3 x4 x5 x6 Using Bayes rule p x1 x2 x3 x4 x5 x6 p x1 p x2 p x3 x1 x2 p x4 x3 p x5 x3 p x6 x4 x5 x1 x2 x3 x4 x5 p x1 p x2 p x3 x1 x2 p x4 x3 p x5 x3 p x6 x4 x5 The conditioning x6 means structure distribution non evidential variables changes example x4 x5 coupled conditioned x3 x4 x5 independent conditioned x3 x6 x4 x5 dependent One attempt work equivalent new forward sam pling structure exercise generally complex running exact inference approach An alternative proceed forward sampling non evidential distribution discard samples match evidential states exercise analogous situation DRAFT April Gibbs Sampling x1 x2 x3 x4 x5 x6 Figure The shaded nodes Markov blanket x4 belief network fig To draw sample p x4 x clamp x3 x5 x6 evidential states draw sample p x4 x3 p x6 x4 x5 Z Z normalisation constant justifies procedure However generally recommended probability sample p x consistent evidence roughly O dimx e dimx e number states evidential variable In principle ease effect discarding sample soon variable state inconsistent evidence Nevertheless number starts required obtain valid sample average large For reason alternative non exact procedures common discuss section Perfect sampling Markov network For Markov network draw exact samples forming equivalent directed representation graph section subsequently ancestral sampling directed graph This achieved choosing root clique consistently orienting edges away clique An exact sample drawn Markov network sampling root clique recursively children clique See potsample m JTsample m demoJTreeSample m Gibbs Sampling The inefficiency methods ancestral sampling evidence motivates alternative techniques An important widespread technique Gibbs sampling generally straightforward implement No evidence Assume joint sample state x1 multivariate distribution p x We consider particular variable xi wish draw sample Using conditioning write p x p xi x1 xi xi xn p x1 xi xi xn Given joint initial state x1 read parental state x11 x x x n draw sample x2i p xi x11 x1i x1i x1n p xi x We assume distribution easy sample univariate We new joint sample xi updated x x11 x x x x n One selects variable xj sample continuing procedure generates set x1 xL samples xl differs xl single component Clearly exact sampler resulting samples highly dependent Whilst Gibbs sampling generally straightforward implement drawback samples strongly dependent Nevertheless discussed section provided marginal sampling distribution correct valid sampler We outline section limit large number samples holds sampler valid For general distribution conditional p xi x depends Markov blanket variable xi For belief network Markov blanket xi p xi x Z p xi pa xi j ch p xj pa xj example fig The normalisation constant univariate distribution straightforward work requirement Z xi p xi pa xi j ch p xj pa xj DRAFT April Gibbs Sampling x2 x Figure A dimensional distribution Gibbs sampling fails The distribution mass shaded quadrants Gibbs sampling proceeds lth sample state xl1 x l sampling p x2 xl1 write xl x l x l x l One continues sample p x1 x2 xl etc If start lower left quadrant proceed way upper right region explored In case continuous variable xi summation replaced integration Due local structure belief network parental parents children states required forming sample update Evidence Evidence readily dealt clamping samples evidential variables evidential states There need sample variables states known One proceeds selecting non evidential variable determining distribution conditioned Markov blanket subsequently drawing sample variable Gibbs sampling Markov chain In Gibbs sampling sample joint variables xl stage l Based produce new joint sample xl This means write Gibbs sampling procedure draws xl q xl xl distribution q xl xl If choose variable update xi random distribution q Gibbs sampling corresponds drawing samples Markov transition q xl xl q xl xl q q xl xl p xl 1i x l j xl 1j x l j q q That select variable sample conditional copying states variables previous sample Our interest stationary distribution q x x p x We carry assuming x continuous discrete case analogous x q x x p x q x q x x p x q x j x j xj p x x p xi x q xi p x x p xi x q p x x p x q p x p x Hence long continue draw samples according distribution q x x limit large number samples ultimately tend draw dependent samples p x Any distribution q suffices visiting variables equally valid choice Technically require q x x p x equilibrium distribution matter state start converge p x fig section discussion issue DRAFT April Gibbs Sampling Structured Gibbs sampling One extend Gibbs sampling conditioning reveal tractable distribution remaining variables For example consider distribution fig 6a p x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 In single site Gibbs sampling condition variables sample remaining variable For example x emphasise known states p x1 x2 x3 x4 x1 x2 x4 x1 x1 x3 However use limited conditioning long conditioned distribution easy sample In case equation condition x3 p x1 x2 x4 x3 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 This written modified distribution fig 6b p x1 x2 x4 x3 x1 x2 x4 x1 As distribution x1 x2 x4 singly connected linear chain Markov network samples drawn exactly explained section A simple approach compute normalisation constant standard techniques example factor graph method One convert undirected linear chain directed graph use ancestral sampling These operations linear number variables conditioned distribution Alternatively form junction tree set potentials choose root form set chain reabsorption junction tree Ancestral sampling performed resulting oriented clique tree This approach taken GibbsSample m In example reveal tractable distribution conditioning x1 p x3 x2 x4 x1 x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 draw sample x2 x3 x4 distribution exact sampler A structured Gibbs sampling procedure draw sample x1 x2 x4 equation sample x3 x2 x4 equation These steps drawing exact conditional samples iterated Note x2 x4 constrained equal values previous sample This procedure generally preferred single site Gibbs updating samples dependent sample See demoGibbsSample m comparison unstructured structured sampling set potentials Remarks If initial sample x1 state space low probability time samples representative single component x updated iteration This motivates called burn stage initial samples discarded In single site Gibbs sampling high degree dependence successive samples variable single site updating version updated stage This motivates subsampling x1 x2 x3x4 x1 x2 x4 b Figure A toy intractable distribution Gibbs sampling conditioning variables ex cept leads simple univariate conditional dis tribution b Conditioning x3 yields new distribution singly connected exact sampling straightforward DRAFT April Markov Chain Monte Carlo MCMC b Figure Two Gibbs samples dimensional Gaussian At stage single component updated For Gaussian low correlation Gibbs sampling likely regions effectively b For strongly correlated Gaussian Gibbs sampling effective rapidly explore likely regions demoGibbsGauss m 10th sample xK xK xK taken rest discarded Due simplicity Gibbs sampling popular sampling methods particularly con venient applied belief networks Markov blanket property1 Gibbs sampling special case MCMC framework MCMC methods bear mind convergence issue generally unknown samples needed reasonably sure sample estimate accurate Gibbs sampling assumes space effectively single co ordinate dates We require state visited infinitely In fig dimensional continuous distribution mass lower left upper right regions In case start lower left region remain explore upper right region This problem occurs regions connected probable Gibbs path A Gibbs sampler perfect sampler provided subsample appropriately distribution factorised variables independent This suggests general Gibbs sampling effective variables strongly correlated For example consider Gibbs sampling strongly correlated variable Gaussian distribution updates slowly space fig It useful find variable transformations render new variables approximately independent apply Gibbs sampling effectively Markov Chain Monte Carlo MCMC We assume multivariate distribution form p x Z p x p x unnormalised distribution Z x p x computationally intractable normal isation constant We assume able evaluate p x x state x p x x Z intractable The idea MCMC sampling sample directly p x different distribution limit large number samples effectively samples p x To achieve forward sample Markov transition stationary distribution equal p x 1The BUGS package www mrc bsu cam ac uk bugs general purpose software sampling belief networks DRAFT April Markov Chain Monte Carlo MCMC Markov chains Consider conditional distribution q xl xl If given initial sample x1 recursively generate samples x1 x2 xL After long time L provided Markov chain irreducible meaning eventually state state aperiodic meaning don t periodically revisit state samples unique stationary distribution q x defined continuous variable q x x q x x q x The condition discrete variable analogous replacing integration summation The idea MCMC given distribution p x find transition q x x p x stationary distri bution If draw samples Markov chain forward sampling samples p x chain converges stationary distribution Note distribution p x transition q x x p x sta tionary distribution This different MCMC sampling methods different characteristics varying suitability particular distribution hand We ve encountered class samplers MCMC family Gibbs sampling corresponds particular transition q x x Below discuss general members family Metropolis Hastings sampling Consider following transition q x x q x x f x x x x x q x x f x x q x x called proposal distribution f x x positive function This defines valid distribution q x x non negative x q x x x q x x f x x x q x x f x x Our interest set f x x stationary distribution q x x equal p x proposal q x x That p x x q x x p x x q x x f x x p x p x x q x x f x x In order holds require changing integral variable x x x q x x f x x p x x q x x f x x p x Now consider Metropolis Hastings acceptance function f x x min q x x p x q x x p x min q x x p x q x x p x defined x x detailed balance property f x x q x x p x min q x x p x q x x p x min q x x p x q x x p x f x x q x x p x Hence function f x x defined ensures equation holds q x x p x stationary distribution How sample q x x Equation interpreted mixture distributions proportional q x x f x x x x mixture coefficient x q x x f x x To DRAFT April Markov Chain Monte Carlo MCMC Algorithm Metropolis Hastings MCMC sampling Choose starting point x1 L Draw candidate sample xcand proposal q x xl Let q xl xcand p xcand q xcand xl p xl xl xcand Accept candidate draw random value u uniformly unit interval u xl xcand Accept candidate xl xl Reject candidate end end end Figure Metropolis Hastings samples bi variate distribution p x1 x2 proposal q x x N x x I We plot iso probability contours p Although p x multi modal dimensionality low modes sufficiently close simple Gaussian pro posal distribution able bridge modes In higher dimensions multi modality problematic See demoMetropolis m draw sample draw sample q x x accept probability f x x Since drawing q x x accepting performed independently probability accepting drawn candidate product probabilities q x x f x x Otherwise candidate rejected sample x x Using properties acceptance function equation following equivalent procedure When q x x p x q x x p x accept sample q x x Otherwise accept sample x q x x probability q x x p x q x x p x If reject candidate x x Note candidate x rejected original x new sample Hence iteration algorithm produces sample copy current sample candidate sample algorithm A rough rule thumb choose proposal distribution acceptance rate Gaussian proposal distribution A common proposal distribution vector x q x x N x x 2I e x x q x x q x x acceptance criterion equation f x x min p x p x If unnormalised probability p x candidate state higher current state p x accept candidate Otherwise accept candidate probability p x p x If DRAFT April Auxiliary Variable Methods x b c Figure Hybrid Monte Carlo Multi modal distribution p x desire samples b HMC forms joint distribution p x p y p y Gaussian c This plot b Starting point x draw y Gaussian p y giving point x y given green line Then use Hamiltonian dynamics white line traverse distribution roughly constant energy fixed number steps giving x y We accept point H x y H x y new sample x red line Otherwise candidate accepted probability exp H x y H x y If rejected new sample x taken copy x candidate rejected new sample taken copy previous sample x See fig demonstration In high dimensions unlikely random candidate sampled Gaussian result candidate probability higher current value exercise Because small jumps small likely accepted This limits speed explore space x increases dependency samples As aside acceptance function highlights sampling different finding optimum Provided x higher probability x accept x However accept candidates lower probability current sample Auxiliary Variable Methods A practical concern MCMC methods ensuring moves effectively significant proba bility regions distribution For methods Metropolis Hastings local proposal distributions local sense unlikely propose candidate far current sample target distri bution isolated islands high density chance island small Conversely attempt proposal local high variance chance landing random high density island remote Auxiliary variable methods use additional dimensions aid exploration certain cases provide bridge isolated high density islands See use auxiliary variables perfect sampling Consider drawing samples p x For auxiliary variable y introduce distribution p y x form joint distribution p x y p y x p x If draw samples xl yl joint distribution valid set samples p x given taking xl If sampled x directly p x y p y x introducing y pointless effect x sampling procedure In order useful auxiliary variable influence sample x Below discuss common auxiliary variable schemes Hybrid Monte Carlo Hybrid MC method continuous variables aims non local jumps sample space jump potentially mode We define distribution DRAFT April Auxiliary Variable Methods Algorithm Hybrid Monte Carlo sampling Start x1 L Draw new sample y p y Choose random forwards backwards trajectory direction Starting xi y follow Hamiltonian dynamics fixed number steps giving candidate x y Accept candidate xi x H x y H x y accept probability exp H x y H x y If rejected sample xi xi end wish sample p x Zx eHx x given Hamiltonian Hx x We define easy auxiliary distribution readily generate samples p y Zy eHy y joint distribution given p x y p x p y Z eHx x Hy y Z eH x y H x y H x H y In standard form algorithm multi dimensional Gaussian chosen auxiliary distribution dim y dim x Hy y yTy The HMC algorithm draws p y subsequently p x y For Gaussian p y sampling straightforward In dynamic step sample drawn p x y Metropolis MCMC sampler The idea point space x y new point x y non trivial distance x y accepted high probability The candidate x y good chance accepted H x y close H x y achieved following contour equal energy H described section Hamiltonian dynamics We wish update x x x y y y small x y Hamiltonian H x y Hx x Hy y conserved H x y H x y We satisfy order considering Taylor expansion H x y H x x y y H x y xT xH x y yT yH x y O x O y Conservation order requires xT xH x y yT yH x y This single scalar requirement different solutions x y satisfy single condition It customary use Hamiltonian dynamics corresponds setting x yH x y y xH x y DRAFT April Auxiliary Variable Methods b c Figure Swendson Wang updating p x j expI xi xj Current sample states nearest neighbour lattice b Like coloured neighbours bonded probability e forming clusters variables c Each cluster given random colour forming new sample small value ensure Taylor expansion accurate Hence x t x t yHy y y t y t xHx x For HMC method xH x y xHx x yH x y yHy y For Gaussian case yHy y y x t x t y y t y t xH x There specific ways implement Hamiltonian dynamics called Leapfrog discretisation accurate simple time discretisation refer reader details In order symmetric proposal distribution start dynamic step choose uniformly We follow Hamiltonian dynamics time steps usually order reach candidate point x y If Hamiltonian dynamics numerically accurate H x y roughly value H x y We Metropolis step accept point x y H x y H x y accept probability exp H x y H x y If rejected initial point x y sample Combined p y sample step general procedure described algorithm In HMC use potential Hx x define candidate samples gradient Hx x An intuitive explanation success algorithm myopic straightforward Metropolis gradient enables algorithm feel way regions high probability contouring paths augmented space One view auxiliary variables momentum variables sample momentum carry low density x regions Provided momentum high escape local regions significant probability fig Swendson Wang Originally SW method introduced alleviate problems encountered sampling Ising models close critical temperature At point large islands state variables form strong correlations appear distribution scenario example Gibbs sampling suited The method generalised models outline procedure Ising model referring reader specialised text extensions The Ising model external fields defined variables x x1 xn xi takes form p x Z j eI xi xj means pairwise Markov network potential contribution e neighbouring nodes j square lattice state contribution We assume encourages neighbours state The lattice based neighbourhood structure makes difficult sample especially encourages large scale islands state DRAFT April Auxiliary Variable Methods Algorithm Swendson Wang sampling Start random configuration x11 x n l L j edge set If xl 1i x l j bind variables xi xj probability e end For cluster formed bonds set state cluster uniformly random This gives new joint configuration xl1 x l n end variables form The aim remove problematic terms eI xi xj use auxiliary real valued bond variables yij edge lattice making conditional p x y easy sample This given p x y p y x p x p y x j eI xi xj Using p y x cancel terms eI xi xj setting p y x j p yij xi xj j zij I yij e I xi xj I yij e I xi xj denotes uniform distribution eI xi xj zij normalisation constant zij e I xi xj Hence p x y p y x p x j eI xi xj I yij e I xi xj eI xi xj j I yij e I xi xj We assume sample bond variables yij If yij draw sample p x y eI xi xj means xi xj constrained state Otherwise yij introduces constraint xi xj Hence yij bind xi xj state To sample bond variables p yij xi xj consider situation xi xj state Then p yij xi xj U yij e A bond occur yij occurs probability p yij xi xj yij zij I yij e e e e Hence xi xj bind xi xj state probability e On hand xi xj different states p yij xi xj U yij yij uniformly distributed Figure Ten successive samples Ising model p x exp j I xi xj close critical temperature The Swendson Wang procedure Starting random initial configuration samples quickly away initial state The samples display characteristic long range correlations close critical temperature DRAFT April Auxiliary Variable Methods Figure The slice given y Ideally slice sampling draw x sample slice green In general intractable complex distribution local approximate slice formed instead fig b c d Figure For current sample x point y sampled p x giving point x y black circle Then interval width w placed x blue bar The ends bar denote point slice green slice red b The interval increased w hits point slice c Given interval sample x taken uniformly interval If candidate x slice red p x y candidate rejected interval shrunk d The sampling interval repeated candidate slice green subsequently accepted After xi xj pairs end graph clusters like state bonded variables The algorithm simply chooses random state cluster probability variables cluster state This useful able update variables strongly dependent This contrast Gibbs sampling variables strongly dependent rarely change state variable neighbours strongly encouraged agree results critical slowing effect Gibbs sampling essentially freezes See algorithm fig fig This technique found application spatial statistics particularly image restoration Slice sampling Slice sampling auxiliary variable technique aims overcome difficulties choos ing appropriate length scale methods Metropolis sampling The brief discussion follows presented We want draw samples p x Z p x normalisation constant Z unknown By introducing auxiliary variable y defining distribution p x y Z y p x p x y dy p x Z dy Z p x p x shows marginal p x y y equal distribution wish draw samples Hence draw samples p x y ignore y samples valid sampling scheme p x To draw p x y use Gibbs sampling drawing p y x p x y Drawing sample p y x means draw value y uniform distribution U y p x Given DRAFT April Importance Sampling Algorithm Slice Sampling univariate Choose starting point x1 step size w L Draw vertical coordinate y uniformly interval p xi Create horizontal interval xleft xright contains x follows Draw r U r xleft x rw xright xi r w Create initial interval p xleft y xleft xleft w Step left end p xright y xright xright w Step right end accept false accept false draw random value x uniformly unit interval xleft xright p x y accept true Found valid sample modify interval xleft xright follows x xi xright x Shrinking xleft x end end end xi x end sample y draws sample x p x y Using p x y p x y p x y distribution x p x y p x y I p x y That x uniformly distributed slice satisfies p x y fig Computing malisation distribution general non trivial principle need search x find p x y The challenge slice sampling sample slice This addressed MCMC techniques Ideally like slice feasible improve mixing chain rate converge stationary distribution If concentrate slice local current x samples space slowly If attempt guess random point long way x check slice unsuccessful We discuss general highlight achieved univariate slice The happy compromise presented algorithm described fig determines appropriate incremental widening initial interval Once largest potential interval determined attempt sample If sample point interval fact slice rejected interval shrunk Importance Sampling Importance sampling technique approximate averages respect intractable distribution p x The term sampling arguably misnomer method attempt draw samples DRAFT April Importance Sampling p x Rather method draws samples simpler importance distribution q x reweights averages respect p x approximated samples q x Consider p x p x Z p x evaluated Z x p x intractable normalisation constant The average f x respect p x given x f x p x x f x p x x p x x f x p x q x q x x p x q x q x Let x1 xL samples q x We approximate average x f x p x L l f x l p xl q xl L l p xl q xl L l f xl wl define normalised importance weights wl p xl q xl L l p xl q xl L l wl In principle reweighing samples q correct result average respect p The importance weight vector w seen measure q fits p q p weight vector uniform values w L Hence variable components weight vector matched q p Since weight measure q matches p q p matched typically small number dominant weights This particularly evident high dimensions As indication effect consider D dimensional multivariate x Using u vector unnormalised importance weights ui p x q xi measure variability components u given dropping momentarily notational dependence x ui uj u2i u2j ui uj averages respect q The mean unnormalised weights ui uj x p x q x q x u2i u2j p2 x q2 x q x p x q x p x For simplicity assume q x p x factorise p x D d p xd q x D d q xd distributions axis aligned co ordinate d Then u2i p x q x D p x Since p x q x p x q p divergence exercise variability weights ui uj p x q x D p x grows exponentially dimension D This means single unnormalised importance weight likely dominate After normalisation typically high dimensions weight vector w single significantly non zero component A method help address weight dominance resampling Given weight distribution w1 wL draws set L sample indices This new set indices certainly contain repeats original low weight samples likely included The weight new samples set uniformly L This procedure helps select fittest samples known Sampling Importance Resampling DRAFT April Importance Sampling v1 v2 v3 v4 h1 h2 h3 h4 Figure A dynamic belief network In ap plications interest emission distribution p vt ht non Gaussian leading formal intractability filtering smoothing Sequential importance sampling Our interest apply importance sampling temporal distributions p x1 t importance samples q x1 t paths x1 t When new observation time t arrives require set sample paths xl1 t q x1 t If assume previous time set paths x l t importance weights wlt find new importance weights w l t simply sampling transition q xt x1 t updating weights needing explicitly sample new paths To consider unnormalised importance weights sample path xl1 t w lt p xl1 t q xl1 t p xl1 t q xl1 t p xl1 t p xl1 t q x l t xl1 t w l1 p xl1 q xl1 Using p x1 t p xt x1 t p x1 t ignore constants equivalently define unnormalised weights recursively w lt w l t l t t lt p xlt xl1 t q xlt xl1 t This means Sequential Importance Sampling SIS need define conditional impor tance distribution q xt x1 t The ideal setting sequential importance distribution q xt x1 t p xt x1 t choice impractical cases discuss Sequential importance sampling known particle filtering Dynamic belief networks Consider distributions hidden Markov independence structure fig p v1 t h1 t p v1 h1 p h1 t t p vt ht emission p ht ht transition v1 t observations h1 t random variables Our interest draw sample paths h1 t given observations v1 t In models HMM straightforward However cases example emission p vt ht intractable normalise use SIS draw samples instead For dynamic belief networks equation simplifies lt p vt hlt p hlt hlt q hlt hl1 t The optimal importance distribution achieved importance transition q ht h1 t p vt ht p ht ht It case drawing samples optimal q ht h1 t difficult In cases transition easy sample common sequential importance distribution q ht h1 t p ht ht case equation lt p vt ht unnormalised weights recursively defined w lt w l t 1p vt hlt A drawback procedure small number iterations particle weights significantly non zero mismatch importance distribution q target distribution p This addressed resampling described section DRAFT April Importance Sampling Particle filtering approximate forward pass Particle filtering viewed approximation exact filtering recursion Using represent filtered distribution ht p ht v1 t exact filtering recursion ht p vt ht ht p ht ht ht A PF viewed approximation equation message ht approximated sum delta spikes ht L l wlt ht h l t wlt normalised importance weights L l 1w l t h l t particles In words message represented weighted mixture delta spikes weight position spikes parameters distribution Using equation equation ht Z p vt ht L l p ht hlt wlt The constant Z normalise distribution ht Although ht simple sum delta spikes general ht delta spikes broadened transition emission factors Our task approximate ht new sum delta spikes Below discuss method achieve explicit knowledge normalisation Z required This useful tracking applications normalisation emission p vt ht unknown A Monte Carlo sampling approximation A simple approach forming approximate mixture delta functions representation equation generate set points sampling In principle sampling method including pow erful MCMC approaches In particle filters importance sampling use generate new particles That generate set samples h1t h L t importance distribution q ht gives unnormalised importance weights w lt p vt hlt L l p h l t hl t w l t q hlt Defining normalised weights wlt w lt l w l t obtain approximation ht L l wlt ht h l t Ideally use importance distribution makes importance weights uniform q ht p vt ht L l p ht hlt wlt DRAFT April Importance Sampling particle weights particle weights b Figure Tracking object particle filter containing parti cles The small circles par ticles scaled weights The correct corner position face given filtered average large circle o likely particle Ini tial position face noise corresponding weights par ticles b Face noisy ground tracked corner posi tion timesteps The Forward Sampling Resampling PF method maintain healthy pro portion non zero weights See demoParticleFilter m However difficult sample directly unknown normalisation emission p vt ht A simpler alternative sample transition mixture q ht L l p ht hlt wlt To samples component l histogram weights w1t w L t Given sample index l draws sample p ht hl t In case unnormalised weights simply w lt p vt hlt This Forward Sampling Resampling procedure demoParticleFilter m following toy example Example A toy face tracking example At time t binary face template dimensional location ht describes upper left corner template At time t position face known fig 15a In subsequent times face moves randomly according ht ht t t N t I dimensional zero mean unit covariance noise vector In addition fraction binary pixels image selected random states flipped The aim try track upper left corner face time We need define emission distribution p vt ht binary pixels vi Consider following compatibility function vt ht v T t v ht DRAFT April Importance Sampling v ht vector representing image clean face placed position ht zeros outside face template Then vt ht measures overlap face template specific location noisy image restricted template pixels The compatibility function maximal observed image vt face placed position ht We define p vt ht vt Tht 1Tht A subtlety ht continuous compatibility function map ht nearest integer pixel representation In fig 15a particles track face The particles plotted corresponding weights For t pixels selected random image states flipped Using forward sampling resampling method successfully track face despite presence background clutter Real tracking applications involve complex issues including tracking multiple objects transformations object scaling rotation morphology changes Nevertheless principles largely tracking applications work seeking compatibility functions based colour histogram template Summary Exact sampling achieved models belief networks forward sampling case evidence inefficient Provided independent samples drawn distribution small number samples required obtain good estimate expectation However drawing independent samples high dimensional non standard distributions computationally extremely difficult Markov chain Monte Carlo methods approximate sampling methods converge drawing samples correct distribution limit large number samples Whilst powerful assessing conver gence method difficult Also samples highly dependent great number samples required obtain reliable estimate expectation Code potsample m Exact sample set potentials ancestralsample m Ancestral sample belief network JTsample m Sampling consistent junction tree GibbsSample m Gibbs sampling set potentials demoMetropolis m Demo Metropolis sampling bimodal distribution metropolis m Metropolis sample logp m Log bimodal distribution demoParticleFilter m Demo particle filtering forward sampling resampling method placeobject m Place object grid compat m Compatibility function demoSampleHMM m Naive Gibbs sampling HMM DRAFT April Exercises Exercises Exercise Box Muller method Let x1 U x1 x2 U x2 y1 log x1 cos 2x2 y2 log x1 sin 2x2 Show p y1 y2 p y1 x1 x2 p y2 x1 x2 p x1 p x2 dx1dx2 N y1 N y2 suggest algorithm sample univariate normal distribution Hint Use change variable result result vectors y y1 y2 x x1 x2 Exercise Consider distribution p x exp sin x x Using rejection sampling q x N x suitable value M p s q x M M e For suitably chosen draw samples p x plot resulting histogram samples Exercise Consider distribution p x1 x6 p x1 p x2 p x3 x1 x2 p x4 x3 p x5 x3 p x6 x4 x5 For x5 fixed given state x5 write distribution remaining variables p x1 x2 x3 x4 x6 explain forward ancestral sampling carried new distribution Exercise Consider Ising model M M square lattice nearest neighbour interactions p x exp j I xi xj Now consider M M grid checkerboard white square label wi black square label bj square associated particular variable Show p b1 b2 w1 w2 p b1 w1 w2 p b2 w1 w2 That conditioned white variables black variables independent The converse true conditioned black variables white variables independent Explain exploited Gibbs sampling procedure This procedure known checkerboard black white sampling Exercise Consider symmetric Gaussian proposal distribution q x x N x x 2qI target distribution p x N x 2pI dim x N Show log p x p x q x x N2q 22p Discuss result relates probability accepting Metropolis Hastings update Gaussian proposal distribution high dimensions DRAFT April Exercises Exercise The file demoSampleHMM m performs naive Gibbs sampling posterior p h1 T v1 T HMM T At Gibbs update single variable ht chosen remaining h variables clamped The procedure starts t sweeps forwards time When end time t T reached joint state h1 T taken sample posterior The parameter controls deterministic hidden transition matrix p ht ht Adjust demoSampleHMM m run times computing mean absolute error posterior marginal p ht v1 T runs Then repeat Finally repeat procedure times different random draws transition emission matrices average errors computing smoothed posterior marginal Gibbs sampling Discuss performance Gibbs sampling routine deteriorates increasing Exercise Consider following MATLAB code snippet c p r rand r c n c c p end sample Explain draws sample state discrete distribution probability p n Explain effectively sample distribution p e contains infinite number discrete states Exercise This exercise discusses technique called Approximate Bayesian Computation ABC aims provide approximation parameter posterior p D cases sample D gener ated likelihood p D likelihood calculated unknown normalisation constant For example specify model xt f xt t t N t easily sam ple dataset D x1 xT needing specify normalisation constant p D Consider drawing samples posterior p D p D p Show q D D q D D p D p q D D D D marginal distribution q D p D Hence following procedure generates sample p D Sample p b Sample candidate dataset D p D c If D D accept candidate sample sample One relax delta function constraint use example q D D N D D 2I chosen Then p D q D l wl l wl q D Dl L l q D Dl Explain approximate sampling procedure related Parzen estimator Note requires ability sample dataset D given explicitly require normalisation p D By considering single D dimensional datapoint x sampled datapoint xi generated underlying distribution N x 2ID D unnormalised weight ui exp x xi DRAFT April Exercises typical ratio weights assessed log ui uj N x 2I N xi 2I N xj 2I 12D Explain normalised weights typically dominated single component rendering sampling method described generally impractical This shows curse dimen sionality methods ABC meaning general techniques need considerable care high dimensional parameter spaces Exercise Soccer teams Aces Bruisers arch rivals They played times season play final game season The goalkeepers teams season final game However team squad players players selected squad team players plus goalkeeper game The file soccer mat contains history team sheets team won game Aces win Bruisers loss Both team coaches use universal rating system player genius superb average muppet worse grandma The outcome assumes goalkeepers equally matched abilities independently game modelled p Aces beats Bruisers ta tb b atai btbi x exp x Here aj j ability player j Aces bj j ability player j Bruisers The players selected play Aces team given tai similarly Bruisers tbi Which best players Aces best Bruisers Given know Bruisers field players numbered best team players Aces field maximise chance winning How relate selecting best Aces players DRAFT April CHAPTER Deterministic Approximate Inference Sampling methods popular known approximate inference In chapter introduction known class deterministic approximation techniques These spectacularly successful branches information sciences origins study large scale physical systems Introduction Deterministic approximate inference methods alternative sampling techniques discussed chapter Drawing exact independent samples typically computationally intractable assessing quality sample estimates difficult In chapter discuss alternatives The Laplace s method simple perturbation technique The second class methods produce rigorous bounds quantities interest Such methods interesting provide certain knowledge sufficient example marginal probability greater order informed decision A class methods consistency methods loopy belief propagation Such methods revolutionised certain fields including error correction It important bear mind single approximation technique deterministic stochastic going beat problems given computational resources In sense insight properties approximations useful matching approximation method problem hand The Laplace approximation Consider distribution continuous variable form p x Z e E x The Laplace method makes Gaussian approximation p x based local perturbation expansion mode x First find mode numerically giving x argmin x E x Then Taylor expansion second order mode gives E x E x x x T E x x x T H x x H E x x Hessian evaluated mode At mode E x approxima tion distribution given Gaussian q x Zq e x x TH x x N x x H Properties Kullback Leibler Variational Inference mean x covariance H Zq det 2H We use expansion estimate integral x e E x x e E x x x TH x x e E x det 2H The Laplace Gaussian fit distribution necessarily best Gaussian approximation As ll criteria based minimal KL divergence p x Gaussian approximation appropriate depending context A benefit Laplace s method relative simplicity compared approximate inference techniques Properties Kullback Leibler Variational Inference Variational methods approximate complex distribution p x simpler distribution q x Given definition discrepancy approximation q x p x free parameters q x set minimising discrepancy This class techniques called mean field methods physics literature A particularly popular measure discrepancy approximation q x intractable distribution p x Kullback Leibler divergence KL q p log q q log p q It straightforward KL q p zero distributions p q identical section Note whilst KL divergence negative upper bound value potentially discrepancy infinitely large Bounding normalisation constant For distribution form p x Z e x KL q p log q x q x log p x q x log q x q x x q x logZ Since KL q p immediately gives bound logZ log q x q x entropy x q x energy called free energy bound physics community The KL q p method provides lower bound normalisation constant The art choose class approximating distributions q entropy energy term tractably computable Bounding marginal likelihood In Bayesian modelling likelihood model M parameters generating data D given p D M p D M likelihood p M prior This quantity fundamental model comparison However cases high dimensional integral difficult perform Using Bayes rule p D M p D M p M p D M DRAFT April Properties Kullback Leibler Variational Inference Figure Fitting mixture Gaussians p x blue single Gaus sian The green curve minimises KL q p corresponding fitting local model The red curve minimises KL p q corresponding moment match ing considering KL q p D M log q q log p D M q log q q log p D M p M q log p D M non negativity Kullback Leibler divergence gives bound log p D M log q q log p D M p M q log p D M q KL q p M This bound holds distribution q equality q p D M Since optimal setting assumed computationally intractable idea variational bounding choose distribution family q bound computationally tractable maximise bound respect free parameters q The resulting bound surrogate exact marginal likelihood model comparison Bounding marginal quantities The Kullback Leibler approach provides lower bound normalisation constants Combined upper bound obtained alternative methods example exercise able bracket marginals l p xi u exercise The tightness resulting bracket gives indication tight bounding procedures Even cases resulting bracket weak example result p cancer true sufficient decision making purposes probability cancer sufficiently large merit action Gaussian approximations KL divergence Minimising KL q p Using simple approximation q x complex distribution p x minimising KL q p tends solution q x focuses local mode p x underestimating variance p x To consider approximating mixture Gaussians equal variance p x N x N x fig single Gaussian q x N x m s2 We wish find optimal m s2 minimise KL q p log q x q x log p x q x If consider case Gaussian components p x separated setting q x centred left mode Gaussian q x appreciable mass close second mode negligible contribution Kullback Leibler divergence In sense approximate p x q x KL q p log q x q x log p x q x log DRAFT April Properties Kullback Leibler Variational Inference Figure A planar pairwise Markov random field set variables x1 x25 representing distribution form j xi xj In statistical physics lat tice models include Ising model binary spin variables xi xi xj e wijxixj On hand setting m correct mean distribution p x little mass mixture captured s2 large giving poor fit large KL divergence Another way view consider KL q p log q x p x q x provided q close p q significant mass ratio q x p x order KL divergence small Setting m means q x p x large q significant mass poor fit The optimal solution case place Gaussian close single mode However modes separated optimal solution necessarily place Gaussian local mode In general optimal Gaussian fit needs determined numerically closed form solution finding optimal mean co variance parameters Minimising KL p q Bearing mind general KL q p KL p q useful understand properties KL p q For fitting Gaussian q x N x m s2 p based KL p q KL p q log p x p x log q x p x 2s2 x m p x log s2 const Minimising respect m s2 obtain m x p x s x m p x optimal Gaussian fit matches second moments p x In case fig mean p x zero variance p x large This solution dramatically different produced fitting Gaussian KL q p The fit found KL q p focusses making q fit p locally exercise KL p q focusses making q fit p global statistics distribution possibly expense good local match Marginal moment matching properties minimising KL p q For simplicity consider factorised approximation q x q xi Then KL p q log p x p x log q xi p xi The entropic term independent q x constant independent q x KL p xi q xi optimally q xi p xi That optimal factorised approximation set factors q xi marginals p xi exercise Another approximating distribution class yields known form approximation exponential family In case minimising KL p q corresponds moment matching exercise In practice generally compute moments p x distribution p x considered intractable fitting q p based KL p q lead practical algorithm approximate inference Nevertheless useful subroutine local approximations particular Expectation Propagation DRAFT April Variational Bounding Using KL q p Figure A distribution pixels The filled nodes indicate observed noisy pixels unshaded nodes Markov Random Field latent clean pixels The task infer clean pixels given noisy pixels The MRF encourages posterior distribution clean pixels contain neighbouring pixels state Variational Bounding Using KL q p In section discuss fit distribution q x assumed family intractable distribution p x As saw case fitting Gaussians optimal q needs found numerically This complex task formally difficult performing inference directly intractable p reader wonder trade difficult inference task potentially difficult optimisation problem The general idea optimisation problem local smoothness properties enable rapidly find reasonable optimum based generic optimisation methods To ideas concrete discuss particular case fitting q formally intractable p section Pairwise Markov random field A canonical intractable distribution pairwise Markov Random Field1 defined binary variables xi D p x Z w b e j wijxixj bixi Here partition function Z w b ensures normalisation Z w b x e j wijxixj bixi Since x2i terms wiix constant loss generality set wii zero A motivational example model described Example Bayesian image denoising Consider binary image y generated corrupting clean image x interest recover clean image given corrupted image We assume noisy pixel generating process takes clean pixel xi flips binary state p y x p yi xi p yi xi eyixi The probability yi xi state e e e Our interest posterior distribution clean pixels p x y We assume clean images reasonably smooth described MRF prior defined lattice fig p x e ij wijxixj wij neighbouring j wij This encodes assumption clean images tend neighbouring pixels state An isolated pixel different state neighbours unlikely prior We joint distribution p x y p x p yi xi Z e ij wijxixj yixi 1Whilst inference general MRF formally computationally intractable exact polynomial time methods known celebrated results mention passing planar MRF model pure interactions b partition function computable polynomial time MAP state attractive planar Ising models w section DRAFT April Variational Bounding Using KL q p fig posterior given p x y p y x p x x p y x p x e ij wijxixj yixi Quantities MAP state posteriori probable image marginals p xi y normali sation constant interest This example equivalent Markov network example On left clean image middle noisy image right likely posterior clean image arg maxx p x y found iterated conditional modes section We discuss compute MAP state example section concentrate technique bound normalisation constant Z quantity useful model comparison Kullback Leibler based methods For MRF KL q p log q q ij wij xixj q bi xi q logZ Rewriting gives bound log partition function logZ log q q entropy ij wij xixj q bi xi q energy The bound saturates q p This little help compute averages xixj p xi p respect intractable distribution p The idea variational method assume simpler tractable distribution q averages computed entropy q Minimising KL divergence respect free parameters q x equivalent maximising lower bound log partition function Factorised approximation A naive assumption fully factorised distribution q x qi xi The graphical model approximation given fig 4a In case logZ log qi qi ij wij xixj q xi xj bi xi q xi b c Figure Naive Mean Field approximation q x qi xi b A spanning tree approxima tion c A decomposable hypertree approxima tion DRAFT April Variational Bounding Using KL q p For factorised distribution q x q xi xixj xi xj j For binary variable use convenient parametrization qi xi ei ei e xi xi qi q xi q xi tanh This gives following lower bound log partition function logZ B H j wij tanh tanh j bi tanh H binary entropy distribution parameterised according equation H log ei e tanh Finding best factorised approximation minimal Kullback Leibler divergence sense corre sponds maximising bound B respect variational parameters The bound B equation generally non convex riddled local optima Finding globally optimal typically computationally hard problem It simply replaced computa tionally hard problem computing logZ equally hard computational problem maximising B Indeed written factor graph structure optimisation problem matches exactly original MRF However hope approximating difficult discrete summation contin uous optimisation problem able bring table effective continuous variable optimisation techniques A particularly simple optimisation technique solve zero derivative bound equation Differentiating equating zero little algebra leads requirement optimal solution satisfies equations bi j wij tanh j One sequentially updating according equation increases B This called asynchronous updating guaranteed lead local minimum KL divergence section Once converged solution identified addition bound logZ approximate xi p xi q tanh Validity factorised approximation When expect naive factorised approximation work Clearly equation wij small distribution p effectively factorised approximation accurate A interesting case variable xi neighbours In case useful write MRF ignoring bias terms bi simplicity p x Z e ij wijxixj Z eD xi D j wijxj Z eD xizi local fields defined zi D j wijxj An interesting question zi distributed We invoke circular self consistent argument Let s assume p x factorised Assuming wij O mean zi zi D j wij xj O DRAFT April Variational Bounding Using KL q p The variance z2i zi D2 D k w2ik xk O D Hence large D variance field zi smaller mean value Since terms xj summation j wijxj independent provided wij extreme conditions validity central limit theorem hold zi Gaussian distributed In particular D increases fluctuations mean diminish write p x Z eD xi zi p xi The assumption p approximately factorised self consistent limit MRFs large number neighbours Hence factorised approximation appear reasonable extreme limits weakly connected system wij ii large densely connected system random weights The fully factorised approximation called naive mean field theory MRF case assumes replace effect neighbours mean field site General mean field equations For general intractable distribution p x discrete continuous x KL divergence factorised approximation q x q xi p x KL q x p x log q xi q xi log p x q xi Isolating dependency single factor q xi log q xi q xi log p x j q xj q xi Up normalisation constant KL divergence q xi distribution pro portional exp log p x j q xj optimal setting q xi satisfies q xi exp log p x j q xj These known mean field equations define new approximation factor terms previous approximation factors Note normalisation constant p x unknown presents problem constant simply absorbed normalisation factors q xi In words replace p x unnormalised p x equation Beginning initial randomly chosen set distributions q xi mean field equations iterated convergence Asynchronous updating guaranteed decrease KL divergence stage Asynchronous updating guarantees approximation improvement For factorised variational approximation equation claim update equation reduces Kullback Leibler approximation error To write single updated distribution qnewi Zi exp log p x j q old j The joint distribution single update qnew qnewi j qoldj Our interest change approximation error single mean field update KL qnew p KL qold p DRAFT April Variational Bounding Using KL q p x1 x2 x3x4 x1 x2 x3x4 b Figure A toy intractable distribution b A structured singly connected approximation Using KL qnew p log qnewi qnewi j log qoldj qoldj log p x j q old j qnewi defining un normalised distribution q xi exp log p x j q old j Ziq new log qnewi qnewi log qoldi qoldi log p j q old j qnewi log p j q old j qoldi log q qnewi logZi log qoldi qoldi log q qnewi log q qoldi logZi log qoldi qoldi log q qoldi KL qoldi qnewi Hence KL qnew p KL qold p updating single component q time guaranteed improve approximation Note result general holding distribution p x In case Markov network guaranteed approximation improvement equivalent guaranteed increase strictly speaking non decrease lower bound partition function Remark Intractable energy Even fully factorised approximation mean field equations tractably implementable For need able compute log p x j q xj For models interest possible additional approximations required Structured variational approximation One extend factorised KL variational approximation non factorised q x Those averages variables computed linear time include spanning trees fig 4b decomposable graphs fig 4c For example distribution fig 5a p x1 x2 x3 x4 Z x1 x2 x2 x3 x3 x4 x4 x1 x1 x3 tractable q distribution fig 5b q x1 x2 x3 x4 Z x1 x2 x1 x3 x1 x4 In case KL q p Hq x1 x2 Hq x1 x3 Hq x1 x4 3Hq x1 j log xi xj q xi xj DRAFT April Variational Bounding Using KL q p Hq X entropy q X Since q singly connected computing marginals entropy straightforward entropy requires pairwise marginals graph neighbours In case apply directly standard mean field update equations constraints xj q xi xj q xj We deal Lagrange multipliers ensure perform ing mean field style update including contributions coupled terms More generally exploit structural approximation use example junction tree algorithm compute required moments However computational expense typically increases expo nentially hypertree width Example Robot arm control inference Control problems cast inference problems example Consider position vt xt yt T n link robot arm dimensional place link n arm unit length angle hi t xt n coshi t yt n sinhi t Our interest use robot arm track given sequence v1 T joint angles ht change time This classical control problem formulate inference problem model p v1 T h1 T p v1 h1 p h1 T t p vt ht p ht ht terms p vt ht N vt n coshi t n sinhi t T 2I p ht ht N ht ht 2I One solution control problem given likely posterior sequence arg maxh1 T p h1 T v1 T Here consider alternative maximum posterior marginal solution time arg maxht p ht v1 T Due non linear observation posterior marginal computed exactly A simple approximation use fully factorised variational distribution p h1 T v1 T q h1 T q h1 T T t n q hi t From general form mean field equations update q hi t given t T log q hi t hi t h t hi t h t coshi t t sinhi t t const h t hi t t xt j coshj t t yt j sinhj t averages respect q marginal distribution Due non linearities marginal distributions non Gaussian However dimensional required aver ages readily computed quadrature The mean field equations iterated convergence See demoRobotArm m details fig demonstration Note planning problem starting point x1 y1 smoothly arm desired end point xT yT special case framework One achieve removing observation terms intermediate times equivalently making observation variances intermediate times extremely large DRAFT April Local KL Variational Approximations b c Figure The desired trajectory end point link robot arm Green denotes time red time b The learned trajectory based fully factorised KL variational approximation c The robot arm sections 2nd timestep time left time right The control problem matching trajectory smoothly changing angle set solved simple approximation Local KL Variational Approximations In fitting model parameterised w data D encounters parameter posteriors form p w D Z N w f w Z N w f w dw A classical example Bayesian logistic regression section N w represents prior weight w f w represents likelihood p D w Z p D In limited special cases function f w simple squared exponential resulting posterior distribution non standard form Inevitably approximations required When dimension parameter vector DRAFT April Local KL Variational Approximations dim w W large finding accurate posterior approximation general non trivial Our particular interest form approximation p w D gives principled lower bound marginal likelihood p D Local approximation In local method replaces f suitable function integral computed Since case integrand composed Gaussian w convenient bound f w exponential quadratic function f w c e wTF w wTf matrix F vector f scalar c depend specific function f variational parameter enables find tightest bound We discuss explicit c F f functions later moment leave unspecified Then equation Z c det exp w T w exp wTF w wTf dw Which expressed Z c e T det exp wTAw wTb dw A F b f Whilst A b functions notationally drop dependency description compact Completing square integrating logZ B B log c T bTAb log det A To obtain tightest bound logZ maximises B respect In practical problems interest f w M s fs w local site functions fs By bounding site individually obtain bound B M The bound optimised numerically respect vector KL variational approximation An alternative local bounding method consider KL approach based fitting Gaussian distribution Defining p w N w f w Z By fitting Gaussian q w N w m S based minimising KL q w p w obtain bound logZ BKL m S BKL m S log q w log det w T w log f w denotes expectation respect q w One numerically finds best parameters m S maximise bound Since entropy Gaussian trivial potentially problematic term evaluating bound log f w A class functions log f w N w m S computationally tractable f w f wTh fixed vector h In case projection wTh Gaussian distributed log f wTh N w m S log f N mTh hTSh DRAFT April Mutual Information Maximisation A KL Variational Approach readily computed dimensional integration routine Explicitly function m S 2BKL m S log det S W log det trace S m m T log f N mTh hTSh Whilst general variational bounds non concave variational parameters provided f log concave BKL m S jointly concave m S By structured covariances S method scalable high dimensional problems Relation KL local bounds The KL local variational methods provide lower bound normalisation term Z equation It interesting understand relation bounds Using bound f w obtain new bound BKL m S B KL m S B KL log q w log det w T w log c wTF w wTf equation written B KL log q w log det log c T wTAw wTb By defining q w N w A 1b A B KL KL q w q w log det log c T bTA 1b log det 2A Since m S appear q w KL term tightest bound given m S set q w q w At setting KL term B KL disappears m S given S F m S f For setting variational parameters bound matches local bound Since BKL m S B KL m S BKL m S B KL m S B Importantly VG bound tightened setting max m S BKL m S BKL m S Thus optimal KL bound provably tighter local variational bound KL bound calculated optimal local moments m S Mutual Information Maximisation A KL Variational Approach Here short interlude discuss application Kullback Leibler variational approach information theory A common goal maximise information transfer measured mutual information definition I X Y H X H X Y DRAFT April Mutual Information Maximisation A KL Variational Approach x y1 y2 y3 y4 Figure An information transfer problem For fixed distribution p x parameterised distributions p yj x wTi x find op timal parameters wi maximise mutual information variables x y Such considerations popular theoretical neuroscience aim understand receptive fields wi neuron relate statistics environment p x Algorithm I am algorithm maximising mutual information I X Y fixed p x adjustable parameters p y x Choose class approximating distributions Q example factorised Initialise parameters repeat new argmax log q x y p x p y x qnew x y argmax q x y Q log q x y p x p y x new converged entropy conditional entropy defined H X log p x p x H X Y log p x y p x y Here interested situation p x fixed p y x adjustable parameters wish set order maximise I X Y In case H X constant optimisation problem equivalent minimising conditional entropy H X Y Unfortunately cases practical interest H X Y computationally intractable We discuss section general procedure based Kullback Leibler divergence approximately maximise mutual information Example Consider neural transmission system xi denotes emitting neuron non firing state firing state yj receiving neuron If receiving neuron fires independently depending emitting neurons p y x p yi x example use p yi x wTi x Given empirical distribution neural firings p x interest set weights wi maximise information transfer fig Since p x fixed requires maximising log p x y p y x p x The quantity p x y p y x p x p y non factorised function y term p y This means conditional entropy typically intractable compute require approximation The information maximisation algorithm Consider KL p x y q x y DRAFT April Mutual Information Maximisation A KL Variational Approach d b c e f Figure Belief propagation derived considering compute marginal variable MRF In case marginal p d depends messages transmitted neighbours d By defining local messages links graph recursive algorithm computing marginals derived text This immediately gives bound x p x y log p x y x p x y log q x y Multiplying sides p y obtain x y p y p x y log p x y x y p x y log q x y From definition left bound H X Y Hence I X Y H X log q x y p x y I X Y From lower bound mutual information arrive information maximisation I am algorithm Given distribution p x parameterised distribution p y x seek maximise I X Y spect A co ordinate wise optimisation procedure presented algorithm The Blahut Arimoto algorithm information theory example special case optimal decoder q x y p y x p x In applications Blahut Arimoto algorithm intractable implement I am algorithm provide alternative restricting q tractable family distributions tractable sense lower bound computed The Blahut Arimoto algorithm analogous EM algorithm maximum likelihood guarantees non decrease mutual information stage update section Similarly I am procedure analogous variational EM procedure step procedure decrease lower bound mutual information Linear Gaussian decoder A special case I am framework use linear Gaussian decoder q x y N x Uy log q x y x Uy T x Uy log det Plugging bound equation optimising respect U obtain x Uy x Uy T U xyT yyT p x y Using setting bound obtain I X Y H X log det xxT xyT yyT yxT N log N dim x This equivalent Linsker s Gaussian approximation mutual information One view Linsker s approach special case I am algorithm restricted linear Gaussian decoders In principle improve Linsker s method considering powerful non linear Gaussian decoders Applications technique neural systems discussed DRAFT April Loopy Belief Propagation xi xjx1 x2 x3 x1 xi xi x x x x x x xi xj xj Figure Loopy Belief Propagation Once node received incoming messages neighbours excluding wants send message send outgoing message neighbour xi xj xj xi xi xj x1 xi xi x2 xi xi x3 xi xi Loopy Belief Propagation Belief Propagation technique exact inference marginals p xi singly connected distributions p x There different formulations BP modern treatment sum product algorithm corresponding factor graph described section An important observation algorithm purely local updates unaware global structure graph This means graph multiply connected loopy apply algorithm happens Provided loops graph relatively long hope loopy BP converge good approximation true marginals When method converges results surprisingly accurate In following loopy BP motivated variational objective To connection classical BP algorithm factor graph sum product algorithm For reason briefly describe classical BP approach Classical BP undirected graph BP derived considering calculate marginal terms messages undirected graph Consider calculating marginal p d b c e f p b c d e f pairwise Markov network fig We denote node state symbol b d b denotes summation states variable b To compute summation efficiently distribute summations follows p d Z b b d b d d d d d f d f f d d e d e c c e c e e e d d define messages n1 n2 n2 sending information node n1 node n2 function state node n2 In general node xi passes message node xj xi xj xj xi xi xj k ne k j xk xi xi See fig At convergence marginal p xi given q xi ne j xj xi xi prefactor determined normalisation The pairwise marginals approximated q xi xj k ne j xk xi xi xi xj k ne j xk xj xj For singly connected distribution p message passing scheme converges marginal corresponds exact result For multiply connected loopy structures belief propagation generally result approximation DRAFT April Loopy Belief Propagation Loopy BP variational procedure A variational procedure corresponds loopy BP derived considering terms standard variational approximation based Kullback Leibler divergence KL q p We example pairwise Markov network defined potentials xi xj p x Z j xi xj j denotes unique neighbouring edges graph edge counted Using approximating distribution q x Kullback Leibler bound logZ log q x q x entropy j log xi xj q x energy Since log xi xj q x log xi xj q xi xj contribution energy depends q x pairwise marginals q xi xj This suggests marginals form natural parameters approximation Can find expression entropy log q x q x terms pairwise marginals Consider case required marginals q x1 x2 q x2 x3 q x3 x4 Either appealing junction tree representation straightforward algebra uniquely express q terms marginals q x q x1 x2 q x2 x3 q x3 x4 q x2 q x3 An intuitive way arrive result examining numerator equation The variable x2 appears twice variable x3 joint distribution replicated variables compensate overcounting x2 x3 dividing marginals In case entropy q x written Hq x log q x q x Hq x1 x2 Hq x2 x3 Hq x3 x4 Hq x2 Hq x3 More generally chapter decomposable graph represented q x c q Xc s q Xs q Xc marginals defined cliques graph Xc variables clique q Xs defined separators intersections neighbouring cliques The expression entropy distribution given sum marginal entropies minus separator entropies Bethe Free energy Consider Markov network corresponding non decomposable graph example cycle p x Z x1 x2 x2 x3 x3 x4 x4 x1 The energy requires approximating distribution defines pairwise marginals q x1 x2 q x2 x3 q x3 x4 q x4 x1 Assuming marginals given find expression entropy joint distribution q x terms pairwise marginals q xi xj In general possible graph contains loops DRAFT April Loopy Belief Propagation junction tree representation result cliques greater size However simple overcounting approximation write q x q x1 x2 q x2 x3 q x3 x4 q x4 x1 q x1 q x2 q x3 q x4 Using approximate entropy Hq x Hq x1 x2 Hq x2 x3 Hq x3 x4 Hq x1 x4 Hq xi In general distribution dimensionally consistent need compensate factor q xi ci ci number neighbours variable xi minus With approximation negative log partition function known Bethe free energy Our interest maximise expression respect parameters q xi xj subject marginal consistency constraints xj q xi xj q xi These constraints enforced Lagrange multipliers ij xi One write Bethe free energy approximated Kullback Leibler divergence constant F q j Hq xi xj ciHq xi j log xi xj q xi xj j xi ij xi q xi xj q xi xj We neglect including Lagrange terms enforce normalisation q add constant terms later absorbed explicitly normalising q Expression longer bound log partition function entropy approximation lower bound true entropy The task minimize equation respect parameters pairwise marginals q xi xj Lagrange multipliers A simple scheme optimise equation use fixed point iteration equating derivatives Bethe free energy respect parameters q xi xj zero likewise Lagrange multipliers Differentiating respect q xi xj equating zero obtain log q xi xj log xi xj ij xi ji xj const q xi xj xi xj ij xi ji xj ij xi exp ij xi Similarly differentiating respect q xi equating zero obtain ci log q xi j ne ij xi const q xi j ne ci ij xi From equation match equation mapping ij xi k ne j xk xi xi We verify assignment satisfies single marginal requirements equation equation j ne ci ij xi j ne k ne j xk xi xi ci j ne xj xi xi Hence fixed point equations minimising Bethe free energy equivalent belief propagation The convergence loopy belief propagation heavily dependent topology graph message updating schedule The potential benefit Bethe free energy viewpoint opens possibility general optimisation techniques BP The called double loop techniques iteratively isolate convex contributions Bethe Free energy interleaved concave contributions At stage resulting optimisiations carried efficiently DRAFT April Expectation Propagation w x y z original graph w x y z Factorised graph p distribution b p distribution c Figure The Markov network left wish approximate marginals p w p x p y p z All tables drawn uniform distribution raised power renormalised On right shown naive mean field approximation factorised structure b There states distribution Shown randomly sampled distribution isolated peaks suggesting distribution far factorised In case MF Gibbs sampling approximations perform poorly c As increased typically state distribution dominates Whilst distribution simple essentially factorised finding maximal single state numerically challenging See demoMFBPGibbs m fig Validity loopy belief propagation For Markov network loop change variable loop eventually reverberates variable However large number variables loop individual neighbouring links extremely strong numerical effect loop small sense influence variable small In cases expect belief propagation approximation accurate An area particular success loopy belief propagation inference error correction based low density parity check codes usually explicitly designed long loop property loopy belief propagation produces good results In examples practical interest loops short example Markov network lattice In cases naive implementation loopy BP likely fail A natural extension cluster variables alleviate strong local dependencies technique called Kikuchi Cluster Variation method More elaborate ways clustering variables considered region graphs Example The file demoMFBPGibbs m compares performance naive Mean Field MF theory belief propagation unstructured Gibbs sampling marginal inference pairwise Markov network p w x y z wx w x wy w y wz w z xy x y xz x z yz y z variables states In experiment tables selected uniform distribution raised power For close zero tables essentially flat variables independent situation MF BP Gibbs sampling ideally suited As increased dependencies variables increase methods perform worse especially MF Gibbs As increased distribution sharply peaked single state posterior effectively factorised fig This suggests MF approximation Gibbs sampling work However finding state computationally difficult methods stuck local minima fig Belief propagation susceptible trapped local minima regime tends outperform MF Gibbs sampling Expectation Propagation The messages schemes belief propagation representable compact form The switching linear dynamical system described chapter instance messages requir DRAFT April Expectation Propagation ing exponential storage This limits BP cases discrete networks generally exponential family messages Expectation Propagation extends applicability BP projecting messages chosen distribution family stage This projection obtained Kullback Leibler measure Consider distribution defined subsets variables Xi form p x Z Xi In EP identifies factors Xi replaced simpler factors Xi render distribution p x tractable One sets free parameters Xi minimising Kullback Leibler divergence KL p p The general approach outlined algorithm To gain intuition steps algorithm consider specific example pairwise Markov network p x Z x1 x2 x2 x3 x3 x4 x4 x1 factor graph depicted fig 12a If replace terms j xi xj approximate factors j xi j xj resulting joint distribution p factorised tractable Since variable xi appears term p need index approximation factors appropriately A convenient way p x Z x1 x2 x1 x2 x2 x3 x2 x3 x3 x4 x3 x4 x4 x1 x4 x1 represented fig 12b The idea EP determine optimal approximation factors self consistent requirement replacing approximation factors exact counterparts difference marginals p Consider approximation parameters x2 x3 To set replace contribution x2 x3 exact factor x2 x3 This gives modified approximation p Z x1 x2 x2 x3 x3 x4 x4 x1 x2 x3 Z p x2 x3 Z The intuition approximation parameters set correctly replacing approximation factors exact counterpart change computation marginals To measure modified p changes p use Kullback Leibler divergence distribution approximation KL p p log p p log p p BP MF Gibbs Figure The absolute error computing marginal p xi graph fig averaged marginals loopy belief propagation BP Gibbs sampling Gibbs naive mean field factorised approximation MF Plotted horizontal axis parameter controls complex ity true distribution fig The vertical axis error averaged random realisations true distribu tion For small distribution essentially factorised methods work reasonably As increases distribution peaked limited number states finding states increasingly difficult All methods use similar computation updates variables DRAFT April Expectation Propagation x1 x2 x3x4 x1 x2 x3x4 b x1 x2 x3x4 c Figure Multiply connected fac tor graph representing p x b Expecta tion propagation approximates terms tractable factor graph The open squares indi cate factors parameters ap proximation The basic EP approximation replace factors p x product factors c Tree structured EP We like set parameters Kullback Leibler divergence minimal Since interest updating x2 x3 isolate contribution parameters Kullback Leibler divergence KL p p log Z log x2 x3 p x2 x3 const Also p factorised constant proportionality factor dependence Z x2 x3 Z x2 x2 x2 x3 x3 x3 Differentiating Kullback Leibler divergence equation respect x2 equating zero obtain x2 x2 x2 x2 x2 p x2 Similarly optimising w r t x3 gives x3 x3 x3 x3 x3 p x3 Approximating Z The updates determine approximation factors proportionality constant This fine wish find approximations marginals missing proportionality constants determined requirement marginal normalised However wish approximate Z care factors To address write optimal updates x2 z3 p x2 x2 x3 z2 p x3 x3 z3 z2 proportionality terms We determine proportionalities requirement term approximation x2 x3 effect normalisation p p That x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 x4 x1 DRAFT April MAP Markov networks substituting updates equation equation reduces z2 3z3 z z z x2 x3 x2 p x2 x2 p x3 x3 x3 z x2 x3 x2 x2 x3 x3 Any choice local normalisations z2 z3 satisfies equation suffices ensure scale term approximation matches For example set z2 z3 z z Once set approximation global normalisation constant p Z Z The gives procedure updating terms x2 x3 One chooses term updates corresponding approximation factors We repeat approximation parameters converged suitable termination criterion reached case non convergence The generic procedure outlined algorithm Comments EP For Markov network example EP corresponds belief propagation sum product form factor graph This intuitively clear EP BP product messages incoming variable proportional approximation marginal variable A difference schedule EP messages corresponding term approximation updated simultaneously x2 x3 BP updated sequentially EP useful extension BP cases BP messages easily represented introducing exact factors approximation p increases complexity approximation p resolved projecting approximation p In case approximating distribution p exponential family minimal Kullback Leibler projection step equates matching moments approximating distribution p See detailed discussion In general need replace terms joint distribution factorised approximations One needs resulting approximating distribution tractable results structured Expectation Propagation algorithm fig 12c EP extensions closely related procedures tree reweighting fractional EP designed compensate message overcounting effects MAP Markov networks Consider Markov network p x Z eE x Then likely state given x argmax x p x argmax x E x For general Markov network naively exploit dynamic programming intuitions find exact solution graph generally loopy Below consider general techniques approximate x DRAFT April MAP Markov networks Algorithm Expectation Propagation approximation p x Z Xi Choose terms Xi tractable distribution p x Z Xi Initialise parameters Xi repeat Select term Xi p update Replace term Xi exact term Xi form p Xi j j Xj Find parameters Xi Xi argmin Xi KL p p Set proportionality terms Xi requiring x Xi j j Xj x j j Xj converged return p x Z Xi Z x Xi approximation p x Z approximates normalisation constant Z Iterated Conditional Modes A simple general approximate solution found follows initialise x random Then select variable xi find state xi maximally improves E x keeping variables fixed One repeats selection local maximal state computation convergence This axis aligned optimisation procedure called Iterated Conditional Modes Due Markov properties s clear improve ICM method simultaneously optimising variables conditioned respective Markov blankets similar approach black white sampling Another improvement clamp subset variables reveal singly connected structure un clamped variables subsequently find exact MAP state un clamped variables max sum algorithm One chooses new subset variables clamp finding approximate solution solving sequence tractable problems Dual Decomposition A general approach decompose difficult optimisation problem set easier problems In approach identify tractable slave objectives Es x s S master objective E x decomposes E x s Es x Then x optimises master problem equivalent optimising slave problem Es xs constraint slaves agree xs x s S This constraint imposed Lagrangian DRAFT April MAP Markov networks b c d e f b c d e f b Figure A directed graph edge weights wij node j wij edge ex ists j b A graph cut partitions nodes groups S blue T red The weight cut sum weights edges leave S blue land T red Intuitively clear assigning nodes state blue red weight cut corresponds summed weights neighbours different states Here high light weight contributions The non highlighted edges contribute cut weight Note edge directions contributes cut L x xs s Es xs s s xs x Finding stationary point w r t x gives constraint s s consider L xs s Es xs sxs Given optimise slave problem x s argmax xs Es xs sxs The Lagrange dual section A given noting maximisation problem Ls s max xs Es xs sxs In case dual bound primal s Ls s E x x solution primal problem x argmax x E x To update use projected subgradient method minimise Ls s s x s chosen positive constant Then project S s s new s s ensures s new s Pairwise Markov networks Consider pairwise Markov network p x eE x E x j fij xi xj gi xi x j denotes set neighbouring variables f xi xj f xj xi This means undirected edge j graph corresponding Markov network contributes term f xi xj 2f xi xj Here terms f xi xj represent pairwise interactions The terms g xi x represent unary interactions written convenience terms pairwise interaction fixed non variable x0 Typically term f xi xj ensure neighbouring variables xi xj similar states term gi xi x bias xi close desired state x DRAFT April MAP Markov networks Iterated conditional modes Such models application image restoration observed noisy image x0 cleaned example fig To seek clean image x clean pixel value xi close observed noisy pixel value x0i whilst similar state clean neighbours In example ICM find approximate likely state simply updating randomly chosen variables time Dual decomposition For binary Markov network E x ij xixjwij cixi xTWx xTc define tractable slave problems Es x xTWsx x Tcs identifying set tree structured matrices Ws W s Ws unary terms cs c s cs The dual decomposition technique proceeds solve slave tree exactly subse quently updates push tree solutions agreement Similar general methods solving trees constraint agreement discussed Attractive binary Markov networks Whilst general efficient exact solution exists general MAP Markov network problem important tractable special case discussed Consider finding MAP Markov network binary variables dom xi positive connections wij wji In case task find assignment x maximises E x j wijI xi xj cixi j denotes neighbouring variables For particular case efficient exact MAP algorithm exists arbitrary topology interactions wij The algorithm translates MAP assignment problem equivalent min s t cut problem efficient algorithms exist In min s t cut need graph positive weights edges This clearly satisfied wij bias term cixi needs addressed Dealing bias terms To translate MAP assignment problem min cut problem need deal additional term cixi First consider effect including new node x connecting existing node weight ci Using binary variables xi I xi xj xixj xi xj 2xixj xi xj adds term ciI xi x ci xix xi x If set x state contributes cixi DRAFT April MAP Markov networks b c d e f s t b c d e f s t b Figure A Graph bidirectional weights wij wji augmented source node s sink node t Each node corresponding bias sign indicated The source node linked nodes corresponding positive bias nodes negative bias sink b A graph cut partitions nodes groups S blue T red S union source node s nodes state T union sink node t nodes state The weight cut sum edge weights S blue T red The red lines indicate contributions cut considered penalties wish find minimal cut Otherwise set x state obtain ci xi cixi const Our requirement weights need positive achieved defining single additional node x We define source node xs set state connect xi positive ci defining wsi wis ci In addition define sink node xt set state connect nodes negative ci xt weight wit wti ci positive For source node clamped xs sink node xt including source sink E x j wijI xi xj const equal energy function equation positive weights Definition Graph Cut For graph G nodes v1 vD weights wij cut partition nodes disjoint groups called S T The weight cut defined sum weights leave S land T fig For symmetric w weight cut corresponds sum weights mismatched neighbours fig 13b That cut x j wijI xi xj Since I xi xj I xi xj define weight cut equivalently cut x j wij I xi xj j wijI xi xj const E x const minimal cut assignment correspond maximising E x In Markov network case translation weighted graph positive interactions requires identify source DRAFT April MAP Markov networks b Figure Noisy greyscale image defined intensity labels pixel b Restored I am age The expansion method suitable interac tions w bias c ensure rea sonable results From variables assigned state S sink variables state T fig Our task find minimal cut S T A fundamental result discrete mathematics min s t cut solution corresponds max flow solution source s sink t There efficient algorithms max flow example O D3 operations graph D nodes This means find exact MAP assignment attractive binary Markov network efficiently O D3 operations In MaxFlow m implement Ford Fulkerson Edmonds Karp Dinic breadth search variant Potts model A Markov network defined variables states xi S called Potts model E x j wijI xi xj ciI xi x assume wij x known This model immediate application non binary image restoration clustering based similarity score This problem translated directly graph cut problem efficient exact algorithm known A useful approach approximate problem sequence binary problems describe Potts binary Markov network translation Consider expansion representation xi si si xoldi si S This restricts xi state xoldi depending binary variable si Using new binary vector variable s restrict x subpart space write new objective function terms s E s j w ijI si sj c isi const w ij This new problem form attractive binary Markov network solved exactly graph cuts procedure This translation imposes constraint x access space enables solve constrained problem efficiently We choose value random find optimal s new In way guaranteed iteratively increase E DRAFT April Further Reading For given xold transformation Potts model objective given si considering I xi xj I si si xoldi sj sj xoldj si sj I xoldi x old j si sjI xoldi si sj I xoldj sisj sisjuij aisi bjsj const uij I xoldi I xoldj I xoldi x old j ai bi defined obvious manner By enumeration straightforward uij Using mathematical identity si sisj I si sj si sj write I xi xj uij I si sj si sj aisi bjsj const Hence terms wijI xi xj translate positive interaction terms I si sj wijuij All unary terms easily exactly mapped corresponding unary terms c isi c defined sum unary terms si This shows positive interaction wij terms original variables x maps positive interaction new variables s Hence find maximal state s graph cut algorithm A related procedure described Example Potts model image reconstruction An example image restoration problem nearest neighbour interactions pixel lattice suitably chosen w c given fig The images non binary optimal MAP assignment computed exactly efficient way The alpha expansion technique combined efficient min cut approach approximate MAP assignment details Further Reading Approximate inference highly active research area increasingly links convex optimisation developed See general overview Summary Deterministic methods offer alternative sampling techniques For continuous distributions perturbation approaches Laplace s method provide simple approxi mation Variational bounding approaches minimal KL divergence provide bounds quantities interest example normalisation constant distribution marginal likelihood These larger class methods derived convex analysis The deterministic bounding approaches applied areas bounding mutual infor mation Consistency methods loopy belief propagation work extremely structure distribution close tree These methods successful information theory error correction DRAFT April Exercises Loopy belief propagation motivated optimisation Bethe free energy objective function method produce bound quantities interest For binary attractive Markov networks find MAP state exactly polynomial time This method subroutine complex multistate problems MAP search problem broken sequence binary MAP problems Code LoopyBP m Loopy belief propagation Factor Graph formalism demoLoopyBP m Demo loopy belief propagation demoMFBPGibbs m Comparison Mean Field belief propagation Gibbs sampling demoMRFclean m Demo analysing dirty picture MaxFlow m Max Flow Min Cut algorithm Ford Fulkerson binaryMRFmap m Optimising binary Markov network Exercises Exercise The file p mat contains distribution p x y z ternary state variables Using BRML toolbox find best approximation q x y q z minimises Kullback Leibler divergence KL q p state value minimal Kullback Leibler divergence optimal q Exercise Consider pairwise Markov network defined lattice given pMRF mat Using BRMLtoolbox Find optimal fully factorised approximation q BP loopy belief propagation based factor graph formalism Find optimal fully factorised approximation q MF solving variational mean field equa tions By pure enumeration compute exact marginals pi Averaged variables compute mean expected deviation marginals j qi x j pi x j BP MF approximations comment results Exercise In LoopyBP m message schedule chosen random Modify routine choose schedule forward reverse elimination sequence random spanning tree Exercise Double Integration Bounds Consider bound f x g x Then f x x f x dx g x x g x dx DRAFT April Exercises f x g x x f x g x x f x x f x dx g x x g x dx The significance double integration summation case discrete variables general procedure generating new bound existing bound Exercise This question concerns deriving standard mean field bound powerful lower bounds Boltzmann machine Starting ex double integration procedure ex ea x By replacing x sTWs s D hTs derive bound partition function Boltzmann machine Z s es TWs Show bound equivalent naive mean field bound partition function Hint optimise respect Discuss generate tighter bounds partition function Boltzmann distribution application double integration procedure Exercise Consider pairwise Markov network p x Z ex TWx bTx symmetric W Consider decomposition W qiWi I qi qi graph corresponding matrix Wi tree Explain form upper bound normalisation Z discuss naive method find tightest upper bound Hint Consider ex e x See Exercise Derive Linkser s bound Mutual Information equation Exercise Consider average positive function f x respect distribution p x J log x p x f x f x The simplest version Jensen s inequality states J x p x log f x By considering distribution r x p x f x KL q r variational distribution q x J KL q x p x log f x q x The bound saturates q x p x f x This shows wish approximate average J optimal choice approximating distribution q x depends distribution p x integrand f x DRAFT April Exercises Furthermore J KL q x p x KL q x f x H q x H q x entropy q x The term encourages q close p The second encourages q close f encourages q sharply peaked Exercise For Markov network defined D binary variables xi D define p x Z ex TWx p xi Z Z Z x1 xi xi xD ex TWx explain bound marginal p xi requires upper lower bounds partition functions Z Exercise Consider model example Implement structured EP approximation based replacing observation factors p vt ht terms form ct exp hTt Atht h T t bt Exercise Consider directed graph capacity edge x y c x y The flow edge f x y exceed capacity edge The aim maximise flow defined source node s defined sink node t In addition flow conserved node source sink y s y t x f x y x f y x A cut defined partition nodes non overlapping sets S T s S t T Show The net flow s t val f net flow S T val f x S y T f x y y T x S f y x val f x S y T f x y flow upper bounded capacity cut The max flow min cut theorem states maximal flow actually equal capacity cut Exercise Potts Ising translation Consider function E x defined set multistate variables dom xi S E x j wijI xi xj ciI xi x wij pixel states x ci known Our interest find approximate maximisation E x Using restricted parameterisation xi si si xoldi si given N maximisation E x equivalent maximising E s binary variables s E s j w ijI si sj c isi const w ij This new problem form attractive binary Markov network solved exactly graph cuts procedure DRAFT April Exercises Exercise Consider approximating distribution exponential family q x Z e Tg x We wish use q x approximate distribution p x KL divergence KL p q Show optimally g x p x g x q x Show Gaussian written exponential form N x Z e Tg x g1 x x g2 x x suitably chosen Hence optimal Gaussian fit N x distribution minimal KL p q sense matches moments x p x x2 p x x 2p x Exercise For pairwise binary Markov network p partition function Z w b x e j wijxixj bixi mean xi p computed generating function approach d dbi logZ w b Z w b x xie j wijxixj bixi xi p similarly covariance given xixj p xi p xj p d2 dbidbj logZ w b By replacing logZ w b mean field bound B equation approximate mean variance given naive mean field theory equivalent obtained replacing logZ lower bound generating function Exercise The naive mean field theory applied pairwise Markov network p x e j wijxixj bixi dom xi gives factorised approximation q x q xi based minimising KL q p Using approximate xixj p xi q xj q j To produce better non factorised approximation xixj p fit non factorised q The linear response method based perturbation expansion free energy Alternatively xixj p p xi xj p xi xj p xj Explain form improved non factorised approximation xixj p Exercise Derive EP updates equation equation DRAFT April Exercises Exercise Consider fitting univariate Gaussian q x N x distribution p x min imising KL q p respect Show provided quantities defined optimal variance mean approximating Gaussian satisfy implicit equations d2 dx2 log p x q x d dx log p x q x Hence relate optimal variance maximal minimal local curvature p x maxx d2 dx2 log p x minx d2 dx2 log p x Consider distribution p x mixture separated Gaussians p x piN x Show good approximation optimal choice set mean approximating distri bution components Show furthermore 2min 2max 2min max minimal maximal variances Gaussian components p x explain optimal variance smaller variance p x DRAFT April Exercises DRAFT April Part VI Appendix APPENDIX A Background Mathematics A Linear Algebra A Vector algebra Let x denote n dimensional column vector components x1 x2 xn The vector elements equal written similarly vector zero values written Definition A scalar product The scalar product w x defined w x n wixi w Tx A The length vector denoted x squared length given x xTx x2 x21 x22 x2n A A unit vector x x The scalar product natural geometric interpretation w x w x cos A angle vectors Thus lengths vectors fixed inner product largest vector constant multiple If scalar product xTy x y orthogonal right angles A set vectors orthonormal mutually orthogonal unit length Definition A Linear dependence A set vectors x1 xn linearly dependent exists vector xj expressed linear combination vectors If solution n ix A n vectors x xn linearly independent Linear Algebra e e e e Figure A Resolving vector components orthogonal directions e e The projection directions lengths directions e e A The scalar product projection Suppose wish resolve vector components orthogonal directions specified unit vectors e e fig A That e e e e We required find scalar values e e A From obtain e e e e e e e e e e A From orthogonality unit lengths vectors e e simply e e A This means write vector terms orthonormal components e e e e e e A The scalar product e projects vector unit direction e The projection vector direction specified general f f f f A Lines space A line dimensions specified follows The vector point line given s equation p su s R A u parallel line line passes point fig A An alternative specification given realising vectors line orthogonal normal line n u n orthonormal That p n p n n A If vector n unit length right hand represents shortest distance origin line drawn dashed line fig A projection normal direction A Planes hyperplanes To define dimensional plane arbitrary dimensional space specify vectors u v lie plane need mutually orthogonal position vector plane fig A Any vector p plane written p su tv s t R A p n u Figure A A line specified position vector line unit vector direction line u In dimensions unique direction n perpendicular line In dimensions vectors perpendicular direction line lie plane normal vector direction line u DRAFT April Linear Algebra nu vp Figure A A plane specified point plane non parallel directions plane u v The normal plane unique direction directed line origin nearest point plane An alternative definition given considering vector plane orthogonal normal plane n p n p n n A The right hand represents shortest distance origin plane drawn dashed line fig A The advantage representation form line Indeed representation hyper planes independent dimension space In addition quantities need defined normal plane distance origin plane A Matrices An m n matrix A collection scalar values arranged rectangle m rows n columns A vector considered n matrix The j element matrix A written Aij conventionally aij Where clarity required write A ij Definition A Matrix addition For matrices A B size A B ij A ij B ij A Definition A Matrix multiplication For l n matrix A n m matrix B product AB l m matrix elements AB ik n j A ij B jk l k m A Note general BA AB When BA AB A B commute The matrix I identity matrix necessarily square s diagonal s For clarity write I am square m m identity matrix Then m n matrix A I am AIn A The identity matrix elements I ij ij given Kronecker delta ij j j A Definition A Transpose The transpose BT n m matrix B m n matrix components BT kj Bjk k m j n A BT T B AB T BTAT If shapes matrices A B C makes sense calculate product ABC ABC T CTBTAT A DRAFT April Linear Algebra A square matrix A symmetric AT A A square matrix called Hermitian A AT denotes complex conjugate operator For Hermitian matrices eigenvectors form orthogonal set real eigenvalues Definition A Trace trace A Aii A eigenvalues A A Linear transformations If define ui vector zeros expect th entry vector expressed x xiui Then linear transformation x given Ax xiAui xiai A ai th column A Rotations The unit vectors T T rotation radians transform vectors cos sin sin cos A form columns rotation matrix R cos sin sin cos A Multiplying vector R Rx rotates vector radians A Determinants Definition A Determinant For square matrix A determinant volume transformation matrix A sign change That hypercube unit volume map vertex transformation The volume resulting object defined determinant Writing A ij aij det a11 a12 a21 a22 a11a22 a21a12 A det a11 a12 a13a21 a22 a23 a31 a32 a33 a11 a22a33 a23a32 a12 a21a33 a31a23 a13 a21a32 a31a22 A The determinant case form a11det a22 a23 a32 a33 a12det a21 a23 a31 a33 a13det a21 a22 a31 a32 A The determinant matrix A given sum terms 1a1idet Ai Ai matrix formed A removing ith row column This form determinant DRAFT April Linear Algebra generalises dimension That define determinant recursively expansion row determinants reduced matrices The absolute value determinant volume transformation det AT det A A For square matrices A B equal dimensions det AB det A det B det I det A det A A Definition A Orthogonal matrix A square matrix A orthogonal AAT I ATA From properties determinant orthogonal matrix determinant corresponds volume preserving transformation Definition A Matrix rank For m n matrix X rank X maximum number linearly independent columns equivalently rows The matrix rank rank equal min m n matrix rank deficient A square matrix rank deficient singular A Matrix inversion Definition A Matrix inversion For square matrix A inverse satisfies A 1A I AA A It possible find matrix A A 1A I case A singular Geo metrically singular matrices correspond projections transform vertices v binary hypercube Av volume transformed hypercube zero Hence det A matrix A form projection collapse means A singular Given vector y singular transformation A uniquely identify vector x y Ax Provided inverses exist AB B 1A A For non square matrix A AAT invertible right pseudo inverse defined A AT AAT A satisfies AA I The left pseudo inverse given A ATA AT A satisfies A A I Definition A Matrix inversion lemma Woodbury formula Provided appropriate inverses exist A UVT A A 1U I VTA 1U VTA A det A UVT det A det I VTA 1U A DRAFT April Linear Algebra Definition A Block matrix inversion For matrices A B C D provided appropriate inverses exist A B C D A BD 1C A BD 1C BD D 1C A BD 1C D 1C A BD 1C BD A A Computing matrix inverse For matrix A b c d inverse matrix elements ad bc d b c A A The quantity ad bc determinant A There ways compute inverse general matrix refer reader specialised texts If wants solve linear system Ax b algebraically solution given x A 1b This suggest needs compute n n matrix A However practice A explicitly required x needed This obtained rapidly greater numerical precision Gaussian elimination A Eigenvalues eigenvectors The eigenvectors matrix correspond natural coordinate system geometric transforma tion represented A easily understood Definition A Eigenvalues Eigenvectors For n n square matrix A e eigenvector A eigenvalue Ae e A Geometrically eigenvectors special directions effect transformation A direction e simply scale vector e For rotation matrix R general direction preserved rotation eigenvalues eigenvectors complex valued Fourier representation corresponds representation rotated basis necessarily complex For n n dimensional matrix including repetitions n eigenvalues corresponding eigenvector We reform equation A A I e A We write equation A Be B A I If B inverse solution e B trivially satisfies eigen equation For non trivial solution problem Be need B non invertible This equivalent condition B zero determinant Hence eigenvalue A det A I A This known characteristic equation This determinant equation polynomial degree n resulting equation known characteristic polynomial Once found eigenvalue corresponding eigenvector found substituting value equation A solving linear equations e It eigenvalue eigenvector unique space corresponding vectors An important relation determinant eigenvalues matrix det A n A DRAFT April Linear Algebra Hence matrix singular zero eigenvalue The trace matrix expressed trace A A For real symmetric matrix A AT eigenvectors ei ej ei Tej eigenvalues j different This shown considering Aei ie ej TAei ej Tei A Since A symmetric ej TA ei Aej Tei j e j Tei ej Tei j ej Tei A If j condition satisfied ej Tei eigenvectors orthogonal Definition A Trace Log formula For positive definite matrix A trace log A log det A A Note logarithm matrix element wise logarithm In MATLAB required function logm In general analytic function f x f M defined power series expansion function On right det A scalar logarithm standard logarithm scalar A Matrix decompositions Definition A Spectral decomposition A real n n symmetric matrix A eigen decomposition A n ieie T A eigenvalue eigenvector ei eigenvectors form orthogonal set ei T ej ij ei T ei A In matrix notation A EET A E e1 en matrix eigenvectors corresponding diagonal eigenvalue matrix More generally square non symmetric diagonalisable A write A EE A Definition A Singular Value Decomposition The SVD decomposition n p matrix X X USVT A dim U n n UTU In Also dim V p p VTV Ip The matrix S dim S n p zeros diagonal entries The singular values diagonal entries S ii non negative The singular values ordered upper The left diagonal element S contains largest singular value Sii Sjj j Assuming n p transpose X computational complexity computing SVD O 4n2p 8np2 9n3 For thin SVD n p p columns U S computed giving decomposition X UpSpV T A Up dimension n p Sp p p diagonal matrix singular values DRAFT April Multivariate Calculus Definition A Quadratic form xTAx xTb A Definition A Positive definite matrix A symmetric matrix A property xTAx vector x called positive semidefinite A symmetric matrix A property xTAx vector x called positive definite A positive definite matrix rank invertible Using eigen decomposition A xTAx ix Tei ei Tx xTei A greater zero eigenvalues positive Hence A positive definite eigenvalues positive Eigenfunctions x K x x x aa x A The eigenfunctions real symmetric kernel K x x K x x orthogonal x x b x ab A x complex conjugate x A kernel decomposition provided eigenvalues countable K xi xj x x j A Then j yiK x xj yj j yi x x j yj yi x zi yi x z A greater zero eigenvalues positive complex z zz If eigenvalues uncountable appropriate decomposition K xi xj s xi s xj s ds A A Multivariate Calculus Definition A Partial derivative Consider function n variables f x1 x2 xn f x The partial derivative f w r t xi defined following limit exists f xi lim h f x1 xi xi h xi xn f x h A The gradient vector f denoted f g f x g x f x1 f xn A DRAFT April Multivariate Calculus x1 x2 f x Figure A Interpreting gradient The ellipses contours constant function value f const At point x gradi ent vector f x points direction maximal increase function A Interpreting gradient vector Consider function f x depends vector x We interested function changes vector x changes small x x vector length small According Taylor expansion function f change f x f x f xi O A We interpret summation scalar product vector f components f f xi f x f x f O A The gradient points direction function increases rapidly To consider direction p unit length vector Then displacement units direction changes function value f x p f x f x p A The direction p function largest change maximises overlap f x p f x p cos f x cos A angle p f x The overlap maximised giving p f x f x Hence direction function changes rapidly f x A Higher derivatives The second derivative n variable function defined xi f xj n j n A usually written 2f xi xj j 2f xi2 j A If partial derivatives 2f xi xj 2f xj xi exist 2f xi xj 2f xj xi A This denoted f These n2 second partial derivatives represented square symmetric matrix called Hessian matrix f x Hf x 2f x12 2f x1 xn 2f x1 xn 2f xn2 A DRAFT April Inequalities Definition A Chain rule Let xj parameterized u1 um e xj xj u1 um f u n j f xj xj u A vector notation u f x u fT x u x u u A Definition A Directional derivative Assume f differentiable We define scalar directional derivative Dvf x f direction v point x Let x x hv Then Dvf x d dh f x hv h j vj f xj x x fTv A A Matrix calculus Definition A Derivative matrix trace For matrices A B A trace AB BT A Definition A Derivative log det A log det A trace log A trace A A A So A log det A A T A Definition A Derivative matrix inverse For invertible matrix A A A T AA A A Inequalities A Convexity Definition A Convex function A function f x defined convex x y f x y f x f y A If f x convex f x called concave An intuitive picture given considering quantity x y As vary traces points x y Hence start point x f x increase trace straight line point y f y Convexity states function f lies DRAFT April Multivariate Optimisation straight line Geometrically means function f x non decreasing Hence d2f x dx2 function convex As example function log x concave second derivative negative d dx log x x d2 dx2 log x x2 A A Jensen s inequality For convex function f x follows directly definition convexity f x p x f x p x A distribution p x A Optimisation Definition A Critical point When order partial derivatives point zero e f point said stationary critical point A critical point correspond minimum maximum saddle point function There minimum f x f x f x x sufficiently close x This requires x stationary point f x The Taylor expansion f optimum given f x hv f x h2vTHfv O h A Thus minimum condition requires vTHfv e Hessian non negative definite Definition A Conditions minimum Sufficient conditions minimum x f x ii Hf x positive definite For quadratic function f x xTAx bTx c symmetric A condition f x reads Ax b A If A invertible equation unique solution x A 1b If A positive definite x corresponds minimum point A Multivariate Optimisation In cases optima functions found algebraic means numerical techniques required The search techniques consider iterative e proceed minimum x sequence steps Perhaps simplest approach minimising multivariate function f x break problem sequence dimensional problems By initialising elements x random selects component x update keeping fixed This coordinatewise optimisation decreases function sequence dimensional minimisations Whilst procedure simple inefficient high dimensions particularly strong dependencies components x For reason useful use gradient based procedures local geometry objective account We consider general class iterative gradient based method kth step step length k direction pk xk xk kpk A DRAFT April Multivariate Optimisation Figure A Optimisation line search steepest descent directions Following steepest way downhill point continuing fi nite time direction doesn t result fastest way A Gradient descent fixed stepsize Locally point xk decrease f x taking step direction g x To gradient descent works consider general update xk xk xf A For small expand f xk Taylor s theorem f xk kpk f xk xf A change f f xf If non infinitesimal possible step true minimum Making small guards means optimization process long time reach minimum A simple idea improve convergence gradient descent include iteration proportion change previous iteration pk gk gk momentum coefficient An unfortunate aspect gradient descent change function value depends coordinate system Consider new coordinate system x My invertible square matrix M Define f y f x Then change function f gradient update f f y yf f y yf A In original coordinate system change x x xf Since yf MT xf x transforming change y change x M y MMT xf x MMT I equal x Similarly change function value f xf x TMMT xf x A orthogonal M equal f xf A Gradient descent line searches An extension idea gradient descent choose direction steepest descent indicated gradient g calculate value step reduces value f moving direction This involves solving dimensional problem minimizing f xk kgk respect k known line search To find optimal step size k th step choose k minimize f xk kpk So setting F f xk pk step solve dimensional minimization problem F Thus choice k satisfy F k Now F k d dh F k h h d dh f xk kpk hpk h d dh f xk hpk h Dpkf xk fT xk pk A So F k means directional derivative search direction vanish new point gives condition gTk 1pk If step size chosen reduce f direction decrease E moving direction moment Thus step component direction right angles previous taken This lead zig zag type behaviour optimisation DRAFT April Multivariate Optimisation A Minimising quadratic functions line search Consider minimising quadratic function f x xTAx bTx c A A positive definite symmetric Although know minimum simple function based linear algebra wish use function toy model complex functions As zoom minimum smooth function increasingly appear quadratic Hence small scales methods work quadratic case work general smooth functions If general function looks roughly quadratic larger scale methods work case One approach search particular direction p find minimum direction We search deeper minimum looking different directions That search firstly line x p function attains minimum This solution b Ax p pTAp f x p pTAp A How choose line search direction pnew It sensible choose successive line search directions p according pnew f x time minimise function line steepest descent However generally optimal choice fig A If matrix A diagonal minimisation straightforward carried independently dimension If find invertible matrix P property PTAP diagonal solution easy f x x TPTAPx bTPx c A x Px compute minimum dimension x separately retransform find x Px The columns matrix P called conjugate vectors Definition A Conjugate vectors The vectors pi k called conjugate matrix A j k j pTi Apj p T Api A The conditions guarantee conjugate vectors linearly independent Assume k j jpj j jpj ipi k j jpj A Now multiplying left pTi A yields ip T Api So zero know p T Api As argument k zero A Gram Schmidt construction conjugate vectors Assume k conjugate vectors p1 pk let v vector linearly independent p1 pk We use Gram Schmidt procedure pk v k j pTj Av pTj Apj pj A clear vectors p1 pk conjugate A positive definite We construct n conjugate vectors positive definite matrix following way We start n linearly independent vectors u1 un We set p1 u1 use A compute p2 p1 v u2 Next set v u3 compute p3 p1 p2 v Continuing manner obtain n conjugate vectors Note stage procedure vectors u1 uk span subspace vectors p1 pk DRAFT April Multivariate Optimisation A The conjugate vectors algorithm Let assume minimising f x xTAx bTx c construct n vectors p1 pn conjugate A use search directions Using iterative solution takes form xk xk kpk A step choose k exact line search k pTkgk pTkApk A This conjugate vectors algorithm geometrical interpretation directional derivative zero new point direction pk zero previous search directions p1 pk known Luenberger expanding subspace theorem In particular fT xn pi n fT xn p1 p2 pn A The square matrix P p1 p2 pn invertible pi conjugate f xn point xn minimum x quadratic function f So contrast gradient descent quadratic function conjugate vectors algorithm converges finite number steps A The conjugate gradients algorithm The conjugate gradients algorithm special case conjugate vectors algorithm construct conjugate vectors fly After k steps conjugate vectors algorithm need construct vector pk conjugate p1 pk In conjugate gradients algorithm makes special choice v f xk The gradient new point xk orthogonal pi k f xk linearly independent p1 pk valid choice v f xk In case xk minimum algorithm terminates Using notation gk f xk equation new search direction given Gram Schmidt procedure equation A pk gk k pTi Agk pTi Api pi A Since gk orthogonal pi k p T k 1gk gTk 1gk So k written k gTk 1gk pTk 1Apk A particular k We want conjugate gradients algorithm previous steps equation A terms sum vanish We shall assume k step k set p1 g1 First note gi gi Axi b Axi b A xi xi iApi A Api gi gi So equation A pTi Agk g T k 1Api g T k gi gi gTk 1gi gTk 1gi A Since pi obtained applying Gram Schmidt procedure gradients gi g T k 1pi gTk 1gi k This shows pTi Agk g T k 1gi gTk 1gi k gTk 1gk k k A Hence equation A simplifies pk gk gTk 1gk k pTkApk pk A DRAFT April Multivariate Optimisation Algorithm A Conjugate Gradients minimising function f x k Choose x1 p1 g1 gk k argmin k f xk kpk Line Search xk xk kpk k g T k 1gk g T k gk pk gk kpk k k end This brought simpler form applying equation A k pk gk gTk 1gk pTkApk pTkApk gTk gk pk gk gTk 1gk gTk gk pk A We shall write form pk gk kpk k gTk 1gk gTk gk A The formula A k Fletcher Reeves Since gradients orthogonal k written k gTk gk gk gTk gk A Polak Ribie formula The choice expressions k importance f quadratic Polak Ribie formula commonly The important point derived algorithm particular case minimising quadratic function f By writing algorithm terms f gradients explicit reference fact f quadratic This means apply algorithm non quadratic functions The similar function f quadratic function confident algorithm find minimum A Newton s method Consider function f x wish find minimum A Taylor expansion second order gives f x f x T f THf O A The matrix Hf Hessian Differentiating right hand respect equivalently completing square find right hand ignoring O term lowest value f Hf H 1f f A Hence optimisation routine minimise f given Newton update xk xk H 1f f A scalar practice improve convergence A benefit Newton method gradient descent decrease objective function invariant linear change co ordinates x My Defining f y f x H f MTHfM yf MT xf change original x system given M y M MTHfM MT xf H 1f xf means change variable function value independent coordinate system DRAFT April Multivariate Optimisation Algorithm A Quasi Newton minimising function f x k Choose x1 H I gk pk H kgk k argmin k f xk kpk Line Search xk xk kpk sk xk xk yk gk gk update H k k k end Quasi Newton methods For large scale problems storing Hessian solving resulting linear system computationally demanding especially matrix close singular An alternative set iteration xk xk kSkgk A For Sk A Newton s method Sk I steepest descent In general good idea choose Sk approximation inverse Hessian Also note important Sk positive definite small k obtain descent method The idea quasi Newton methods try construct approximate inverse Hessian H k information gathered descent progresses set Sk H k As seen quadratic optimization problem relationship gk gk A xk xk A Defining sk xk xk yk gk gk A equation A yk Ask A It reasonable demand H k 1yi si k A After n linearly independent steps H n A For k n infinity solutions H k satisfying equation A A popular choice Broyden Fletcher Goldfarb Shanno BFGS update given H k H k yTk H kyk yTk sk sks T k sTkyk sky T k H k H kyks T k sTkyk A This rank correction H k constructed vectors sk H kyk The direction vectors pk H kgk produced algorithm obey pTi Apj j k H k 1Api pi k A The storage requirements Quasi Newton methods scale quadratically number variables methods tend smaller problems Limited memory BFGS reduces storage l latest updates computing approximate Hessian inverse equation A In contrast memory requirements pure Conjugate Gradient methods scale linearly dimension x As algorithm derived quadratic function mind final form algorithm depends f gradients applied non quadratic functions f DRAFT April Constrained Optimisation Lagrange Multipliers A Constrained Optimisation Lagrange Multipliers Consider problem minimising f x subject single constraint c x A formal treatment problem scope notes requires understanding conditions optimum found As informal argument imagine identified x satisfies constraint c x How tell x minimises function f We allowed search lower function values x directions consistent constraint For small change change constraint c x c x c x A Let explore change f direction c x f x f x f x A We looking point x direction change f c minimal This dual requirement represented find x minimise f x c x A This simple quadratic form optimising gives f x c x f x c x A Hence constrained optimum f x c x A scalar We formulate requirement look x constitute stationary point Lagrangian L x f x c x A Differentiating respect x obtain requirement f x c x differentiating respect c x In multiple constraint case ci x find stationary point Lagrangian L x f x ici x A A Lagrange Dual Consider primal problem minimise f x subject c x A The Lagrange dual defined L min x f x c x A By construction x L f x c x A Now consider optimal x solves primal problem equation A Then L f x c x f x A DRAFT April Constrained Optimisation Lagrange Multipliers step follows x solves primal problem meaning c x Since L f x optimal given solving unconstrained dual problem max L A In addition providing bound enables bracket optimal primal value L f x f x c x A dual possesses interesting property concave irrespective f convex This follows construction function f x c x concave point x More explicitly consider L min x f x c x f x c x c x A Similarly L f x c x c x A Averaging equation A equation A taking minimum x sides L L L A shows L concave DRAFT April Bibliography L F Abbott J A Varela K Sen S B Nelson Synaptic Depression Cortical Gain Control Science D H Ackley G E Hinton T J Sejnowski A Learning Algorithm Boltzmann Machines Cognitive Science R P Adams D J C MacKay Bayesian Online Changepoint Detection Cavendish laboratory department physics University Cambridge Cambridge UK E Airoldi D Blei E Xing S Fienberg A latent mixed membership model relational data In LinkKDD Proceedings 3rd international workshop Link discovery pages New York NY USA ACM E M Airoldi D M Blei S E Fienberg E P Xing Mixed membership stochastic blockmodels Journal Machine Learning Research D L Alspach H W Sorenson Nonlinear Bayesian Estimation Using Gaussian Sum Approximations IEEE Transactions Automatic Control S Amari Natural Gradient Learning Over Under Complete Bases ICA Neural Computation I Androutsopoulos J Koutsias K V Chandrinos C D Spyropoulos An experimental comparison naive Bayesian keyword based anti spam filtering personal e mail messages In Proceedings 23rd annual international ACM SIGIR conference Research development information retrieval pages New York NY USA ACM S Arora C Lund Hardness approximations In Approximation algorithms NP hard problems pages PWS Publishing Co Boston MA USA F R Bach M I Jordan Thin junction trees In T G Dietterich S Becker Z Ghahramani editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press F R Bach M I Jordan A probabilistic interpretation canonical correlation analysis Computer Science Division Department Statistics University California Berkeley Berkeley USA Y Bar Shalom Xiao Rong Li Estimation Tracking Principles Techniques Software Artech House Norwood MA D Barber Dynamic Bayesian Networks Deterministic Tables In S Becker S Thrun K Obermayer editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Barber Learning Spiking Neural Assemblies In S Becker S Thrun K Obermayer editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Barber Are Classifiers performing equally A treatment Bayesian Hypothesis Testing IDIAP RR IDIAP Rue de Simplon Martigny CH Switerland May IDIAP RR BIBLIOGRAPHY BIBLIOGRAPHY D Barber The auxiliary variable trick deriving kalman smoothers IDIAP RR IDIAP Rue de Simplon Martigny CH Switerland December IDIAP RR D Barber Expectation Correction smoothing Switching Linear Gaussian State Space models Journal Machine Learning Research D Barber Clique Matrices Statistical Graph Decomposition Parameterising Restricted Positive Definite Matrices In D A McAllester P Myllymaki editors Uncertainty Artificial Intelligence number pages Corvallis Oregon USA AUAI press D Barber F V Agakov Correlated sequence learning network spiking neurons maximum likelihood Informatics Research Reports EDI INF RR Edinburgh University D Barber F V Agakov The I am Algorithm A variational approach Information Maximization In Advances Neural Information Processing Systems NIPS number D Barber C M Bishop Bayesian Model Comparison Monte Carlo Chaining In M C Mozer M I Jordan T Petsche editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Barber C M Bishop Ensemble Learning Bayesian Neural Networks In Neural Networks Machine Learning pages Springer D Barber S Chiappa Unified Inference Variational Bayesian Linear Gaussian State Space Models In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Barber W Wiegerinck Tractable Variational Structures Approximating Graphical Models In M S Kearns S A Solla D A Cohn editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Barber C K I Williams Gaussian processes Bayesian classification hybrid Monte Carlo In M C Mozer M I Jordan T Petsche editors Advances Neural Information Processing Systems NIPS pages Cambridge MA MIT Press R J Baxter Exactly solved models statistical mechanics Academic Press M J Beal F Falciani Z Ghahramani C Rangel D L Wild A Bayesian approach reconstructing genetic regulatory networks hidden factors Bioinformatics A Becker D Geiger A sufficiently fast algorithm finding close optimal clique trees Artificial Intelligence A J Bell T J Sejnowski An Information Maximization Approach Blind Separation Blind Decon volution Neural Computation R E Bellman Dynamic Programming Princeton University Press Princeton NJ Paperback edition Dover Publications Y Bengio P Frasconi Input Output HMMs sequence processing IEEE Trans Neural Networks A L Berger S D Della Pietra V J D Della Pietra A maximum entropy approach natural language processing Computational Linguistics J O Berger Statistical Decision Theory Bayesian Analysis Springer second edition D P Bertsekas Nonlinear Programming Athena Scientific 2nd edition D P Bertsekas Dynamic Programming Optimal Control Athena Scientific second edition J Besag Spatial Interactions Statistical Analysis Lattice Systems Journal Royal Statistical Society Series B J Besag On statistical analysis dirty pictures Journal Royal Statistical Society Series B J Besag P Green Spatial statistics Bayesian computation Journal Royal Statistical Society Series B DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY G J Bierman Measurement updating U D factorization Automatica N L Biggs Discrete Mathematics Oxford University Press K Binder A P Young Spin glasses Experimental facts theoretical concepts open questions Rev Mod Phys Oct C M Bishop Neural Networks Pattern Recognition Oxford University Press C M Bishop Pattern Recognition Machine Learning Springer C M Bishop M Svense n Bayesian hierarchical mixtures experts In U Kjaerulff C Meek editors Proceedings Nineteenth Conference Uncertainty Artificial Intelligence pages Morgan Kaufmann F Black M Scholes The Pricing Options Corporate Liabilities Journal Political Economy D Blei A Ng M Jordan Latent Dirichlet allocation Journal machine Learning Research R R Bouckaert Bayesian belief networks construction inference PhD thesis University Utrecht S Boyd L Vandenberghe Convex Optimization Cambridge University Press Y Boykov V Kolmogorov An experimental comparison min cut max flow algorithms energy mini mization vision IEEE Trans Pattern Anal Mach Intell Y Boykov O Veksler R Zabih Fast approximate energy minimization graph cuts IEEE Trans Pattern Anal Mach Intell C Bracegirdle D Barber Switch reset models Exact approximate inference In Proceedings The Fourteenth International Conference Artificial Intelligence Statistics AISTATS volume M Brand Incremental singular value decomposition uncertain data missing values In European Conference Computer Vision ECCV pages J Breese D Heckerman Decision theoretic troubleshooting A framework repair experiment In E Horvitz F Jensen editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann H Bunke T Caelli Hidden Markov models applications computer vision Machine Perception Artificial Intelligence World Scientific Publishing Co Inc River Edge NJ USA W Buntine Theory refinement Bayesian networks In Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann A Cano S Moral Advances Intelligent Computing IPMU chapter Heuristic Algorithms Triangulation Graphs pages Number Lectures Notes Computer Sciences Springer Verlag O Cappe E Moulines T Ryden Inference Hidden Markov Models Springer New York A T Cemgil Bayesian Inference Non negative Matrix Factorisation Models Technical Report CUED F INFENG TR University Cambridge July A T Cemgil B Kappen D Barber A Generative Model Music Transcription IEEE Transactions Audio Speech Language Processing E Challis D Barber Concave Gaussian Variational Approximations Inference Large Scale Bayesian Linear Models In Proceedings The Fourteenth International Conference Artificial Intelligence Statistics AISTATS JMLR H S Chang M C Fu J Hu S I Marcus Simulation based Algorithms Markov Decision Processes Springer S Chiappa D Barber Bayesian Linear Gaussian State Space Models Biosignal Decomposition Signal Processing Letters DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY S Chib M Dueker Non Markovian Regime Switching Endogenous States Time Varying State Strengths Econometric Society North American Summer Meetings Econometric Society August C K Chow C N Liu Approximating discrete probability distributions dependence trees IEEE Transactions Information Theory P S Churchland T J Sejnowski The Computational Brain MIT Press Cambridge MA USA D Cohn H Chang Learning probabilistically identify authoritative documents In P Langley editor International Conference Machine Learning number pages Morgan Kaufmann D Cohn T Hofmann The Missing Link A Probabilistic Model Document Content Hypertext Connectivity Number pages Cambridge MA MIT Press A C C Coolen R Ku hn P Sollich Theory Neural Information Processing Systems Oxford University Press G F Cooper E Herskovits A Bayesian Method Induction Probabilistic Networks Data Machine Learning A Corduneanu C M Bishop Variational Bayesian Model Selection Mixture Distributions In T Jaakkola T Richardson editors Artifcial Intelligence Statistics pages Morgan Kaufmann M T Cover J A Thomas Elements Information Theory Wiley R G Cowell A P Dawid S L Lauritzen D J Spiegelhalter Probabilistic Networks Expert Systems Springer D R Cox N Wermuth Multivariate Dependencies Chapman Hall J C Cox S A Ross M Rubinstein Option Pricing A Simplified Approach Journal Financial Economics N Cristianini J Shawe Taylor An Introduction To Support Vector Machines Cambridge University Press P Dangauthier R Herbrich T Minka T Graepel Trueskill time Revisiting history chess In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press H A David The method paired comparisons Oxford University Press New York A P Dawid Influence diagrams causal modelling inference International Statistical Review A P Dawid S L Lauritzen Hyper Markov Laws Statistical Analysis Decomposable Graphical Models Annals Statistics P Dayan L F Abbott Theoretical Neuroscience MIT Press P Dayan G E Hinton Using Expectation Maximization Reinforcement Learning Neural Computation T De Bie N Cristianini R Rosipal Handbook Geometric Computing Applications Pattern Recogni tion Computer Vision Neuralcomputing Robotics chapter Eigenproblems Pattern Recognition Springer Verlag R Dechter Bucket Elimination A unifying framework probabilistic inference algorithms In E Horvitz F Jensen editors Uncertainty Artificial Intelligence pages San Francisco CA Morgan Kaufmann A P Dempster N M Laird D B Rubin Maximum Likelihood Incomplete Data EM Algorithm Journal Royal Statistical Society Series B Methodological S Diederich M Opper Learning Correlated Patterns Spin Glass Networks Local Learning Rules Physical Review Letters R Diestel Graph Theory Springer DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY A Doucet A M Johansen A Tutorial Particle Filtering Smoothing Fifteen years later In D Crisan B Rozovsky editors Oxford Handbook Nonlinear Filtering Oxford University Press R O Duda P E Hart D G Stork Pattern Classification Wiley Interscience Publication J Durbin The fitting time series models Rev Inst Int Stat R Durbin S R Eddy A Krogh G Mitchison Biological Sequence Analysis Probabilistic Models Proteins Nucleic Acids Cambridge University Press A Du ring A C C Coolen D Sherrington Phase diagram storage capacity sequence processing neural networks Journal Physics A J M Gutierrez E Castillo A S Hadi Expert Systems Probabilistic Network Models Springer Verlag J Edmonds R M Karp Theoretical improvements algorithmic efficiency network flow problems Journal ACM R Edwards A Sokal Generalization fortium kasteleyn swendson wang representation monte carlo algorithm Physical Review D A E Elo The rating chess players past present Arco New York second edition R F Engel GARCH The Use ARCH GARCH Models Applied Econometrics Journal Economic Perspectives Y Ephraim W J J Roberts Revisiting autoregressive hidden Markov modeling speech signals IEEE Signal Processing Letters February E Erosheva S Fienberg J Lafferty Mixed membership models scientific publications In Proceedings National Academy Sciences volume pages R E Fan P H Chen C J Lin Working Set Selection Using Second Order Information Training Support Vector Machines Journal Machine Learning Research P Fearnhead Exact Efficient Bayesian inference multiple changepoint problems Technical report Deptartment Mathematics Statistics Lancaster University G H Fischer I W Molenaar Rasch Models Foundations Recent Developments Applications Springer New York M E Fisher Statistical Mechanics Dimers Plane Lattice Physical Review B Frey Extending Factor Graphs Unify Directed Undirected Graphical Models In C Meek U Kjrulff editors Uncertainty Artificial Intelligence number pages Morgan Kaufmann N Friedman D Geiger M Goldszmidt Bayesian Network Classifiers Machine Learning S Fru hwirth Schnatter Finite Mixture Markov Switching Models Springer M Frydenberg The chain graph Markov property Scandanavian Journal Statistics T Furmston D Barber Solving deterministic policy po mpds expectation maximisation tifreeze In First international workshop learning data mining robotics LEMIR September In conjunction ECML PKDD T Furmston D Barber Variational methods Reinforcement Learning In Teh Y W M Titter ington editors Proceedings The Thirteenth International Conference Artificial Intelligence Statistics AISTATS volume pages Chia Laguna Sardinia Italy May JMLR T Furmston D Barber Efficient Inference Markov Control Problems In Uncertainty Artificial Intelligence number Corvallis Oregon USA T Furmston D Barber Lagrange Dual Decomposition Finite Horizon Markov Decision Processes In European Conference Machine Learning ECML DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY A Galka O Yamashita T Ozaki R Biscay P Valdes Sosa A solution dynamical inverse problem EEG generation spatiotemporal Kalman filtering NeuroImage P Gandhi F Bromberg D Margaritis Learning markov network structure independence tests In Proceedings SIAM International Conference Data Mining pages M R Garey D S Johnson Computers Intractability A Guide Theory NP Completeness W H Freeman Company New York A Gelb Applied optimal estimation MIT press A Gelman G O Roberts W R Gilks Efficient Metropolis jumping rules In J O Bernardo J M Berger A P Dawid A F M Smith editors Bayesian Statistics volume pages Oxford University Press S Geman D Geman Stochastic relaxation Gibbs distributions Bayesian restoration images In Readings uncertain reasoning pages San Francisco CA USA Morgan Kaufmann Publishers Inc M G Genton Classes kernels machine learning A statistics perspective Journal Machine Learning Research W Gerstner W M Kistler Spiking Neuron Models Cambridge University Press Z Ghahramani M J Beal Variational Inference Bayesian Mixtures Factor Analysers In S A Solla T K Leen K R Mu ller editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press Z Ghahramani G E Hinton Variational learning switching state space models Neural Computation A Gibbons Algorithmic Graph Theory Cambridge University Press M Gibbs Bayesian Gaussian processes regression classification PhD thesis University Cambridge W R Gilks S Richardson D J Spiegelhalter Markov chain Monte Carlo practice Chapman Hall M Girolami A variational method learning sparse overcomplete representations Neural Computation M Girolami A Kaban On equivalence PLSI LDA In Proceedings 26th annual international ACM SIGIR conference Research development information retrieval pages New York NY USA ACM Press M E Glickman Parameter estimation large dynamic paired comparison experiments Applied Statistics A Globerson T Jaakkola Approximate inference planar graph decomposition In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press D Goldberg D Nichols B M Oki D Terry Using collaborative filtering weave information tapestry Communications ACM G H Golub C F van Loan Matrix Computations Johns Hopkins University Press 3rd edition M C Golumbic I Ben Arroyo Hartman Graph Theory Combinatorics Algorithms Springer Verlag C Goutis A graphical method solving decision analysis problem IEEE Transactions Systems Man Cybernetics P J Green B W Silverman Nonparametric Regression Generalized Linear Models volume Monographs Statistics Applied Probability Chapman Hall D M Greig B T Porteous A H Seheult Exact maximum posteriori estimation binary images Journal Royal Statistical Society Series B DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY G Grimmett D Stirzaker Probability Random Processes Oxford University Press second edition S F Gull Bayesian data analysis straight line fitting In J Skilling editor Maximum entropy Bayesian methods Cambridge pages Kluwer A K Gupta D K Nagar Matrix Variate Distributions Chapman Hall CRC Boca Raton Florida USA D J Hand K Yu Idiot s Bayes Not So Stupid After All International Statistical Review D R Hardoon S Szedmak J Shawe Taylor Canonical Correlation Analysis An Overview Applica tion Learning Methods Neural Computation D O Hebb The organization behavior Wiley New York D Heckerman A Tutorial Learning With Bayesian Networks Technical Report MSR TR Microsoft Research Redmond WA March Revised November D Heckerman D Geiger D Chickering Learning Bayesian Networks The Combination Knowledge Statistical Data Machine Learning R Herbrich T Minka T Graepel TrueSkillTM A Bayesian Skill Rating System In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press H Hermansky Should recognizers ears Speech Communication J Hertz A Krogh R Palmer Introduction theory Neural Computation Addison Wesley T Heskes Convexity arguments efficient minimization Bethe Kikuchi free energies Journal Artificial Intelligence Research D M Higdon Auxiliary variable methods Markov chain Monte Carlo applications Journal American Statistical Association G E Hinton R R Salakhutdinov Reducing dimensionality data neural networks Science T Hofmann J Puzicha M I Jordan Learning dyadic data In M S Kearns S A Solla D A Cohn editors Advances Neural Information Processing Systems NIPS pages Cambridge MA MIT Press R A Howard J E Matheson Influence diagrams Decision Analysis Republished version original report J C Hull Options Futures Other Derivatives Prentice Hall A Hyva rinen J Karhunen E Oja Independent Component Analysis Wiley Aapo Hyva rinen Consistency Pseudolikelihood Estimation Fully Visible Boltzmann Machines Neural Computation M Isard A Blake CONDENSATION Conditional Density Propagation Visual Tracking International Journal Computer Vision T S Jaakkola M I Jordan Variational probabilistic inference qmr dt network Journal Artificial Intelligence Research T S Jaakkola M I Jordan Bayesian parameter estimation variational methods Statistics Computing R A Jacobs F Peng M A Tanner A Bayesian approach model selection hierarchical mixtures experts architectures Neural Networks R G Jarrett A note intervals coal mining disasters Biometrika E T Jaynes Probability Theory The Logic Science Cambridge University Press DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY F Jensen F V Jensen D Dittmer From Influence Diagrams Junction Trees In Proceedings 10th Annual Conference Uncertainty Artificial Intelligence UAI pages San Francisco CA Morgan Kaufmann F V Jensen F Jensen Optimal Junction Trees In R Lopez de Mantaras D Poole editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann F V Jensen T D Nielson Bayesian Networks Decision Graphs Springer Verlag second edition M I Jordan R A Jacobs Hierarchical mixtures experts EM algorithm Neural Computation B H Juang W Chou C H Lee Minimum classification error rate methods speech recognition IEEE Transactions Speech Audio Processing L P Kaelbling M L Littman A R Cassandra Planning acting partially observable stochastic domains Artificial Intelligence H J Kappen An introduction stochastic control theory path integrals reinforcement learning In Proceedings 9th Granada seminar Computational Physics Computational Mathematical Modeling Cooperative Behavior Neural Systems volume pages American Institute Physics H J Kappen F B Rodr guez Efficient learning Boltzmann machines linear response theory Neural Compution H J Kappen W Wiegerinck Novel iteration schemes Cluster Variation Method In T G Dietterich S Becker Z Ghahramani editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press Y Karklin M S Lewicki Emergence complex cell properties learning generalize natural scenes Nature November G Karypis V Kumar A fast high quality multilevel scheme partitioning irregular graphs Siam Journal Scientific Computing P W Kasteleyn Dimer Statistics Phase Transitions Journal Mathematical Physics S A Kauffman At Home Universe The Search Laws Self Organization Complexity Oxford University Press Oxford UK C J Kim Dynamic linear models Markov switching Journal Econometrics C J Kim C R Nelson State Space models regime switching MIT Press G Kitagawa The Two Filter Formula Smoothing implementation Gaussian sum smoother Annals Institute Statistical Mathematics U B Kjaerulff A L Madsen Bayesian Networks Influence Diagrams A Guide Construction Analysis Springer N Komodakis N Paragios G Tziritas MRF Optimization Dual Decomposition Message Passing Revisited In IEEE 11th International Conference Computer Vision ICCV pages A Krogh M Brown I Mian K Sjolander D Haussler Hidden Markov models computational biology Applications protein modeling Journal Molecular Biology S Kullback Information Theory Statistics Dover K Kurihara M Welling Y W Teh Collapsed Variational Dirichlet Process Mixture Models In Proceedings International Joint Conference Artificial Intelligence volume pages J Lafferty A McCallum F Pereira Conditional random fields Probabilistic models segmenting labeling sequence data In C E Brodley A P Danyluk editors International Conference Machine Learning number pages San Francisco CA Morgan Kaufmann H Lass Elements Pure Applied Mathematics McGraw Hill reprinted Dover S L Lauritzen Graphical Models Oxford University Press DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY S L Lauritzen A P Dawid B N Larsen H G Leimer Independence properties directed Markov fields Networks S L Lauritzen D J Spiegelhalter Local computations probabilities graphical structures application expert systems Journal Royal Statistical Society B D D Lee H S Seung Algorithms non negative matrix factorization In T K Leen T G Dietterich V Tresp editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press M A R Leisink H J Kappen A Tighter Bound Graphical Models In Neural Computation volume pages MIT Press V Lepar P P Shenoy A Comparison Lauritzen Spiegelhalter Hugin Shenoy Shafer Architectures Computing Marginals Probability Distributions In G Cooper S Moral editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann U Lerner R Parr D Koller G Biswas Bayesian Fault Detection Diagnosis Dynamic Systems In Proceedings Seventeenth National Conference Artificial Intelligence AIII pages U N Lerner Hybrid Bayesian Networks Reasoning Complex Systems Computer science department Stanford University R Linsker Improved local learning rule information maximization related applications Neural Networks Y L Loh E W Carlson M Y J Tan Bond propagation algorithm thermodynamic functions general dimensional Ising models Physical Review B H Lopes M West Bayesian model assessment factor analysis Statistica Sinica T J Loredo From Laplace To Supernova Sn 1987A Bayesian Inference In Astrophysics In P F Fougere editor Maximum Entropy Bayesian Methods pages Kluwer D J C MacKay Bayesian interpolation Neural Computation D J C MacKay Probable Networks plausible predictions review practical Bayesian methods supervised neural networks Network Computation Neural Systems D J C MacKay Introduction Gaussian Processes In Neural Networks Machine Learning volume NATO advanced study institute generalization neural networks machine learning pages Springer August D J C MacKay Information Theory Inference Learning Algorithms Cambridge University Press U Madhow Fundamentals Digital Communication Cambridge University Press K V Mardia J T Kent J M Bibby Multivariate Analysis Academic Press H Markram J Lubke M Frotscher B Sakmann Regulation synaptic efficacy coincidence postsynaptic APs EPSPs Science A McCallum K Nigam J Rennie K Seymore Automating construction internet portals machine learning Information Retrieval Journal G McLachlan T Krishnan The EM Algorithm Extensions John Wiley Sons G McLachlan D Peel Finite Mixture Models Wiley Series Probability Statistics Wiley Interscience E Meeds Z Ghahramani R M Neal S T Roweis Modeling Dyadic Data Binary Latent Factors In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS volume pages Cambridge MA MIT Press M Meila An Accelerated Chow Liu Algorithm Fitting Tree Distributions High Dimensional Sparse Data In I Bratko editor International Conference Machine Learning pages San Francisco CA Morgan Kaufmann DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY M Meila M I Jordan Triangulation continuous embedding In M C Mozer M I Jordan T Petsche editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press B Mesot D Barber Switching Linear Dynamical Systems Noise Robust Speech Recognition IEEE Transactions Audio Speech Language Processing N Meuleau M Hauskrecht K E Kim L Peshkin Kaelbling L P T Dean C Boutilier Solving Very Large Weakly Coupled Markov Decision Processes In Proceedings Fifteenth National Conference Artificial Intelligence pages T Mills The Econometric Modelling Financial Time Series Cambridge University Press T Minka Expectation Propagation approximate Bayesian inference In J Breese D Koller editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann T Minka A comparison numerical optimizers logistic regression Technical report Microsoft Research research microsoft com minka papers logreg T Minka Divergence measures message passing Technical Report MSR TR Microsoft Research Ltd Cambridge UK December A Mira J Mller G O Roberts Perfect slice samplers Journal Royal Statistical Society Series B Statistical Methodology C Mitchell M Harper L Jamieson On complexity explicit duration HMM s Speech Audio Processing IEEE Transactions May T Mitchell Machine Learning McGraw Hill J Mooij H J Kappen Sufficient conditions convergence Loopy Belief Propagation IEEE Information Theory J W Moon L Moser On cliques graphs Israel Journal Mathematics A Moore A tutorial kd trees Technical report Available http www cs cmu edu awm papers html J Moussouris Gibbs Markov Random Systems Constraints Journal Statistical Physics R M Neal Probabilistic inference Markov Chain Monte Carlo methods CRG TR Dept Computer Science University Toronto R M Neal Markov Chain Sampling Methods Dirichlet Process Mixture Models Journal Computational Graphical Statistics R M Neal Slice sampling Annals Statistics R E Neapolitan Learning Bayesian Networks Prentice Hall A V Nefian L Luhong P Xiaobo X Liu C Mao K Murphy A coupled HMM audio visual speech recognition In IEEE International Conference Acoustics Speech Signal Processing volume pages H Nickisch M Seeger Convex Variational Bayesian Inference Large Scale Generalized Linear Models International Conference Machine Learning D Nilsson An efficient algorithm finding m probable configurations probabilistic expert system Statistics Computing D Nilsson J Goldberger Sequentially finnding N best list Hidden Markov Models Internation Joint Conference Artificial Intelligence IJCAI A B Novikoff On convergence proofs perceptrons In Symposium Mathematical Theory Automata New York volume pages Brooklyn N Y Polytechnic Press Polytechnic Institute Brooklyn DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY F J Och H Ney Discriminative training maximum entropy models statistical machine transla tion In Proceedings Annual Meeting Association Computational Linguistics pages Philadelphia July B A Olshausen D J Field Sparse coding overcomplete basis set A strategy employed V1 Vision Research A V Oppenheim R W Shafer M T Yoder W T Padgett Discrete Time Signal Processing Prentice Hall edition M Ostendorf V Digalakis O A Kimball From HMMs Segment Models A Unified View Stochastic Modeling Speech Recognition IEEE Transactions Speech Audio Processing P Paatero U Tapper Positive matrix factorization A non negative factor model optimal utilization error estimates data values Environmetrics A Palmer D Wipf K Kreutz Delgado B Rao Variational EM algorithms non Gaussian latent variable models In B Scho lkopf J Platt T Hoffman editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press V Pavlovic J M Rehg J MacCormick Learning switching linear models human motion In T K Leen T G Dietterich V Tresp editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press J Pearl Probabilistic Reasoning Intelligent Systems Networks Plausible Inference Morgan Kaufmann J Pearl Causality Models Reasoning Inference Cambridge University Press B A Pearlmutter L C Parra Maximum Likelihood Blind Source Separation A Context Sensitive Gen eralization ICA In M C Mozer M I Jordan T Petsche editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press K B Petersen O Winther The EM algorithm independent component analysis In IEEE International Conference Acoustics Speech Signal Processing volume pages J P Pfister T Toyiozumi D Barber W Gerstner Optimal Spike Timing Dependent Plasticity Precise Action Potential Firing Supervised learning Neural Computation J Platt Fast Training Support Vector Machines Using Sequential Minimal Optimization In B Scho lkopf C J C Burges A J Smola editors Advances Kernel Methods Support Vector Learning pages MIT Press I Porteous D Newman A Ihler A Asuncion P Smyth M Welling Fast collapsed Gibbs sampling Latent Dirichlet Allocation In KDD Proceeding 14th ACM SIGKDD international conference Knowledge discovery data mining pages New York NY USA ACM J E Potter R G Stern Statistical filtering space navigation measurements In American Institute Aeronautics Astronautics Guidance Control Conference volume pages Cambridge Mass August W Press W Vettering S Teukolsky B Flannery Numerical Recipes Fortran Cambridge University Press S J D Prince J H Elder Probabilistic Linear Discriminant Analysis Inferences About Identity In IEEE 11th International Conference Computer Vision ICCV pages L R Rabiner A tutorial hidden Markov models selected applications speech recognition Proc IEEE C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press H E Rauch G Tung C T Striebel Maximum Likelihood estimates linear dynamic systems American Institute Aeronautics Astronautics Journal AIAAJ T Richardson P Spirtes Ancestral Graph Markov Models Annals Statistics D Rose R E Tarjan E S Lueker Algorithmic aspects vertex elimination graphs SIAM Journal Computing DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY F Rosenblatt The Perceptron A Probabilistic Model Information Storage Organization Brain Psychological Review S T Roweis L J Saul Nonlinear Dimensionality Reduction Locally Linear Embedding Science D B Rubin Using SIR algorithm simulate posterior distributions In M H Bernardo K M Degroot D V Lindley A F M Smith editors Bayesian Statistics Oxford University Press D Saad M Opper Advanced Mean Field Methods Theory Practice MIT Press R Salakhutdinov S Roweis Z Ghahramani Optimization EM Expectation Conjugate Gradient In T Fawcett N Mishra editors International Conference Machine Learning number pages Menlo Park CA AAAI Press L K Saul M I Jordan Exploiting tractable substructures intractable networks In D S Touretzky M Mozer M E Hasselmo editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press L Savage The Foundations Statistics Wiley R D Schachter Bayes ball The rational pastime determining irrelevance requisite information belief networks influence diagrams In G Cooper S Moral editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann B Scho lkopf A Smola K R Mu ller Nonlinear Component Analysis Kernel Eigenvalue Problem Neural Computation N N Schraudolph D Kamenetsky Efficient Exact Inference Planar Ising Models In D Koller D Schuur mans Y Bengio L Bottou editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press E Schwarz Estimating dimension model Annals Statistics M Seeger Gaussian Processes Machine Learning International Journal Neural Systems M Seeger Expectation propagation exponential families Technical report Department EECS Berkeley www kyb tuebingen mpg de bs people seeger J Shawe Taylor N Cristianini Kernel Methods Pattern Analysis Cambridge University Press S Siddiqi B Boots G Gordon A Constraint Generation Approach Learning Stable Linear Dynamical Systems In J C Platt D Koller Y Singer S Roweis editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press T Silander P Kontkanen P Myllyma ki On Sensitivity MAP Bayesian Network Structure Equivalent Sample Size Parameter In R Parr L van der Gaag editors Uncertainty Artificial Intelligence number pages Corvallis Oregon USA AUAI press S S Skiena The algorithm design manual Springer Verlag New York USA E Smith M S Lewicki Efficient auditory coding Nature P Smolensky Parallel Distributed Processing Volume Foundations chapter Information processing dynamical systems Foundations harmony theory pages MIT Press Cambridge MA G Sneddon Studies atmospheric sciences chapter A Statistical Perspective Data Assimilation Numerical Models Number Lecture Notes Statistics Springer Verlag P Sollich Bayesian Methods Support Vector Machines Evidence Predictive Class Probabilities Machine Learning D X Song D Wagner X Tian Timing Analysis Keystrokes Timing Attacks SSH In Proceedings 10th conference USENIX Security Symposium USENIX Association A S Spanias Speech coding tutorial review Proceedings IEEE Oct P Spirtes C Glymour R Scheines Causation Prediction Search MIT press edition DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY N Srebro Maximum Likelihood Bounded Tree Width Markov Networks In J Breese D Koller editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann H Steck Constraint Based Structural Learning Bayesian Networks Finite Data Sets PhD thesis Technical University Munich H Steck Learning Bayesian Network Structure Dirichlet Prior vs Data In D A McAllester P Myl lymaki editors Uncertainty Artificial Intelligence number pages Corvallis Oregon USA AUAI press H Steck T Jaakkola On Dirichlet prior Bayesian regularization In S Becker S Thrun K Obermayer editors NIPS pages MIT Press G Strang Linear Algebra It is Applications Brooks Cole M Studeny On mathematical description probabilistic conditional independence structures PhD thesis Academy Sciences Czech Republic M Studeny On non graphical description models conditional independence structure In HSSS Workshop Stochastic Systems Individual Behaviours Louvain la Neueve Belgium January C Sutton A McCallum An introduction conditional random fields relational learning In L Getoor B Taskar editors Introduction Statistical Relational Learning MIT press R S Sutton A G Barto Reinforcement Learning An Introduction MIT Press R J Swendsen J S Wang Nonuniversal critical dynamics Monte Carlo simulations Physical Review Letters B K Sy A Recurrence Local Computation Approach Towards Ordering Composite Beliefs Bayesian Belief Networks International Journal Approximate Reasoning T Sejnowski The Book Hebb Neuron R E Tarjan M Yannakakis Simple linear time algorithms test chordality graphs test acyclicity hypergraphs selectively reduce acyclic hypergraphs SIAM Journal Computing S J Taylor Modelling Financial Time Series World Scientific second edition Y W Teh D Newman M Welling A Collapsed Variational Bayesian Inference Algorithm Latent Dirichlet Allocation In J C Platt D Koller Y Singer S Roweis editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press Y W Teh M Welling The unified propagation scaling algorithm In T G Dietterich S Becker Z Ghahramani editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press R Tibshirani Regression shrinkage selection lasso Journal Royal Statistical Society B H Tijms Understanding Probability Cambridge University Press M Tipping C M Bishop Mixtures probabilistic principal component analysers Neural Computation M E Tipping Sparse Bayesian Learning Relevance Vector Machine Journal Machine Learning Research M E Tipping Sparse Bayesian learning relevance vector machine Journal Machine Learning Research M E Tipping C M Bishop Probabilistic principal component analysis Journal Royal Statistical Society Series B D M Titterington A F M Smith U E Makov Statistical analysis finite mixture distributions Wiley M Toussaint S Harmeling A Storkey Probabilistic inference solving PO MDPs Research Report EDI INF RR University Edinburgh School Informatics DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY M Tsodyks K Pawelzik H Markram Neural Networks Dynamic Synapses Neural Computation L van der Matten G Hinton Visualizing Data t SNE Journal Machine Learning Research P Van Overschee B De Moor Subspace Identification Linear Systems Theory Implementations Applications Kluwer V Vapnik The Nature Statistical Learning Theory Springer New York M Verhaegen P Van Dooren Numerical Aspects Different Kalman Filter Implementations IEEE Transactions Automatic Control T Verma J Pearl Causal networks Semantics expressiveness In R D Schacter T S Levitt L N Kanal J F Lemmer editors Uncertainty Artificial Intelligence volume pages Amsterdam North Holland T O Virtanen A T Cemgil S J Godsill Bayesian extensions nonnegative matrix factorisation audio signal modelling In IEEE International Conference Acoustics Speech Signal Processing pages G Wahba Support Vector Machines Repreducing Kernel Hilbert Spaces Randomized GACV pages MIT Press M J Wainwright M I Jordan Graphical models exponential families variational inference Founda tions Trends Machine Learning H Wallach Efficient training conditional random fields Master s thesis Division Informatics University Edinburgh Y Wang J Hodges B Tang Classification Web Documents Using Naive Bayes Method 15th IEEE International Conference Tools Artificial Intelligence pages S Waterhouse D Mackay T Robinson Bayesian methods mixtures experts In D S Touretzky M Mozer M E Hasselmo editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press C Watkins P Dayan Q learning Machine Learning Y Weiss W T Freeman Correctness Belief Propagation Gaussian Graphical Models Arbitrary Topology Neural Computation M Welling T P Minka Y W Teh Structured Region Graphs Morphing EP GBP In F Bacchus T Jaakkola editors Uncertainty Artificial Intelligence number pages Corvallis Oregon USA AUAI press J Whittaker Graphical Models Applied Multivariate Statistics John Wiley Sons W Wiegerinck Variational approximations mean field theory Junction Tree algorithm In C Boutilier M Goldszmidt editors Uncertainty Artificial Intelligence number pages San Francisco CA Morgan Kaufmann W Wiegerinck T Heskes Fractional Belief Propagation In S Becker S Thrun K Obermayer editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press C K I Williams Computing infinite networks In M C Mozer M I Jordan T Petsche editors Advances Neural Information Processing Systems NIPS pages Cambridge MA MIT Press C K I Williams D Barber Bayesian classification Gaussian processes IEEE Trans Pattern Analysis Machine Intelligence C Yanover Y Weiss Finding M Most Probable Configurations Using Loopy Belief Propagation In S Thrun L Saul B Scho lkopf editors Advances Neural Information Processing Systems NIPS number pages Cambridge MA MIT Press J S Yedidia W T Freeman Y Weiss Constructing free energy approximations generalized belief propagation algorithms Information Theory IEEE Transactions July DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY S Young D Kershaw J Odell D Ollason V Valtchev P Woodland The HTK Book Version Cambridge University Press A L Yuille A Rangarajan The concave convex procedure Neural Computation J H Zhao P L H Yu Q Jiang ML estimation factor analysis EM non EM Statistics Computing O Zoeter Monitoring non linear switching dynamical systems PhD thesis Radboud University Nijmegen DRAFT April BIBLIOGRAPHY BIBLIOGRAPHY DRAFT April Index M coding N max product expansion recursion recursion recursion maximum likelihood Hopfield network naive mean field absorbing state absorption influence diagram acceptance function active learning acyclic adjacency matrix algebraic Riccati equation ancestor ancestral ordering ancestral sampling antifreeze approximate inference belief propagation Bethe free energy double integration bound expectation propagation graph cut Laplace approximation switching linear dynamical system variational approach variational inference AR model auto regressive model ARCH model Artificial Life asymmetry asynchronous updating auto regressive model ARCH GARCH switching time varying auxiliary variable sampling average backtracking bag words batch update Baum Welch Bayes Information Criterion Bayes factor model selection theorem Bayes rule Bayes theorem Bayesian decision theory hypothesis testing image denoising linear model mixture model model selection Occam s razor outcome analysis Bayesian Dirichlet score Bayesian linear model BD score BDeu score BDeu score belief network chest clinic belief network asbestos smoking cancer cascade divorcing parents dynamic noisy AND gate noisy logic gate noisy OR gate sigmoid structure learning training Bayesian belief propagation loopy belief revision max product INDEX INDEX Bellman s equation Bessel function beta distribution function Bethe free energy bias unbiased estimator bigram binary entropy binomial coefficient binomial options pricing model bioinformatics black white sampling black box Black Scholes option pricing Blahut Arimoto algorithm Boltzmann machine restricted bond propagation Bonferroni inequality Boolean network Bradley Terry Luce model bucket elimination burn calculus canonical correlation analysis constrained factor analysis canonical variates causal consistency causality calculus influence diagrams post intervention distribution CCA canonical correlation analysis centering chain graph chain component chain rule chain structure changepoint model checkerboard chest clinic missing data decisions children directed acyclic graph Cholesky chord chordal Chow Liu tree classification Bayesian boundary error analysis linear parameter model multiple classes performance random guessing softmax clique decomposition graph matrix cliquo Cluster Variation method clustering collaborative filtering collider directed acyclic graph commute compatibility function competition model Bradley Terry Luce Elo TrueSkill competition models concave function conditional entropy conditional likelihood conditional mutual information conditional probability conditional random field conditioning loop cut set conjugate distribution exponential family Gaussian prior conjugate gradient conjugate gradients algorithm conjugate vector conjugate vectors algorithm connected components connected graph consistent consistent estimator control convex function correction smoother correlation matrix cosine similarity coupled HMM covariance matrix covariance function exponential construction Gibbs isotropic DRAFT April INDEX INDEX Mate rn Mercer kernel neural network non stationary Ornstein Uhlenbeck periodic rational quadratic smoothness squared exponential stationary CPT conditional probability table CRF conditional random field critical point cross validation cumulant curse dimensionality cut set conditioning cycle D map dependence map DAG directed acyclic graph data anomaly detection catagorical dyadic handwritten digits labelled monadic numerical ordinal unlabelled data compression vector quantisation decision boundary decision function decision theory decision tree decomposable degree degree belief delta function Dirac delta function Kronecker density estimation Parzen estimator dependence map descendant design matrix determinant deterministic latent variable model differentiation digamma function digit data Dijkstra s algorithm dimension reduction linear supervised dimensionality reduction linear non linear Dirac delta function directed acyclic graph ancestor ancestral order cascade children collider descendant family immorality moralisation parents direction bias directional derivative Dirichlet distribution Dirichlet process mixture models discount factor discriminative approach training discriminative approach discriminative training dissimilarity function distributed computation distribution Bernoulli beta binomial Categorical change variables conjugate continuous density Dirichlet discrete divergence double exponential empirical average expectation exponential exponential family canonical gamma mode Gauss gamma Gauss inverse gamma Gaussian canonical exponential form conditioning DRAFT April INDEX INDEX conjugate entropy mixture multivariate normalisation partitioned propagation system reversal univariate inverse gamma inverse Wishart isotropic joint kurtosis Laplace marginal mode moment multinomial normal Poisson Polya scaled mixture skewness Student s t uniform Wishart domain double integration bound dual parameters dual representation dyadic data dynamic Bayesian network dynamic synapses dynamical system linear non linear dynamics reversal early stopping edge list efficient IPF eigen decomposition equation function problem spectrum value Elo model emission distribution emission matrix empirical independence empirical distribution empirical risk penalised empirical risk minimisation energy entropy differential Gaussian EP expectation propagation error function estimator consistent evidence marginal likelihood hard likelihood soft uncertain virtual Evidence Procedure exact sampling expectation average expectation correction expectation maximisation algorithm antifreeze belief networks E step energy entropy failure case intractable energy M step mixture model partial E step partial M step variational Bayes Viterbi training expectation propagation exponential family canonical form conjugate extended observability matrix face model factor analysis factor rotation probabilistic PCA training EM SVD factor graph factor loading family directed acyclic graph feature map filtering finance optimal investment DRAFT April INDEX INDEX option pricing finite dimensional Gaussian Process Fisher information Fisher s linear discriminant Floyd Warshall Roy algorithm forward sampling Forward Backward forward filtering backward sampling Forward Sampling Resampling gamma digamma distribution function GARCH model Gaussian canonical representation distribution moment representation sub super Gaussian mixture model Bayesian collapsing mixture EM algorithm infinite problems k means Parzen estimator symmetry breaking Gaussian process classification Laplace approximation multiple classes regression smoothness weight space view Gaussian sum filtering Gaussian sum smoothing generalisation generalised pseudo Bayes generative approach model training generative approach Gibbs sampling Glicko GMM Gaussian mixture model Google gradient descent Gram matrix Gram Schmidt procedure graph acyclic adjacency matrix chain chain structured chord chordal clique clique matrix cliquo connected cut cycle decomposable descendant directed disconnected edge list factor loop loopy multiply connected neighbour path separation set chain singly connected skeleton spanning tree tree triangulated undirected vertex degree graph cut algorithm graph partitioning Gull MacKay iteration Hamilton Jacobi equation Hamiltonian dynamics Hammersley Clifford theorem handwritten digits Hankel matrix harmonium restricted Boltzmann machine Heaviside step function Hebb Hebb rule hedge fund Hermitian Hessian hidden Markov model recursion recursion coupled direction bias discriminative training duration model entropy filtering DRAFT April INDEX INDEX input output likelihood likely state MAP pairwise marginal Rauch Tung Striebel smoother smoothing parallel sequential viterbi Viterbi algorithm hidden variables HMM hidden Markov model Hopfield network augmented capacity Hebb rule heteroassociative maximum likelihood perceptron pseudo inverse rule sequence learning hybrid Monte Carlo hyper Markov hyper tree hyperparameter hyperplane hypothesis testing Bayesian error analysis I map independence map ICA identifiability identity matrix IID independent identically distributed I am algorithm information maximisation algorithm immorality importance distribution sampling particle filter resampling sequential weight incidence matrix independence Bayesian conditional empirical map Markov equivalent mutual information naive Bayes parameter perfect map independent identically distributed independent components analysis indicator function indicator model induced representation inference bond propagation bucket elimination causal cut set conditioning HMM linear dynamical system MAP marginal Markov decision process max product message passing mixed MPM sum product algorithm transfer matrix variable elimination influence diagram absorption asymmetry causal consistency chest clinic decision potential fundamental link information link junction tree forgetting assumption partial order probability potential solving utility utility potential information link information maximisation information retrieval information maximisation algorithm innovation noise input output HMM inverse modus ponens IPF iterative proportional fitting efficient Ising model Markov network approximate inference isotropic isotropic covariance functions item response theory Iterated Conditional Modes iterative proportional fitting iterative scaling Jeffrey s rule Jensen s inequality Joseph s symmetrized update DRAFT April INDEX INDEX jump Markov model switching linear dynamical system junction tree absorption algorithm clique graph computational complexity conditional marginal consistent hyper tree influence diagram marginal likelihood likely state normalisation constant potential running intersection property separator strong strong triangulation tree width triangulation k means Kalman filter Kalman gain KD tree kernel covariance function classifier kidnapped robot Kikuchi KL divergence Kullback Leibler divergence KNN nearest neighbour Kronecker delta Kullback Leibler divergence kurtosis labelled data Lagrange dual multiplier Lagrangian Laplace approximation latent ability model latent Dirichlet allocation latent linear model latent semantic analysis latent topic latent variable deterministic model lattice model LDA regularised LDS linear dynamical system leaky integrate fire model Leapfrog discretisation learning active anomaly detection Bayesian belief network belief networks EM Dirichlet prior inference nearest neighbour online query reinforcement semi supervised sequences sequential structure supervised unsupervised learning rate likelihood bound marginal model approximate pseudo likelihood decomposable line search linear algebra linear dimension reduction canonical correlation analysis latent semantic analysis non negative matrix factorisation probabilistic latent semantic analysis supervised unsupervised Linear Discriminant Analysis linear discriminant analysis regression penalised regularised linear dynamical system cross moment dynamics reversal filtering identifiability inference learning likelihood likely state numerical stability Riccati equations smoothing subspace method switching symmetrising updates DRAFT April INDEX INDEX linear Gaussian state space model linear model Bayesian classification factor analysis latent regression linear parameter model Bayesian linear perceptron linear separability linear transformation linearly independent linearly separable Linsker s Gaussian approximation localisation logic Aristotle logistic regression logistic sigmoid logit loop loop cut set loopy loss function squared zero loss matrix Luenberger expanding subspace theorem Mahalanobis distance manifold linear low dimensional MAP probable posteriori prob able posteriori MAR missing random margin soft marginal generalised marginal likelihood approximate marginalisation Markov blanket chain order stationary distribution equivalent global hyper local model pairwise random field approximation Markov chain absorbing state PageRank Markov chain Monte Carlo auxiliary variable hybrid Monte Carlo slice sampling Swendson Wang Gibbs sampling Metropolis Hastings proposal distribution structured Gibbs sampling Markov decision process Bellman s equation discount factor partially observable planning policy iteration reinforcement learning stationary deterministic policy temporally unbounded value iteration Markov equivalence Markov network Boltzmann machine continuous state temporal discrete state temporal Gibbs distribution Hammersley Clifford theorem pairwise potential Markov random field alpha expansion attractive binary graph cut map Potts model matrix adjacency block inverse Cholesky clique Gram Hankel incidence inversion inversion lemma orthogonal positive definite pseudo inverse rank stochastic matrix factorisation max product DRAFT April INDEX INDEX N probable states max sum maximum cardinality checking maximum likelihood belief network Chow Liu tree counting empirical distribution factor analysis Gaussian gradient optimisation Markov network ML II naive Bayes properties MCMC Markov chain Monte Carlo mean field theory asynchronous updating Mercer kernel message passing schedule message passing Metropolis Hastings acceptance function Metropolis Hastings sampling minimum clique cover missing random completely missing data mixed inference mixed membership model mixing matrix mixture Gaussian mixture model Bernoulli product Dirichlet process mixture expectation maximisation factor analysis Gaussian indicator approach Markov chain PCA mixture experts MN Markov network mode model auto regressive changepoint deterministic latent variable faces leaky integrate fire linear mixed membership mixture Rasch model selection approximate moment moment representation momentum monadic data money financial prediction loadsa moralisation probable posteriori probable path multiple source multiple sink probable state N probable MRF Markov random field multiply connected multiply connected distributions mutual information approximation conditional maximisation naive Bayes Bayesian tree augmented naive mean field theory nearest neighbour probabilistic network flow network modelling neural computation neural network depression dynamic synapses leaky integrate fire Newton update Newton s method forgetting assumption node extremal simplicial non negative matrix factorisation normal distribution normal equations normalised importance weights observed linear dynamical system Occam s razor One m encoding online learning optimisation Broyden Fletcher Goldfarb Shanno conjugate gradients algorithm conjugate vectors algorithm DRAFT April INDEX INDEX constrained optimisation critical point gradient descent Luenberger expanding subspace theorem Newton s method quasi Newton method option pricing ordinary squares orthogonal orthogonal squares orthonormal outlier complete representation complete representations overcounting overfitting PageRank pairwise comparison models pairwise Markov network parents directed acyclic graph speech tagging Partial Least Squares partial order partially observable MDP particle filter partition function partitioned matrix inversion Parzen estimator path blocked PC algorithm PCA Principal Components Analysis perceptron logistic regression perfect elimination order perfect map independence perfect sampling planning plant monitoring plate Poisson distribution policy iteration stationary deterministic Polya distribution POMDP partially observable MDP positive definite matrix parameterisation posterior Dirichlet potential Potts model precision prediction auto regression financial non parametric parameteric predictive variance predictor corrector Principal Components Analysis algorithm high dimensional data kernel latent semantic analysis missing data probabilistic principal directions printer nightmare missing data prior probabilistic latent semantic analysis conditional EM algorithm probabilistic PCA probability conditional function density frequentist posterior potential prior subjective probit probit regression projection proposal distribution Pseudo Inverse pseudo inverse Hopfield network pseudo likelihood quadratic form quadratic programming query learning questionnaire analysis radial basis functions Raleigh quotient Random Boolean networks Rasch model Bayesian Rauch Tung Striebel reabsorption region graphs regresion linear parameter model regression DRAFT April INDEX INDEX logisitic regularisation reinforcement learning rejection sampling relevance vector machine reparameterisation representation dual complete sparse complete resampling reset model residuals responsibility restricted Boltzmann machine Riccati equation ridge regression risk robot arm robust classification Rose Tarjan Lueker elimination running intersection property sample mean variance sampling ancestral forward filtering backward sampling Gibbs importance multi variate particle filter rejection univariate Sampling Importance Resampling scalar product scaled mixture search engine self localisation mapping semi supervised learning lower dimensional representations separator sequential importance sampling sequential minimal optimisation set chain Shafer Shenoy propagation shortest path shortest weighted path sigmoid logistic sigmoid belief network sigmoid function approximate average simple path simplicial nodes Simpson s Paradox singly connected singular Singular Value Decomposition thin skeleton skewness slice sampling smoothing softmax function spam filtering spanning tree sparse representation spectrogram speech recognition spike response model squared Euclidean distance squared loss standard deviation standard normal distribution stationary distribution stationary Markov chain stochastic matrix stop words strong Junction Tree strong triangulation structure learning Bayesian network scoring PC algorithm undirected structured Expectation Propagation subsampling subspace method sum product sum product algorithm supervised learning semi classification regression support vector machine chunking training support vectors SVD Singular Value Decomposition SVM support vector machine Swendson Wang sampling switching AR model switching Kalman filter switching linear dynam ical system switching linear dynamical system changepoint model collapsing Gaussians DRAFT April INDEX INDEX expectation correction filtering Gaussian sum smoothing generalised Pseudo Bayes inference computational complexity likelihood smoothing symmetry breaking system reversal tagging tall matrix term document matrix test set text analysis latent semantic analysis latent topic probabilistic latent semantic analysis TF IDF time invariant LDS Tower Hanoi trace log formula train set training batch discriminative generative generative discriminative HMM linear dynamical system online transfer matrix transition distribution transition matrix tree Chow Liu tree augmented network tree width triangulation check greedy elimination maximum cardinality strong variable elimination TrueSkill uncertainty complete representation undirected graph undirected model learning hidden variable latent variable uniform distribution unit vector unlabelled data unsupervised learning utility matrix money potential zero loss validation cross value value iteration variable hidden missing visible variable elimination variance variational approximation factorised structured variational Bayes expectation maximisation varimax vector algebra vector quantisation visualisation Viterbi Viterbi algorithm Viterbi alignment Viterbi training volatility Voronoi tessellation web modelling website analysis whitening Woodbury formula XOR function zero loss DRAFT April Front Matter Notation List Preface BRML toolbox Contents I Inference Probabilistic Models Probabilistic Reasoning Probability Refresher Interpreting Conditional Probability Probability Tables Probabilistic Reasoning Prior Likelihood Posterior Two dice individual scores Summary Code Basic Probability code General utilities An example Exercises Basic Graph Concepts Graphs Numerically Encoding Graphs Edge list Adjacency matrix Clique matrix Summary Code Utility routines Exercises Belief Networks The Benefits Structure Modelling independencies Reducing burden specification Uncertain Unreliable Evidence Uncertain evidence Unreliable evidence Belief Networks Conditional independence The impact collisions Graphical path manipulations independence d Separation Graphical distributional dependence Markov equivalence belief networks Belief networks limited expressibility Causality Simpson s paradox The calculus Influence diagrams calculus Summary Code Naive inference demo Conditional independence demo Utility routines Exercises Graphical Models Graphical Models Markov Networks Markov properties Markov random fields Hammersley Clifford Theorem Conditional independence Markov networks Lattice Models Chain Graphical Models Factor Graphs Conditional independence factor graphs Expressiveness Graphical Models Summary Code Exercises Efficient Inference Trees Marginal Inference Variable elimination Markov chain message passing The sum product algorithm factor graphs Dealing Evidence Computing marginal likelihood The problem loops Other Forms Inference Max Product Finding N probable states Most probable path shortest path Mixed inference Inference Multiply Connected Graphs Bucket elimination Loop cut conditioning Message Passing Continuous Distributions Summary Code Factor graph examples Most probable shortest path Bucket elimination Message passing Gaussians Exercises The Junction Tree Algorithm Clustering Variables Reparameterisation Clique Graphs Absorption Absorption schedule clique trees Junction Trees The running intersection property Constructing Junction Tree Singly Connected Distributions Moralisation Forming clique graph Forming junction tree clique graph Assigning potentials cliques Junction Trees Multiply Connected Distributions Triangulation algorithms The Junction Tree Algorithm Remarks JTA Computing normalisation constant distribution The marginal likelihood Some small JTA examples Shafer Shenoy propagation Finding Most Likely State Reabsorption Converting Junction Tree Directed Network The Need For Approximations Bounded width junction trees Summary Code Utility routines Exercises Making Decisions Expected Utility Utility money Decision Trees Extending Bayesian Networks Decisions Syntax influence diagrams Solving Influence Diagrams Messages ID Using junction tree Markov Decision Processes Maximising expected utility message passing Bellman s equation Temporally Unbounded MDPs Value iteration Policy iteration A curse dimensionality Variational Inference Planning Financial Matters Options pricing expected utility Binomial options pricing model Optimal investment Further Topics Partially observable MDPs Reinforcement learning Summary Code Sum Max partial order Junction trees influence diagrams Party Friend example Chest Clinic Decisions Markov decision processes Exercises II Learning Probabilistic Models Statistics Machine Learning Representing Data Categorical Ordinal Numerical Distributions The Kullback Leibler Divergence KL q p Entropy information Classical Distributions Multivariate Gaussian Completing square Conditioning system reversal Whitening centering Exponential Family Conjugate priors Learning distributions Properties Maximum Likelihood Training assuming correct model class Training assumed model incorrect Maximum likelihood empirical distribution Learning Gaussian Maximum likelihood training Bayesian inference mean variance Gauss Gamma distribution Summary Code Exercises Learning Inference Learning Inference Learning bias coin Making decisions A continuum parameters Decisions based continuous intervals Bayesian methods ML II Maximum Likelihood Training Belief Networks Bayesian Belief Network Training Global local parameter independence Learning binary variable tables Beta prior Learning multivariate discrete tables Dirichlet prior Structure learning PC algorithm Empirical independence Network scoring Chow Liu Trees Maximum Likelihood Undirected models The likelihood gradient General tabular clique potentials Decomposable Markov networks Exponential form potentials Conditional random fields Pseudo likelihood Learning structure Summary Code PC algorithm oracle Demo empirical conditional independence Bayes Dirichlet structure learning Exercises Naive Bayes Naive Bayes Conditional Independence Estimation Maximum Likelihood Binary attributes Multi state variables Text classification Bayesian Naive Bayes Tree Augmented Naive Bayes Learning tree augmented Naive Bayes networks Summary Code Exercises Learning Hidden Variables Hidden Variables Missing Data Why hidden missing variables complicate proceedings The missing random assumption Maximum likelihood Identifiability issues Expectation Maximisation Variational EM Classical EM Application Belief networks General case Convergence Application Markov networks Extensions EM Partial M step Partial E step A failure case EM Variational Bayes EM special case variational Bayes An example VB Asbestos Smoking Cancer network Optimising Likelihood Gradient Methods Undirected models Summary Code Exercises Bayesian Model Selection Comparing Models Bayesian Way Illustrations coin tossing A discrete parameter space A continuous parameter space Occam s Razor Bayesian Complexity Penalisation A continuous example curve fitting Approximating Model Likelihood Laplace s method Bayes information criterion BIC Bayesian Hypothesis Testing Outcome Analysis Outcome analysis Hindep model likelihood Hsame model likelihood Dependent outcome analysis Is classifier A better B Summary Code Exercises III Machine Learning Machine Learning Concepts Styles Learning Supervised learning Unsupervised learning Anomaly detection Online sequential learning Interacting environment Semi supervised learning Supervised Learning Utility Loss Using empirical distribution Bayesian decision approach Bayes versus Empirical Decisions Summary Exercises Nearest Neighbour Classification Do As Your Neighbour Does K Nearest Neighbours A Probabilistic Interpretation Nearest Neighbours When nearest neighbour far away Summary Code Exercises Unsupervised Linear Dimension Reduction High Dimensional Spaces Low Dimensional Manifolds Principal Components Analysis Deriving optimal linear reconstruction Maximum variance criterion PCA algorithm PCA nearest neighbours classification Comments PCA High Dimensional Data Eigen decomposition N D PCA Singular value decomposition Latent Semantic Analysis Information retrieval PCA With Missing Data Finding principal directions Collaborative filtering PCA missing data Matrix Decomposition Methods Probabilistic latent semantic analysis Extensions variations Applications PLSA NMF Kernel PCA Canonical Correlation Analysis SVD formulation Summary Code Exercises Supervised Linear Dimension Reduction Supervised Linear Projections Fisher s Linear Discriminant Canonical Variates Dealing nullspace Summary Code Exercises Linear Models Introduction Fitting A Straight Line Linear Parameter Models Regression Vector outputs Regularisation Radial basis functions The Dual Representation Kernels Regression dual space Linear Parameter Models Classification Logistic regression Beyond order gradient ascent Avoiding overconfident classification Multiple classes The Kernel Trick Classification Support Vector Machines Maximum margin linear classifier Using kernels Performing optimisation Probabilistic interpretation Soft Zero One Loss Outlier Robustness Summary Code Exercises Bayesian Linear Models Regression With Additive Gaussian Noise Bayesian linear parameter models Determining hyperparameters ML II Learning hyperparameters EM Hyperparameter optimisation gradient Validation likelihood Prediction model averaging Sparse linear models Classification Hyperparameter optimisation Laplace approximation Variational Gaussian approximation Local variational approximation Relevance vector machine classification Multi class case Summary Code Exercises Gaussian Processes Non Parametric Prediction From parametric non parametric From Bayesian linear models Gaussian processes A prior functions Gaussian Process Prediction Regression noisy training outputs Covariance Functions Making new covariance functions old Stationary covariance functions Non stationary covariance functions Analysis Covariance Functions Smoothness functions Mercer kernels Fourier analysis stationary kernels Gaussian Processes Classification Binary classification Laplace s approximation Hyperparameter optimisation Multiple classes Summary Code Exercises Mixture Models Density Estimation Using Mixtures Expectation Maximisation Mixture Models Unconstrained discrete tables Mixture product Bernoulli distributions The Gaussian Mixture Model EM algorithm Practical issues Classification Gaussian mixture models The Parzen estimator K Means Bayesian mixture models Semi supervised learning Mixture Experts Indicator Models Joint indicator approach factorised prior Polya prior Mixed Membership Models Latent Dirichlet allocation Graph based representations data Dyadic data Monadic data Cliques adjacency matrices monadic binary data Summary Code Exercises Latent Linear Models Factor Analysis Finding optimal bias Factor Analysis Maximum Likelihood Eigen approach likelihood optimisation Expectation maximisation Interlude Modelling Faces Probabilistic Principal Components Analysis Canonical Correlation Analysis Factor Analysis Independent Components Analysis Summary Code Exercises Latent Ability Models The Rasch Model Maximum likelihood training Bayesian Rasch models Competition Models Bradley Terry Luce model Elo ranking model Glicko TrueSkill Summary Code Exercises IV Dynamical Models Discrete State Markov Models Markov Models Equilibrium stationary distribution Markov chain Fitting Markov models Mixture Markov models Hidden Markov Models The classical inference problems Filtering p ht v1 t Parallel smoothing p ht v1 T Correction smoothing Sampling p h1 T v1 T Most likely joint state Prediction Self localisation kidnapped robots Natural language models Learning HMMs EM algorithm Mixture emission The HMM GMM Discriminative training Related Models Explicit duration model Input Output HMM Linear chain CRFs Dynamic Bayesian networks Applications Object tracking Automatic speech recognition Bioinformatics Part speech tagging Summary Code Exercises Continuous state Markov Models Observed Linear Dynamical Systems Stationary distribution noise Auto Regressive Models Training AR model AR model OLDS Time varying AR model Time varying variance AR models Latent Linear Dynamical Systems Inference Filtering Smoothing Rauch Tung Striebel correction method The likelihood Most likely state Time independence Riccati equations Learning Linear Dynamical Systems Identifiability issues EM algorithm Subspace Methods Structured LDSs Bayesian LDSs Switching Auto Regressive Models Inference Maximum likelihood learning EM Summary Code Autoregressive models Exercises Switching Linear Dynamical Systems Introduction The Switching LDS Exact inference computationally intractable Gaussian Sum Filtering Continuous filtering Discrete filtering The likelihood p v1 T Collapsing Gaussians Relation methods Gaussian Sum Smoothing Continuous smoothing Discrete smoothing Collapsing mixture Using mixtures smoothing Relation methods Reset Models A Poisson reset model Reset HMM LDS Summary Code Exercises Distributed Computation Introduction Stochastic Hopfield Networks Learning Sequences A single sequence Multiple sequences Boolean networks Sequence disambiguation Tractable Continuous Latent Variable Models Deterministic latent variables An augmented Hopfield network Neural Models Stochastically spiking neurons Hopfield membrane potential Dynamic synapses Leaky integrate fire models Summary Code Exercises V Approximate Inference Sampling Introduction Univariate sampling Rejection sampling Multivariate sampling Ancestral Sampling Dealing evidence Perfect sampling Markov network Gibbs Sampling Gibbs sampling Markov chain Structured Gibbs sampling Remarks Markov Chain Monte Carlo MCMC Markov chains Metropolis Hastings sampling Auxiliary Variable Methods Hybrid Monte Carlo Swendson Wang Slice sampling Importance Sampling Sequential importance sampling Particle filtering approximate forward pass Summary Code Exercises Deterministic Approximate Inference Introduction The Laplace approximation Properties Kullback Leibler Variational Inference Bounding normalisation constant Bounding marginal likelihood Bounding marginal quantities Gaussian approximations KL divergence Marginal moment matching properties minimising KL p q Variational Bounding Using KL q p Pairwise Markov random field General mean field equations Asynchronous updating guarantees approximation improvement Structured variational approximation Local KL Variational Approximations Local approximation KL variational approximation Mutual Information Maximisation A KL Variational Approach The information maximisation algorithm Linear Gaussian decoder Loopy Belief Propagation Classical BP undirected graph Loopy BP variational procedure Expectation Propagation MAP Markov networks Pairwise Markov networks Attractive binary Markov networks Potts model Further Reading Summary Code Exercises End Matter VI Appendix Background Mathematics Linear Algebra Vector algebra The scalar product projection Lines space Planes hyperplanes Matrices Linear transformations Determinants Matrix inversion Computing matrix inverse Eigenvalues eigenvectors Matrix decompositions Multivariate Calculus Interpreting gradient vector Higher derivatives Matrix calculus Inequalities Convexity Jensen s inequality Optimisation Multivariate Optimisation Gradient descent fixed stepsize Gradient descent line searches Minimising quadratic functions line search Gram Schmidt construction conjugate vectors The conjugate vectors algorithm The conjugate gradients algorithm Newton s method Constrained Optimisation Lagrange Multipliers Lagrange Dual Bibliography Index\n",
            "5 []\n",
            "5 gsl_stats March Modeling Data gsl_stats March gsl_stats March Modeling Data Tools Techniques Scientific Computing Ben Klemens PRINCETON UNIVERSITY PRESS PRINCETON AND OXFORD gsl_stats March Copyright Princeton University Press Published Princeton University Press William Street Princeton New Jersey In United Kingdom Princeton University Press Oxford Street Woodstock Oxfordshire OX20 1TW All Rights Reserved Klemens Ben Modeling data tools techniques scientific computing Ben Klemens p cm Includes bibliographical references index ISBN hardcover alk paper Mathematical statistics Mathematical models I Title QA276 K546 dc22 British Library Cataloging Publication Data available This book composed LATEX The publisher like acknowledge author volume providing camera ready copy book printed Printed acid free paper press princeton edu Printed United States America gsl_stats March We believe deprived books reason Russell Wattenberg founder Book Thing The author pledges donate royalties Book Thing Baltimore non profit gives books schools students libraries readers kinds gsl_stats March gsl_stats March CONTENTS Preface xi Chapter Statistics modern day PART I COMPUTING Chapter C Lines Variables declarations Functions The debugger Compiling running Pointers Arrays pointer tricks Strings Errors Chapter Databases Basic queries Doing queries Joins subqueries On database design Folding queries C code gsl_stats March viii CONTENTS Maddening details Some examples Chapter Matrices models The GSL s matrices vectors apop_data Shunting data Linear algebra Numbers gsl_matrix gsl_ve tor internals Models Chapter Graphics plot Some common settings From arrays plots A sampling special plots Animation On producing good plots Graphs nodes flowcharts Printing LATEX Chapter More coding tools Function pointers Data structures Parameters Syntactic sugar More tools PART II STATISTICS Chapter Distributions description Moments Sample distributions Using sample distributions Non parametric description Chapter Linear projections Principal component analysis OLS friends Discrete variables Multilevel modeling gsl_stats March CONTENTS ix Chapter Hypothesis testing CLT The Central Limit Theorem Meet Gaussian family Testing hypothesis ANOVA Regression Goodness fit Chapter Maximum likelihood estimation Log likelihood friends Description Maximum likelihood estimators Missing data Testing likelihoods Chapter Monte Carlo Random number generation Description Finding statistics distribution Inference Finding statistics parameter Drawing distribution Non parametric testing Appendix A Environments makefiles A Environment variables A Paths A Make Appendix B Text processing B She shall scripts B Some tools scripting B Regular expressions B Adding deleting B More examples Appendix C Glossary Bibliography Index gsl_stats March gsl_stats March PREFACE Mathematics provides framework dealing precisely notions Computation provides framework dealing precisely notions Alan J Perlis Abelson et al p xvi SHOULD YOU USE THIS BOOK This book intended complement standard stats textbook ways First descriptive inferential statistics kept separate beginning sentence chapter I believe fusing number because confusion statistics students Once descriptive modeling given space models necessarily preparation test options blossom There myriad ways convert subjective understanding world mathematical model including simulations models like Bernoulli Poisson distributions tradi tional probability theory ordinary squares knows If options aren t simple models combined form multi level models describe situations arbitrary complexity That basic linear model Bernoulli Poisson models simple situations building blocks let produce descriptive models The approach concludes multilevel models e g Eliason Pawitan Gelman Hill gsl_stats March xii PREFACE Second stats texts aim complete possible completeness thick spine impression value money textbook reference book need guaranteed But s hard learn reference book So I havemade solid effort provide narrative important points statistics directly implies book incomplete relative encyclopdic texts For example moment generating functions interesting narrative tangential story I mention Computation The manner book complements traditional stats textbook acknowledges working data time working computer time The better un derstand computing able data faster able The politics software All software book free software meaning freely downloaded dis tributed This book focuses porta bility replicability need purchase license time switch computers code portable If redistribute functioning program wrote based GSL Apophenia need redistribute compiled final program source code write program If publishing academic work If situation dis tribute output analysis obli gations This book reliant POSIX compliant sys tems systems built ground writing running replicable portable projects This exclude current operating system OS current members Microsoft Win dows family OSes claim POSIX compliance OSes ending X Mac OS X Linux UNIX People like characterize comput ing fast paced changing churn syntactic surface The fundamental concepts conceived mathemati cians eye sim plicity elegance pencil paper math long anybody remember Time spent learning funda mentals pay matter exciting new language everybody happens month I spent life ignor ing fundamentals computing hacking projects package language month C Mathematica Oc tave Perl Python Java Scheme S PLUS Stata R probably I ve forgotten Albee p explains s neces sary long distance way order come short distance correctly distance I ve gone arrive writing book data oriented computing general basic computing language For purpose mod eling data I found C easier pleasant language purpose built alternatives especially I worked I ignore advice books written 1980s apply techniques I learned scripting languages gsl_stats March PREFACE xiii WHAT IS THE LEVEL OF THIS BOOK The short answer intended graduate student independent searcher supplement standard year stats text later study Here ways answer question Ease use versus ease initial use The majority statistics students trying slog department s stats requirement look data set If sole concern ease initial use want stats package textbook focus proficiency immediate intuition Conversely book solving today s problem quickly physically possible getting better understanding data handling com puting statistics Ease long term use follow therefrom Level computing abstraction This book takes fundamentals computing seriously reinventing wheels statistical computing For example Numerical Recipes C Press et al classic text describing algorithms seeking optima efficiently calculating determinants making random draws Normal distribution Being classic packages implement algorithms level book build packages replicate effort Computing experience You taken computer science course experience basics dealing computer writing scripts stats package scripting language like Perl Python Computational detail This book includes working sample programs Code clarifies English text ambiguities details place program execute correctly Also code rewards curious readers explore data find changes pro cedure robust productively break code That means book computing system agnostic So devo tee stats package look book Although I shy away C specific details syntax book focuses conceptual issues common computing environments If look C code finish book better grounding effectively working preferred programming language Linear algebra You reasonably familiar linear algebra ex pression like X foreign There countably infinite number linear algebra tutorials books stats text appendices online book include Statistical topics The book s statistical topics particularly advanced trendy OLS maximum likelihood bootstrapping staples year grad level stats But creatively combining building blocks able model data situations arbitrary complexity 1I learned things excellently written narrative Gonick Smith gsl_stats March gsl_stats March Modeling Data gsl_stats March gsl_stats March STATISTICS IN THE MODERN DAY Retake falling snow drifting flake Shapeless slow unsteady opaque A dull dark white day s pale white And abstract larches neutral light Nabokov lines Statistical analysis goals directly conflict The find pat terns static given infinite number variables observe discover relations patterns human sense The second goal fight apophenia human tendency invent patterns random static Given found pattern handful variables verify product lucky draw overactive imagination Or consider complementary dichotomy objective versus subjective The objective called probability e g given assumptions Central Limit Theorem conclusion true mathematical certainty The subjective called statistics e g claim observed quantity A linear function observed quantity B useful Nature interest This book writing subjective models based human standing world works heavily advised objective formation including mathematical theorems observed data 1Of course human gathered data perfectly objective try best gsl_stats March CHAPTER The typical scheme begins proposing model world estimating parameters model observed data evaluating fit model This scheme includes descriptive step describing pattern inferential step testing indications pattern valid It begins subjective model heavily advised objective data Figure shows model flowchart form First descriptive step data parameters fed function simple correlated b complex set interrelations function spits output Then comes testing step evaluating output based criterion typically matches portion data Our goal find parameters produce output best meets evaluation criterion Parameters Data Data Function Output Evaluation Figure A flowchart distribution fitting linear regression maximum likelihood methods multilevel modeling simulation including agent based modeling data mining non parametric modeling methods Online source diagram models dot The Ordinary Least Squares OLS model popular familiar example pic tured Figure If familiar cover Chapter Let X indicate independent data parameters y dependent data Then function box consists simple equation yout X evaluation step seeks minimize squared error y yout X y X yout y yout Figure The OLS model special case Figure For simulation function box complex flowchart variables combined non linearly parameters feed unpredictable ways The final step evaluate simulation output corresponds real world phenomenon explained The key computational problem statistical modeling find parameters gsl_stats March STATISTICS IN THE MODERN DAY beginning flowchart output best evaluation end That given function evaluation Figure seek routine data produce optimal parameters Figure In OLSmodel simple equation solution problem best X X 1X y But complex models simulations multilevel models strategically try different sets parameters hunt best ones Data Estimation Parameters X y X X 1X y best Figure Top parameters input model Figure output estimation routine Bottom estimation OLS model simple equation And s book develop models parameters tests dis cover verify interesting patterns data But setup incredibly versa tile different function specifications setup takes forms Among minor asides book cover following topics vari ants Figure Probability known distributions model data Projections summarizing dimensional data dimensions Estimating linear models OLS Classical hypothesis testing Central Limit Theorem CLT ferret apophenia Designing multilevel models model s output input parent model Maximum likelihood estimation Hypothesis testing likelihood ratio tests Monte Carlo methods describing parameters Nonparametric modeling comfortably fits parametric form smoothing data distributions Bootstrapping describe parameters test hypotheses gsl_stats March CHAPTER THE SNOWFLAKE PROBLEM OR A BRIEF HISTORY OF STATISTICAL COMPUTING The simplest models list parameters like Binomial n p distribution built n identical draws success probability p Chapter But draws real world rarely identical snowflakes exactly alike It nice outcome variable like annual income determined entirely variable like education know dozen enter picture like age race marital status geographical location et cetera The problem design model accommodates sort complexity manner allows actually compute results Before computers com mon best analysis variance methods ANOVA ascribed variation potential causes Sections The computational milestone circa early 1970s arrived civilian computers power easily invert matrices process necessary linear models The linear models ordinary squares dominant Chapter The secondmilestone circa themid 1990s arrivedwhen desktop computing power sufficient easily gather local information pin global op timum complex function thousands millions evaluations function The functions methods handle general linear models write optimize models millions inter acting agents functions consisting sum thousand sub distributions Chapter The ironic result computational power allows return simple models like Binomial distribution But instead specifying fixed n p entire population observation value n function individual s age race et cetera value p different function age race et cetera Section The models Part II listed order complexity The infinitely quotable Albert Einstein advised simple possible simpler The Central Limit Theorem tells errors Normally dis tributed case dependent variable basically linear log linear function variables If descriptions violence reality data culled OLS method use general techniques persuasive But assumptions apply longer need assume linearity overcome snowflake problem gsl_stats March STATISTICS IN THE MODERN DAY THE PIPELINE A statistical analysis guided series transformations data raw form originally written simple summary question interest The flow statistics textbook tradition picked halfway analysis assumes data set correct form But pipeline goes original messy data set final estimation statistical model It built functions incrementally transform data manner like removing missing data selecting subset data summarizing single statistic like mean variance Thus think book catalog pipe sections filters plus discussion fit elements form stream raw data final publishable output As pipe sections listed ordinary squares maximum likelihood procedures book covers techniques directly transforming data computing statistics welding sections program Structuring programs modular functions stack frames Programming tools like debugger profiler Methods reliability testing functions making robust Databases produce data format need Talking external programs like graphics packages generate visualiza tions data Finding pre existing functions quickly estimate parameters model data Optimization routines work use Monte Carlo methods getting picture model millions random draws To things concrete sample code book available book s Web site linked http press prin eton edu titles html This means learn running modify ing examples cut paste modify sample code analyses running quickly The programs listed given complete discussion pages book read bus beach encouraged read book sitting computer run sample code happens given different settings explore Figure gives typical pipeline raw data final paper It works number different layers abstraction segments involve manipulating individual numbers segments low level numerical manipulation given op gsl_stats March CHAPTER Input Data SQL Database Appendix B C Matrix Ch Output parametersPart II Plots graphs Ch Figure Filtering input data outputs Online source datafiltering dot erate database tables matrices segments matrix operations given run higher level hypothesis tests The lowest level Chapter presents tutorial C programming language self The work lowest level abstraction covering difficult adding columns numbers The chapter discusses C facilitates development use libraries sets functions written past programmers provide tools work higher higher levels abstraction ignore details lower levels For number reasons discussed book relies C program ming language pipe fitting certain section find useful appendices chapter databases comes mind keeping welding pipe section programming language system Dealing large data sets Computers today able crunch numbers hun dred times faster decade ago data sets crunch thousand times larger Geneticists routinely pull genetic markers thousand patients The US Census Bureau s sample covers million people Thus layer abstraction provides specialized tools dealing data sets databases query language organizing data Chapter presents new syntax talking database Structured Query Language SQL You find types data manipulation filtering difficult traditional languages stats packages trivial pleasant SQL 2Why book omit linear algebra tutorial include extensive C tutorial Primarily use linear algebra changed century use C evolved libraries available If writing C code early 1980s standard library writing low level In present day process writing code joining libraries writing scratch I felt existing C tutorials books focused heavily process writing scratch perpetuating myth C appropriate low level bit shifting The discussion C introduces tools like package managers debugger utility early possible start calling existing libraries quickly easily possible gsl_stats March STATISTICS IN THE MODERN DAY As Huber p explains Large real life problems require com bination database management data analysis Neither database manage ment systems traditional statistical packages task The solution build pipeline Figure includes database management statistical analysis sections Much graceful data handling knowing pipeline place filtering operation The database appropriate place filter bad data join data multiple sources aggregate data group means sums C matrices appropriate filtering operations like earlier took data applied function like X X 1X y measured yout y Because data probably come pre loaded database Appendix B discusses text manipulation techniques database expects data set use commas data separated erratic tabs able quickly surmount problem analysis Computation The GNU Scientific Library works numerical computation layer abstraction It includes tools procedures com monly statistics linear algebra operations looking value F t distributions finding maxima likelihood functions Chapter presents basics data oriented use GSL The Apophenia library primarily covered Chapter builds layers abstraction provide functions level data analysis model fitting hypothesis testing Pretty pictures Good pictures essential good research They reveal patterns data look like mere static data pre sented table numbers effective means communicating peers persuading grantmakers Consistent rest book Chapter cover use Gnuplot Graphviz packages freely available computer right Both entirely automatable graph plot like C programs autogenerate manipulate amusing ways send program colleague Madras problem reproducing modifying plots Once basics animation real time graphics simulations easy 3Following suggestion Thomson I chosen gender representative agents book flipping coin gsl_stats March CHAPTER WHY C You surprised book modern statistical computing based language composed Why use C instead special ized language package like SAS Stata SPSS S Plus SAGE SIENA SU DAAN SYSTAT SST SHAZAM J K GAUSS GAMS GLIM GENSTAT GRETL EViews Egret EQS PcGive MatLab Minitab Mupad Maple Mplus Maxima MLn Mathematica WinBUGS TSP HLM R RATS LISREL Lisp Stat LIMDEP BMDP Octave Orange OxMetrics Weka Yorick This book advocate statistical computing general computing lan guage I time better idea modern numerical analysis best old language One effects programming language stable long mythology builds Sometimes mythology outdated false I seen professional computer programmers writers claim simple structures like linked lists need written scratch C Section proof takes times long write program C recently written language like R people C write device drivers low level work high level work This section partly intended dispel myths Is C hard language C hard language With basic 80s era compiler easily hard catch mis takes But programmers decades identify pitfalls build tools catch Modern compilers warn issues debuggers let interact program runs catch quirks C s reputation hard language means tools evolved easy language Computational speed Using stats package sure beats inverting matri ces hand computation goes stats packages relatively slow slowness useful statistical methods infeasible R Apophenia use C code Fisher exact test makes good basis timing test Listings programs C R respectively run Fisher exact test million times data set You C program bit verbose steps taken lines C code lines R code identical lines 4Out courtesy citations omitted This section makes frequent comparisons R partly salient common stats package partly I know having daily basis years 5That download source code R s fisher test function find set procedures written C Save minor modifications code underlying apop_test_fisher_exa t function line line identical gsl_stats March STATISTICS IN THE MODERN DAY include apop h int main int test_ct 5e6 double data apop_data testdata apop_line_to_data data test_ct apop_test_fisher_exact testdata Listing C code time Fisher exact test It runs test million times Online source timefisher test_ct 5e6 data c testdata matrix data nrow test_ct fisher test testdata Listing R code test Listing Online source Rtimefisher longer C C program preliminary code R script On laptop Listing runs minutes Listing work minutes thirty times long So investment little verbosity extra stars semicolons returns thirty fold speed gain Nor isolated test case I t count times people told stories analysis simulation took days weeks stats package ran minutes rewrote C Even moderately sized data sets real computing speed opens new possibili ties drop typically false assumptions needed closed form solutions favor maximum likelihood Monte Carlo methods The Monte Carlo examples Section produced billion draws t distributions stats package t produce thousand draws second t work unfeasibly slow 6These timings actually based modified version fisher test omits additional R calculations If Fisher test loop editing R s code R C speed ratio 7If produce random draws t distributions batch draws rt 5e6 df R takes mere times long comparable C code But need produce individually 5e6 draw rt df R takes times long comparable C code On laptop R gsl_stats March CHAPTER Simplicity C super simple language It is syntax special tricks poly morphic operators abstract classes virtual inheritance lexical scoping lambda expressions arcana meaning learn Those features certainly helpful place C proven sufficient writing impressive programs like Mac Linux operating systems stats packages listed Simplicity affords stability C oldest programming languages common use today8 stability brings benefits First reason ably assured able verify modify work years Since C written countless stats packages come gone changes syntax effectively new languages Either way try follow trends hard drives dozens scripts t run anymore Meanwhile correctly written C programs 1970s compile run new PCs Second people decades write good libraries libraries build libraries It syntax language allows easily handle complex structures tasks vocabulary case C continually expanded new function libraries With statistics library hand C code Listing R code Listing work high level abstraction Alternatively need precision use C s low level bit twiddling shunt individual elements data There embarrassing presenter answers question anomaly data analysis Stata didn t function correct Yes I heard real live presentation real live researcher But C s higher level lower level libraries equally accessible work level laziness precision called given situation Interacting C scripts Many stats packages listed provide pleas ing interface let run regressions mouse clicks Such systems certainly useful certain settings ask ing quick questions new data set But un replicable analysis based clicking arbitrary sequence screen buttons useful analysis In context building repeatable script takes data far possible pipeline raw format final published output developing batch mode produced draws rate sec C produced draws rate sec 8However oldest honor goes FORTRAN This noteworthy claim C common use today merely inertia path dependency et cetera But C displaced number languages ALGOL PL I inertia making clear improvements incumbents gsl_stats March STATISTICS IN THE MODERN DAY 11s ript interpreter developing program compiler equivalent especially compilation modern computer takes order seconds With debugger distance smaller jump C code change intermediate values interact program way stats package Graphical interfaces stats packages C debuggers tend similar design But C ugly C means best language possible purposes Dif ferent systems specialized syntaxes communicating programs handling text building Web pages producing certain graphics But data analysis C effective It syntactic flaws append semicolons line frustrated while3 But Perl requires semicolons line Perl Python Ruby Type declarations detail remember alternatives warts Perl basically requires declare type variable use R guess type meant use guess wrong thinking element list like integer C s printf statements look terribly confusing authors Ruby Python striving programmer friendly syntax possible chose use C s printf syntax alternatives easier eyes harder use In short C measured initial ease use But logic mess stars braces course decades C proven suited designing pipelines data analysis linking libraries disparate sources describing detailed computation intensive models TYPOGRAPHY Here notes typographic conventions book Seeing forest trees On hand good textbook narrative plots definite course field On hand fields countless interesting useful digres sions paths Sections marked cover details skipped reading They necessarily advanced sense difficult unmarked text distractions main narrative gsl_stats March CHAPTER Q1 Questions exercises marked like paragraph The exercises thought experiments It happens think understand sit actually host hairy details turn Especially outset exercises relatively simple tasks let face hairy details real world complications enter situation Exercises later chapters involved require writing modifying longer segments code Notation X boldface capital letters matrices With exceptions data matrices book organized rows single observation column variable x lowercase boldface indicates vector Vectors generally column num bers transpose x row y typically vector dependent variables exception need generic data vectors case x y x A lowercase variable bold scalar e single real number X transpose matrix X Some authors notate XT X data matrixXwith mean column subtracted meaning column X mean zero If X column ones regression techniques constant column left unmodified X n number observations data set discussion typically number rows X When ambiguity n subscripted I The identity matrix A square matrix ones diagonal zeros Greek letters indicate parameters estimated boldface vector parameters The common letter slip standard deviation mean The variance carat parameter indicates empirical estimate parameter derived data Typically read e g sigma hat beta hat o N Read epsilon distributed Normal distribution parameters gsl_stats March STATISTICS IN THE MODERN DAY P A probability density function LL The log likelihood function ln P S The Score vector derivatives LL I The information matrix matrix second derivatives LL E The expected value aka mean input P x The probability x given true P x x The probability density function holding x fixed Mathematically simply P x given situation thought function Ex f x Read expectation x given function form like x f x P x dx Because integral x Ex f x function x teletype typefa e indicates text typed directly text file understood valid she shall script C commands SQL queries et cetera sample_file Slanted teletype text indicates placeholder text insert variable text read literally You read code let sample_file file hard drive Then type sample_file command prompt b Read equivalent b defined b b Read proportional b 3e6 Engineers write scientific notation called exponential E notation 3e6 Many computing languages including C SQL Gnuplot recognize E notated numbers 9Others use different notation For example Efron Hinkley p The log likelihood function l x log density function thought function See page philosoph ical considerations underlying choice notation gsl_stats March CHAPTER Every section ends summary main points set like para graph There said strategy flipping ahead summary end section reading section The summary introduction This book discuss methods estimating testing param eters model data It cover means writing computer including tech niques manage data plot data sets manipulate matrices estimate statistical models test claims parameters Credits Thanks following people added higher quality richness book Anjeanette Agro graphic design suggestions Amber Baum extensive testing critique The Brookings Institution s Center Social Economic Dynamics includ ing Rob Axtell Josh Epstein Carol Graham Emily Groves Ross Hammond Jon Parker Matthew Raifman Peyton Young Dorothy Gambrel author Cat Girl Lonely Planet data Rob Goodspeed National Center Smart Growth Research Educa tion University Maryland Washington Metro data Derrick Higgins comments critique Perl commands page Lucy Day Hobor Vickie Kearn editorial assistance making working Princeton University Press pleasant experience Guy Klemens wide range support fronts Anne Laumann tattoo data set Laumann Derick Abigail Rudman deft librarianship gsl_stats March I COMPUTING gsl_stats March gsl_stats March C This chapter introduces C general concepts good program ming script writers overlook The function based approach stacks frames debugging test functions overall good style immediately applica ble virtually programming language use today Thus chapter C help better programmer programming language As syntax C chapter cover subset C keywords book use Some keywords basi cally archaic designed days compilers needed help user optimize code Other elements like bit shifting operators useful writing operating system hardware device driver With parts C directly manipulate hexadecimal memory addresses omitted find C simple language suited simulations handling large data sets An outline This chapter divides main parts Sections start small covering syntax individual lines code assign ments arithmetic declare variables Sections introduce functions describing C built idea modular functions independently built evaluated Sections cover pointers C specificmeans handling computer memory complements C s means handling functions large data structures The remainder chapter offers tips writing bug free code 1For comparison C keywords writing Java gsl_stats March CHAPTER Tools You need number tools work including C compiler debugger facility libraries functions Some systems pre installed especially benevolent system adminis trator taking care things If fortunate need gather tools The online appendix book site linked http press prin eton edu titles html guide pro cess putting complete C development environment tools gathering requisite libraries Q2 Check C environment compiling running Hello world clas sic program adapted Kernighan Ritchie Download sample code book link http press prin eton edu titles html Decompress zip file directory created com pile program command g hello_world If IDE manual compilation instructions If went program directory named hello_world From command line ex ecute hello_world You want try makefile find code directory See instructions head file If need troubleshooting help online appendix ask local computing guru copy paste error messages favorite search engine LINES The story begins smallest level single line code Most work level familiar written programs language including instructions like assignments basic arith metic conditions loops comments For common programming elements learning C simply question details syntax Also C typed language meaning need specify variable function integer real vector Thus lines simple type declarations syntax covered section 2A pedantic note standards book makes effort comply ISO C99 standard IEEE POSIX standard The C99 standard includes features appear great majority C textbooks like designated initializers compiler support features C99 new compiler s long The POSIX standard defines features common modern operating system notable pipe Appendix B details The focus g I expect readers The command line switches g command obviously specific compiler users compilers need check compiler manual corresponding switches However C code compile C99 POSIX compliant compiler Finally g switch relevant footnote std gnu99 basically puts compiler C99 POSIX mode gsl_stats March C ASSIGNMENT Most work simple assignments For example ratio b find value divided b value ratio The indicates assignment assertion equality paper computer scientists write ratio b nicely gets image ratio taking value b There semicolon end line need semicolon end exceptions You use usual operations As basic algebraic custom evaluated seven TWO TYPES OF DIVISION There ways answer question What divided The common answer remainder Many programming lan guages including C second approach Dividing integer integer gives answer fractional thrown modulo operator finds remainder So Is k number If k zero Splitting process parts provides touch additional precision because machine write integers precisely approximate real numbers like Thus machine s evaluation slightly different But special handling division integers guaranteed integers b b zero b b b exactly But cases want The solution mean integer mean real number happens inte ger value adding decimal point desired Get habit adding decimal points integer division famous source hard debug errors Page covers situation slightly detail meantime convenient parts language 3The number because compiler complaints like line syntax error missing semicolon line 4In practice check evenness GSL_IS_EVEN GSL_IS_ODD lude gsl gsl_math h GSL_IS_EVEN k do_something gsl_stats March CHAPTER INCREMENTING It incredibly common operation forma b common C special syntax b This slightly readable involves redundancy All arith metic operators form following lines equiv alent expressions b equivalent b b equivalent b b equivalent b b equivalent b The common operation incrementing decrementing C offers following syntax typing equivalent equivalent CONDITIONS C need FALSE TRUE keywords Boolean operations expression zero false true The standard operations comparison Boolean algebra appear somewhat familiar form b greater b b b b greater equal b b equal b b equals b b equal b b b b b All evaluate zero depending expression parens true false 5There pre increment form Pre post incrementing differ situations bad style avoided Leave operations separate line stick whichever form looks nicer gsl_stats March C The comparison equality involves equals signs row One equals sign b assign value b variable intended Your compiler warn cases probably wrong heed warnings The operators convenient feature sufficient determine entire expression true won t bother b For example code fragment sqrt square root negative number If zero evaluation expression half true evaluation stops If expression sufficient evaluate expression second evaluated determine Why parentheses First parentheses indicate order operations pencil paper math Since comparisons evaluate zero b d b d sense C You probably meant order operations table memorized won t sure C thinks mean b d Second primary use conditionals flow control causing pro gram repeat lines condition true execute lines condition false In cases need parentheses conditions forget confusing compiler error IF ELSE STATEMENTS Here fragment code compile showing syntax conditional evaluations b sqrt b If positive b given value s square root zero negative b given value zero The condition evaluated parentheses following statement curly braces evaluated 6The order operations table available online encouraged look If tryman operator command prompt Most people remember basics like multiplication division come addition subtraction rely order operations table ordering merely sending future readers check table gsl_stats March CHAPTER condition true evaluated condition false You exclude curly braces lines surround exactly line point confuse because regret leaving You exclude lines don t need common likely because trouble The statement line following smaller parts larger ex pression semicolon clause hap pens true similarly clause If semicolon statement statement execute null statement Your compiler warn Q2 Modify hello_world print greeting expression true print different message choos ing false Did C think meant evaluates evaluates LOOPS Listing shows types loop slightly redundant include stdio h int main int printf Hello n printf Hi n printf Hello n return Listing C provides types loop loop loop loop Online source flow gsl_stats March C The simplest loop The interpretation straightforward expression parentheses line true mustn t forget parentheses execute instructions brackets lines Loops based counter common syntax loop The loop lines exactly equivalent loop lines gathers instructions incrementing counter single line You compare loop subelements parentheses evaluated evaluated loop runs second tested beginning iteration loop evaluated end loop After section arrays limit form recognize mean step array There way text editor produce form keystrokes Finally want guarantee loop run use loop semicolon end line conclude thought The loop Listing equivalent loops But want iteratively evaluate function converges Naturally want run function The form like error evaluate_function error 1e Example birthday paradox The birthday paradox staple undergraduate statistics classes The professor writes birth date student class finds chance given pair students birthday odds good match class overall Listing shows code find likelihood student shares person s birthday likelihood students share birthday Most world s programs need square root functions likepow sqrt included standard C library They separate math library refer command line Thus compile program 7It mystifies TV talk hosts according Paulos p gsl_stats March CHAPTER include math h include stdio h int main double no_match double matches_me int ct printf People t Matches t Any match n ct ct ct matches_me pow ct no_match ct printf t 3f t t 3f n ct matches_me no_match return Listing Print odds students share birthday students room share birthday Online source birthday gcc birthday c lm o birthday lm indicates math library o indicates output program named birthday default More linking libraries follow Lines introductory material discussed including preface lude ing external files list dramatis person variables named no_mat h mat hes_me t Line prints header line labeling columns numbers loop producing easy read know t means print tab n means newline Line tells counter t start count reaches As math easier calculate complement odds body shares birthday The odds person share person s birthday odds people share person s birthday et cetera Thus odds t additional peo ple birthday person ct You calculation line As odds second person share person s birthday The odds additional person shares birthday given share birthday odds 8We assume away leap years fact odds born given day exactly children born summer gsl_stats March C share birthday This expression best produced incrementally In introductory material no_ mat h initialized line element sequence headed Expression gets multiplied no_mat h step loop Line prints results The input printf function discussed detail inputs indicate printed counter mat hes_me no_mat h Q2 Modify loop verify program prints correct values class student COMMENTS Put long block comments head file head function describe file function complete sentences Describe function expects come function The common wisdom indicates comments focus code self explanatory clearly written code The primary audience comment months When shopping black boxes plug project auditing data referee finally got paper note self head function pay immense dividends Long comments begin slash star continue long want end star slash The stars slashes useful commenting code If like temporarily remove lines program happen don t want delete entirely simply code compiler think comment ignore However slight problem approach comment commented You sequence like code 9The sample code book attempts example good code respects documentation real world code book documentation gsl_stats March CHAPTER Line A Line B Line C We hoped lines commented compiler ignore sees That means Line A Line B ignored Line C read code malformed code You need watch commenting large blocks code But small blocks syntax commenting individual lines code deserve note this_is_code Everything line slashes ignored Later meet preprocessor modifies program s text compilation It provides solution commenting large blocks comments embedded The compiler following code preprocessor skip statement evaluates zero endif This function void do_nothing endif PRINTING C prints screen string text memory file syntax The formatting works like Mad Libs party game Price Stern First format specifier showing output blanks filled 10Q If C didn t quirk allowed comments inside comments different quirk watch instead gsl_stats March C My noun adjective Then user provides specific instance noun adjective filled case left exercise reader Since C programming language party game syntax little terse Instead number line string intprintf uses s number line Here complete example include stdio h int main int position char Steven printf s number line n position return The printf function actually defined default definition stan dard input output header luded line Lines variable declarations defining types variables lines foreshadow section Finally line actual print statement insert Steven placeholder s insert second placeholder It printSteven number line plus invisible newline Here odd characters need work insert integer g insert real number general format s insert string text plain percent sign n begin new line t tab quote won t end text string newline continue text string line gsl_stats March CHAPTER There format specifiers great deal control want printing tables example refer number detailed references need man printf command line At point want flip book find examples ofprintf verify print promise Assignment uses single equals sign assignee value The usual arithmetic works Conditions b b b equals signs control flow Conditional flow uses form ondition do_if_true do_if_false The basic loop loop this_is_true do_ When iterating array loop makes iteration clearer j j limit j printf pro essingitem n j Write comments future self VARIABLES AND THEIR DECLARATIONS Having covered verbs line code execute nouns variables You use x z paper declaring let x R2 z C You leave reader guess mean x use readers misunderstand referee wonder come declare x C strict referee requires declare type variable The declaration consists listing type variable variable e g int a_variable counter double stuff This snippet declared variables lines We initialize ounter zero declared gsl_stats March C The variables a_variable unknown values right As sume contained declared uninitialized value Since step variable typically assignment declare initialize line burden declaring types basically means putting type head line variable lines sample code page Here comprehensive list useful basic types C int integer 3double real number 3e8 har character b C An int count billion simulation involves billion agents uses counting trillions case use long int type There ways extend shrink size numbers basically worth caring A double counts 1e308 significantly common estimates number atoms universe circa 1e80 long double type case need precision size Section offers detailed notes numbers represented Finally notice variable names words letters Using English variable names number best thing code readable Imagine life spent flipping journal articles trying remember M m stood Why impose 11I reluctant mention later distinction global static local variables Global static variables automatically initialized zero NULL local variables But suffer fewer painful debugging sessions ignore fact habit explicitly initializing needs initialization 12The int type bit systems bits Though norm doubt change future safe bet write code assumption int counts 13The double short double precision floating point number float half precision range double Why floating point The computer represents real form comparable scientific notation n 10k n represents number decimal point fixed location k represents location decimal point Multiplying doesn t change number n causes decimal point float different position The float type especially worth bothering GSL s matrices vectors default holding doubles C s floating point functions internally operate doubles For example atof function ASCII text floating point number actually returns double 14The exception indices counters loops j k gsl_stats March CHAPTER Arrays Much art describing real world consists building aggregates basic types larger structures The simplest aggregate array simply numbered list items type To declare list integers use int a_list Then refer items list use square brack ets For example assign value seven element array use a_list Why element list Because index offset element The element zero items away a_list a_list second element The reasoning system evident section pointers D arrays simply require indices int a_2d_list But details implementation D arrays difficult use practice Chapter introduce gsl_matrix provides advantages raw D array Just initialize scalar value declaration array e g double data You bother counting elements array C smart Q2 Write program create array integers fill array squares the_array hold Then print message like squared element array Use Hello World program template start Q2 The element Fibonacci sequence defined second defined element n defined sum elements n n Thus element fourth fifth et cetera The ratio nth element n st element converges value known golden ratio Demonstrate convergence producing table elements sequence ratio nth n st element n gsl_stats March C DECLARING TYPES You define types For example lines declare new type triplet declare triplets tri1 tri2 typedef double triplet triplet tri1 tri2 This primarily useful designing complex data types collections subelements conjunction stru t keyword For example typedef struct double real double imaginary complex complex b You variables type omplex use real b I am ginary refer appropriate constituents complex numbers Listing repeats birthday example stores class size s data astru t Lines define structure hold variable indicating probabil ity somebody matching person s birthday variable giving probability people share birthday Those lines defined type line declares variable days type Since number brackets array ofbday_stru ts In line none_mat h element days given value Lines assign values elements days days Having calculated values stored organized manner easy lines print values Initializing As array initialize elements struct value declaration line The option comparable array syntax remember order stru t s elements assignments order For example complex complex pioverfour gsl_stats March CHAPTER include math h include stdio h typedef struct double one_match double none_match bday_struct int main int ct upto bday_struct days upto days none_match ct ct upto ct days ct one_match pow ct days ct none_match days ct none_match ct printf People t Matches t Any match n ct ct upto ct printf t 3f t t 3f n ct days ct one_match days ct none_match return Listing The birthday example Listing rewritten stru t hold day s data Online source bdaystru t initialize 0i pioverfour 1i This probably best way initialize stru t elements known order The option use designated initializers best defined exam ple The initializations equivalent complex real complex pioverfour imaginary real In case imaginary given initialized zero In second case elements order problem Designated initializers prove invaluable dealing structures like apop_ model large number elements particular order Two final notes designated initializers They arrays interspersed unlabeled elements The line int isprime gsl_stats March C initializes array zero length determined ini tialized element setting elements index prime number The ones label slot index follow sequentially index given Structs syntactically simple little good programming goes designing structs sense good reflection reality This book filled including purpose built structures like bday_stru t structures defined libraries like GSL gsl_matrix mentioned TYPE CASTING There minor complications assigning value type variable different type When assign adouble value integer int decimal point dropped Also range floats doubles ints necessarily match unpredictable results large numbers decimal point If confident want assign variable type variable putting type cast parentheses variable For example ratio double int ratio cast integer If want accept truncation assign floating point real integer int n explicitly tell compiler meant making cast e g n int ratio Type casting solves division integers problem head chap ter If num den ints ratio double num den division real integer produce real number expected There ways getting effect num int plus double double Then num den division real integer works expected don t forget parens And numbers constant add decimal point anint floating point real number Finally note casting double int numbers truncated rounded As lead discussion functions function uses type casting correctly round numbers 15In real world use rint math h round integer rounded_val rint unrounded_number gsl_stats March CHAPTER int round double unrounded Input real number output number rounded nearest integer unrounded return int unrounded return int unrounded All variables declared use Until variable given value know value You assign initial value variable declaration line int Arrays simply declared including size brackets variable int array Refer elements offset element array array You declare new types including structures amalgamate simpler types typedef stru t double length waist intleg_ t pants After declaring variable structure pants utoffs refer structure elements dot utoffs leg_ t An integer divided integer integer By putting decimal number real number divi sion works expected FUNCTIONS The instruction inverse matrix words long refers sequence steps typically require pages fully describe Like fields mathematics progresses development new vocab ulary like phrase inverse We comprehensibly express complex statement like variance X X didn t need write exactly squaring transposition X inverse Similarly process writing code describing pro cedures involved building specialized vocabulary describing gsl_stats March C procedures trivial Adding new nouns vocabulary simple task discussed basic nouns stru ts aggregate larger concepts This section covers functions single verbs encapsulate larger pro cedure include math h include stdio h typedef struct double one_match double none_match bday_struct int upto void calculate_days bday_struct days void print_days bday_struct days int main bday_struct days upto calculate_days days print_days days return void calculate_days bday_struct days int ct days none_match ct ct upto ct days ct one_match pow ct days ct none_match days ct none_match ct void print_days bday_struct days int ct printf People t Matches t Any match n ct ct upto ct printf t 3f t t 3f n ct days ct one_match days ct none_match Listing The birthday example broken logical functions Online source bdayfns The second birthday example Listing hard read mess mul tiple loops Listing presents program function math print output The main function lines describes procedure great clarity declare array bday_stru ts calculate val ues days print values exit The functions main refers lines short easier read long string gsl_stats March CHAPTER code Listing Simply functions provide structure relatively unstructured mess Structure takes space listing lines code unstructured version But consider format book reading right uses stylistic features paragraphs chapter headings indenta tion space Brevity good thing means typically worth effort minimize redundancy search simple brief algorithms But brevity come cost clarity By eliminat ing intermediate variables subfunctions reduce entire program single line code liner virtually impossible debug modify simply understand No trees killed add lines white space function headers screen code additional structure save time dealing code later Functional form Have look function headers line func tion lines In parens inputs func tion aka arguments look like familiar declarations The main function takes arguments functions arguments case argument declarations comma separated list Or consider function declaration round function int round double unrounded If ignore argument list parens int round looks like declaration It indicates function return integer value need integer For example assign function output variable int round Declaring function You declare existence function separately function lines Listing Themain function idea expect comes func tions lines functions appear later You compiler gets immense mileage declaration functions compile main knowing functions return leaving inner workings black box The void type If function returns declare type void Such func tions useful effects changing values inputs like al ulate_days printing data screen external gsl_stats March C file like print_days You functions inputs following valid declarations functions void do_something double double do_something_else void double do_something_else The equivalent t forget parentheses entirely compiler think declaring variable instead function Q2 Write function header void print_array int in_array int array_size takes integer array size array prints array screen Modify square printing program earlier use function output How write program Given blank screen program write begin Write outline based function headers For example birthday example begin writing main func tion describes broad outline calculating probabilities print ing screen In writing outline need write inputs outputs intent number functions Then begin filling function When writing function s body rest program mind focus making sure black box working exactly produce right output When functions correctly job main outline fully fleshed working program You want black boxes entirely predictable error free best way small autonomous Flip book look structure longer sample programs You notice functions run lines especially discounting introductory material declaring variables checking inputs FRAMES The manner computer evaluates functions abides principle encapsulating functions focusing context func tion time When function called computer creates frame function Into frame placed variables declared file function defined including files luded copies variables passed arguments The function run variables frame blithely ignorant rest program It math making note return value gsl_stats March CHAPTER calculates destroys entirely erasing variables created frame copies variables frame Variables passed argument frame global variables come unscathed return value sent function called frame existence include stdio h printf double globe global variable double factorial int a_c a_c globe a_c a_c return globe int main void int printf factorial f n factorial printf n printf globe f n globe return Listing A program calculate factorials Online source allbyval One way think terms stack frames The base stack function named main For example program Listing computer ignores function fa torial instead starting work line finds main function It creates main frame starts working reading declaration creating variable main frame The global variable declared line main frame Then line told print value fa torial means evaluate expression This function commands program halt start working evaluating func tion fa torial So system freezes main frame generates frame thefa torial function jumps line Think new frame leaving topmost frame visible active The value copied a_ global variable globe frame function math returns calculated value globe 16Why globe double factorial integer Because double uses exponential notation necessary range larger int You try long int replacing printf s int placeholder long int placeholder li fails gsl_stats March C Having returned value fa torial frame contents discarded Since copy value sent frame value10 function returns copy a_ decremented zero The main frame stack pick left printing value screen calls printf function create frames turn Finally main function finishes work frame destroyed leaving stack finished program Call value A common error forget global variables func tion frames copies variables function s argu ment list frame When fa torial function called system puts copy a_ function modifies copy a_ Meanwhile globe copy real thing changed inside function changes globally This output got ran program showed On hand fa torial function mangle a_ affecting original hand want functions change inputs This global variables tempting resist Section better alternative explain bday_stru t examples worked Static variables There exception rule local variables destroyed frame define stati variable When function s frame destroyed program makes note value static variable function called static variable start value This provides continuity function function knows variable Static variable declarations look like declarations wordstati You initialization declaration line taken consideration time function called Here sample function enter data points array It assumes calling function knows length survey_data bounds checking ac cordingly void add_a_point double number double survey_data static int count_so_far survey_data count_so_far number count_so_far gsl_stats March CHAPTER The time function called ount_so_far initialized zero number passed survey_data ount_so_far cremented The second time function called program remem ber ount_so_far second value survey_ data want The main function All programs function namedmain program begin executing base stack frames The consistency checks operating system called program expect main declared forms int main void int main int argc char argv include stdlib h include stdio h int main int argc char argv argc printf Give command run n return int return_value system argv printf The program returned n return_value return return_value Listing A she shall program primarily intended running programs rudimentary Online source simpleshell The second form discussed page Listing provides quick example use inputs main It uses C s system function program The usage program like simpleshell ls a_directory Conceptually little difference calling function wrote calling main function foreign program In case system func tion ls effectively putting ls program s main function current stack More generally think computer s entire func tioning boot shutdown evaluation set stacks frames At boot system starts C program named init program child init child child init child child child et cetera gsl_stats March C There fork function generates second stack runs concurrently parent system runs programs The system function pass return value subprogram s main The general custom main returns went positive integer indicates type error Because people concerned return value main current C standard assumes main returns zero indication given programs book away having return statement main function SCOPE When function running variables frame visible variables rest program dormant inaccessible This good thing don t want bear mind current state variables program GNU Scientific Library standard library knows Block scope You declare variables inside loop int works expect declaring int set iteration loop int max works butg complain specify std std gnu99 flags compiler A variable declared inside bracketed block header pair curly braces like loop destroyed end bracketed code This known block scope Function level scope thought special case block scope Block scope occasionally convenient especially int form bear mind won t able refer block internal variable loop ends A variable s scope set func tions variable A variable declared inside function visible inside function If variable declared file variable global file function file variable If declared header file including important caveat page function file ludes header variable The strategy deciding scope variable small possible If function uses variable means declare variable inside function possibly stati variable If variable functions declare variable main function pass argument functions use If variable single file infrequently changed let globally available file putting file outside function bodies 17In bash she shall default POSIX systems return value program run stored e ho print value gsl_stats March CHAPTER Finally variable parts program consisting multiple files declare header file globally available file ludes header file page There temptation declare variable global worry scope issues This makes maintaining writing code difficult sure tweak black box named fun tion_a won t change workings inside black box named fun tion_b Next month want use fun tion_a new program written verify rest program affects question cutting pasting black box file involved analysis original program Good coding form involves breaking problems functions writing function independent entity The header function form fun tion_typefun tion_name p1_type p1_name p2_type p2_name The computer evaluates function independent entity It maintains stack frames activity current frame When program starts build frame function named main complete program function Global variables passed new frame copies pa rameters passed If variable frame scope accessed 18This appropriate time answer common intro C question What difference C C There confusion compatible syntax similar explaining C double plus language s author references Newspeak language George Orwell s Orwell Stroustrup p The key difference C adds second scope paradigm C s file function based scope object oriented scope In system functions bound objects object effectively stru t holding variables functions Variables private object scope functions bound object public scope object scope In C think file object variables declared inside file private declared header file public Only functions declaration header file called outside file But real difference C C philosophy C intended allow mixing styles programming object oriented coding C includes number features type scope called namespaces templates tools representing abstract structures large standard library templates Thus C represents philosophy keeping language simple unchanging possible means passing useful additions C represents inclusive philosophy choosing additional features conveniences parsimony gsl_stats March C THE DEBUGGER The debugger somewhat mis named A better interrogator let us interact ask questions program look line executed pause check value variables jump ahead program insert extra command The main use powers find fix bugs actively debugging want run program inside debugger The joy segfaults There ways program break For example attempt calculate computer halt Or declared array int data attempt read data This location memory ints distance past end space allocated array One possibility data happens fall space interpreted integer computer processes junk location wrong Or data point area protected memory memory operating system dissertation In case referring data halt program greatest haste destroys valuable This segmentation fault segfault short attempted refer memory outside segment allocated program Below section pointers en counter null pointer definition points Mistakenly trying read data null pointer pointing halts program complaint attempting dereference null pointer A segfault far clearest way computer tell mis coded need fire debugger It like refusing compile refer undeclared variable If declared eipts anddata setting iepts data value probably error A language saves trouble making declarations refuses segfault produce new variable expand array insert errors output catch 19In fact find worst thing happen error like read data program segfault continue bad data break lines later This rare event evident need use memory debugger find error page gsl_stats March CHAPTER The debugging process To debug program run_me debugger typegdb run_me command line You given gdb prompt You need tell compiler include names variables functions compiled file adding g flag compiler command line For example instead g hello use g g hello If debugger com plains t find debugging symbols means forgot g switch Because g slow program makes debugging possible use time compile If know program segfault halt start gdb run program typing run gdb s prompt wait break When returned gdb s prompt interrogate program The thing want know program You ba ktra e command abbreviate bt orwhere It stack function calls pending program stopped The frame main program started Ifmain called function frame et cetera Often program break internals piece code write mallopt Ignore You find bug inmallopt Find topmost frame code wrote At point best thing look listing code window look line debugger pointed Often simply knowing line failed error painfully obvious If error evident debugger look vari ables You need aware frame working know set variables disposal You default frame stack change frame number command frame f short You traverse stack commands whereup goes parent function goes child function Once frame want information variables You list local variables info lo als information arguments function info args argument information frame description Or print variable think frame print var_name briefly p var_name You 20Asking favorite search engine gdb gui turn number graphical shells built gdb Some stand programs like ddd integrated IDEs They discussed work exactly like gdb involve mouse Also GDB offers conveniences described See Stallman et al story gsl_stats March C print value expression scope p sqrt var p apop_show_ matrix m display square root var matrix m provided variables functions available scope working Or p stddev sqrt var set variable stddev given value print result Generally execute line C code makes sense given context print command GDB special syntax viewing elements array If like elements array items use p items Breaking stepping If program things wrong kind segfault need find places halt program Do break command For program file code simply line number break stop program line evaluated For programs based files need specify file breakfile2 Or specify function debugger stop line function s header E g break al ulate_days You want program break certain conditions iterator reaches I e break ounter All breakpoints given number list info break You delete break point number command del Once set breakpoints run r run program reaches break point apply interrogation techniques You want carefully step s step line evaluated mean backing current function going subfunction nwill step function involve backtracking run stopping subframes created e subfunctions called u going line function debugger run subfunctions loops forward progress current function continue break point end program 21You set watchpoints tell gdb watch variable halt variable changes e g wat hmyvar Watchpoints commonly breakpoints suffer scope issues 22A mnemonic device remembering s slowest means stepping n slightly faster u faster fastest In order spell snu English word implications stepping slowly gsl_stats March CHAPTER jump lineno jump given line number repeat line variables tweaked skip lines Odd things happen jump frame working use jump single function return exit given frame resume parent frame You return value like return var Just hitting enter repeat command won t hitting n step lines Q2 Break bdayfns Listing debug Modify line days days Recompile Be sure include g flag Run program observe output Start debugger If program segfaulted type run wait failure insert breakpoint break al ulate_days run Check backtrace stack What evidence find things right A note debugging strategy Especially numeric programs strategy debugging find point chain logic things look askew Below code include assertions check things gone astray debugger s break inspect system provides means searching earliest misstep But intermediate steps inspected debugging difficult Say writing roots quadratic equation x b b2 4ac 2a erroneously code root firstroot b sqrt b b c wrong There basically intermediate steps b system spits bad value firstroot Now instead wrote firstroot b firstroot sqrt b b c firstroot firstroot wrong gsl_stats March C If know output wrong interrogate sequence clues error Say As step find value offirstroot change line runs From error obvious line firstroot firstroot firstroot Such chain logic impossible line version routine However typical reader second version unattractively verbose A draft code err inelegant verbosity easy de bugability You incrementally tighten code earns trust repeatedly producing correct results Q2 A triangular number number like et cetera Fermat s polygonal number theorem states natural number expressed sum trian gular numbers For example Demonstrate program finds triangular numbers number Write function int triangular int takes index returns ith triangular number E g triangular return Write main test Use function write function int find_next_ triangular int returns index smallest triangular number larger input Modify main test Write function void find_triplet int int takes number puts triangular numbers sum inout You use find_next_triangular find largest triangu lar number try write nested loops search range zero maximum found If loop loop loop finds numbers sum function canreturn cutting loops Finally write main function declares array ints filled find_triplet runs loop calls find_triplet integer prints result 23The line version second error spotting left quick exercise reader gsl_stats March CHAPTER The debugger allow view intermediate results point program s execution You wait program segfault use break insert breakpoints You execute print expression variable pvariable Once program stopped use s n u step program speeds COMPILING AND RUNNING The process compiling program text machine executable instructions relies hea vily system frames If function A calls function B compiler write instructions creating executing function A knowing function B declaration It simply create frame series instructions function B Since frames separate compiler focus creating time link later What type To point minimal command line com pile programs specify Say want compiler include symbols debugging g warn potential coding errors Wall use C99 POSIX standards std gnu99 compile source files file1 file2 plus sqlite3 standard math library lsqlite3 lm finally output resulting program file named run_me o run_me You specify command line gcc g Wall std gnu99 file1 c file2 c lsqlite3 lm o run_me This lot type separate program designed facilitate compiling After setting makefile describe project able simply type instead mess You benefit reading Appendix A point Or decide gsl_stats March C write alias she shall write batch file use IDE s compilation features Multiple windows come handy code window compile inevitable compilation errors source code time Some text editors IDEs features compile program step errors returned The components Even refer process compilation ac tually embodies separate programs preprocessor com piler linker The sub programs embody steps developing set frames pre processor inserts header files declaring functions compilation step uses declarations convert C code machine instructions build execute standalone frame linking step locates disparate frames system knows look function THE PREPROCESSING STEP The preprocessordoes text wrote convert text There dozen types text substitutions preprocessor number use dumping contents header files source files When preprocessor processing file main sees lines include gsl gsl_matrix h include a_file h finds gsl_matrix h a_file h header files puts entire contents verbatim point file You expansion run g E flag preprocessor passes expanded code compiler For example gsl_matrix h header file declares gsl_ matrix type dozen functions act preprocessor inserts declarations program use structure functions d written The angle bracket form lude gsl gsl_matrix h indicates pre processor look pre specified include path header use headers library files Appendix A details The lude a_ file h form searches current directory header use header files wrote 24As technical detail generally ignore practice preprocessor compiler typically program linker typically separate program 25The lude a_file h form searches include path actually use home grown system ludes In practice forms serve indication find given header file authors use form redundant gsl_stats March CHAPTER Header aggregation The Apophenia library provides convenience header aggregates header likely By placing lude apop h file need clude standard headers normally include program numerical analysis stdio h stdlib h math h gsl_anything h This means ignore headers code snippets chapter Of course need include headers written compiler complains undeclared function header evidently included apop h Manyprogramming languageshave way declare variables hav ing global scope meaning ev ery function use variable Technically C mechanism Instead best I file global scope meaning ev ery function single file variable declared file Header files allow simulate truly global scope finer control want If vari ables global entire program create file named globals h declarations file details By putting lude globals h file project variables declared project global If variables pro ess code files project global scope overkill lude pro ess h code files need Q2 In prior exercises wrote program function create ar ray numbers squares page function print_ array print values page Move print_array new text file utility_fns Write corresponding line file utility_fns h print_ array s header lude utility_fns h main square printing program Modify square calculating code print_array Compile files e g g your_main utility_ fns Run compiled program verify Variables headers The system putting file scope variables base file global scope variables h file ugly detail A function declaration merely advice compiler function gsl_stats March C certain inputs outputs multiple files reread declaration program suffers harmless redundancy But variable declaration com mand compiler allocate space listed If head h includes declarationint x file1 includes head h set aside int s worth mem ory x file2 includes header set aside memory named x So bit memory referring use x later code C s solution extern keyword tells compiler declaration follows memory allocation purely informative Simply normal declaration extern int x extern long double y work Then file declare variables e g int x long double y Thus files lude header know variable x x s scope files given header variable allocated To summarize function declarations typedefs header file included multiple files Variables need declared usual file want files declare header file extern keyword THE COMPILATION STEP The compilation stage consists taking file turn writing machine readable object file sofile1 result file1 o file2 compile file2 o These object files self encapsulated files include table symbols de clared file functions variables types actual machine code tells computer allocate memory sees variable set run frame sees function The preprocessor inserted declara tions external functions variables compiler run consistency checks goes The instructions function include instruction like point create frame gsl_matrix_add input variables executing instruc tion require knowledge gsl_matrix_add looks like separate frame current frame business meddling THE LINKING STEP After compilation step hand number standalone frames Some o files compiler created libraries system The linker collects elements single executable function s instructions tell computer evaluate gsl_matrix_add computer problem locating loading function Your primary interaction linker gsl_stats March CHAPTER telling find libraries l commands compiler command line lgsl lgsl blas lm et cetera Note library s header file object file separate entities meaning distinct ways library function wrong To include function standard math library like sqrt need tell preprocessor include header file lude math h code tell linker link math library l flag command line case lm Appendix A detail debug lude statements l flags Finding libraries An important art C programming knowing find libraries work online hard drive The library know standard library Being standard installed computer compiler If documentation hard drive try info glib easily find online It worth giving documentation skim know wheels reinvent The GNU UNESCO website gnu org holds hefty array libraries free download sour eforge net hosts order projects varying quality To hosted Sourceforge project agree code public fold find work Finally start writing library month s project probably overlap working Simply functions relating topi file named topi useful declarations separate header file named topi h You start creating utility library exercise page Compilation step process The step consists text ex pansions like replacing lude header h entire con tents header h Therefore public variable declarations extern key word function declarations header files The step consists compilation source file converted object o file gsl_stats March C Therefore source file consist set standalone func tions depend file s contents declarations cluded headers The final step linking references functions libraries object files reconciled Therefore find use libraries functions set tasks imagine POINTERS Pointers change life If dealt spend quantity time puzzling wondering anybody need bother And finally comfortable difference data location data wonder wrote code Pointers embody concept location data concept deal time I know location http nytimes om expect I location I information today s events I gave colleagues email address years ago new information send location When inclined I check location new information Some libraries regimented books located need book probability Library Congress classification QA273 librarian tell upstairs bookshelf left The librarian know information probability location information Returning computer moment declare int k com puter going k memory Perhaps microscope find chip transistors bot tom You point Lacking finger point computer use illegible hexadec imal location deal hexadecimal directly lose ignoring implementation thinking pointers precise finger book s number The confusing location data data After write QA273 slip paper easily P A B P A B P B gsl_stats March CHAPTER Further location information location Before computers took card catalog library card catalog place location data stored look location book It happens arrive QA273 shelf find wood block message taped saying oversized books end aisle In situations problem distinguishing information location data data But computing context guide Is large integer data memory address location data C s syntax little clear confusion variable like k integer data location integer data But C uses location data solve number problems key function calls allow inputs modified implementation arrays Call address v value First quick review functions called function computer sets separate frame function puts frame copies variables passed function The function thing produces return value Then entire frame destroyed including copies variables A copy return value gets sent main program remains defunct frame This setup known value values passed function allows stable implementation paradigm standalone frames But k array million doubles making copy time common function noticeable time Also want function change variables sent Pointers fix problems The trick instead sending function copy variable send copy location variable copy book s number slip paper hand function In Figure picture shows situation function main program pointer location holding number Then picture function called pointer argument form like fn_ pointer There fingers original copy pointing spot function knows copy Given copy finger easy function change value pointed seven When function returns picture copy finger destroyed changes undone The original finger hasn t changed pointing place pointing pointing modified value 26This number actually address lifted debugger listed 0x08049588 The 0x prefix indicates number represented hexadecimal gsl_stats March C Figure Before function modifies pointed value Returning C syntax rules pointers To declare pointer integer use int k Outside declarations refer integer pointed use k Outside declarations refer pointer use k The declaration int p means p pointer integer integer non declaration line like p p refers integer value p points There actually logical justification syntax I present tends confuse clarifies Instead bear mind star effectively means different declarations non declaration use To example let declaring new pointer p2 initialized point address p Then declaration beint p2 p p2 declared pointer p pointer The spaces stars matter use whichever int k int k orint k like best General custom prefers form minimizes chance write int k b allocate pointer named k int named b meant int k b allocate pointers The star means multiply There ambiguity bothers use parentheses Listing shows sample program uses C s pointer syntax implement address trick Figure gsl_stats March CHAPTER include stdio h printf include malloc h malloc int globe global variable int factorial int a_c a_c globe a_c a_c return globe int main void int malloc sizeof int printf factorial printf n factorial printf n printf globe n globe free return Listing A version factorial program address Online source allbyadd In main function pointer address integer indicated star declaration line mallo discussed To print integer pointed line use The header function list declarations line fa torial int a_ tells function takes pointer integer nameda_ Thus non declaration use like lines a_ integer Now address trick Figure When fa torial Line pointer gets passed The computer builds frame copy copy location integer Both main frame a_ fa torial frame point piece data Line a_ tells computer address a_ decrement value finds When frame destroyed a_ goes undone slot memory hold decremented value Because integer points changed effect calling fa torial function saw line printed ran program gsl_stats March C Dealing memory Finally contend pointer initialization line int malloc sizeof int Malloc function declared stdlib h short memory allocate By library metaphor mallo builds bookshelves later hold data Just idea int variable given value idea address points initialize The function mallo low level work finding free slot memory claiming computer uses returning address The input mallo quantity memory need case size integer sizeof int The ampersand Every variable address declared pointer The ampersand finds ad dress ount integer ount pointer integer The ampersand star inverses ount ount imply symmetric star appear code ampersand ampersand appear declaration function header As mnemonic ampersand sign address begin letter A There actually character istics given pointer lo cation finger point ing type int memory reserved pointer sizeof int bytes integer The location computer look hexadecimal ad dresses But need bear mind type size pointer If treat data pointed int pointer pointing double computer read good data garbage read variables space allocated program read garbage segfault By way int k fail initialization declaration line pointer value pointer holds Given k pointer integer lines correct int k malloc sizeof int k k malloc sizeof int One convenience help allocating pointers allo read clear allocate run mallo return appropriate address set space zero running k Sample usage int k calloc sizeof int gsl_stats March CHAPTER The syntax requires explicitly state want space size integer You need information mallo process putting zero double pointer different putting zero int pointer Thus allo requires arguments allo element_ ount sizeof element_type Finally allocation de allocation responsibility The de allocation comes simply calling free k pointerk When program ends operating system free memory peo ple free pointers end main point good form leave computer freeing Q2 Write function named swap takes pointers int variables exchanges values First write main function simply declares ints point ers se ond gives values prints values turns Check compiles Then write swap function accepts pointers noth ing That write header let body Call function main Do need use box page Check program compiles Finally write swap function Hint include local variableint temp Add printf end main sure function worked Q2 Modify swap program variables main point ers ints Add allocations mallo declaration line Which need send swap Do need modify swap function Q2 Modify allbyadd declaration line integer pointer That replace current int mallo inta Make necessary modifications main program running Do modify fa torial function gsl_stats March C A variable hold address location memory By passing address function function modify location memory copies variables passed functions A star declaration means variable pointer e g int k A star non declaration line indicates data location pointer points e g two_times k k The space pointer pointing needs prepared mallo e g int integer_address mallo sizeof int When certain pointer free e g free integer_address ARRAYS AND OTHER POINTER TRICKS You use pointer array stead pointing single integer example point sequence integers Listing shows sample code declare array fill square numbers include stdlib h include stdio h int main int array_length int squares malloc array_length sizeof int int array_length squares Listing Allocate array fill squares Online source squares The syntax declaring array exactly matches allocating single pointer needed allocate block size sizeof int stead single sizeof int Referring element squares uses identical syntax automatically declared arrays beginning chap ter Internally types array sequence blocks memory holding certain data type gsl_stats March CHAPTER Q2 The listing squares exciting output Add second loop print table squares screen printing index value squares array position Q2 After loop squares holds plain integer Thus refer integer s address putting Extend version squares use swap function swap values squares squares But despite similarities arrays pointers identical automatically allocated memory manually allocated Given declarations double a_thousand_doubles double a_thousand_more_doubles malloc sizeof double declares automatically allocated array int automatically al located allocation de allocation variable respon sibility compiler The second allocates memory locationa_thousand_doubles decide free In function arguments interchange syntaxes These equivalent int a_function double our_array int a_function double our_array But careful function frees automatically allocated array passed parent assigns new value mallo stepping C s turf segfault ARRAYS OF STRUCTS Before stru t complex numbers referred elements dot real orb imaginary For pointer structure use instead dot Here examples definition omplex structure page complex ptr_to_cplx malloc sizeof complex ptr_to_cplx real ptr_to_cplx imaginary complex array_of_cplxes malloc sizeof complex array_of_cplexes real gsl_stats March C If error like request member real structure union dot vice versa Use feedback understand misunderstood switch try REALLOCATING If know howmany items array probably won t bother pointers instead use intfixed_list declaration leave memory allocation issues computer But try elements list recall means putting fixed_list memory machine hadn t allocated segfault If sure size array need expand array Listing program find prime numbers amusing tricks thrown Since don t know primes find need use reallo The program runs hit ctrl c dumps complete list point Line SIGINT signal hitting ctrl c sends program By default halts program immediately line tells system line function line receives signal Thus loop ginning line running hit ctrl c program continue line Lines Check testme evenly divisible primes The sec ond element loop run condition includes conditions Line Computers TV movies fast moving counters screen giving impression important happening line shows The newline n actually steps carriage return beginning line plus line feed line The r character carriage return line feed current line rewritten printf The fflush function tells system sure written screen Lines Having found prime number add list step process reallocate list item larger element space add counter holding size array The argument reallo pointer space needs resizing second argument new size The new block memory primes array far end allocated garbage filled space ready fill data gsl_stats March CHAPTER include math h include stdio h include signal h include malloc h int ct keepgoing int primes NULL void breakhere keepgoing int main int testme isprime signal SIGINT breakhere keepgoing isprime isprime sqrt testme ct isprime testme primes isprime printf r testme fflush NULL primes realloc primes sizeof int ct primes ct testme ct testme printf n ct printf t primes printf n Listing Find prime numbers array Online source primes SOME COMMON FAUX PAS Figure shows errors pointer handling The step picks second step Figure function called pointer argument Then function frees copy pointer pointing frees original finger pointing Next allocates new space moving copy finger point new location But function finishes depicted final step copy pointer destroyed way refer mallo ed space main program We pointer main frame space space pointer Returning library metaphor moment calling function wrote number slip paper handed function function went shelves moved things But function mechanism telling caller things shelves mess Listing repeat prime finding code cute tricks removed gsl_stats March C Figure How mess pointers include math h include stdio h include malloc h void add_a_prime int addme int ct int primes primes realloc primes sizeof int ct primes ct addme ct int main int ct j testme isprime max int primes NULL j j max j isprime isprime sqrt testme ct isprime testme primes isprime add_a_prime testme ct primes testme ct printf t primes printf n Listing Find prime numbers array Online source primes2 process adding prime relegated separate function The wrong way implement function lines void add_a_prime_incorrectly int addme int ct int primes primes realloc primes sizeof int ct primes ct addme ct gsl_stats March CHAPTER This commits faux pas changing value copy primes allocating new data new location called function idea changes primes pointing invalid location The correct method shown Listing It passes pointer primes pointer location location data The calling function sends location card catalog called function revise locations listed card catalog makes changes shelves The syntax confusing compare t treated Because function modify line sends location t function header extra star instead int t refers address int t Similarly function sends array s address primes function adds star header int primes Q2 Kernighan Pike point reallo slow better calling time array extended Instead suggest time array length n reallo ed size doubled 2n Thus array eventually elements reallo ed adding second fourth eighth data points total reallocations instead Rewrite code figure primes2 implement method selective reallocation Arrays internally represented pointers The int sarray form creates automatically allocated array com puter creates destroys array int harray mallo sizeof int form creates manually allocated array allocate deallocate Refer array elements cases square brackets notation harray Arrays structs declared arrays basic variables Refer elements pointer struct You expand manually allocated array reallo Just copies normal variables passed functions copies pointers sent Therefore careful modifying pointer subfunction gsl_stats March C STRINGS C s handling text strings simple elegant unpleasant awkward Although book oriented programs manipulating numbers building Web pages text words phrases inevitable mathiest programs So section cover basics system deals variable length text There approaches dealing C s awkwardness text You skim standard library documentation know functions available common string operations You use higher level library provided data type text Glib s GString type Chapter Glib Or leave C entirely text manipulation number command line tools text focused language like Ruby Perl Appendix B Nonetheless methods based C s raw string handling core raw C methods hand worth getting know C s string handling prefer higher level methods C implements lines text hello arrays individual characters followed invisible null character written That think single word shorthand array h e l l o This means think terms arrays dealing strings characters The implication elegant use arrays represent text expectations assignment won t work Here examples char hello char hello2 Hi hello hello2 This probably meant hello Hi Nor Line shows strings declared array style declarations static form mallo Line shows arrays integers specify list array initialize array later But common error think line copy text Hi intohello pointers integers actual function line pointer operation instead copying data pointed hello2 location pointed hello simply copies location hello2 When change text pointer later pointing location change Along similar vein line behave expect 27Line compile string Hi held memory pointerhello point However pointer literal text onst pointer meaning time try change hello program crash gsl_stats March CHAPTER Instead series functions strings copy handle strings strlen There lengths associated string pointer spacemallo ed pointer second number characters string terminating character appears Size free memory sponsibility strlen your_string return number text characters your_string strn py Continuing example right way copy data intohello include string h strncpy hello Hi strncpy hello hello2 The argument total space mallo ed hello strlen hello strn Rather overwriting string append e concatenate string strncat base_string addme freespace For example strn hello hello2 leave Hi Hello hello Q2 The key problem pointers strings editing string comes step problem measure length string changed reallo string appropriate size finally change string Write function astrn py har base har opyme copy opyme base Internally use strlen reallo andstrn py execute steps string extension All dis cussion pointing pointers inside function applies function needs pointer pointer argument Once function working write function astrn executes procedure string concatenation After ve tested functions add library utilities gsl_stats March C 67snprintf The snprintf function works like printf statements prints output string instead screen The fill blanks syntax printf works exactly manner strings include string h int string_length char write_to_me string_length char Steven int position snprintf write_to_me string_length person s number line n position However step process measuring new string calling reallo finally modifying string especially painful snprintf hard know long printf style format specifier blanks filled One way sure room adding text simply allocate absurd space string like megabyte memory har hello You won t notice wasted memory modern computer method error prone brilliant idea loop add little text million data points string asprintf If GNU C library BSD s standard C library Apo phenia use asprintf allocates string big handle inputs runs snprintf Here simple example memory allocation sight print line char line asprintf line s number line Steven Once asprintf probably line memory need send location pointer pointer You comfortably asprintf loop worrying flow Here snippet write string counts 28The nice feature snprintf secure common functions like str py andsprintf check length input easy inadvertently overwrite important bits memory input string including location instruction executed Unsafe string handling functions common security risk allowing execution malicious code 29Because asprintf free current space taken string reallocating new version loop memory leak In situations isn t leak matter need use int har tmp string asprintf string s string free tmp gsl_stats March CHAPTER char string NULL asprintf string initialize non NULL string int asprintf string s string If don t asprintf hand hack way guess ing final length filled string running measure reallo write procedure directly e g char string NULL char int_as_string int int newlen strlen string string realloc string newlen snprintf int_as_string strncat string int_as_string newlen The C standard defines sizeof har safe write newlen place sizeof har newlen See Listing page example extending string inside loop Q2 Modify primes2 write string named primes_so_far printing screen As step program printf primes_ so_far Q2 Modify program exercise print primes digit seven Hint written number pstring compare element pstring s array characters single character str mp Because strings arrays form Joe Jane sense Instead str mp function goes arrays charac ters input determines character character equal You encouraged use str mp If s1 s2 identical strings thenstr mp s1 s2 different str mp s1 s2 There 30It nice use snprintf string newlen s string usingsnprintf insert string string behaves erratically This reason find system withasprintf 31ISO C standard Committee Draft ISO IEC TC2 par gsl_stats March C rationale str mp function effectively subtracts string equal difference zero equal difference nonzero But great majority humans read str mp s1 s2 mean s1 s2 following To translate En glish sentiment C need use str mp s1 s2 Experi enced programmers world regularly wrong Q2 Add function library convenience functions compare strings return nonzero value strings identical zero strings identical Feel free use str mp internally Strings actually arrays hars think pointer terms dealing There number functions facilitate copying adding printing strings use need know long string edit ERRORS The compiler warn syntax errors seen debugger help find runtime errors best approach errors sure happen This section presents methods code robust reliable They dovetail notes writing function time making sure function task TESTING THE INPUTS Here simple function mean input ar ray double find_means double int length doublemean int length mean return mean length What happens user calls function NULL pointer It crashes What happens length It crashes You easy time pulling 32Normally d assign double mean loop beginning I slightly odd initialization sake example How approaches differ zero length array gsl_stats March CHAPTER debugger drilling point sent bad values easier program told error Below version find_means save trips debugger It introduces new member printf family fprintf prints files streams Streams discussed Appendix B suffices note writing stderr stream fprintf appropriate means displaying errors double find_means double int length NULL fprintf stderr You sent NULL pointer find_means n return NAN length fprintf stderr You sent invalid length find_means n return NAN doublemean int length mean return mean length This took typing display brevity mathematicians ad mire gains clarity usability Listing conditions inputs provides touch additional documentation function expects If misuse function know error heartbeat The perfect inserting quick tests left hand test validity right hand execute validity test passes For example let user gives list element indexes add counter chosen array elements The quick way simply use operator array evens array But array index invalid break So add tests testing evenness array_len array evens array gsl_stats March C If bounds program throws moves When failing silently OK types tests perfect system complain loudly encounters failure tool list assert assert The assert macro makes claim claim false program halts point This mathematical assertions housekeeping like checking NULL pointers Here input checking function rewritten assert include assert h double find_means double int length assert NULL assert length doublemean int length mean return mean length If assertion fails program halt notice failure print screen On gcc based system error message look thing like assert your_program c find_means Assertion length failed Aborted Some people comment assertions feel program adequately debugged typically saves time defeats purpose having assertions begin sure ll find bug If d like compare timing assertions DNDEBUG flag compiler add command line compile program assert statements skipped Q2 The method taking mean runs risks overflow errors array million elements mean grow million times average value divided natural scale Rewrite function calculates incremental mean function mean date element Given sequencex1 x2 x3 mean x1 second x2 x3 et cetera Be sure appropriate assertions inputs For solution GSL gsl_ve tor_mean function code apop_db_sqlite gsl_stats March CHAPTER TEST FUNCTIONS The best way know function working correctly test separate function sole purpose test main function A good test function tries cover obvious strange possibilities vector element unexpected values Do corner cases input counter zero maximum because function fail It worth checking absolutely wrong inputs like find_means array4 fail appropriately Here function runfind_means paces void test_find_means double array1 int length assert find_means array1 length double array2 INFINITY assert find_means array2 length INFINITY double array3 assert find_means array3 length double array4 assert find_means array4 Writing test functions numerical computing significantly harder writing general computing excuse skipping testing stage Say write function invert matrix It tall heap scrap paper manually check answer typical matrix But know inverse identity matrix inverse zero matrix NaN You know thatX X anyXwhereX defined Errors slip tests look broad properties special cases best especially ornery computation simple diagnostics find surprising number errors Some programmers actually write test functions This manner writing outline filling details Write comment block explaining function write test program gives examples comment block described prose Finally write actual function When function passes tests Once test functions run supple mentary test program Right short program calls thetest_find_means function write functions tests added program appropriate Then add test run old tests time Peace mind ensue For ulti mate peace mind test functions beginning main gsl_stats March C analysis They microsecond run fails easier debug function failed course main routine Q2 Write test function incremental mean program d written Did function pass try Some programmers Donald Knuth famous example bug log listing errors committed If function didn t pass test time entry bug log Before written function expectations behave express set tests func tion pass You expectations function s behavior run time assert expectations ensure met Q2 This chapter stuck standard library installed default C compiler The remainder book rely number li braries commonly available POSIX standard installed separately including Apophenia GNU Scientific Library SQLite If writing simulations need GLib library data structures presented Chapter Now compiled number programs C source foreign good time install auxiliary libraries Most available packagemanager somemay installed C source code See online appendix linked book s web site http press prin eton edu titles html notes find ing installing packages Appendix A notes preparing environment gsl_stats March DATABASES There way voice presence Where information flows Rumi p Structured Query Language SQL1 specialized language deals flow information Some things like joining multiple data sets pain traditional techniques matrix manipulation easy query database language Meanwhile operations like matrix multiplication inver sion SQL queries With database tables C matrices data analysis technique unstoppable As broad rule try data manipulation like pulling subsets data merging multiple data tables SQL Then step pull perfectly formatted data memory matrix statistical analysis Because SQL specialized language deals information flows nearly complex C Here valid SQL sele t age gender year survey That s proper English It goes downhill terms properness worst difficult look SQL query idea rows columns output table look like 1Some people pronounce SQL sequel ess queue ell The official ISO IEC standard comment correct gsl_stats March DATABASES Like C SQL merely language left programmers world write code parse SQL return data SQL queries Just book leans g interpret C code recommends SQLite library D Richard Hipp interpret code written SQL SQLite provides library functions parse SQL queries uses instructions read write specific format file binary trees Chapter Any program uses SQLite function library reading writing file format SQLite files traded dozens programs As C utilities problem selecting SQLite database viewer use options The SQLite library comes command line program sqlite3 alternatives reminiscent table view standard stats package spreadsheet ask search engine sqlite browser sqlite GUI These programs immediate feedback queries input let verify tables creating C code expected Why SQLite lite Because SQL oriented databases designed multiple users firm s customers employees With multiple users come issues simultaneous access security add complications basic process querying data SQLite designed user time exactly right typical data analysis project If hope use database system need learn typically vendor specific commands locking permissions This chapter primarily consist overview SQL follow tools Section describe Apophenia library functions facilitate SQL database SQLite mySQL C program Q3 Check SQLite executable development libraries cor rectly installed In online code supplement find SQLite formatted database named data wb db listing GDP popula tion countries world Verify open database tools e g sqlite3 data wb db com mand prompt execute view results querysele t pop Once working query interpreter follow discussion chapter For cutting pasting convenience queries chapter queries file online code supplement gsl_stats March CHAPTER Data format A database holds tables Each column table repre sents distinct variable For example health survey include columns subject s age weight height Expect units different column column Each row table typically represents observation For example survey row data single person There mechanism SQL naming row common plain column named row_ identifier ial_se urity_no serves purpose The asymmetry columns rows evident syntax SQL You select columns column real mechanism selecting arbitrary subset columns select rows characteristics real mechanism select rows Your C matrices generally expected similar format page notes Most world s data sets format If data set best bet convert fighting SQL s design notes crosstabs page tips converting common alternative data format BASIC QUERIES SQL s greatest strength selecting subsets data set If need data countries World Bank data set data wb db populations million ask thusly select pop population You read like English know means columns find rows table named pop population row equal return columns rows 2If row_name variable select rows row_name Joe simply selecting rows characteristic having row_name variable value Joe That column names bona fide names row names data gsl_stats March DATABASES Commas semicolons In SQL semicolons terminators given com mand You send SQL commands ending semicolon Many SQLite based programs forgive omitting final semi colon Commas separators meaning ele ment comma separated list comma For example write query like sele t ountry pop population error like syntax error near ferring comma sepa rating columns Generally sele t statement gives list columns output table clause declares source data comes clause lists restrictions rows output And s Every query run parts order column specifi cation data source row specifica tion This simple means specify ing rows columns source data allows huge range possibili ties Select The sele t clause specify columns table output The easiest list means columns Other options Explicitly list columns sele t ountry population Explicitly mention table s pulling data sele t pop population gdp ountry This unnecessary essential dealing multiple tables Rename output columns sele t pop ountry ountry gdp gdp_in_millions_usd If alias pop ountry ountry need use namepop ountry future queries bit annoying Generate new columns For example convert GDP dollars GDP British pounds conversion rate writing sele t ountry gdp gdp_in_GBP The gdp_in_GBP subclause essential hope refer column future From The clause specifies tables pulling data The simplest case single table data_tab specify tables necessary data_tab1 data_tab2 You alias tables easier reference The clause data_tab1 d1 3You row restrictions case query parts null gsl_stats March CHAPTER 3data_tab2 d2 gives short names tables lines likesele t d1 age d2 height Another option data subqueries Borrowing C s annoyances SQL accepts C style block comments form It trouble nested block comments C p With line comments dashes ignored compara ble slashes C mySQL users need dashes space Also following C s lead dividing integers produces integer real number hu mans expect Thus calculating ount1 ount2 cast columns real number adding ount1 ount2 return real number The add zero trick works turn string number1990 SQL ast keyword easier use trick adding Aliasing generally optional convenient case necessary arises joining table For simply note syntax datat1 data t2 let refer data table en tirely independent tables Notice way aliased sele t section form sele tlong_ ol_des ription l d section long_file_namelfn Where The clause chance pick rows interest With clause query return line line original table columns returned match specified sele t clause For example try sele t gdp data wb db database You use Boolean operators know love usual d1 age d2 height d1 weight SQL ally assignments variables clause d1 weight test equality assignment SQLite easygoing accept C format d1 weight SQL parsers like mySQL forgiving con sider double equals error You select based text way select number ountry United States Any string SQL keyword table column single tick quotation marks 4The actually optional sele t clause improves readability 5Again SQLite forgiving accept C style double tick quotation marks However beneficial SQL uses single ticks C uses double ticks snprintf q sele t ountry Qatar requires unsightly backslashes double tick quotation marks snprintf q sele t ountry Qatar gsl_stats March DATABASES Case matters United States united states However need case insensitive like keyword The clause ountry like united states match fully capitalized country lower case version The like keyword accept wild cards _ match single character match set characters Both ountry like unit ates ountry like united_states match United States The clause refers root data output meaning readily refer columns mention sele t clause Q3 Use clause population table find current popula tion home country Once know select countries populous country Generalizing equality inequalities want group elements range For keywords Say want United States China output Then ask columns country short list select gdp country United States China The keyword typically makes sense text data numeric data probably want range Here countries GDP billion select gdp gdp Q3 Write query replicate query usedbetween A query consists parts columns output data source rows output The columns specified sele t statement You pull columns data sele t specify indi vidual columns like sele t b b ratio gsl_stats March CHAPTER The data source clause typically list tables The row specification generally clause list con ditions rows meet It missing possible rows returned include series conditions like b b DOING MORE WITH QUERIES Beyond basic sele t format sele t query clude auxiliary clauses refine output Here complete format sele t query section explore clause clause select distinct columns tables conditions group columns having group_conditions order columns limit n offset n PRUNING ROWS WITH distin t The data metro db file includes listing stations color subway line s station lies The query sele t line lines produces massive redundancy dozen stations line color appears dozen times table The distin t keyword tell SQL engine rows exact duplicates return copy row In case try select distinct line lines The distin t word prunes rows placed sele t portion program This reads like English breaks story thesele t statement specifies columns statement specifies rows gsl_stats March DATABASES AGGREGATION Here number rows gdp table data wb db select count row_ct gdp This produces table column row listing total number rows data table Q3 How rows sele t pop gdp produce The explana tion answer appear section joins You probably want refinement like know howmuch data region use group clause select class count countries_per_class classes group class After ount common aggregation commands sum avg These existing row argument For example data tattoo db database single table representing telephone survey tattoos To average number tattoos person broken race use query select race avg tattoos ct tattoos tattoos group race Feel free specify multiple group clauses For example modify query sort race age changing group ra e groupby ra e tattoos year birth When want analyze output interested apop_db_to_ rosstab function page Q3 In pre ip table data limate db database yearmonth col umn encodes dates forms like mean August Fortunately SQL standard round function produce plain year round Use round group avg find average precipitation p p year You use ount distin t keyword find row table This useful producing weights observation type gsl_stats March CHAPTER Function Standard SQL mySQL SQLite Apophenia abs avg count max min round sum acos asin atan cos exp ln log10 pow rand sin sqrt stddevs tan variancep stdp stddev_popp stddev_ samps var_samps var_popp ran vars skews kurtosiss kurts aRound SQL standard instead provides floor eil Table Standard SQL offers mathematical functions different systems offer different extensions The p s subscripts indicate functions populations samples box page query produce tabulation respondents tattoo survey race birth year select distinct race tattoos year birth birthyear count weight tattoos group race birthyear With group command levels elements items groups want subsets As subset items clause Similarly exclude groups query having keyword For example query produced lot low weighted groups What groups ount We t answer usingwhere weight weight column data table post aggregation table This having keyword comes select distinct race tattoos year birth birthyear count weight tattoos group race birthyear having weight SQL extensions That s aggregators standard SQL So imple menters SQL standard typically add additional functions standard Table list including aggregation functions gsl_stats March DATABASES like var R R functions like log The table focuses numeric functions standard mySQL include functions manipulation text dates sundry types data online references details Bear portability mind functions careful stick SQL standard hope use queries context If want stay standard data C vector matrix use apop_ ve tor_log apop_ve tor_exp apop_ve tor_skew apop_ve tor_var desired statistics matrix SORTING To order output table add order clause For example view list country populations alphabetical order use select pop order country You multiple elements clause order ountry pop If ties variable broken second The keyword des short descending reverse order variable s sorting Sample usage order ountry des pop GETTING LESS Especially interactively interrogating database want table constructed sele t clause The output million lines long gist use limit clause For example follow ing query return rows pop table select pop limit You want later rows add offset keyword For example select pop limit offset return rows discarding rows Thus rows Beyond making interactive querying easier limit offset gsl_stats March CHAPTER clauses break tables giving problems manageable pieces probably C loop You limit offset query thing query If union family combine sele t statements yourlimit clause end applies aggregate table Random subsets The limit clause gives sequential subset data representative If problem random draw subset data Ideally provide query like sele t data rand draw data SQLite Apophenia mySQL provide rand function works exactly For function row draws uniform random number zero CREATING TABLES There ways create table One reate state ment insert statement single row data The reate statement requires list column names insert state ment requires list data element column begin create table newtab age insert newtab values Joe insert newtab values Jill insert newtab values Bob commit The begin ommit wrapper way means happen memory final commit The program run faster program 6Standard SQL s random function absolutely painful SQLite s version currently produces number tween reader recognize So need pull random number divide shift familiar range compare limit Standard SQL provide exponentiation requires bit shifting operator I promised need read x 2x That said sele t data random pull approximately data set 7After read Section wonder stream random numbers produced database There stream database Apophenia maintains internally To initialize seed seven use apop_db_rng_init If function database RNG auto allocates use seed zero 8SQLite pleasant property columns basically type Other database engines insist table declarations look little like C functions e g reate table newtab var har ageint database engine documentation details gsl_stats March DATABASES crashes middle lost The optimal speed secu rity trade left exercise reader If hundreds thousands inserts certainly better putting data text file C function apop_text_to_db command line program The form useful situations creating table mid program example page The method creating table saving results query Simply reate table newtab_name head query like save create table tourist_traps select country lonely_planet pp Q3 The riders table data metro db database includes average boardings station Washington Metro system year opening Create riders_per_year table column year column total average boardings system given year DROPPING A TABLE The converse table creation table dropping drop table newtab See apop_table_exists C p delete tables desired ROWID Sometimes need unique identifier output row This difficult create scratch SQLite inserts row named rowid It simple integer counting number rows appear query sele t table But querysele t rowid table hidden row numbers appear output 9mySQL users need explicitly ask column creating table A statement like reatetable newtab id_ olumn int auto_in rement info1 har info2 double cre ate table typical columns fill plus id_ olumn system fill Afterinsert newtab values Joe insert newtab values Jane ta ble row Joe id_ olumn Jane id_ olumn gsl_stats March CHAPTER Q3 Using order rowid find rank home country s GDP countries World Bank database METADATA What tables database What column names Stan dard SQL provides easy way answer questions database engine specific means SQLite gives database table named sqlite_master provides information It includes type ob ject index table type column column query generated object sql column MySQL users page In practical terms table primarily good getting lay unfamil iar database quick sele t sqlite_master open database hurts If SQLite command line table command exactly program Thus command sqlite3mydb db table lists available tables s hema command gives information sqlite_master MODIFYING TABLES SQL primarily oriented filtering style pro gram design e g query filter data table pro duce new table bad data removed query filter resulting table produce aggregate table select elements aggregate table produce new table et cetera But want modify table place sending filter produce new table especially table million entries long SQL provides operations modify table place delete Unlike drop acts entire table delete acts individual rows database For example remove columns missing GDP data use query destroy data sample databases copy e g reate table gdp2 sele t gdp delete gdp gdp insert The obvious complement deleting lines inserting You saw insert context creating table inserting elements item item You insert query form insert intoexisting_table sele t gsl_stats March DATABASES 87update The update query replace data column new data For example World Bank refrained estimating Iraq s population US Central Intelligence Agency s World Factbook estimates Here change Iraq s population pop table to26783 update pop set population country Iraq You limit queries fewer rows limit clause gives sequential snippet random draws The SQL standard includes simple aggregation commands avg sum ount SQL implementations pro vide nonstandard aggregators queries called functions When aggregating add group clause indicate aggregation grouped Sort output order clause You create tables reate insert commands probably better reading table text file Usedrop delete table SQLite gives row rowid hidden ask explicitly JOINS AND SUBQUERIES So far cutting table ei ther selecting subset rows group ing rows SQL s great strength building tables joining data disparate sources The joining process based join keyword simply specifying multiple data sources section query describing mesh section If specify tables line lacking restrictions database return joined line pair lines Let table column data table column data b c sele t gsl_stats March CHAPTER table1 table2 produce output table combination rows a1 b1 a2 b2 a3 b3 Such product quickly gets overwhelming exercise page saw joining countries World Bank data s pop table countries gdp table produces pages rows Thus clause essential It is typical use join arises column table represents identical information Out rows join including matched Qatar Ghana Cameroon Zimbabwe interested match Qatar Qatar Cameroon Cameroon That want rowswhere pop ountry gdp ountry query makes sense restriction added select pop country pop population gdp GDP pop gdp pop country gdp country You table dot column format column names essential In sele t clause specifying output columns use eitherpop ountry gdp ountry definition identical unconcerned country names want numeric data omit names entirely Q3 Add calculation sele t portion query find GDP capita country Be sure calculated column like gdp_per_ ap order gdp_per_ ap gsl_stats March DATABASES Q3 The World Bank data includes classification country Countries receiving World Bank assistance WB calls client countries classed region e g Middle East North Africa countries binned generic class like Lower middle income economies Find total GDP capita eachWorld Bank grouping Here join country columns gdp lasses table country columns pop lasses table Add total GDP region divide total population region Example time lag The form columns match far common type join creative uses joins For example common time series analysis include value variable time t data influenced value time t The data limate db database includes table deviation century long norm aggregate worldwide temperatures Smith Reynolds methods caveats discussion A quick sele t temp upward trend data years zero years hover What month month change look like The step dealing fact separate year month columns One solution deal year month moves time smooth crements This creates problem comparing floating point values reliable wind like test value exactly equals fail As variant solves problem instead dividing months multiply years comparing integers select R year R month R temp L temp temp L temp R R year R month L year L month The salient feature data set happens The long term shift result large number small month month changes 10Chapter cover graphing try apop_plot_query data limate db sele t tempfrom temp command line visual indication trend gsl_stats March CHAPTER Q3 Perhaps larger change larger time span Calculate year year differences Create annualized table columns year averagetemp months year Join table lagged year You won t worry unreliable float comparisons recall SQLite thinksyear string treat year number Having looked year long differences try decades Create de ades table average decade Hint group round year Join table lagged years Are differences ginning pattern Given data sorted matching rows rowid select L temp R temp temp L temp R R rowid L rowid SPEEDING IT UP Now seen join tables cover avoid joining tables If tables million elements joining clause like b requires 1e6 1e6 1e12 trillion comparisons This impossibly slow number tricks avoid making 1e12 comparisons Indices You ask SQL engine create index table intend use join later The commands create index pop_index population country create index gdp_index gdp country 11Say mean join million subjects ID number sele t t1 t2 t1 t2where t1 id t2 id forget include clause Then asked system cre ate trillion entry table hours weeks Thus step speeding inordinately slow query try tricks section sure actually wrote query intended write gsl_stats March DATABASES index pop gdp tables ountry column The index pop_index basically irrelevant gibberish sounds nice Once created index join indexed columns goes faster system longer 1e12 comparisons Basically look value var left table check right table s index list elements value That instead million comparisons join element index lookup The lookup process building tree took time processes order millions operations millions squared The tree internally structured binary tree Chapter discussion b trees There standard SQL syntax indexing multiple columns e g reate indexpop_index2 pop ountry population goes lexicographic der This index item ountry second column population backup ordering want join second column prepare creating index puts column position Subqueries Among SQL s nicest tricks allows input tables queries For example large averageWorld Bank grouping Answering question step process ount category average You run query produce table counts save table run query table find averages create table temptab select count ct classes group class select avg ct temptab But generating temporary table SQL allows simply insert thesele t statement directly query select avg ct select count ct classes group class The query inside clause return table table data source like table If query output needs alias result usual sele t t1 allow refer query s output t1 query gsl_stats March CHAPTER Q3 On page found home country s population coun tries populations greater Use subquery query Hint replace number query returns ele ment Subsetting foreign table If look World Bank data large number countries small islands million people Say unconcerned countries want GDP countries population Q3 Write query pull GDP countries population greater million standard left old right old join syntax head section But join exercise necessary particularly concerned population se eliminate rows It logical fit query clause clause typically select subset rows Indeed query directly clause select gdp country select country pop population The subquery return list country names main query use directly typed This typically faster join operation need left table row count right table row count comparisons The boost efficiency implies slight restrictions clause list table subquery refer sub query s columns output Joining loop The time takes especially large join linear number rows primarily real world reasons hardware software engineering If computer store data points needed query fast memory need plenty swapping forth different physical locations computer But computer able store hundredth thousandth data set fast memory painfully slow query run finite gsl_stats March DATABASES time breaking series shorter queries Here example work Baum et al We gathered genetic markers SNPs number pools subjects wanted mean pool Omitting details database included pools table subject id poolid pool elements table individual ids SNP labels values tens millions values Even creating appropriate indices straight join select pools poolid poolid SNP avg val val var val var genes pools genes id pools id group pools poolid SNP taking hours Our solution use C loop plus subsetting foreign table avoid join taking long There steps process create blank table filled list poolids use insert sele t add poolid s data main table The details functions discussed steps evident code snippet apop_query create table t poolname SNP val var apop_data names apop_query_to_text select distinct poolid pools int names textsize apop_query insert t n select s SNP avg val var val n genes n id select id pools poolid s n group SNP n names names This allowed aggregation process run minutes The week bought better hardware As noted natural grouping like pools example afor loop limit offset form break long table smaller pieces STACKING TABLES You think joining tables setting table right table But need stack There keywords gsl_stats March CHAPTER Union Sandwiching union complete queries select id age zip data_set_1 union select id age zip data_set_2 produce results query stacked directly second query Be careful tables number columns Union If row duplicated tables union operation throws copy duplicate lines like sele t distin t includes duplicates Replacing union union retain duplicates Interse t As guess putting interse t sele t state ments returns single copy lines appear tables Ex ept This subtraction returning elements table appear second Notice asymmetry second table appear You output query clause parent query You join tables listing multiple tables clause When need specify clause possibly thedistin t keyword prevent having unreasonably long output table If intend join elements speed join immensely creating index If join takes long sidestep sele t old sele t form C loop Tables stacked union union interse t andex ept ON DATABASE DESIGN Say reading existing data gathering simulation data collected real world Here considerations sugges tions design database summarizing common wisdom best way think database tables gsl_stats March DATABASES The basic premise type object single table object single row table Figure shows table observations generic study involving sub jects treatments information measured times The simple table design typical spreadsheet designed This version row subject row observations information subjects treatments observations pools mixed Figure shows structure better suited databases For statistical studies key object observation gets table observations The objects study subjects pools treatments tables By giving element table ID number table easily cross reference This setup advantages Minimize redundancy This rule number database design book article written goes reducing data redundancy minimized normal form Codd If human enter redundant data creates chances error opportunities failure come data needs modified somebody notices actually subjects pool In single table form information pool repeated everymember pool having separate table pools means pool s information listed exactly Ask non observation questions There reasons ask questions based treat ments pools setup obser vation based table facilitate From multiple tables easy ask questions focus data treatments pools join operations observation pool subject treatment IDs Gelman Hill p point separating subjects groups facil itates multilevel models group parameters submodel estimated parameters estimate overall model This sort modeling covered later chapters Use power row subsets Figure includes multiple observations line morning evening measurements But went observations hourly observations hours Remember way arbitrarily select subset columns columns gsl_stats March CHAPTER subjid value_morn value_eve poolcount pooldate t_type t_dosage NaN control NaN NaN control NaN NaN control NaN NaN control NaN case case case case case case Figure Spreadsheet style monolithic table redundancy obsid subjid value time morn morn morn morn morn morn morn morn morn morn NaN eve NaN eve NaN eve NaN eve eve eve eve eve eve eve subjid poolid treatmentid poolid poolcount pooldate treatmentid t_type t_dosage control NaN case case case Figure Database style table object type row object gsl_stats March DATABASES named 1AM 2AM difficult use If needed mean morn ing observations d need like sele t 12AM 1AM 2AM 3AM table Figure hour column simply use select avg value observations time like time depending format choose time If chance observations compared aggre gated probably recorded different rows column For hours subjects table rows nearly pleasing human digestible spreadsheet But rarely need look data easily construct crosstab need viaapop_db_to_ rosstab Even worse having data points type separate columns having data points type separate tables cases table controls table Or political scientist wants study county level data United States including variables correlations tax rates votes Senators educational outcomes Because DC county subdivisions residents Congressional representation DC data fit form data states commonwealths United States But correct approach nonetheless DC data table counties states creating table DC table states worse separate table state It easy sele t alldata senate_vote null DC s lack representation affect analysis Databases spreadsheets They typically designed tables millions rows necessary Each type object observations treatments groups single table object single row table Bear mind tools designing table layouts It easy join tables find subsets tables create spreadsheet like crosstabs data tables 12By way sele t alldata population sele t population fromalldata state DC won t work return states population DC zero Senators zero Representatives Wyoming Senators Representative population census data gsl_stats March CHAPTER FOLDING QUERIES INTO C CODE This section covers functions Apophenia library cre ate query database All functions wrappers functions SQLite mySQL libraries dirty work sufficiently com plete need use functions SQLite mySQL C libraries directly The details main discussion apply SQLite mySQL users page list differences IMPORTING The command need apop_open_db If file like apop_open_db study db database live hard drive This slower memory exist stop restart program programs able use file information debugging run program reading data Conversely null argument apop_open_ db NULL database kept memory run faster disappear program exits Apophenia uses database time theapop_merge_dbs SQLite s atta h functions Command line utilities Apophenia includes handful command line util ities handling SQLite databases need write blown C program apop_text_ to_db reads text file database table apop_ merge_dbs send tables database apop_plot_query send query output di rectly Gnuplot apop_db_to_ rosstab table SQLite database produce crosstab All simply wrappers cor responding Apophenia functions For utili ties use h parameter detailed structions e g apop_plot_query h Unless program generat ing data probably importing data text file The apop_text_to_db func tion try command line box The line text file column names remaining rows data If data file right format rarely Appendix B text massaging techniques When queries run apop_ lose_db close database If send function apop_ lose_db SQLite minute clean da tabase exiting leaving smaller file disk sending zero doesn t bother step Of course database memory s moot forget close database consequence The queries The simplest function apop_query takes single text argu ment query This line runs query returns appropriate reate insert queries gsl_stats March DATABASES int page_limit apop_query create table tourist_traps select country lonely_planet pp page_limit A string easiest human read broken lines end line backslash reach end string The example use alternative As example shows Apophenia s query functions accept printf style arguments page easily write queries based C calculations There series functions query database result C variable This function run given query return resulting table analysis int page_limit apop_data tourist_traps apop_query_to_text select country lonely_planet pp page_limit C merges consecutive strings sele t ountry merged sele t ountry We use split string lines But careful include whitespace sele t ountry merges sele t ountryfrom After snippet tourist_traps allocated filled data ready use query returned data case NULL It worth checking forNULL output query return There apop_query_ functions types meet chapter including apop_ query_to_matrix pull query gsl_matrix apop_query_to_text pull query text apop_data set apop_query_to_data pull data matrix apop_query_to_ve tor apop_query_to_ float pull column number returned table gsl_ ve tor double For immediate feedback use apop_data_show dump data screen apop_data_print print file database If want quick screen picture table try apop_data_show apop_query_to_data select table gsl_stats March CHAPTER Listing gives idea quickly data brought database table C matrix The use structures handled detail Chapter application ap function mystify reading book sequentially But main function sense opens database sets apop_opts db_name_ olumn appropriate value uses apop_ query_to_data pull data set It is steps math results screen include apop h void percap gsl_vector double gdp_per_cap gsl_vector_get gsl_vector_get gsl_vector_set gdp_per_cap column gdp_per_cap int main apop_opts verbose apop_db_open data wb db strcpy apop_opts db_name_column country apop_data d apop_query_to_data select pop country country pop population pop gdp GDP GDP GDP_per_cap pop gdp pop country gdp country apop_matrix_apply d matrix percap apop_data_show d apop_opts output_type d apop_data_print d wbtodata_output Listing Query populations GDP apop_data structure calculate GDP capita C routines Online source wbtodata Line As SQL tables special means handling row names apop_data sets row column labels You set apop_ opts db_name_ olumn column specially treated holding row names sake importing apop_data set Lines The final table columns pop GDP GDP cap query asks columns filled ones This known planning ahead difficult resize gsl_matrixes apop_data sets query table appropriate size fill column dummy data correct values C matrix Data db To C matrices database tables plain old print functions like apop_data_print apop_matrix_print Lines Listing write data table table named wbtodata_ output Say tomorrow decide prefer data dumped file change d f away gsl_stats March DATABASES Crosstabs In spreadsheetworld tables form X Y dimensions labeled case X dimension year Y dimension location x y point measurement taken year location Conversely convenient form data database columns year location statistic After write query sele tstatisti tab year separate column year Converting forms annoyance Apophenia provides functions conversions forth apop_db_to_ rosstab andapop_ rosstab_to_db Imagine data table columns height width height values like middle width takes values like left andright Then query create table anovatab select height width count ct group height width produce table looking like height width ct left right middle left middle right left right Then command apop_data anova_tab apop_db_to_crosstab anovatab height width ct anova_tab data form Left Right Up Middle Down gsl_stats March CHAPTER You print table summary use run ANOVA tests Sec tion The apop_ rosstab_to_db function goes direction online reference details Q3 Use command line program apop_db_to_ rosstab corre sponding C function data limate db database produce ta ble temperatures row year column month Import output favorite spreadsheet program Multiple databases For SQL C dot means subelement Just C stru t named person subelement namedperson height column dbname tablename olname The typical database system including mySQL SQLite begins da tabase open alias main allows attach additional databases For SQLite syntax simply atta h database newdb db asdbalias refer tables dbalias tablename form For mySQL don t need atta h command refer tables mySQL databases dbname tablename form time Aliases help retain brevity Instead db table old format column query assigns aliases db table parts clause uses alises sele t clause attach database newdb n select t1 c1 t2 c2 main firsttab t1 n othertab t2 Given attached databases main new easily copy tables create table new tablecopy select main origial Apophenia provides convenience functions apop_db_merge apop_ db_merge_table facilitate copying In memory databases faster close program want database hard drive To best worlds use memory database bulk work write database disk end program e g gsl_stats March DATABASES int main void apop_db_open NULL open db memory do_hard_math remove on_disk db apop_db_merge on_disk db remove standard C library function delete file Removing file merging prevented duplication data du plicate tables appended overwritten Open SQLite database memory apop_db_open NULL hard drive apop_db_open filename Import data apop_text_to_db If don t need output use apop_query send queries da tabase engine Use apop_query_to_ data matrix ve tor text float write query result formats MADDENING DETAILS Data clean text books faster computers noth ing help fact everybody different rules data written Here tips dealing common frustrations data importation use Appendix B offers tools Spaces column names Column names short punctu ation underscores Instead column likePer ent male treatment ases showing signs nausea brief like male_t1_moderate create documentation table describes exactly abbreviation means Not everybody follows advice creates small frustration The query sele t ent males treatment data pro duce table literal string ent males treatment repeated row far meant The solution use dot notation specify table sele t data ent males treatment gsl_stats March CHAPTER 3as males_t1 data correctly return data column alias easier use Text numbers In cases need text numeric data data set As chapter apop_data structure includes slots text numbers need specify column goes The argument apop_query_to_mixed_ data function specifier consisting letters n v m t indicating column read output apop_data s vector matrix column text column For example apop_query_to_mixed_data nmt sele t b data ounter use column row names b ounter column matrix column text elements This provides maximal flexibility requires knowing exactly query output Now text apop_data set In cases data unordered discrete data thing turn series dummy variables See page example Missing data Everybody represents missing data differently SQLite uses NULL indicate missing data Section real numbers C NAN values use facilitated GSL s GSL_NAN macro The typical input data set indicates missing value text marker like NaN NA arbitrary indicator When reading text set apop_opts db_nan regular expression matches missing data marker If unfamiliar regular expressions Appendix B tutorial For examples Apophenia s default NaN string matching NaN nan NAN strcpy apop_opts db_nan NaN Literal text strcpy apop_opts db_nan Missing Matches periods Periods special regexes need backslashes strcpy apop_opts db_nan 13Why doesn t Apophenia automatically detect type column Because stresses replicability impossible replicably guess column types One common approach stats packages look row data use cast entire column element column NAN numeric data wind text vice versa depending arbitrary rules The system search entire column text presume count text elements means entire column text error prone Next month new data set comes columns auto typed text auto typed numbers scripts written data set break Explicitly specifying types work outguessing system s attempts cleaning real world data frequently takes work gsl_stats March DATABASES The searched text entire string plus minus surrounding quota tion marks white space None match NANCY missing persons Once database NULL right place Apophenia s functions read databases gsl_matrixes apop_data C struc tures translate database NULLs floating point GSL_ NANs Mathematically operation unknown data produces unknown result need ensure data set complete mak ing estimations based data The naive approach simply delete ob servation complete Allison points naive approach known jargon listwise deletion somewhat reasonable approach espe cially reason suspect pattern missing data correlated dependent variable study Missing data covered detail page Implementing listwise deletion SQL simple given data ol1 data ol2 add data ol1 null data ol2 null clause query If numeric data summarize towhere data ol1 data ol2 null Q3 Using notes data tattoo db file query apop_ data set number tattoos number piercings political af filiation subject Make sure NaNs converted zeros point chain Print table screen apop_data_show sure correctly place Then query list po litical parties data set Hint sele t distin t Write loop run list finding mean number tattoos piercings Democrats Republicans Would person survey far tattoos anybody eliminate person outlier clause restricting tattoo count Outer join Another possibility row data entirely missing table The World Bank database includes lonely_planet table list ing number pages given country s Lonely Planet tourist guidebook Antarctica page guidebook GDP negligible population query 14Systematic relationships missingness independent variables concern gsl_stats March CHAPTER select pp gdp lonely_planet lp gdp lp country gdp country return Antarctica line corresponding line thegdp table The solution outer join includes data table plus data second table blank necessary Here join include Antarctica output The condition joining tables joinon l ountry gdp ountry appears different location norm entire left outer join clause describes single table data source select pp gdp lonely_planet lp left outer join gdp l country gdp country l country like A Q3 The query left outer join includes data left table exclude data right table As writing SQLite supports systems support right outer join include entries right table outer join include entries tables Using union keyword generate reference table country names Lonely Planet GDP tables Then use left outer joins beginning reference table produce complete data set mySQL As SQLite Apophenia supports mySQL mySQL somewhat better massive data sets work mySQL server running permission access database place Your packagemanager installing mySQL server client develop ment libraries easy mySQL s maintainers placed online comprehensive manual tutorial Once mySQL set system need changes set she shall s APOP_DB_ENGINE environment variable mysql code set apop_opts db_engine m You switch forth SQLite mySQL variable m database operations mySQL engine database operations sent 15As discussed Appendix A probably want add export APOP_DB_ENGINE mysql bashr systems mySQL gsl_stats March DATABASES SQLite engine This useful transferring data For example apop_opts db_engine m apop_db_open mysqldb apop_data d apop_query_to_data select get_me apop_opts db_engine l apop_db_open sqlitedb apop_opts output_type d print database apop_data_print d put_me SQLite s concept database single file hard drive database memory Conversely mySQL server stores databases central repository location concern end users It concept memory database As noted SQL system rules metatadata From themysql prompt query mySQL server complete list databases databases attach use dbname type mysqldbname command prompt attach dbname outset You use tables list tables current database like SQLite prompt s tables command use tables your_db tables your_db attaching Given table use olumns your_table column names your_table mySQL digresses SQL standard different manners SQLite s means digressing standard SQLite somewhat forgiving details punctuation taking equivalent double ticks single ticks equivalent mySQL de mands single single ticks After sele t reate mySQL s results need internally processed lest error commands executed order Apophe nia s functions handle processing odd effects sending string holding multiple semicolon separated queries apop_ query functions Similarly trouble begin ommit wrap pers bundle queries mySQL s internal cache management wrappers unnecessary mySQL includesmanymore functions SQL standard number additional utilities For example LOAD command read text file quickly apop_text_to_db 16Or use command line program mysqlshow things slightly pleasant format gsl_stats March CHAPTER SQL represents missing data NULL marker queries clude conditions like old null Data files use came mind mark missing data setapop_opts db_nan regular expression appropriate data If appears table like joint tables use outer join ensure names appear SOME EXAMPLES Here examples C code SQL calls neatly interact TAKING SIMULATION NOTES Say running simulation like notes state period The following code open file hard drive create table add entry period The begin commit wrapper puts data chunks elements tired waiting halt program walk away data point double sim_output apop_db_open sim db apop_table_exists results See apop_query create table results period output begin int max_periods sim_output run_sim apop_query insert results values g sim_output 1e4 apop_query commit begin apop_query commit apop_db_close The apop_table_exists command checks table exists If second argument example table deleted created anew subsequently second argument zero function simply returns answer question table exist leaves table intact It especially useful statements Every 1e4 entries system commits entered far begins new batch With SQLite systems add significant speed mySQL 17Sometimes behavior leave database unclean state If try SQLite command va uum gsl_stats March DATABASES batch management begins ommits omitted mySQL databases EASY T TESTS People East West coasts United States joke t tell difference states middle This perfect chance t test incomes North Dakota significantly different incomes South Dakota First test algorithm English code Let data set income counties North Dakota let second income counties South Dakota If n estimated mean variance actual count elements North South data sets stat N S 2N nN S nS tnN nS That given ratio t distribution nN nS degrees freedom The final step look statistic standard t tables found standard statistics textbook Of course looking data job computer instead ask GSL tailed confidence level page details double confidence gsl_cdf_tdist_Q stat nN nS If onfiden e large reject null hypothesis North South Dakotan incomes county different Otherwise isn t information confidence Listing translates process C Lines comprise queries read gsl_ve tor Both ask data clause restricting query pull North Dakotan counties clause restricting query South Dakota Lines vital statistics vectors count mean variance Given line translation Equation Finally line confidence calculation line prints percentage gsl_stats March CHAPTER include apop h int main apop_db_open data census db gsl_vector n apop_query_to_vector select in_per_capita income state select state geography North Dakota gsl_vector s apop_query_to_vector select in_per_capita income state select state geography South Dakota double n_count n size n_mean apop_vector_mean n n_var apop_vector_var n s_count s size s_mean apop_vector_mean s s_var apop_vector_var s double stat fabs n_mean s_mean sqrt n_var n_count s_var s_count double confidence gsl_cdf_tdist_Q stat n_count s_count printf Reject null g confidence n confidence Listing Are North Dakota incomes different South Dakota incomes Answering long way Online source ttest long No easier But easy Apophenia provides high level function math Listing The code identical line line calls apop_t_test function takes vectors input returns apop_data structure output listing relevant statistics Line prints entire output structure line selects single confidence statistic tailed hypothesis incomeND incomeSD DUMMY VARIABLES The ase command SQL Say data true false yes One way turn zero variable apop_data_to_dummies function matrix This works partly luck y n T F English y T map n F map zero But survey affirmative negative mapping backward intuition Then ase statement column definitions produce column binaryq affirmative zero select id case binaryq affirmative end other_vars datatable gsl_stats March DATABASES include apop h int main apop_db_open data census db gsl_vector n apop_query_to_vector select in_per_capita income state select state geography North Dakota gsl_vector s apop_query_to_vector select in_per_capita income state select state geography South Dakota apop_data t apop_t_test n s apop_data_show t output set printf n confidence g n apop_data_get_ti t conf tail value Listing Are North Dakota incomes different South Dakota incomes Online source ttest To extreme turn variable discrete dered district numbers following example series dummy variables It requires writing separate ase statement value variable s loops Again demonstration code Use apop_data_to_dummies practice Listing creates series dummy variables technique On lines build_a_query function queries list districts Then query writes select statement line ase State state_ state_name Line uses obfuscatory page print comma items end sele t clause Line pulls data massive query line runs OLS regression returned data You set apop_opts verbose head main function dis play query executes Lines parameter estimates suppress gigantic variance covariance matrix Note loop starting line goes When including dummy variables exclude baseline value pre vent X singular excluding means Alabama baseline Q Rewrite loop use state baseline Or set loop run range zero end array watch disaster befall analysis gsl_stats March CHAPTER include apop h char build_a_query char q NULL apop_data state apop_query_to_text select Name state State id geography sumlevel asprintf q select in_per_capita income int state textsize asprintf q s case state s end s c n q state text state text state textsize asprintf q s income n q return q int main apop_db_open data census db apop_data d apop_query_to_data build_a_query apop_model e apop_estimate d apop_ols e covariance NULL don t apop_model_show e Listing A sample loop creates SQL creates dummy variables Online source statedummies There standard loops assigning variables matrix style manipulation SQL need things C analysis Functions exist transfer data databases matrices incorporate database queries directly C code gsl_stats March MATRICES AND MODELS My freedom consists moving narrow frame I signed undertakings Whatever diminishes constraint diminishes strength The constraints imposes frees s self chains shackle spirit Stravinsky p Recall C language provides basic basics addition division provided library So data oriented mathematics need library handle matrices vectors There available book uses GNU Scientific Library GSL The GSL recommended actively supported work platforms C Beyond functions useful statistics includes functions useful engineering physics book mention The reference documentation readily available online book form Gough Also book co evolvedwith Apophenia library builds GSL statistics oriented work This chapter goes basics dealing GSL s matrices vectors Although insisting matrices vectors specific rigid form constraint constraint makes productive work possible The predictable form structures makes easy write functions allocate fill multiply invert convert gsl_stats March CHAPTER THE GSL S MATRICES AND VECTORS Quick s times Thanks calculators bit rusty multiplication Listing produces multiplication table include apop h int main gsl_matrix m gsl_matrix_alloc gsl_matrix_set_all m int m size1 Apop_matrix_row m one_row gsl_vector_scale one_row int m size2 Apop_matrix_col m one_col gsl_vector_scale one_col apop_matrix_show m gsl_matrix_free m Listing Allocate matrix multiply row column different value produce multiplication table Online source multipli ationtable The matrix allocated introductory section line It surprise allo giving indication memory allocated matrix In case matrix rows columns Row comes Column like order Roman Catholic Randy Choirboy RC Cola Line matrix level operation set element matrix The rest file works row column time The loop lines begins Apop_matrix_row macro pull single row puts vector named one_row Given vector one_row line multiplies element When happens columns line multiplication table Line displays constructed matrix screen Line frees matrix The system automatically frees matrices end program Some consider good style free matrices allocated memory consider freeing end main waste time 1Due magic discussed vectors allocated Apop_matrix_row _ old exist need freed gsl_stats March MATRICES AND MODELS Naming conventions Every function GSL library begin gsl_ argument functions object acted Most GSL functions affect matrix begin gsl_ matrix_ operate vectors begin gsl_ve tor_ The libraries book stick standard Apophenia s functions begin apop_ great majority begin data type apop_data_ apop_model_ GLib s functions begin g_ obje t g_tree_ g_list_ et cetera This custom important C general purpose language design ers library idea libraries authors calling program If libraries function named data_allo break C s library loaded matrix vector operations clearly verbose dundant comparable operations languages purpose built matrix manipulation But C s syntax provide advantages notably verbose redundant As discussion debugging strategy page spacing operations debugging numerical algorithms painful When type function clue function function correctly The authors Mathematica package chose use abbreviations answer question applies The answer consistency There general convention function names spelled English words standard mathematical abbreviation The great advantage scheme predictable Once know function usually able guess exactly If names abbreviated remember shortening standard English words Wolfram p The naming convention makes indices helpful For example index GSL s online reference gives complete list functions operate vectors alphabetized gsl_ve tor_ index book gives partial list useful functions 2There awkward detail naming scheme functions Apophenia library act gsl_ matrixes gsl_ve tors Those names beginning apop_matrix apop_ve tor compromis ing library main input gsl_stats March CHAPTER Q4 Don t delay look gsl_ve tor_ gsl_matrix_ sections index book GSL s online reference skim sort operations The Apophenia package num ber higher level operations worth getting know look apop_ve tor_ apop_matrix_ apop_data_ sections If find naming scheme verbose write wrap functions require typing For example write file my_ onvenien e_fns include void mset gsl_matrix m int row int col double data gsl_matrix_set m row col data void vset gsl_vector v int row double data gsl_vector_set v row data You need header file my_ onvenien e_fns h include gsl gsl_matrix h include gsl gsl_vector h void mset gsl_matrix m int row int col double data void vset gsl_vector v int row double data define VECTOR_ALLOC vname length gsl_vector vname gsl_vector_alloc length For simple functions rename define page define vget v row gsl_vector_get v row define mget m row col gsl_matrix_get m row col After throwing lude my_ onvenien e_fns h pro gram able use abbreviated syntax vget v It s sthetic code legible changes But option find function s form annoying write pleasant wrapper function personal library hides annoying parts BASIC MATRIX AND VECTOR OPERATIONS The simplest operations matrices vectors element element operations adding elements matrix The GSL provides functions expect things Each modifies argument gsl_stats March MATRICES AND MODELS gsl_matrix_add b aij aij bij j gsl_matrix_sub b aij aij bij j gsl_matrix_mul_elements b aij aij bij j gsl_matrix_div_elements b aij aij bij j gsl_matrix_scale x aij aij x j N x R gsl_matrix_add_constant x aij aij x j N x R gsl_vector_add b ai ai bi gsl_vector_sub b ai ai bij gsl_vector_mul b ai ai bi gsl_vector_div b ai ai bi gsl_vector_scale x ai ai x N x R gsl_vector_add_constant x ai ai x N x R apop_vector_log ai ln ai apop_vector_log10 ai log10 ai apop_vector_exp ai e ai The functions multiply divide matrix elements given slightly lengthier names minimize potential confused process mul tiplying matrix matrix AB inverse AB Those operations require functions computational firepower introduced Q4 Rewrite structured birthday paradox program page agsl_matrix instead stru t currently uses allo allo matrix main pass functions Replace lude directives apop h Replace title printing line print_days withapop_matrix_show data_matrix Put gsl_matrix_set commands loop al ulate_days set number people likelihood match ing likelihood match opposed minus likelihood bdayfns Apply map Beyond simple operations doubt want transform data creative ways For example func tion Listing double indicating taxable income return US income taxes owed assuming head household dependents tak ing standard deduction Internal Revenue Service This function applied vector incomes produce vector taxes owed gsl_stats March CHAPTER include apop h double calc_taxes double income double cutoffs INFINITY double rates double tax int bracket income Head household standard deduction income exemption self plus dependents income tax rates bracket GSL_MIN income cutoffs bracket cutoffs bracket income cutoffs bracket bracket return tax int main apop_db_open data census db strncpy apop_opts db_name_column geo_name apop_data d apop_query_to_data select geo_name Household_median_in income income sumlevel order household_median_in desc Apop_col_t d income income_vector d vector apop_vector_map income_vector calc_taxes apop_name_add d names tax owed v apop_data_show d Listing Read median income US state find taxes family median owe Online source taxes Lines Listing demonstrate use apop_data structure explained detail For suffices know line produces gsl_ve tor named ome_ve tor holding median household income state The bulk program specification tax rates al _taxes function In exercise page plot function The program bother find length arrays declared lines The utoffs array final value guarantees loop lines exit point Similarly add final value like NULL NAN3 end list rewrite loop s header tofor int data NAN This means remember sentinel value end list need remember fix counter time fix array 3NAN read number introduced introduced page gsl_stats March MATRICES AND MODELS Threading Even low end laptops ship processors capable simultaneously operating stacks frames map apply functions split work multiple processors Setapop_opts thread_ ount desired number threads probably number processor cores system functions apportion work processors appropriately When threading careful writing global vari ables thousand threads modifying global variable order outcome likely un defined When writing functions threading best bet variables passed explicitly read You write loop apply al _tax function ele ment income vector turn But apop_ve tor_map function Let c al _taxes function ome_ve tor apop_ve tor_map line turns c assigned vector element data set d ve tor Line displays output But apop_ve tor_map beginning Apophenia provides small family functions map apply function data set The index functions relegated manual pages list examples idea You saw apop_ve tor_map ome_ve tor al _taxes gsl_ve tor returns vector Or apop_ve tor_apply ome_ ve tor al _taxes replace element ome_ve tor al _taxes element One sees functions header like double log_likelihood gsl_ ve tor indata takes data vector returns log likelihood Then row dataset vector representing separate observation apop_ matrix_map dataset log_likelihood return vector log likeli hoods observation Functions _map_ _sum like apop_matrix_map_all_sum return sum f item item matrix vector For example apop_ matrix_map_all_sum m gsl_isnan return total number elements m NAN Continuing log likelihood example apop_ matrix_map_sum dataset log_likelihood total log likeli hood rows Another example family appeared earlier Listing page usedapop_matrix_apply generate vector GDP capita matrix GDP population You express matrices vectors gsl_matrix gsl_ ve tor structures Refer elements gsl_matrix_set gsl_matrix_get similarly apop_data sets gsl_ve tors gsl_stats March CHAPTER v c0 c1 c2 t0 t1 t2 r0 r1 r2 Figure The vector column matrix text gets numbering system Row names shared elements Once data set forms operate matrix vector functions like gsl_matrix_add b orgsl_ve tor_s ale x Use apop_ matrix ve tor _ map apply family functions send row vector matrix function turn apop_data The apop_data structure joining data types gsl_ve tor gsl_matrix table strings anapop_name structure The conceptual layout given Figure The vector columns matrix columns text named Also rows named set row names presumption row structure holds information single observation Think vector 1st element matrix text elements having addresses There means creating apop_data set including apop_query_ to_data apop_matrix_to_data apop_ve tor_to_data creating blank slate apop_data_allo For example Listing apop_query_to_data read table anapop_data set setting apop_opts db_name_ olumn option col umn line query set row names data Line sets ve tor element data set line adds element ve tor slot names element set You easily operate subelements structure If matrix_man ipulate function requires gsl_matrix your_data apop_data struc ture matrix_manipulate your_data matrix Similarly gsl_stats March MATRICES AND MODELS manipulate names table text data directly The size text data stored textsize element Sample usage apop_data set apop_query_to_text int r r set textsize r int c c set textsize c printf s t set text r c printf n There consistency checking sure number row names theve tor size matrix size1 equal If want vector elements matrix structure columns free In fact typical case elements assigned values If ve tor_size zero apop_data newdata_m apop_data_alloc vector_size n_rows n_cols initialize elements newdata NULL produce n_rows n_ ols matrix set names Alternatively n_rows ve tor_ size positive vector element initialized matrix set NULL Get set point You use GSL tools dissect gsl_ matrix element apop_data struct similarly theve tor element In addition suite functions setting getting element apop_data set names Let t title numeric index refer row column coordinate t t t t form apop_data_get your_data j apop_data_get_ti your_data rowname j apop_data_get_it your_data colname apop_data_get_tt your_data rowname colname apop_data_set your_data j new_value apop_data_set_ti your_data rowname j new_value apop_data_ptr your_data j apop_data_ptr_ti your_data rowname j 4Seasoned C programmers recognize usage similar union gsl_ve tor gsl_ matrix har array apop_data set hold simultaneously C programmers observe structure allows form polymorphism write function takes anapop_data input operates gsl_ve tor gsl_matrix depending notNULL input gsl_stats March CHAPTER The apop_data_ptr form returns pointer given data point read write increment et cetera It mimics gsl_matrix_ptr gsl_ve tor_ptr functions thing respective data structures As think vector 1st element matrix example apop_data_set_ti your_data rowname operates apop_data structure s vector matrix This facilitates forms likefor int data matrix size2 runs entire row including vector matrix These functions use case insensitive regular expression matching find right imprecise column request Appendix B dis cusses regular expressions greater detail suffices know approximate p val match P value p val andp values For example flip ttest listed page Line showed output t test list named elements meaning output set s rownames ve tor elements Line pulled single named element vector Forming partitioned matrices You copy entire data set stack data matrices stack rows stack data matrices right stack columns stack data vectors apop_data newcopy apop_data_copy oldset apop_data newcopy_tall apop_data_stack oldset_one oldset_two r apop_data newcopy_wide apop_data_stack oldset_one oldset_two c apop_data newcopy_vector apop_data_stack oldset_one oldset_two v Again generally better data manipulation database If tables database instead apop_data sets vertical horizontal stacking commands equivalent select oldset_one union select oldset_two select t1 t2 oldset_one t1 oldset_two t2 gsl_stats March MATRICES AND MODELS Q4 The output exercise page table tattoos piercings political affiliation Run Probit regression determine political affiliation affects count piercings The function apop_data_to_dummies produce new data ma trix column category Stack matrix right original table Send augmented table apop_probit estimate function The output categorical variables indicates effect relative omitted category Encapsulate routine function code wrote function takes data text column number returns augmented data set dummy variables The apop_data structure combines vector matrix text array names elements You pull named items data set estimation apop_data_get_ti family SHUNTING DATA Igor Stravinsky advocated constraints head chapter points Rigidity slightly yields like Justice swayed mercy beauty earth None function point sense operate specific structure like gsl_matrix gsl_ve tor coding easier flexibility easily switching constrained forms To end section presents suggestions converting data formats book It exciting read prefer section reference use necessary Table provides key method appropriate given con version From pairs marked dot left exercise reader particularly difficult require going format example double apop_data set double gsl_matrix apop_data As proven steps format 5Stravinsky p citing GK Chesterton The furrows Alarms discursions gsl_stats March CHAPTER F ro m To Te xt fil e D b ta bl edouble gsl_ve torgsl_matrixapop_data Text file C F F Db table Q Q Q Qdouble C F F gsl_ve tor P P F C F Fgsl_matrix P P F V C Fapop_data P P F S S C Table A key methods conversion Copying structures The computer quickly copy blocks bother ing comprehend data contains function memmove safe variant mem py For example borrowing omplex structure Chapter complex real imaginary complex second memmove second sizeof complex The computer location blindly copy finds location se ond size omplex struct Since se ond identical data constituent parts guaranteed identical But small caveat element stru t pointer pointer copied data memory For example gsl_ve tor includes data pointer memmove result identical structs point data If want use view Method V want copy need memmove base gsl_ve tor data array This sets stage series functions mem py modeled C s basic memmove mem py functions handle internal pointers correctly Method C Copying The gsl_ _mem py functions assume destination copying allocated al lows reuse space carefully oversee memory The 6How remember order arguments computer scientists think terms data flowing left right C dest sour e R dest sour e pseudocode dest sour e Similarly copying functions data flow end line beginning memmove dest sour e gsl_stats March MATRICES AND MODELS 125apop_ _ opy functions allocate copy step declare copy line easily embed copy filtering operation Text file Text file Just use system s file copy command The apop_system function acts like standard C system command accepts printf style arguments apop_system cp s s from_file_name to_file_name gsl_ve tor gsl_ve tor gsl_vector copy gsl_vector_alloc original size gsl_vector_memcpy copy original gsl_vector copy2 apop_vector_copy original double double Let original_size length original array double copy1 malloc sizeof double original_size memmove copy1 original sizeof double original_size double copy2 original_size memmove copy2 original sizeof original gsl_matrix gsl_matrix gsl_matrix copy gsl_matrix_alloc original size1 original size2 gsl_matrix_memcpy copy original gsl_matrix copy2 apop_matrix_copy original apop_data apop_data apop_data copy1 apop_data_alloc original vector size original matrix size1 original matrix size2 apop_data_memcpy copy1 original apop_data copy2 apop_data_copy original Method F Function These functions designed convert format There ways express matrix doubles The analog pointer declare list pointers pointers analog automatically allocated array use double subscripts double method_one malloc sizeof double size_1 int size_1 method_one malloc sizeof double size_2 double method_two size_1 size_2 The method inconvenient The second method convenient let us allocate matrix But minuti 7The sizeof function types send array element tosizeof If original array doubles sizeof original sizeof double whilesizeof original sizeof double use sizeof original argument formemmove However incredibly error prone places C send object pointer object function warning error In cases modest complexity difference array element easy confuse hard debug gsl_stats March CHAPTER discussed Kernighan Ritchie p method hassle worth Instead declare data single line listing entire row sec ond et cetera intervening brackets Then use apop_line func tions convert matrix For example page text db table The number states file row names second column names Finally colnames present provide argument har apop_text_to_db original txt tablename NULL text apop_data apop_data copyd apop_text_to_data original txt double gsl_ve tor gsl_matrix double original gsl_vector copv apop_array_to_vector original original_size gsl_matrix copm apop_array_to_matrix original original_size1 original_size2 double gsl_matrix double original int orig_vsize orig_size1 orig_size2 gsl_matrix copym apop_line_to_matrix original orig_size1 orig_size2 double apop_data apop_data copyd apop_line_to_data original orig_vsize orig_size1 orig_size2 gsl_ve tor double double copyd apop_vector_to_array original_vec gsl_ve tor n gsl_matrix gsl_matrix copym apop_vector_to_matrix original_vec gsl_ve tor gsl_matrix apop_data apop_data copydv apop_vector_to_data original_vec apop_data copydm apop_matrix_to_data original_matrix Method P Printing Apophenia s printing functions actually func tions dump data screen file database system pipe Appendix B overview pipes Early putting analysis want print results screen later want save temporary results database month colleague ask text file output major changes output changing character code The choices apop_opts output_type variable apop_opts output_type s default print screen apop_opts output_type f print file apop_opts output_type d store database table apop_opts output_type p write pipe apop_opts output_pipe gsl_stats March MATRICES AND MODELS The screen output generally human readable meaning different column sizes notes conveniences terminal understand going The file output generally oriented allowing machine read output meaning stricter formatting The second argument output functions string Output screen pipe ignores outputting file file writing database table gsl_ve tor gsl_matrix apop_data text file apop_opts output_type t apop_vector_print original_vector text_file_copy apop_matrix_print original_matrix text_file_copy apop_data_print original_data text_file_copy gsl_ve tor gsl_matrix apop_data db table apop_opts output_type d apop_vector_print original_vector db_copy apop_matrix_print original_matrix db_copy apop_data_print original_data db_copy Method Q Querying The way data database query db table db table apop_query create table copy select original db table double gsl_ve tor gsl_matrix apop_data double d apop_query_to_float select value original gsl_vector v apop_query_to_vector select original gsl_matrix m apop_query_to_matrix select original apop_data d apop_query_to_data select original Method S Subelements Sometimes function overkill pull subelement main data item Notice way data subelement gsl_ve tor necessarily copied directly double stride wrong Section details Instead use copying functions Method F apop_data gsl_matrix gsl_ve tor my_data_set matrix my_data_set vector 8File names tend periods periods table names produce difficulties When printing database file dots stripped sv table out_put gsl_stats March CHAPTER Method V Views Pointers reasonably easy natural look subsets matrix Do want matrix representsXwith row lopped Then set matrix data pointer points second row Since new matrix pointing data original changes affect matrices want copy submatrix s data new location However easy finding second row pointing gsl_matrix includes information data e metadata number rows columns Thus macros help pull row column submatrix larger matrix For example m agsl_matrix Apop_matrix_row m row_v Apop_matrix_col m col_v Apop_submatrix m submatrix produce gsl_ve tor named row_v holding row named ol_v holding fifth column gsl_matrix named submatrix th element original For apop_data set names disposal use Apop_row m row_v Apop_ old m ol_v pull given vectors matrix element apop_data structure row col umn number Apop_row_t m fourth row row_v Apop_ ol_t m sixth olumn ol_v pull rows columns titles The macros work bit magic internally declare automatically allocatedgsl_matrix ve tor requisite metadata declare pointer selected like pointer matrix vector However macros automatically allocated memory need free matrix vector generated macro Thus provide quick disposable view portion matrix If need permanent record copy view regular vector matrix methods prior pages e g gsl_ve tor permanent_ opy apop_ ve tor_ opy temp_view 9These macros based GSL functions slightly convenient For example gsl_ve tor v gsl_matrix_ old a_matrix ve tor apop_ve tor_show v If macro misbehaving macros fall form gsl_stats March MATRICES AND MODELS LINEAR ALGEBRA Say transition matrix showing system row state column state For example Figure transition matrix showing formats converted formats Omitting labels marking transition dot Figure zero following transition matrix Listing shows brief program read data set text file dot product t display result include apop h int main apop_data t apop_text_to_data data markov apop_data apop_dot t t apop_data_show Listing Two transitions transition matrix Online source markov Before discussing syntax apop_dot detail program s output This tells example ways transition state second steps verify The apop_dot function takes arguments apop_data structures flag matrix indicating t transpose matrix v use vector element use matrix For example X matrix gsl_stats March CHAPTER apop_dot X X t find X X function takes dot product X version transposed second If data set matrix component dot product matrix element NULL ve tor component There exactly transposition flags matrices If element vector taken row second element vector column In cases element matrix need flag indicate use apop_data set sve tor element v use transposed matrix t use matrix written character If elements vectors probably better usinggsl_blas_ddot use apop_dot output anapop_data set vector element length Q4 The quadratic form X YX appears frequently statistical work Write function header apop_data quadrati _form apop_ data x apop_data y takes gsl_matrixes returns quadratic form Be sure check y square dimension x size1 Vector vector Given vectors x y gsl_blas_ddot returns x1y1 x2y2 xnyn Rather outputting value x y function s return value takes location double places output E g ifx y gsl_ve tor s use double dotproduct gsl_blas_ddot x y dotproduct 10Why tell computer transpose Some feel send indicate transposition meant vice versa system able determine Say vector multiply data sets second You write simple loop int apop_dot data v At smart system realizes committed faux pas matrix dot column vector works transposition So corrects telling data Withdata transposition works rows columns So correct Good luck catching debugging gsl_stats March MATRICES AND MODELS Q4 Write table displaying sum squares n2 n Write function takes n allocates gsl_ve tor size n fills vector n calculates returns v v gsl_blas_ddot finally frees v Write loop n calls function prints n returned value Verify work printing n n 2n alongside calculation sum squares n An example Cook s distance Cook s distance estimate data point affects regression Cook The formula Ci j y r j y rij p MSE p number parameters MSE mean squared error overall regression y rj jth element predicted value y based overall regression y rij jth element predicted value y based regres sion excluding data point That find Cook s distance data points need separate regression data sets excluding different data point The formula simply quantifies predictions main regression change significantly excluding given data point The procedure provides good opportunity matrix shunting linear algebra need functions produce subsets functions calculate y X find squared differences MSE The function Listing It produces series data sets row missing The function named jackknife procedure uses delete loop calculating covariances correcting bias Lines use submatrix produce view main matrix starting 11The Jackknife discussed book online documentation Apophenia s apop_ ja kknife_ ov gsl_stats March CHAPTER include apop h typedef double math_fn apop_data gsl_vector jack_iteration gsl_matrix m math_fn do_math int height m size1 gsl_vector gsl_vector_alloc height apop_data reduced apop_data_alloc height m size2 APOP_SUBMATRIX m height m size2 mv gsl_matrix_memcpy reduced matrix mv int height gsl_vector_set do_math reduced height APOP_MATRIX_ROW m onerow gsl_matrix_set_row reduced matrix onerow return Listing Iteratively produce size1 submatrices omitted row data Online source ja kiteration position size m size1 m size2 original matrix row missing uses gsl_matrix_mem py copy new matrix The loop repeats view copy procedure row row It begins row zero omitted overwrites row zero copy aka row original It copies original row overwriting copy original row end matrix Line calls function sent argument See page notes writing functions functions inputs including meaning typedef line Now matrix shunting way Listing provides addi tional functions linear alegbra The sum_squared_diff function calculates Li Ri The line finds L R second line applies function gsl_pow_2 element L R squares element returns post squaring sum The proje t function taking dot product yest X By giving single 12The GSL provides efficient power calculators gsl_pow_2 gsl_pow_9 catch functiongsl_pow_int value exponent raise value integer exponent efficient manner general purpose pow gsl_stats March MATRICES AND MODELS include apop h typedef double math_fn apop_data gsl_vector jack_iteration gsl_matrix math_fn apop_data ols_data gsl_vector predicted double p_dot_mse double sum_squared_diff gsl_vector left gsl_vector right gsl_vector_sub left right destroys left vector return apop_vector_map_sum left gsl_pow_2 gsl_vector project apop_data d apop_model m return apop_dot d m parameters v vector double cook_math apop_data reduced apop_model r apop_estimate reduced apop_ols double sum_squared_diff project ols_data r predicted p_dot_mse apop_model_free r return gsl_vector cooks_distance apop_model apop_data c apop_data_copy data apop_ols prep data ols_data data predicted project data p_dot_mse c matrix size2 sum_squared_diff data vector predicted return jack_iteration c matrix cook_math int main apop_data dataset apop_text_to_data data regressme apop_model est apop_estimate dataset apop_ols printf plot n strcpy apop_opts output_delimiter n apop_vector_show cooks_distance est Listing Calcluate Cook s distance running regressions Compile withja kiteration Online source ooks line function hide details self documenting indication code s intent The ook_math function calculates Equation value It called directly passed ja k_iteration apply function gsl_stats March CHAPTER submatrices The ooks_distan e function produces copies data set untainted copy regression style version dependent variable ve tor element data set column matrix ones After main calls ooks_distan e calls linear algebra proce dures ja k_iteration calls ook_math submatrix list Cook s distance point set use search outliers The main function produces Gnuplot ready output run e g ook gnuplot persist Some researchers prefer sort data points plot ting e try sending output vector gsl_ve tor_sort plotting Q4 Add bad data points data regressme file like sim ulate outliers erroneous data Does Cook s distance bad data stand especially large MATRIX INVERSION AND EQUATION SOLVING Inverting matrix requires significantly computa tion element element operations But modern day big deal old laptop invert matrix seconds version step typical OLS regression matrix blink eye Apophenia provides functions find determinants inverses GSL BLAS s triangular decomposition functions named apop_matrix_inverse apop_matrix_determinant apop_det_and_inv Exam ples function located book e g calcula tion OLS coefficients page Sometimes bother inversion For example write OLS parameters X X X Y implement solving X X X Y involves inversion If xpx matrix X X xpy vector X Y gsl_linalg_HH_solve xpx xpy betav return vector apop_data apop_data apop_dot Vector vector gsl_blas_ddot Inversion apop_matrix_inverse apop_matrix_determinant orapop_det_and_inv gsl_stats March MATRICES AND MODELS NUMBERS Floating point numbers special values important INFINITY INFINITY NAN Data oriented readers interested NAN read number appropriate way represent missing data Listing shows necessary vo cabulary All statements true print associated statements include math h NaN handlers include stdio h printf int main doublemissing_data NAN double big_number INFINITY double negative_big_number INFINITY isnan missing_data printf missing_data missing data point n isfinite big_number printf big_number finite n isfinite missing_data printf missing_data isn t finite n isinf negative_big_number printf negative_big_number negative infinity n Listing Some functions test infinity NaNs Online source notanumber Because floating point numbers values division zero won t crash program Assigning double d result d INFINITY d result d set NAN However integers luxuries try int way ofArithmeti ex eption ore dumped Comparison NAN value fails double blank NAN blank NAN This evaluates false blank blank This evaluates false isnan blank Returns correct way check NaN value 13Pedantic note standards These values defined C99 standard machines support IEEE IEC floating point standards standards respectively taken given best knowledge current hardware supports INFINITY NAN Recall g requires std gnu99 C99 features GSL provides GSL_POSINF GSL_NEGINF GSL_NAN work non C99 non IEEE systems gsl_stats March CHAPTER GSL constant approx GSL constant approx e M_E M_PI log2 e M_LOG2E M_PI_2 log10 e M_LOG10E M_PI_4 ln M_LN2 M_1_PI ln M_LN10 M_2_PI M_SQRT2 M_SQRTPI M_SQRT1_2 M_2_SQRTPI M_SQRT3 ln M_LNPI Euler constant M_EULER Table The GSL defines number useful constants Predefined constants There number useful constants defined GSL preprocessor defines listed Table It generally better form use constants descriptive raw decimal values defined greater precision want type PRECISION As finite means writing real numbers roundoff er ror computer s internal representation numbers The computer basically stores non integer numbers scientific notation For forgotten notation written 14159e0 14159e2 Numbers exactly digit decimal point exponent chosen ensure case Your computer works binary floating point numbers type float anddouble form d 2n d string ones zeros n exponent The scale number overall magnitude expressed exponent n The floating point system express wide range scales equal ease easy express picometers 3e million kilometers 3e9 The precision significant digits information held d 14e 14e9 significant decimal digits information There fixed space d space exceeded n adjusted suit change probably means loss precision d To small base 14The GSL gets constants BSD UNIX able find GSL available The exceptions M_SQRT3 M_SQRTPI GSL specific 15This oversimplifies details basically irrelevant users For example digit d computer normally doesn t bother storing gsl_stats March MATRICES AND MODELS 2i 26765e 88861e 60694e 22302e 03704e 90909e 58225e 87259e 27339e 05494e 14952e 40992e 26014e 90109e 66801e 4997e 45271e 18305e 07151e 33264e inf inf Table Multiplying large columns numbers eventually fail example space d digits 89e0 892e0 525e1 The final truncated zero fit d given space Precision easily lost manner lost regained One general rule thumb implied writing precision sensitive function act float use double variables internally taking doubles use long double internally The loss precision especially acute multiplying long list numbers This discussed chapter maximum likeli hood estimation page likelihood function involves exactly multiplication Say column thousand values half Then product thousand elements strains adouble represent Table shows table powers represented double For modest number data points double throws towel calls 2i These referred overflow error underflow error respectively For like try home Listing shows code produce table repeats experiment long double doesn t doublings halvings gsl_stats March CHAPTER include math h include stdio h int main printf Powers held double n int printf t g t g n ldexp ldexp printf Powers held long double n int printf t Lg t Lg n ldexpl ldexpl Listing Find computer s representation 2i large Online source powersoftwo The program uses ldexp family functions manipulate floating point representation number directly probably bad form The printf format specifier long double type Lg The solution problem finding product large number elements calculate log product product page If need calculate million decimal points need find library work numbers arbitrary precision Such libraries typi cally work representing numbers data structure listing ones place tens place hundreds place extending list direction necessary Another alternative rational arithmetic leaves numbers int int form long possible Either system need provide add subtract multiply divide routines act data structures C s built operators Unfortunately added layer complexity means arithmetic operations fast procedures implemented special purpose registers processor hardware long series library calls So math efficiently large matrices stuck finite precision rely heavily numbers maybe significant digits For purposes estimating testing parameters model real world data OK If numbers differ significant dig versus rarely reason numbers significantly different Even hypothesis test indicates different difficult convince referee 16The g Lg format specifiers round large values change f Lf precise value calculations exponential notation gsl_stats March MATRICES AND MODELS CONDITIONING Most matrix routines badly determinant near zero eigenvalues different orders magnitude One way because problems data column order order In finite precision arithmetic numbers wide range smaller number simply swallowed 14e10 92e 14e10 Thus try ensure column data approximately order magnitude calculations Say theory mean fingernail thickness influenced location s population You modify scale pulling data database select population nail_thickness health_data modify gsl_matrix APOP_COL data pop gsl_vector_scale pop APOP_COL data nails gsl_vector_scale nails These notes conditioning C specific Any mathematics package hopes work efficiently large matrices use finite precision arithmetic problems I shall conditioned data matrices For muchmore precision issues standardmachine representation num bers Goldberg COMPARISON Floating point numbers exact representations real number probability zero Simply bit fuzz expect number little bit It normally problem 1e fuzz safely ignored Polhill et al point fuzziness numbers problem comparisons problems create odd effects simu lations agent based models After long series floating point operations comparison form t probably work For example Listing calculates standard IEEE arithmetic finds zero There labor intensive solutions problem like long ints 17This example web site affiliated authors paper http www ma aulay uk fearlus floating point gsl_stats March CHAPTER include stdio h int main double t t t t t printf By IEEE floating point standard n Listing The IEEE standard imply Online source fuzz sensible solution bear mind comparison precise example agents die wealth zero maybe 1e Otherwise model robust agents iota negative wealth Floating point numbers values INFINITY INFINITY NAN Multiplying column thousand numbers break log product summing logs column s ele ments Reporting results based fifth significant digit spuri ous Try scale variables factor thousand Exact comparisons floating point numbers fail testf fabs f 1e gsl_matrix AND gsl_ve tor INTERNALS First warning intent sec tion circum vent GSL s access functions asgsl_matrix_get gsl_ve tor_set Doing bad form inviting errors making code difficult read Instead notes useful 18Recall b b b exactly long int b b6 19If concerned overhead functions define GSL_RANGE_CHECK_ OFF preprocessor directive adding DGSL_RANGE_CHECK_OFF compilation com gsl_stats March MATRICES AND MODELS understanding data structures way giving han dle operations easy operations difficult Here relevant section declaration gsl_matrix structure typedef struct size_t size1 size_t size2 size_t tda double data int owner gsl_matrix Figure Within submatrix element step element steps element As know size1 size2 simply count rows columns Thedata pointer single pointer stream numbers Since memory addresses linear Figure closer actually memory row data followed immediately afterward second row row forming long row data By adding line breaks humans think long row data actually grid like second half Figure Stepping row means simply stepping sizeof double units stepping matrix column means stepping sizeof double size2 steps current element For example reach element matrix processor skip rows skip items jump sizeof double steps base element Modern computers proactive data gathering When read data slower types memory check neighbors If code rel atively predictable system gather bit data time mand line Between compiler s optimization routines function reduce appropriate array operation gsl_stats March CHAPTER crunching current data element The gsl_matrix structure works won derfully system steps predictable fixed size processor good chance correctly guessing data faster caches Now matrix following infor mation size1 size2 tda data location owner With tda equal size2 jumping column require jump size double tda If wanted pull submatrix begins resulting submatrix data look like size1 size2 tda data location original matrix owner We use matrix exactly matrix For example elements row step sizeof double forward base element pointed data beginning column jump sizeof double tda Thus process pulling subset data merely required finding point writing arbitrary limits size1 size2 No actual data copied This gsl_matrix_row APOP_ROW APOP_COL APOP_SUBMATRIX routines work The owner variable important multiple sub matrices pointing data Since submatrix owner data GSL allow free data set points The gsl_ve tor similar structure including starting point stride indicate far jump element Thus taking row column subset matrix merely requires writing correct coordinates lengths The GSL s structures good fast access element 20The abbreviation tda stands trailing dimension array For gsl_ve tor analogous element stride gsl_stats March MATRICES AND MODELS fixed jump relative current element traversing rows columns They exceptionally good describing submatrices subvectors merely requires writing new coordinates They handle matrix easily matrix matrix They good non contiguous subsets like second fifth columns data set relative jumps column identical Similarly good holding types data jumps sizeof int sizeof double Also space allocated element way efficiently represent sparse matrices Systems deal variable sized jumps pros cons reversed For example solution let overall table list column vectors column type But traversing row involve jumping memory common operations like finding sum row significantly slower operation Other designers different goals different means representing data matrix gsl_matrix means best needs But goal allowing hardware process rows columns homogeneous data maximal efficiency The GSL s matrix vector structures good efficient computation element fixed size fixed distance neighboring elements It easy contiguous subvectors submatrices structures Doing requires copying bits metadata data There simple way non contiguous subsets gsl_ matri es gsl_ve tors You need copy data manually e loop manipulations da tabase data gsl_matrix form MODELS Recall sentence summary statistical analysis page introduction estimate parameters model data The Apophenia library provides functions data structures exactly level abstraction form apop_model apop_data structures functions operate gsl_stats March CHAPTER You met apop_data structure lent hand operations matrix algebra layer abstraction remainder chapter introduces apop_model structure provides similar forms strength con straint encapsulates model information uniform manner allows models interchangeably functions model input al lows sensible defaults filled necessary A great deal statistical work consists converting combining existing models form new ones That models filtered produce models data filtered provide new information We read estimation filtering un parameterized model parameterized Bayesian updating discussed thoroughly page takes prior model likelihood function data outputs new model input round filtering new data comes Another example discussed imposition constraint begin esti mating general model generate new model constraint imposed parameters estimate The difference log likelihoods constrained unconstrained models hypothesis testing The structure model struct In usage book model intermedi ates data parameters From model directions X Given data estimate parameters ii X Given parameters generate artificial data e g random draws model find expected value iii X p Given data parameters estimate likelihood probability To examples form descriptive problem estimating covariance OLS parameters Monte Carlo methods use form ii producing million draws model given fixed parameters Bayesian estimation based form iii describing posterior probability given data param eters likelihoods maximum likelihood estimation For common models apop_models written including dis tributions like Normal Multivariate Normal Gamma Zipf et cetera gen eralized linear models like OLS WLS probit logit Because standardized form sent model handling functions applied data sequence For example fit data Gamma Lognormal Exponential distribution compare outcomes exercise page gsl_stats March MATRICES AND MODELS Every model estimated form apop_model est apop_estimate data apop_normal Examples form appear book look code later section pages example Discussion directions making random draws data given parame ters finding likelihoods given data parameters delayed chapters Monte Carlo methods maximum likelihood estimation respec tively Changing defaults A complete model includes model s functions environment functions evaluated Gentleman Ihaka The apop_model includes outputs functions need replicate For purposes Apophenia s model estimations N separate model anN maximum likelihood model optimization step conjugate gradient method separate identical model esti mated simplex algorithm But generic stru t indended hold settings models faces complica tion different methods estimation require different settings The choice conjugate gradient simplex algorithm meaningless instrumental vari able regression list instrumental variables makes sense maxi mum likelihood search Apohenia standard apop_model struct open space attaching differ ent groups settings needed If model s defaults need tweaking add MLE OLS histogram settings group change details need changing group Again examples usage syntax step processs abound online documentation book pages Writing It nice specify model single form leave computer work best way implement directions head section far computational nirvana Recall example OLS pages Chapter The form model find value y X minimized gave hint correct form direction X X 1X y Other models probit model elaborated Chapter begin similar X type forms closed form solution gsl_stats March CHAPTER Thus writing model computable form requires writing pro cedure directions estimate function log_likelihood p probability function draw function random draws You fill solve closed form leave Apophenia fill computationally intensive default procedures rest include apop h apop_model new_OLS static apop_model new_ols_estimate apop_data d apop_model params APOP_COL d v apop_data ydata apop_data_alloc d matrix size1 gsl_vector_memcpy ydata vector v gsl_vector_set_all v affine column ones apop_data xpx apop_dot d d t apop_data inv apop_matrix_to_data apop_matrix_inverse xpx matrix apop_model apop_model_copy new_OLS data d parameters apop_dot inv apop_dot d ydata return apop_model new_OLS A simple OLS implementation estimate new_ols_estimate int main apop_data dataset apop_text_to_data data regressme apop_model est apop_estimate dataset new_OLS apop_model_show est Listing A new implementation OLS model Online source newols For example listing shows new implementation OLS model The math OLS covered detail page In case estimate function specified The procedure simply matter pulling column data replacing ones calculating X X 1X y Lines allocate apop_model set parameter element correct value Line keeps pointer original data set short program comes handy The allocation output line needs model de clared The solution circularity simply declaration gsl_stats March MATRICES AND MODELS model line Given function line initializes model process simple thanks designated initializers p Although main function file typical model deserves file By stati keyword line function known file wish worrying cluttering global space But pointer function model object routine file use apop_new_ OLS estimate form function apop_estimate line internally Lines provide complete header external file need use new model structure apop_model provided apop h Q4 Compare new_OLS model apop_ols model Modify Listing declare array apop_models Declare element apop_ols second new_OLS The reverse won t work new_OLS destroys input data Write loop fill second array pointers models estimate models Calculate display difference betweenestimate parameters ve tor andestimate parameters ve tor AN EXAMPLE NETWORK DATA The default models apop_ols orapop_probit row data assumed observation column data dependent vari able remaining columns independent variable For models merely distribution rule row equals observation necessary data matrix form This provides maximum flexibility produce data But data describing ranks score place second place things interesting data appears multiple forms For example classroom student wrote ID number best friend tallied list student numbers First need count student appeared gsl_stats March CHAPTER id_no count In SQL select id_no count ct surveys group id_no order ct desc If talking city sizes favorite rank type analysis list size largest city second largest et cetera The labels relevant analysis simply send row counts popular second popular et cetera Each row data set classroom like column number represents ranking tallied As mentioned add groups settings model tweak havior In case models commonly rank analysis signal model getting rank ordered data For example apop_model rank_version apop_model_copy apop_zipf Apop_settings_add_group rank_version apop_rank NULL apop_model_show apop_estimate ranked_draws rank_version Alternatively data sets provided entry listing rank observation There s s s et cetera In city size example imagine drawing people uniformly random cities writing person drawn largest city second largest et cetera Here order written data matter You pass data directly estimation routines adding group settings e g apop_estimate ranked_draws apop_gamma gsl_stats March MATRICES AND MODELS Q4 The nominee column file data lassroom exactly sort data run network density analysis Read data database Query vector ranks apop_data set query like Transpose matrix hint gsl_matrix_transpose_mem py because apop_zipf s estimate function requires classroom row nth ranked nth column This data set includes classroom row data Call apop_estimate yourdata apop_zipf resulting estimate How Zipf model fit data MLE MODELS To examples different style model notes writing models based maximum likelihood estimation Write likelihood function It is header look like static double apop_new_log_likelihood gsl_vector beta apop_data d Here beta holds parameters maximized d fixed parameters data This function return value log likelihood function given parameters data In cases natural express probabilities log form times terms plain probability use works best func tions calculate needed Declare model apop_model new_model The Me distribution log_likelihood new_log_likelihood If probability instead log likelihood hook model p new_p The numbers size parameter structure format apop_data_allo size vector rows ma trix columns matrix If placed number columns input data set s matrix e your_ gsl_stats March CHAPTER 4data matrix size2 This use OLS regres sion example parameter independent variable With little like apop_estimate your_data new_model work apop_estimate defaults maximum likelihood search explicitly specified new_model estimate function For better estimations write gradient log likelihood function If provide closed form gradient function system fill blank numerically estimating gradients slower precision Cal culating closed form gradient usually hard typically requiring derivatives See Listing existing apop_model example showing details syntax SETTING CONSTRAINTS A constraint imposed author model declared arbitrary cutoff t spend evaluating likelihood function fails ln Thus system needs search near border going past needs able arbitrarily impose constraint unconstrained function Apophenia s solution add constraint function gets checked actual function evaluated It things constraint violated nudges point evaluated valid area imposes penalty subtracted final likelihood system know optimum The unconstrained maximization routines continuous function search find optimum parameter limits To concrete example Listing adds apop_normal model con straint function ensure parameters dimensional input greater given values Observe constraint function manages requisite steps First checks constraints quickly returns zero binds Then bind sets return vector inside constrained region Finally returns distance Manhattan metric input point point returned The unconstrained evaluation system repeatedly try points closer closer zero penalty point penalty continuously de cline approach point 21if model exotic parameter count needs determined run time use prep method apop_model allocation 22This akin common penalty function methods turning constrained problem unconstrained Avriel formal technique commonly explained involves series optimizations penalty approaches zero series progresses It hard computer find limit sequence best expect series estimations decreasing penalties apop_estimate_restart help process 23If need calculate distance point constraint functions apop_ve tor_ distan e apop_ve tor_grid_distan e gsl_stats March MATRICES AND MODELS include apop h double linear_constraint apop_data d apop_model m double limit0 limit1 tolerance 1e try GSL_EPSILON_DOUBLE double beta0 apop_data_get m parameters beta1 apop_data_get m parameters beta0 limit0 beta1 limit1 return create valid return vector return penalty apop_data_set m parameters GSL_MAX limit0 tolerance beta0 apop_data_set m parameters GSL_MAX limit1 tolerance beta1 return GSL_MAX limit0 tolerance beta0 GSL_MAX limit1 tolerance beta1 int main apop_model constrained apop_model_copy apop_normal constrained estimate NULL constrained constraint linear_constraint apop_db_open data climate db apop_data dataset apop_query_to_data select pcp precip apop_model free apop_estimate dataset apop_normal apop_model constr apop_estimate dataset constrained apop_model_show free apop_model_show constr double test_stat free llikelihood constr llikelihood printf Reject null constraint effect g confidence n gsl_cdf_chisq_P test_stat Listing An optimization constrained optimization likelihood ratio test comparing Online source normallr In real world set linear constraints apop_linear_ onstraint func tion takes set contrasts form F test requisite work For example use look budget constraint Listing The main portion program likelihood ratio test comparing constrained unconstrained versions Normal distribution Lines copy basic Normal model add constraint function copy The estimate routine Normal doesn t use constraint invalid unconstrained case line erases Lacking explicit estimate routine model line resorts maximum likelihood estimation MLE The MLE routine takes model s constraint gsl_stats March CHAPTER account Lines hypothesis test Basically twice difference log likeli hoods distribution page covers details AN EXAMPLE UTILITY MAXIMIZATION The process maximizing function subject constraints extensively outside statistical applications economic agents maximizing welfare physical systems maximizing entropy With little abuse opti mization routines use solve model involving maximization subject constraints This section gives extended example numerically solves Econ style utility maximization problem The consumer s utility consumption pair x1 x2 U x x Given prices P1 P2 B dollars cash budget constraint requires P1x1 P2x2 B Her goal maximize utility subject budget constraint The data e apop_data set fixed elements element vector matrix structured like budget price1 price2 Once models written estimation function calcu lating marginal values Overall program overkill prob lem solved derivatives framework problems analytic solutions consumers stochastic utility function dynamic optimizations graceful closed form Because estimation finds slopes optimum gives comparative statics answering questions change final decision given marginal rise price P1 P2 Listing shows model numerical optimization model simple Listing shows analytic version model The e on101_estimate routine sets optimization settings callsapop_maximum_likelihood The budget constraint turn she shall apop_linear_ onstraint That function requires constraint matrix look like matrix equations sent F tests page In case equations budget p11 p22 All inequalities form meaning cost goods gsl_stats March MATRICES AND MODELS include apop h apop_model econ_101 static apop_model econ101_estimate apop_data choice apop_model p Apop_settings_add_group p apop_mle p Apop_settings_add p apop_mle tolerance 1e Apop_settings_add p apop_mle step_size 1e return apop_maximum_likelihood choice p static double budget apop_data beta apop_model m double price0 apop_data_get m data price1 apop_data_get m data cash apop_data_get m data apop_data constraint apop_data_alloc apop_data_fill constraint cash price0 price1 return apop_linear_constraint m parameters vector constraint static double econ101_p apop_data d apop_model m double alpha apop_data_get d beta apop_data_get d qty0 apop_data_get m parameters qty1 apop_data_get m parameters return pow qty0 alpha pow qty1 beta apop_model econ_101 Max Cobb Douglass subject budget constraint estimate econ101_estimate p econ101_p constraint budget Listing An agent maximizes utility Online source e on101 budget negated However statemets positive positive natural easy express terms Convert ing system inequalities familiar vector matrix pair gives budget p1 p2 The budget consraint listed memory leak won t notice scale specifies constraint time For larger projects ensure onstraint allocated declaring stati initially setting NULL allocating NULL The analytic version model Listing straightforward translation gsl_stats March CHAPTER include apop h apop_model econ_101_analytic apop_model econ_101 define fget r c apop_data_get fixed_params r c static apop_model econ101_analytic_est apop_data fixed_params apop_model pin apop_model est apop_model_copy econ_101_analytic double budget fget p1 fget p2 fget alpha fget beta fget double x2 budget alpha beta p2 x1 budget p2 x2 p1 est data fixed_params est parameters apop_data_alloc apop_data_fill est parameters x1 x2 est llikelihood log econ_101 p fixed_params est return est static void econ101_analytic_score apop_data fixed_params gsl_vector gradient apop_model m double x1 apop_data_get m parameters double x2 apop_data_get m parameters double alpha fget beta fget gsl_vector_set gradient alpha pow x1 alpha pow x2 beta gsl_vector_set gradient beta pow x2 beta pow x1 alpha apop_model econ_101_analytic Analytically solve Cobb Douglass maximization subject budget constraint vbase estimate econ101_analytic_est score econ101_analytic_score Listing The analytic version Online source e on101 analyti solution constrained optimization If familiar Lagrange multipliers little difficulty verifying equations expressed routines The routines natural slots estimating parameters estimating vector parameter derivatives The term score defined Chapter point notice use abuse notation score defined derivative log utility function function derivative utility function The preprocessor provide quick conveniences abbreviates long function pull parameters fget Section page gives details preprocessor s use caveats The process wrapping library functions standardizedmodel routines oth gsl_stats March MATRICES AND MODELS include apop h apop_model econ_101 econ_101_analytic void est_and_score apop_model m apop_data params gsl_vector marginals gsl_vector_alloc apop_model e apop_estimate params m apop_model_show e printf nThe marginal values n apop_score params marginals e apop_vector_show marginals printf nThe maximized utility g n exp e llikelihood int main double param_array dummy apop_data params apop_line_to_data param_array sprintf apop_opts output_delimiter n est_and_score econ_101 params est_and_score econ_101_analytic params Listing Given models main program series calls Online source e on101 main erwise putting place pays main function Listing Notably est_and_s ore function run knowing model internals run closed form numeric search ver sions model It displays maximized utility continuing metaphor likelihood utility e on101model s maximization returned log utility llikelihood element output model The e on101 analyti model calculates parameters calculating util ity need write separate calculation fill utility calculation e on101 model Thanks calling models functions easy produce variants existing models The public parts e on101 e on101 analyti models don t bother header file instead simply declare models e on101 main The model files compiled separately linked Make file Appendix A OBJECTS line listing o files Q4 Use models produce plot marginal change x0 x1 expands gsl_stats March CHAPTER The apop_model aggregates methods estimating parameters data drawing data given parameters estimating likelihoods given Most apop_models data structure observation row variable column If model includes de pendent variable column Given prepackaged model estimate parameters model putting data appropriate apop_data struc ture apop_estimate data your_model This produce apop_model interrogate display screen If closed form calculations model elements avail able means write model But functions apop_models input best effort fill missing methods For example score function mandatory use gradient based optimization methods gsl_stats March GRAPHICS Graphs friendly Tukey p Graphics places computing world agreed standard instead dozen standards including JPG PNG PDF GIF TLAs You find computer readily handles including quick displays screen find logging remotely command line university s servers support SVG graphics Some journals insist graphics EPS format require JPGs GIFs The solution graphics portability problem use handful external programs easily available package manager plain text output image panoply graphics formats The text graphics programs open freely available gcc confident code portable new computers But chapter plotting programs control text driven program C If prefer create graphics portions analytic pipeline separate package like stats packages listed introduction use techniques 1There politics strictly true maintainers Gnuplot allow modify code distribute modified package independently e fork code base The project entirely unrelated GNU project simply compromise names main authors preferred nplot llamaplot gsl_stats March CHAPTER Appendix B takes different approach converting data executable script command line tools modify text If data going com putations transformations better she shall script Ap pendix B instead writing C program chapter The plot producing program chapter primarily concerned Gnuplot It is language basically verbs set plot plus variants unset replot et cetera To feel language typical Gnuplot script consists string set commands eliminate legend set title et cetera final plot command actual work unset key set title US national debt set term postscript color set debt eps plot data debt lines In code supplement find plots file provides plots chapter cut paste able form You find data debt file plotted example But words means sending plot samples Gnuplot PRELIMINARIES As SQLite mySQL command line interpreter Gnuplot s commands gnuplot interactively try different set commands options look best After finished shopping best settings script produce perfect plot better write C program autogenerate script produce perfect plot There ways Gnuplot read commands script e g file named plotme From she shall command line run gnuplot persist plotme This runs script exits plot persists screen Without persist option plot disappear split second If writing file looking plot screen persist option unnecessary Run gnuplot options prompt type load plotme This leaves Gnuplot prompt experiment settings The hybrid gnuplot plotme command line This executes structions plotme leaves Gnuplot prompt play different settings gsl_stats March GRAPHICS 159set term Meaning Output goalx11 Window system POSIX OSes display screenwindows Window system Windows OSes display screenaqua Window system Mac OS X display screenpng Portable network graphics browsergif Graphics interchange format browser word processorsvg scalable vector graphics browser word processorps Postscript encapsulated postscript PDFlatex LATEX graphics sub language LATEX docs Table Some terminal types Gnuplot supports If Gnuplot prompt exit exit command ctrl d On systems interact plot spinning D plots mouse zooming selected subregions D plots Q5 Check Gnuplot installation Write line text file named plotme text reads plot sin x Execute script methods Once works try national debt example If system unable display plots screen read alternative output formats set term AND set Gnuplot defaults putting plots screen useful looking data necessarily useful communicating peers Still worse systems capable screen output There potential solutions problem The set terminal command dictates language write Table presents common options Gnuplot s help system de scribes deatils e g help set term latex helpset term posts ript The default typically screen format appropriate system The set command provides file write For example Gnuplot file set term postscript color set a_plot eps 2The popular JPG format listed compression designed work photographic images makes lines text look fuzzy Use plots graphs resort gsl_stats March CHAPTER head script display screen instead write designated file Postscript format Comments Gnuplot follows commenting standards scripting languages line ignored For example script begins set terminal posts ript olor set printme eps lines ignored Gnuplot display plots screen usual If later decide print plot delete s script write printme eps You rest assured mat ter way view graphics experimentation find For example dialing remote server able copy graphics publi _html directory file publicly readable hmod 644plot png view plot web browser Or produce Postscript output run ps2pdf produe PDF file open familiar PDF viewer Now know run Gnuplot script view output script plot The plot command set basic shape plot To plot basic scatterplot columns data file thedata debt file online code supplement simply use plot data debt You dump columns data datafile necessary For example portion data debt file includes columns year debt deficit To plot columns data use plot datafile This produces scatterplot X values column Y values column Notice Gnuplot uses index numbering instead offset numbering column zero Say want single data series maybe column plot datafile With column given plot assume X values ordinal series data Y values replot You want multiple data sets plot Gnuplot easily replot command Just change use plot replot Page presents notes function gsl_stats March GRAPHICS set xrange plot sin x replot cos x replot log x x x Gnuplot understands variable x refer axis y second If parametric plot example Figure page variables t u v Gnuplot knows functions standard C math library For example sequence produce set pleasing curves Notice x common math package notation x2 splot The plot command prints flat D plots To plot D surfaces use splot All applies directly dimensions If data set columns plot splot datafile If data set columns specify want form like splot datafile There crosstab like case data s row column dimensions represent X Y axes directly st element data file height Southwest corner plot n n th element height Northeast corner To plot data use splot datafile matrix Surface plotting goes hand hand pm3d palette mapped D option produces pleasing color gradient surface For example simple ex ample produces sort plot advertisements math programs Again run Gnuplot prompt system supports able use mouse spin plot set pm3d splot sin x cos y pm3d Here extended script produce Figure This example appears slightly modified form agentgrid gnuplot code supplement The simulation produced available request set term postscript color set plot eps set pm3d contour map use set pm3d map unset colorbox set xlabel percent acting set ylabel value emulation n gsl_stats March CHAPTER percent acting v lu e o f e m u la ti o n n percent acting va lue em ula tio n n Figure Two views density plot thousand simulations group agents Agents act iff ti nki C ti Normally distributed private preference ki percentage people acting n agent s preference emulating C Uniformly distributed cutoff The vertical axis shows density simulations given percent acting value n When value emulation low outcomes unimodally distributed outcomes bimodally distributed emulation valuable set palette gray set xtics set ytics splot datafile matrix pm3d Lines send output Postscript file instead screen Given line olor modifier optional case Line tells Gnuplot prepare palette mapped surface necessary splot command line Line deletes legend pm3d case known colorbox Line sets labels demonstrates Gnuplot commands separate line separated semicolon As script ending line semicolon optional Line sets color scheme appropriate book printed black white There palettes default omitting setpalette line entirely works purposes Lines seven fix axis labels Gnuplot defaults index column row label The format requires text label followed index label placed notes Changing line set pm3d map produces overhead view surface known contour plot second plot Figure gsl_stats March GRAPHICS The short version The sample lines relatively brief huge information line For example set style data bars set style function lines set linetype set xrange set yrange plot data title data replot sin x title sine rewritten plot data bars title data sin x lines linetype title sine plot data w bars title data sin x w l lt title sine All settings discussed The purpose example style information plot command mixed line defining plot The replot command technically optional add additional steps plot line comma Finally line abbreviate replacing lines w l This minimalism versus clarity tradeoff encouraged stick clear version SOME COMMON SETTINGS At point produce basic plot data function But mind different look Gnuplot s default means need set commands final plot This section catalogs common settings For information set tings comprehensive Gnuplot documentation accessed inside Gnuplot command line program help optionally followed headers e g help set style helpset pointtype Finally interactively experimenting settings en couraged reading section bear mind replot command word options settings effect gsl_stats March CHAPTER 5set style The basic style plot simple line points bar plot box es error bars possibilities Gnuplot keeps track types style function plotting set style fun tion data plot ting set style data For example plot looks like bar chart try set style data boxes followed line plot yourdata As equivalent slightly shorter form plot yourdata withboxes useful separate style setting plot content Other favorite data styles include lines dots impulses lines x axis data level steps continuous line takes diagonals linespoints line actual data point marked errorbars discussed If plotting function like plot sin x use set style fun tionlines dots impulses et cetera Because separate styles functions data easily plot data overlaid function different style set pointtype set linetype You set width colors lines points display balls tri angles boxes stars et cetera The pointtype linetype commands handful commands differ screen Postscript PNG formats depending easy different formats You terminal test command E g set terminal postscript set testpage ps test Among things test page displays numbered catalog points lines available given terminal set title set xlabel set ylabel These simple commands label X Y axes plot If plot going figure paper paper caption title optional rarely excuse omitting axis labels Sample usage 3As writing Gnuplot s default plotted lines use linetype red linetype2 green Seven percent males females red green colorblind won t able dis tinguish line Try e g plot sin x replot os x linetype bypass linetype2 green producing red blue plot gsl_stats March GRAPHICS set xlabel Time days set ylabel Observed density picograms liter set title Density time set key Gnuplot puts legend plot default Most time rea sonably intelligent legend gets way Your option case turn key entirely unset key The moderate option key combination left right outside set horizontal position set vertical ask help set key details precise meaning positions experiment The key benefits border set key box For surface plots pm3d key thermometer displaying range colors values To turn use unset olorbox See help set olorbox moving box changing orientation set xrange set yrange Gnuplot generally good job selecting default range plot manually override set range min max Sometimes want leave end range set Gnuplot fix end case use indicate automatically set bound For example set yrange fixes plot let us lower end plot fall You want axes backward Say data represents rankings best set yrange reverse place plot autoset plot past largest ranking data set set xti s AND set yti s Gnuplot reasonably sensible defaults axeswill labeled youmay set tick marks directly To provide list parens text position label lines seven code page Producing hand annoying indication producing Gnuplot code C simple routine write set yti s line gsl_stats March CHAPTER static void deal_with_y_tics FILE f double min double max double step int j fprintf f set ytics double n_min n_max n_step fprintf f g j n_step n_max fprintf f fprintf f n ASSORTED Here settings find handy unset border Delete border plot unset grid Make plot minimalist set size square Set axes equal length screen paper set format y 3g You use printf strings format axis labels set format y Or turn printing Y axis entirely set zero 1e Set limit point rounded zero default 1e FROM ARRAYS TO PLOTS The scripts gave file fromwhich read data plot data debt Alter natively plot tells Gnuplot plot data placed immediately theplot command With trick process turning matrix basic plot trivial In fact principle simple ways I am plementing Write file Let data apop_data set fifth columns like plot Then need create file plot command line preceded series staticset commands fill remainder data plotted Below basic code create Gnuplot file Since virtually Gnuplot variant code dissected detail FILE f fopen plot_me w f exit fprintf f set key set ylabel picograms liter n set xrange n fprintf f plot title columns n fclose f apop_matrix_print data matrix plot_me gsl_stats March GRAPHICS The argument fopen file written second option append w write anew case want start clean file use w The fopen function easily fail reasons including mistyped directory permissions disk Thus encouraged check fopen worked use If file handle NULL went wrong need investigate As lines syntax fprintf similar rest printf family argument indicates writing rest standard printf line include usual insertions g You separate Gnuplot commands semicolon newline fprintf statement convenient However need end line data plot line newline Since Gnuplot let us select columns clause need pare data set memory Notice column Gnuplot The title clause shows Gnuplot accepts double quotes single quotes text file names labels Single quotes special C makes easier enter text Line closes file confusion line writes data file It prints matrix instead apop_data structure names labels written Since apop_matrix_print defaults appending matrix appears plot header lines wrote file You need set apop_opts output_append overwrite At end plot_me executable Gnuplot forms likegnuplot persist plot_me Instant gratification The method involved writing commands data file running Gnuplot want pro duce plots program runs This useful simulations hint OK program runs impress friends funders This easy pipe named UNIX s running data water metaphor Appendix B exposition The command popen things runs specified program pro duces data pipe sends stream data produced program running child program Any commands write pipe read child typed commands program directly Listing presents sample function open write pipe gsl_stats March CHAPTER include apop h void plot_matrix_now gsl_matrix data static FILE gp NULL gp gp popen gnuplot persist w gp printf Couldn t open Gnuplot n return fprintf gp reset plot n apop_opts output_type p apop_opts output_pipe gp apop_matrix_print data NULL fflush gp int main apop_db_open data climate db plot_matrix_now apop_query_to_matrix select year month temp temp Listing A function open pipe Gnuplot plot vector Online source pipeplot The popen function takes location Gnuplot executable w indicate writing Gnuplot reading Most sys tems accept simple program gnuplot search program path location Appendix A paths If gnuplot path need explicit location like usr lo al bin gnuplot In case find Gnuplot lives machine commandwhi h gnuplot The popen function returns FILE assigned gp Since gp declared static variable popen called whengp NULL persist multiple calls function peatedly function produce new plots window If gp NULL popen went wrong This worth checking time pipe created But pipe created properly function continues familiar process writing plot matrix file The reset command Gnuplot line ensures time function new plot strange interactions plot Lines set output type p output pipe gp apop_ matrix_print uses global variables know write pipe instead file stdout Notice resemblance form form write file gsl_stats March GRAPHICS A FILE pointer point program expecting input file program writes exactly manner They closed f lose The difference file opens fopen program opens popen Thus option named apop_opts output_ pipe easily point file e g FILE f fopen plotme w apop_opts output_pipe f One final detail piped output kept buffer space memory system promises eventually write file end pipe This improves performance want plot system deems buffer worth writing The function line fflush tells system send elements gp buffer pipeline The function works expecting standard output screen way fflush stdout The main drawback producing real time plots computer plot pops grabs focus half second significantly slow program Thus want settle occasional redisplays periods simulation form like int period period period gsl_vector output run_simulation period plot_vector_now output Q5 Now good time plot data series Query column table data tattoo db giving birth year column mean number tattoos respon dents birth year second Dump data Gnuplottable file fprintf tech niques Plot file add set commands fprintf file produce nicer looking plot E g try boxes impulses Modify program display plot immediately pipes How written program initially minimize effort required switch writing file pipe How graph look restrict query include nonzero number tattoos 4Alternatively fflush NULL flush buffers For programs like ones book handful streams open doesn t hurt use fflush NULL cases gsl_stats March CHAPTER Self executing files This chapter possible paths data set script executed external program s Those experiencedwith POSIX systems know script executable The line script begin special marker fol lowed interpreter read file script given execute permissions Listing shows program produces self executing Gnuplot script plot sin x You use system plotme script execute end program include apop h include sys stat h chmod int main char filename plot_me FILE f fopen filename w fprintf f usr bin gnuplot persist n plot sin x fclose f chmod filename Listing Write file self executing Gnuplot script Run script plot_me command line Online source selfexe ute replot REVISITED replot plot mechanism incompatible because Gnuplot needs read data replotting stream reread Instead replotting data sets write data separate file refer file file read Gnuplot To clarify C snippet writes file asks Gnuplot read file apop_data_print data datafile FILE f fopen gnuplot w fprintf f plot datafile title data column n replot datafile title data column n fclose f Also outputting paper device replotting tends mess Set output terminal final replot plot datafile replot datafile replot datafile set term postscript color set four_lines eps replot datafile gsl_stats March GRAPHICS A Gnuplot command file basically consists group set com mands specify plot look single plot D version splot D version command You run external text driven programs C fopen popen Using plot form data plotted command file immediately command You use andapop_data_print produce plot files data A SAMPLING OF SPECIAL PLOTS For system basically set plot command Gnuplot surprisingly versatile Here specialized visualizations yond basic D plot LATTICES Perhaps plotting pairs columns time sufficient want bulk displaying variable plotted For use apop_plot_latti e function include eigenbox h int main apop_plot_lattice query_data Listing The code produce Figure Link code Listing p Online source latti e Listing produces file gnuplot persist produces plot Figure Yes line code need link data querying code Listing p Each variable plotted e g upper middle plot shows males females versus state population middle left plot shows mirror image diagonal line plot What lattice plot good Some getting lay land data snooping Given perfectly random variables good chance pair lattice plots look demonstrates nice correlation A formal regression chosen pair variables likely verify initial visual impression For conflict page gsl_stats March CHAPTER m_per_100_f median_age population Figure The Census data queried page ERROR BARS The typical error bar parts center limit bot tom limit Gnuplot supports type data directly set styledata errorbars You provide necessary information variety mats common x y center y y x y center y range range typically standard deviation Listing produces Fig ure takes second approach querying month mean temperature month standard deviation temperature month Plotting data shows typical annual cycle temperatures regular fluc tuation variances temperature HISTOGRAMS A histogram set X Y values like plotting requires special tools However Gnuplot list data form histogram C send final histogram Gnuplot Fortunately apop_plot_histogram binning Have look Listing page example turning list data items histogram shown Figure gsl_stats March GRAPHICS include apop h int main apop_db_open data climate db apop_data d apop_query_to_data select yearmonth round yearmonth month avg tmp stddev tmp precip group month printf set xrange plot errorbars n apop_matrix_show d matrix Listing Query month average variance plot data errorbars Prints stdout pipe output Gnuplot errorbars gnuplot persist Online source errorbars te m p F month Monthly temp Figure Monthly temperature Q5 The leading digit number simply significant digit lead ing digit leading digit Benford s law Benford states digit d leading digit frequency Fd ln d d Check data set choosing population col umn World Bank data Total_area column US Cen sus data The formal test exercise page chapter plotting produce histogram chosen data set verify slopes sharply downward Hint d 2e7 int log10 d 1e7 gsl_stats March CHAPTER b ir th y ea r tattoos b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b Figure Each point represents person LOG PLOTS You plot data log scale transforming gets Gnuplot transforming plot Log plots work best log base scale typical natural loga rithm readers immediately convert scale 1e2 1e et cetera From C use log10 x function calculate log10x data gsl_ve tor use apop_ve tor_log10 trans form entire vector In Gnuplot simply set logs ale y use log scaled Y axis set logs alex use log scaled X axis set logs ale xy Q5 Redo data debt plot beginning chapter log scale PRUNING AND JITTERING Plotting entire data set detrimental reasons One range problem data point Y 1e20 throws darn plot If interactive screen plot select smaller region better plot point begin The second reason pruning data set large single gsl_stats March GRAPHICS page The black blob plotting million data points single piece paper informative In case want use random subset data Both problems selecting data easy handle SQL A simple sele t plotme value 1e7will eliminate val ues greater million page showed select random subset data In Gnuplot add keyword plot plot data plot fifth data point This quick easy care patterns data Now consider graphing number tattoos person year birth Because discrete values expect people share year birth tattoo count meaning plot people exactly point A point tattoo represent person include apop h gsl_rng r void jitter double gsl_rng_uniform r int main apop_db_open data tattoo db gsl_matrix m apop_query_to_matrix select tattoos ct tattoos ct tattoos year birth yr tattoos yr ct r apop_rng_alloc apop_matrix_apply_all m jitter printf set key set xlabel tattoos n set ylabel birth year n plot pointtype n apop_matrix_show m Listing By adding bit noise data point plot reveals data Online source jitter 5Another strategy getting ink page change point type default cross dot For typical terminal plot pointtype gsl_stats March CHAPTER One solution add small noise observation points fall exactly probability near zero Figure shows plot Without jittering exactly point tattoo column year jittering evident generally people tattoo born earlier later years half sample tattoos born Further process jittering simple Listing shows code produce plot Lines simple query lines produce Gnuplot header file printed stdout run program jitter gnuplot In blocks line applies jitter function element matrix That function lines simply adds random number input Comment line plot jitter Q5 Modify code Listing jittering data points SQL query instead gsl_matrix Apophenia construct grid plots plotting variable apop_plot_latti e With Gnuplot s set style errorbars command plot range spread data point You need aggregate data histograms outside Gnuplot plotting easy data set If data set exceptional range log data C log10 plot log Gnuplot set logs ale y trim data SQL e g sele t ols table wherevar 1e20 If data falls grid e g integer valued rows columns add jitter plot reveal density point gsl_stats March GRAPHICS ANIMATION Perhaps dimensions need Gnuplot easily supports animation stack ma trices plot However media paper support animation meaning output generally dependent display method The GIF format provides animation supported major web browsers movies online There details help plotting multiple data sets First Gnu plot reads e line indicate end data set Second Gnuplot allows define constants simple equals sign e g command p creates variable p sets Third Gnuplot pause p com mand wait p seconds drawing plot Tying want Gnuplottable file looks like set pm3d p splot pm3d data e pause p splot pm3d data e pause p splot pm3d data e You run resulting output file Gnuplot command line usual If second pause long short need change single value p head file change delay You write plots paper oriented output format case plot separate page You page screen viewer print flipbook When writing file set p use delaying outputs In addition example Listings page page demonstrate production animations 6For GIFs need request animation set term line e g set term gif animate delay100 The number end pause frames hundredths second gsl_stats March CHAPTER Figure The Game Life left original colony right colony periods The vaguely V shaped figures groups farthest left right known gliders travel step forward periods An example game life There tradition agent based modeling built plotting agents grid pioneered Ep stein Axtell A simple predecessor Conway s Game Life cellular automaton discussed length Gardner The game played grid point grid host single blob These blobs somewhat delicate blob zero neighbors dies loneli ness neighbors dies overcrowding If cell surrounded exactly blobs new blob born cell These simple rules produce complex interesting patterns At left Figure called r pentomino simple configuration blobs At right outcome periods life Listing presents code run game Because program prints Gnuplot commands stdout run life gnuplot The game uses grids completed grid period named tive code incomplete grid soon represent state life period named ina tive Both grids initialized zero lines lines define r pentomino Line Gnuplot header line tells Gnuplot data points coming Themain work period preparing inactive grid al _ grid function It sets zero loops indexed rows j indexed columns checks point grid borders The area_pop function calculates population space line needs gsl_stats March GRAPHICS include apop h int area_pop gsl_matrix int row int col int j row row j col j col j gsl_matrix_get j return void calc_grid gsl_matrix active gsl_matrix inactive int size int j s live gsl_matrix_set_all inactive size j j size j live gsl_matrix_get active j s area_pop active j live live s s live s gsl_matrix_set inactive j printf n j int main int gridsize periods gsl_matrix t active gsl_matrix_calloc gridsize gridsize gsl_matrix inactive gsl_matrix_calloc gridsize gridsize gsl_matrix_set active gsl_matrix_set active gsl_matrix_set active gsl_matrix_set active gsl_matrix_set active printf set xrange n set yrange n gridsize gridsize periods printf plot points pointtype n calc_grid active inactive gridsize t inactive inactive active active t printf e n pause n Listing Conway s game life Online source life subtract population central point population point s neighborhood Notice loops area_pop al _grid consider edges grids That means area_pop need concern edge gsl_stats March CHAPTER conditions like The rules Game Life summarized statement lines There blob point currently blob neighbors living blob neighbors If statement finds blob space period prints point Gnuplot marks currently inactive grid Lines classic swap active inactive grids temp location help exchange Since shunting addresses data operation takes zero time Q5 Other rules life death produce interesting results For example rewrite rules living blob stays alive neighbors space birth neighbor Or try staying alive rules birth rules ON PRODUCING GOOD PLOTS Cleveland McGill offer suggestions producing plots purpose perceiving patterns static Their experiments aimed people compare data presented formats arrived ordering graphical elements likely allow accurate perception layouts inhibited accurate perception Position common scale e g height means temper ature plot page ii Position identical nonaligned scales comparing points separate graphs iii Length e g height error bars temperature plot page iv Angle v Slope close vertical horizontal vi Area vii Volume Density Color saturation e g continuous scale light blue dark blue viii Color hue e g continuous scale red blue Data presented techniques high scale possible Pie charts representing data angle area bad idea better ways present data Gnuplot provide means set ting data dependent color given color Cleveland gsl_stats March GRAPHICS McGill s list resort They run experiments animation eyes cells exclusively dedicated sensing motion sensible movement included list rank highly US national debt 1e 2e 3e 4e 5e 6e 7e 8e 9e N ti o n l d e b t 5e 5e 5e 6e 5e 7e 5e 8e 5e 9e N ti o n l d e b t 1e 2e 3e 4e 5e 6e A n n u l d e fi c Online source debt gnuplot As angles slopes consider plots US national debt The plot y axis starting sec ond y axis starting trillion Is rate change larger smaller rate change Was growth constant decelerat ing It diffi cult answer questions graph somewhat flat change gles small perceived bad discerning slopes Differences slope second scale visible The lesson plots patterns clearly axes set slope The plot US national deficit This gov ernment spends income rate change debt The rate change graphs slope graphs height plot year position common scale number Cleveland McGill s list The answers questions obvi ous rate change debt slowed quickly rose double rates Darrell Huff classic How Lie With Statistics Huff Geis Chapter different goal makes different recommendations He points graphs tell different narrative The second graph tells story national debt rapidly increasing height point times height The graph shows debt rose gsl_stats March CHAPTER fast multiplying rate Huff concludes story best told zero y axis picture means blank space page advice directly contradicts advice So choice given plot depends context intent rules like avoiding pie charts continuous color scales valid situations GRAPHS NODES AND FLOWCHARTS In common conversation typically mean word graph plot data like diagram point chapter The mathematician s definition graph set nodes connected edges Figure Gnuplot plot network data like visualize Graphviz package use Like tools book Graphviz installs gracefully package manager The package includes executables notable dot andneato Both input files dot produces flowchart definite beginning end neato produces amorphous plots aim group nodes links connecting The programs similar Gnuplot plain text description nodes edges produce output file plethora graphics formats The syntax input files entirely different Gnuplot s concept familiar elements describe settings interspersed data elements For example flip page look Figure Produce graph figure following input dot digraph rankdir LR node shape box Data Estimation Parameters The lines set parameters stating graph read left right instead default nodes boxes instead default ellipses The line s defines nodes link looks like text version Figure The command line dot Tpng graphdata dot output png gsl_stats March GRAPHICS Figure The social network Junior High classroom produces graph like Figure PNG format At point tools need autogenerate graph For example n n grid position j indicates link agents j zero indicates link Then simple loop convert data neato plottable file series rows form like e g node32 node12 void produce_network_graph apop_data link_data char outfile FILE g fopen outfile w fprintf g digraph n int link_data matrix size1 int j j link_data matrix size2 j apop_data_get link_data j fprintf g node node n j fprintf g n fclose g In code supplement find file named data lassroom lists survey results Junior High classroom LA students listed 7The second half Figure produced exactly graph plus psfrag package replace text like Data OLS specific math shown figure gsl_stats March CHAPTER best friends The ego column student writing best friend nominee column gives number best friend Figure graphs classroom neato following options graph digraph node label shape circle height width edge arrowhead open arrowsize A patterns immediately evident graph right group students form complete clique interested rest class The student clique absent day survey nominated friend nominations similarly student lower left Most graph large clumps students closely linked left center probably representing boys girls There students nominated best friends right students popular Q5 Write program replicate Figure Read data lassroom file apop_data set eitherapop_text_to_db query apop_text_to_data Open output file copy header Write single loop write line output file line data set Base printing function page Close file Running program produce neato formatted file From command line run neato Tps my_output eps The output graph look like Figure In dot manual man dot command line variant programs produce different types graph What classroom look like dot ir o twopi The exercise page way produce Graphviz readable output file gsl_stats March GRAPHICS Internal use If sticking philosophy coding small simple functions code look like set elements linked function calls exactly sort network Graphviz written If feel code files getting bit complex use Graphviz big picture For example database growing involved queries merge tables new tables queries split tables tables et cetera For query creates table easy write line lines like base_tab hild_tab Then dot sort individual links relatively coherent flow raw data final output You graph calling relationships functions C code start manually scanning code know program Ask package manager doxygen generates documentation specially formatted comments source file If configured correctly use Graphviz include graphs doc umentation The online code supplement includes examples Graphviz work including code create Figures The Graphviz package produces graphs list nodes edges Such lists easy autogenerate C You use Graphviz track relationships func tions code tables database PRINTING AND LATEX This book focuses tools write replicable portable analyses step described handful human legible text files sent programs behave manner computer The TEX document preparation system set macros built LATEX extend pipeline final writeup For example easily write script run analysis regenerate final document updated tables plots 8SQL sort metadata systems describing table s contents detail e g apop_data structure s title element But set metadata table column table description tables generated table Such table reasonably easy maintain need add insert metadata query query generates table Q Write function table output flowchart demonstrating flow data database tables 9To answer questions probably wondering yes book LATEX document Most plots produced set term latex Gnuplot minimize complications sending Postscript press Save pointer box diagrams C chapter Student s hand drawn menagerie snowflake gsl_stats March CHAPTER A complete tutorial LATEX entire book written dozens times But chapter s discussion pipeline raw data output graphs incomplete mention unpleasant details plots LATEX You options putting plot TEXed paper native LATEX Postscript Native format Just set output device screen Postscript printer send file written LATEX s graphics sub language One plus fonts identical docu ment resolution TEX times finer wavelength visible light On minus features color currently available Producing plot LATEX format requires setting term settings type output set term latex set plot tex Just dump C file lude include Gnuplot output input command documentclass article usepackage latexsym begin document begin figure input outfile tex caption This figure autogenerated C program end figure end document Another common complaint Y axis label isn t rotated properly The solution provides good example insert arbitrary LATEX code Gnu plot code First Gnuplot file set label set instructions LATEX understand Let arbitrary label following command write label tell LATEX rotate appropriately head chapter I point producing entire book tools discusses The snowflake generated covering triangle uniform area distribution dots randomly selected color size rotating triangle form figure Therefore patterns sided rotational symmetry purely apophenia And experience helping build data publication pipelines detail discussed section rotating Y axis label common complaint 10By way single plot set command optional use pipe gnuplot plotme outfile tex gsl_stats March GRAPHICS set ylabel rotatebox Your lambda Two final notes complete example rotatebox graphi x package needs called document preamble usepackage latexsym graphicx Second dvi viewers support rotation viewing TEX s native dvi format rotation won t appear Use pdflatex dvips view output print The Postscript route Which brings second option including graphic Postscript Use graphicx package incorporate plot E g documentclass article usepackage graphicx begin document begin figure rotatebox scalebox includegraphics outfile eps caption This figure autogenerated C program end figure end document Notice frequently need rotate plot scale figure reasonable size The option generating PDFs use epstopdf First convert youreps files pdf files command line In bash try eps epstopdf Then LATEX header add usepackage pdftex epsfig gsl_stats March CHAPTER The benefit method run pdflatex my_do ument incident drawback versions figure clut tering directory regenerate PDF version graphic time regenerate Postscript version The alternative Postscript generating document latex a_report dvips a_report dvi a_report ps ps2pdf a_report ps Either method lot typing way automate process themake program discussed detail Appendix A Listing sample makefile producing PDF document Postscript route As C programs makefile place generate final PDF documents typing command line The creative reader readily combine makefile sample C makefile page regenerate final report time analysis data updated Hint add gen_all target depends targets That target s actions blank DOCNAME a_report pdf DOCNAME pdf DOCNAME dvi DOCNAME tex latex DOCNAME latex DOCNAME DOCNAME ps DOCNAME dvi dvips f DOCNAME dvi DOCNAME ps DOCNAME pdf DOCNAME ps ps2pdf DOCNAME ps DOCNAME pdf clean rm f DOCNAME blg DOCNAME log DOCNAME ps Listing A makefile producing PDFs LATEX documents Online source Makefile tex Once plot looks good screen send output file set term set commands For printing probably want use set term posts ript For online presentation use set term png set term gif For inserting LATEX document use Postscript setterm latex gsl_stats March MORE CODING TOOLS If good handle Chapter need write advanced programs But C world unto hundreds utilities facilitate better coding features programmer wishes delve This chapter covers additional programming topics details C environment As earlier chapters syntax C specific norm programming languages sort features structures discussed chapter useful regardless language The statistician reader likely skim chapter focus Section readers working simulations agent based models certainly need use structures techniques described The chapter roughly divides parts After Section covers functions operate functions Section use functions build struc tures hold millions items find agent based model Section shows manners programs parame ters outside world including parameter files enivornment variables command line Sections cover additional resources life computer easier including syntactic tricks C additional programs useful programmers gsl_stats March CHAPTER FUNCTION POINTERS A data point d stored memory refer address d Similarly func tion f stored memory refer address What pointer function good It let us write Functions ac cept function pointer use pointed function Functions calling functions confusing I capitalize Function indicate parent function takes lower case function input For example Func tion search largest value input function given range bootstrap Function statistic calculating function data set return variance statistic TYPES Before start writing Functions act functions need type input function consideration If function expects ints compiler needs know block attempts send function string array The syntax declaring function pointer based syntax declaring function Say want write Function array doubles plus function apply function element array returning array ints Then input function form like int double_to_int double x A Recall pointer declaration like declaration pointed type star like int goes declaring function pointers extra parens Here type function pointer takes adouble returns pointer int identical line A addition star parens int double_to_int double x B By way function returned int instead plain int declaration int double_to_int double x The type declarations word int But know define function type declaration function header line A Function applies function array doubles header like int apply double v int instance_of_function double x C gsl_stats March MORE CODING TOOLS Putting typedef work Are confused Each component basically makes sense cluttered confusing There way typedef By putting word line B typedef int double_to_int double x created new type named double_to_int use like type Now line C simplifies int apply double v double_to_int instance_of_function include apop h typedef double dfn double double sample_function double return log sin void plot_a_fn doublemin doublemax dfn plotme double val FILE f popen gnuplot persist w f printf Couldn t find Gnuplot n fprintf f set key n plot lines n double min max max min val plotme fprintf f g t g n val fprintf f e n int main plot_a_fn sample_function Listing A demonstration Function takes function R R plots Online source plotafun tion Listing shows program plot function form R R Gnuplot program described Chapter With typedef place syntax easy You don t need extra stars ampersands declaration Function function Function pointed function like gsl_stats March CHAPTER Line To life easier dfn type declared file Line The header sample_fun tionmatches format dfn func tion type e double double Line The plot_a_fn Function specifies takes function type dfn Line Using passed function simple function line gives indication plotme way special Line Finally main plot_a_fn called The sample_ fun tion passed For example look ja kiteration page Q6 Turn plotafun tion library function callable programs Comment main function Write header plotafun tion h necesary type func tion definitions Write test program ludes plotafun tion h plots al _taxes function taxes p Modify makefile produce final program creating linking your_ ode o plotafun tion o Q6 Define type dfn line Listing Then write Function header void apply dfn fn double array int array_len takes arguments function array length array changes element array fn array Apophenia provides comparable functions page details Test Function creating array natural numbers transforming list squares You pass functions function arguments pass arrays numbers The syntax declaring function pointer like syntax declaring function parens preceded star gsl_stats March MORE CODING TOOLS Defining new type describe function helps immensely This requires putting typedef function pointer declaration summary point Once typedef place declare Functions functions use passed functions parent Func tion expect Given typedef need stars ampersands operations DATA STRUCTURES Say million observations store computer You want find given item quickly add delete elements easily worry compli cated organization system There options balancing goals choosing trivial This section consider array linked list binary tree They implemented Glib library general use functions ev ery C programmer implement It includes features string han dling conveniences modules handle data structures de scribed The extended example provides documentation example initializing adding removing finding elements structures package manager happy install complete docu mentation Glib AN EXAMPLE This game consists series meetings pairs birds compete r utils resource If doves meet split resource evenly If dove hawk meet dove backs hawk gets resource If hawks meet hawks fight destroying c utils resources finally splitting left Table shows payoff table summarizing outcomes For pairing row player gets payoff column player gets second 1Glib provides common data structure known hash table technique easy data retrieval It converts piece data string number jump string s data table quickly Binary trees tend work better context agent based modeling I omitted hashes chapter See Kernighan Pike Chapter extended example hash tables produce nonsense text It intended amusement compare Exquisite Corpse type game played Pierce p commonly produce spam email Q Try implementing Kernighan Pike s nonsense generator Glib s hash tables string hash functions list structures 2The util unit measurement quantity utility agent gets action gsl_stats March CHAPTER dove hawk dove r r r hawk r r c r c Table The payoff matrix hawk dove game If c r prisoner s dilemma With c r game commonly known prisoner s dilemma contrived story separated prisoners choose providing evidence prisoner remaining silent It is key feature dove cooperating makes agent worse hawk cooperating literature calls defection The equilibrium P D game cooperates destroying resources period societal optimum cooperates producing r utils utility total time On add evolutionary twist bird success ful spawn chicks In interaction bird gets equal better payoff hawk dove time hawks approach population In simulation bird s odds reproducing proportional percentage total flock wealth bird holds odds dying inversely proportional To simulate game need flock birds Have look header filebirds birds h online code supplement It begins describing basic structure rest functions depend describing single bird typedef struct char type int wealth int id bird The header lists types function The functions relevant action simulation startup births deaths actual plays hawk dove game The second group functions flock management counting flock iterating member flock The set functions implemented Listing Each function taken individually sense play_hd_game takes birds modifies payoff according game rules bird_plays takes single bird finds opponent play et cetera gsl_stats March MORE CODING TOOLS include birds h include time h gsl_rng r int periods int initial_pop int id_count void play_hd_game bird row bird col double resource cost row type d col type h col wealth resource row type h col type d row wealth resource row type d col type d col wealth resource row wealth resource hawk v hawk col wealth resource cost row wealth resource cost void bird_plays void void dummy_param bird find_opponent gsl_rng_uniform_int r id_count play_hd_game bird new_chick bird parent bird malloc sizeof bird parent type parent type gsl_rng_uniform r type d type h wealth gsl_rng_uniform r id id_count id_count return void birth_or_death void void t bird b cast void bird int total_wealth t b wealth total_wealth gsl_rng_uniform r add_to_flock new_chick b b wealth total_wealth gsl_rng_uniform r free_bird b void startup int initial_flock_size flock_init r apop_rng_alloc time NULL printf Period tHawks tDoves n int initial_flock_size add_to_flock new_chick NULL gsl_stats March CHAPTER int main startup initial_pop int periods flock_plays count Listing The birds Online source birds birds Now flock management routines implemented times array list binary tree ARRAYS An array simple data representation write element right The matrices vectors book data arrays type The system retrieve item array faster data structure process consists simply going fixed location reading data On hand adding deleting elements array difficult simulation reallo time list expands If lucky reallo array current location simply find free space array grow If lucky array moved entirety new spacious home An array hole middle elements deleted freeing memory There solutions pleasant The element list moved space requiring copy shrinking array loss order elements If order important element shifted notch item deleted item slot item slot et cetera Thus death mean memmove execute thousands millions copy operations Listing marks dead birds setting id This means program runs memory dead elements rest system check marker use As finding element add_to_flo k takes pains ensure id array index match finding bird given id number trivial As code code consists large number short functions meaning reasonably easy understand read write function gsl_stats March MORE CODING TOOLS include birds h bird flock int size_of_flock hawks doves void flock_plays int size_of_flock flock id bird_plays flock NULL void add_to_flock bird b size_of_flock b id flock realloc flock sizeof bird size_of_flock memcpy flock b id b sizeof bird free b void free_bird bird b b id bird find_opponent int n flock n id return flock n return NULL int flock_size return size_of_flock int flock_wealth int total size_of_flock flock id total flock wealth return total double count int period int tw flock_wealth hawks doves size_of_flock flock id birth_or_death flock tw size_of_flock flock id flock type h hawks doves printf t t n period hawks doves return doves hawks void flock_init flock NULL size_of_flock Listing The birds array version The fatal flaw birds copied eliminated Dead birds eventually pile Online source birds arrayflo k gsl_stats March CHAPTER Q6 Because play_hd_game function sets r c hawks lose utils fight marginally better dove meeting hawk game prisoner s dilemma After run ning simulation times feel equilibrium number birds change c equilibrium proportion doves changes For convenience sample makefile included birds directory code supplement Finally rename main one_run wrap function takes value c returns proportion doves end simulation Send function plot_a_fun tion function earlier chapter produce plot Q6 Rewrite arrayflo k delete birds instead mark dead Use memmove close holes array renumber birds id matches array index Keep counter current allocated size greater number birds reallo array necessary LINKED LISTS A linked list set stru ts connected pointers The firststru t includes pointer points element pointer points element et cetera Figure Bird Bird Bird Bird NULL Figure The archetypal linked list Online source list dot The linked list favorite agent based simulation birth death easy handle To add element linked list create new node replace NULL pointer end list pointer new node Deleting node simple delete bird simply reroute pointer bird points bird free memory holding bird But real failing linked list trouble finding arbitrary element In array finding thousandth element easy flo k You code Listing glib provides g_list_nth_data function return nth element list makes look simple way function find thousandth member flo k start head list steps In fact compile run program gsl_stats March MORE CODING TOOLS runs slowly array tree versions include birds h include glib h GList flock int hawks doves void flock_plays g_list_foreach flock bird_plays NULL void add_to_flock bird b flock g_list_prepend flock b void free_bird bird b flock g_list_remove flock b free b bird find_opponent int n return g_list_nth_data flock n void wealth_foreach void void total int total bird wealth int flock_wealth int total g_list_foreach flock wealth_foreach total return total int flock_size return g_list_length flock void bird_count void void v bird b b type h hawks doves double count int period int total_wealth flock_wealth hawks doves g_list_foreach flock birth_or_death total_wealth g_list_foreach flock bird_count NULL printf t t n period hawks doves return doves hawks void flock_init flock NULL Listing The birds linked list version The fatal flaw finding given bird requires traversing entire list time Online source birds listflo k The g_list_forea h function implements exactly sort apply function list setup implemented Section It takes list function internally applies function element The folks wrote Glib library known thebird structure write linked list hold The lution void pointers pointer type associated point location holding data type whatsoever For example gsl_stats March CHAPTER 6bird_ ount takes void pointers element held list second sort user specified data case ignored The step void pointer casting correct type For example line bird_ ount bird b points b address asin b type associated normal As adding removing Glib implementation list takes pointer GList pointer data added returns new pointer GList The input output pointers identical guaranteed use form reassign list new value add de lete For example flock starts flo k_init NULL given non NULL value add_to_flo k BINARY TREES The binary tree takes linked list step giving node outgoing pointers instead As Figure think pointers left pointer right pointer The branching allows tree structure The directions element trivial tobird5 start head bird1 left right But data points linked list need seven steps element average steps In tree longest walk steps average steps Generally linked list require order n steps find item b tree require order ln n steps Knuth pp The tree arrangement needs sort order elements system knows left right step In case id bird provides natural means ordering For text data str mp provide simi lar ordering More generally key value given element tree structure function comparing keys Eariler saw interesting implementation set binary trees data base Since databases require fast access element natural internally structure data binary tree exactly SQLite mySQL operate internally new index tree This adds complication need associate tree function comparing keys In code g_tree_new initializes tree ompare_birds function 3For follow reference notice Knuth presents equation sum path lengths calls internal path length He finds order n ln n O n complete binary trees average path length ln n O gsl_stats March MORE CODING TOOLS bird left right bird left right bird left right bird left right bird left right bird left right bird left right bird left right Figure The archetypal binary tree Online source btree dot The onst keyword The onst modifiers header ompare_ keys indicate data pointers point changed course function As fact appeared un til page onst keyword optional good form provides check functions don t hadn t intended However conforming function specifica tions like GLib s function header key comparison functions need use If error like subfunction discards qualifiers pointer target type need rewrite subfunction takes onst inputs modify What birds sameid Then way order uniquely way reliably store trieve Thus key element unique The added complication tree solves problems As list inserting delet ing elements require majorreallo ing minor internal reshuffling branches tree length With key short chains finding element faster include birds h include glib h GTree flock NULL int hawks doves static gint compare_keys const void L const void R const int Lb L const int Rb R return Lb Rb 4There exist tree implementations require unique keys requirement GLib Similarly databases strict requiring table field representing key gsl_stats March CHAPTER static gboolean tree_bird_plays void key void void v bird_plays NULL return void flock_plays g_tree_foreach flock tree_bird_plays NULL void add_to_flock bird b g_tree_insert flock b id b bird find_opponent int n return g_tree_lookup flock n int flock_size return g_tree_nnodes flock static gboolean wealth_foreach void key void void t int total t total bird wealth return int flock_wealth int total g_tree_foreach flock wealth_foreach total return total static gboolean tree_bird_count void key void void v bird type h hawks doves return GList dying_birds void free_bird bird b dying_birds g_list_prepend dying_birds b static gboolean tree_birth_or_death void key void void t birth_or_death t return static void cull_foreach void b void v bird a_bird b g_tree_remove flock a_bird id free a_bird double count int period int total_wealth flock_wealth hawks doves dying_birds NULL g_tree_foreach flock tree_birth_or_death total_wealth g_list_foreach dying_birds cull_foreach NULL g_list_free dying_birds g_tree_foreach flock tree_bird_count NULL printf t t n period hawks doves return doves hawks void flock_init flock g_tree_new compare_keys Listing The birds binary tree version The fatal flaw complication maintaining key bird Online source birds treeflo k gsl_stats March MORE CODING TOOLS Culling flock especially difficult tree internally sort element added deleted impossible delete elements traversing tree In implementation Listing free_bird function actually freed bird adds dying birds GList post traversal step goest GList cull marked birds tree There means organizing large data sets collec tions agents agent based model Arrays simply sequential blocks structs Pros easy imple ment 000th element step Cons easy way add delete reorganize elements A linked list sequence structs includes pointer element list Pro adding deleting resorting elements trivial Con Getting 000th element takes steps A binary tree like linked list struct left right successor Pros adding deleting marginally diffi cult linked list getting 000th element takes steps Con Each element accessed unique key adding complication PARAMETERS Your simulations analyses require tweaking You want try agents want program load data set text file database run use data database later runs This section cover cavalcade means setting parameters specifica tions increasing order ease use difficulty implementation The option default sorts set variables file header file This trivial implement need recompile time change parameters Interactive The second option interactively parameters user vias anf fgets Listing shows program asks data user returns manipulated data The s anf function basically works likeprintf reverse reading text given format pointers variables Unfortunately system tends fragile real world stray comma period entirely throw format string The fgets function gsl_stats March CHAPTER read entire line string quirks In short interactive input features good quick interrogations bit fun heavily relied include stdio h include string h strlen int main float indata char s printf Give number scanf g indata printf Your number squared g n indata indata printf OK string max length n fgets s stdin eat newline fgets s stdin printf Here backward n int strlen s printf c s printf n Listing Reading inputs command line Online source getstring Environment variables These variables passed she shall aka com mand prompt program They relatively easy set generally variables infrequently changing like username Environment variables discussed length Appendix A Parameter files There libraries read parameter files consistent rest chapter Listing shows file Glib s key file format read program Listing The configuration file human language like English modify want recompiling code provides permanent record parameters run quickly switch sets variables The payoff Listing line printing distribution parameter mean distribution given parameter The program point finds items Line seven indicates section Listing following code read By commenting line seven uncommenting line code read Exponential section Below setting onfig variable command line difficult Line reads entire glib onfig file keys structure If gsl_stats March MORE CODING TOOLS gkeys c reads file chi squared configuration distribution Chi squared parameter exponential configuration distribution Exponential parameter Listing A configuration style Glib s key files Online source glib onfig include glib h include apop h int main GKeyFile keys g_key_file_new GError e NULL char config chi squared configuration char config exponential configuration double distribution double double g_key_file_load_from_file keys glib config e fprintf stderr e message double param g_key_file_get_double keys config parameter e e fprintf stderr e message char g_key_file_get_string keys config distribution e e fprintf stderr e message strcmp Chi squared distribution gsl_cdf_chisq_Pinv strcmp Exponential distribution gsl_cdf_exponential_Pinv printf Mean s distribution parameter g g n param distribution param Listing A program reads Listing Online source gkeys goes wrong line prints error message stored e Properly program exit point sake brevity return lines omitted Now keys holds values config file lines indi vidual values The g_key_file_get functions filled key struc ture section variable place errors They return requested value error gsl_stats March CHAPTER Unfortunately way specify functions text file lines set function pointer distribution according config file Q6 Rewrite code Listing set parameters database command line Write text file columns configuration parameters data Read file apop_text_to_db beginning main Write function header double get_param har onfig har p queries database parameter named p configuration group onfig returns value Then modify program distribution parameter get_param function Command line Reading parameters command line ef fort implement parameter setting options dynamic allowing change parameters time run program You write batch files Perl Python she shall type language run program different parameter variants record variants The main function takes inputs produces output like The output integer returned end main typically zero success positive integer indicating type failure The inputs integer giving number command line elements har array strings listing command line elements Like function specification types non negotiable internal choose arguments arbitrary However universal custom arg argument count argv argument values This ingrained custom expect names Listing shows rudimentary use arg argv Here sample usage command line 5They alphabetical order parameters main provides easy way remember count comes 6Perl uses argv Python uses sys argv Ruby uses ARGV All structures automatically track array lengths languages uses arg variable gsl_stats March MORE CODING TOOLS include stdio h int main int argc char argv int argc printf Command line argument s n argv Listing This program simply print command line arguments given Online source argv home klemens argv fo ur cinco command line argument home klemens argv command line argument command line argument command line argument command line argument fo ur command line argument cinco Argument zero argv command Some cre ative programs run differently referred different names norm skip argv After elements argv command line broken spaces dashes numbers sort text As parsing fo ur space preceded backslash taken character like argument separator Argument shows pair quotation marks single argument backslash turns quotation mark plain text For purposes need set program options command line For example program run different actions main like following int main int argc int argv argc printf I need command line argument n return strcmp argv read read_data strcmp argv analysis_1 run_analysis_1 strcmp argv analysis_2 run_analysis_2 gsl_stats March CHAPTER Q6 The allbyval program Listing page calculated factorial number hard coded main Rewrite number command line fa torial find Hint atoi function converts text string integer represents example atoi getopt For complex situations use getopt parses command lines switches form x It standard C library fully portable Listing shows program display series exponents As explained message lines set minimum series m maximum M increment Specify base exponents switches Sample usage demonstrates spaces switch data optional getopt m M4 There steps process lude unistd h Specify set letters indicating valid single letter switches crunched string like line Listing If switch takes additional info switch h indicate colon letter Write loop getopt line act based value har getopt returned argv text youwill want specify numbers The functions atoi atol atof convert ASCII text int long int double respectively 7The GNU version standard C library provides sublibrary named argp provides functions automatically correspondingly complex portable Glib subsection devoted command line arguments provides features long library installed 8Getopt readily handles negative numbers arguments flag m negative number options look like flag e g getopt m M looks flag named The special flag indicates getopt stop parsing flags getopt m M work This useful handling files begin dash e g given file named a_mistake delete rm a_mistake gsl_stats March MORE CODING TOOLS include stdio h printf include unistd h getopt include stdlib h atof include math h powf double min max double incr base void show_powers double min max incr printf g g g n base powf base int main int argc char argv char c opts M m h char help A program powers function Usage n t tgetopt options number n h t This help n m t The minimum exponent start n M t The maximum exponent finish n t Increment n argc printf help return c getopt argc argv opts c h printf help return c m min atof optarg c M max atof optarg c incr atof optarg optind argc base atof argv optind show_powers Listing Command line parsing getopt Online source getopt optarg sets variable optind indicate position argv visited Thus line able check non switch argu ments remaining line parse remaining argument withoutgetopt s help gsl_stats March CHAPTER The program provides human assistance If user gives h switch leaves switches entirely program prints help message exits Every variable user forget set command line default value Q6 Listing page hard coded plot range x x Modify use getopt minimum maximum user zero defaults Provide help user uses h flag There ways change program s settings having recompile program Environment variables covered Appendix A lightweight means setting variables she shall program use There libraries parsing parameter files use SQL pull settings database The main function takes arguments listed command line C functions like getopt help parse arguments program settings SYNTACTIC SUGAR Returning C syntax ways Chapter For example rewrite lines b j b single expression b j The seventh element array k called k k truly perverse k That book looks great deal C syntax useful graceful confuses easily clarifies This section goes details C syntax strictly necessary decent odds occasionally useful In fact demonstrated handful times remainder book If interest piqued like learn C works alternatives mentioned authoritative surprisingly readable reference language Kernighan Ritchie gsl_stats March MORE CODING TOOLS The obfuscatory There way write statement b first_val second_val equivalent b first_val second_val Both components condition condition true condition false However legible anybody knows basic English second takes reader second parse time sees On hand second version compact The condensed form primarily useful right assignment For example new_ hi k function Listing p saw following snippet gsl_rng_uniform r type d type h Using obfuscatory lines reduced type gsl_rng_uniform r d h Macros As lude ing files preprocessor text substitu tion simply replaces set symbols Text substitution things C t entirely For example encountered detail C global variables constant size compiler sets Thus attempt compile following program 9Global static variables initialized program calls main meaning al located evaluating non constant expressions code Local variables allocated needed runtime based evaluated expressions usual gsl_stats March CHAPTER int array_size int array_size int main error like variable size type de lared outside anyfun tion The easy alternative simply leave declaration file initialization main fix problem define The following program compile properly preprocessor substitute number ARRAY_SIZE compiler touches code define ARRAY_SIZE int ARRAY_SIZE int main Do use equals sign semicolon defines You define function type text substitutions For example code GSL_MIN macro gsl gsl_math h header file define GSL_MIN b b b It expand instance code GSL_MIN b expression parens GSL_MAX similarly defined This macro evaluated differently function A function evalu ates arguments calls function f guaranteed evaluate exactly f A macro works substituting block text regard text means If macro define twice x x twi e expands equal twi e We arrive rule macro writing macro definition parentheses prevent unforseen interactions tween text inserted rest macro gsl_stats March MORE CODING TOOLS Repeated evaluation common problem look For example GSL_MIN b expands b b meaning incremented twice function Again solution use macros resort second sure calls macros simple possible The thing macro better function type argu ment preprocessor shunts text regard text represents type variable For example recall form reallocating pointer array doubles var_array realloc var_array new_length sizeof double This rewritten macro create simpler form define REALLOC ptr length type ptr realloc ptr length sizeof type like REALLOC var_array new_length double It gives moving break needs luded file code readable This macro gives demonstration importance parens parens like REALLOC ptr double allocate sizeof double bytes memory instead sizeof double bytes If need debug macro E flag g run preprocessor expands You probably want run g E onefile The custom macro names capitals You rely code encouraged stick standard writing reminder macros relatively fragile tricky Apophenia s macros written caps looks like yelling initial capital Short statements summarized line ondi tion true_value false_value form You use preprocessor define constants short func tions gsl_stats March CHAPTER MORE TOOLS Since C widely ecosystem tools built helping easily write good code Beyond debugger programs life programmer easier MEMORY DEBUGGER The setup mistake memory handling early program fatal program con tinues bad data Later program innocuous bad data segfault This pain trace gdb packages designed handle problem If GNU standard library probably usingg use she shall command export MALLOC_CHECK_ set MALLOC_CHECK_ enviornment variable Appendix A environment variables When set set zero library uses usualmallo When set library uses version mallo checks common errors like double freeing errors reports onstderr When variable set system halts error exactly want running gdb Another common entirely portable alternative Electric Fence library available package manager It provides different version mallo crashes mis allocations mis reads To use simply recompile program efence library adding lefen e compilation command LINKFLAGS line makefile Appendix A REVISION CONTROL The idea revision control system RCS project lives sort database known reposi tory When want work check copy project making changes check project repository delete copy The repository makes note change check copy program looked weeks ago easily check current copy This pleasant psychological benefits Don t worry experimenting code copy break check fresh copy repository Also matches confidence mak ing major changes code finding results precisely match results month gsl_stats March MORE CODING TOOLS Finally revision control packages facilitate collaboration coauthors If changes sufficiently far apart e g working function coauthor RCS merge changes single working copy If unable work clearly demarcated list changes accept reject This method works text files life papers written LATEX HTML text based format For example book revision control There universal standard revision control software Subversion pack age readily available package manager For usage Subversion s detailed manual describing set operation command line ask search engine GUIs written Subversion THE PROFILER If feel program running slowly step fixing measurement The profiler times long function takes execute know functions focus clean efforts First need add flag compilation include profiler symbols pg Then execute program produce file named gmon directory machine readable timings profiler use Unlike debugger s g option pg option slow program significantly writes gmon use g pg necessary Finally gprof my_exe utable produce human readable table fromgmon See manual man gprof details reading output As debugger profiler points time taken program need alleviate bottleneck comes obvious If trying programs run optimizing speed far mind But nonetheless interesting exercise run mod estly complex program profiler like debugger s backtrace output provides useful view functions 10If program fast profiler rename main internal_main write new main function loop internal_main thousand times 11gprof outputs stdout use usual she shall tricks manipulate output piping output pager gprof my_exe utable dumping text file gprof my_exe utable outfile view text editor gsl_stats March CHAPTER Optimization The g compiler number things code run faster For example change order lines code executed assign x y z replace instance x y z To turn optimization use O3 flag compiling g That s O optimization zero There O1 O2 long optimizing The problem optimization makes debugging difficult The program jumps making stepping odd trip instance x replaced check value It happens memory allocation duties right things went OK optimization suddenly program crashes optimization A memory debugger provide clues scour code find problem Thus O3 flag final step reasonably confident code debugged Q6 Add pg switch makefile birds directory check tim ing different versions It help comment printf function run simulation periods How O3 flag change timings gsl_stats March II STATISTICS gsl_stats March gsl_stats March DISTRIBUTIONS FOR DESCRIPTION This chapter covers methods describing data set number strate gies increasing complexity The approach Section consists simply looking summary statistics series observations single variable like mean variance It imposes structure data sort The level structure assume data drawn distribution instead finding mean variance instead use data estimate param eters describe distribution The simple statistics distributions chapter sufficient describe complex models real world chain multiple distributions form larger model Chap ter slightly different approach modeling multidimensional data set projecting subspace dimensions MOMENTS The step analyzing data set quick lay land variables generally lie How far wander As variable A goes variable B follow ESTIMATOR VOCABULARY A statistic output function takes data typically form f Rn R That statistic takes data summarizes single dimension Common statistics include mean x variance max x covariance x y regression parameter regression X y written form X y gsl_stats March CHAPTER Thus means describing data set writing mean described generation statistics One goal writing statistic dimension reduction simply summarizing data human comprehensible summary statistics data set s mean variance Another goal case use statistics data X estimate statistic population Let P signify population When US Census Bureau said August press release1 million people United States health insurance meant count people Current Population Survey health insurance sample statistic indicated count people United States health insurance population statistic million Is estimate statistic based sample data f X valid estimate population statistic f P There ways precise question For example X grows larger Do exist estimates smaller variance After discussing desirable qualities estimator begin simple statistics Evaluating estimator From given population possi ble samples X1 X2 means possible calculated statistics f X1 f X2 There means describing collection statistics precise accurate estimate true value You sections follow intuitive means estimating population statistic work scales fail Unbiasedness The expected value discussed great detail equals true population value E Variance The variance expected value squared distance expected value E E The variance discussed detail Mean squared error MSE E Below MSE equals variance plus square bias So unbiased estimator meaning E MSE equivalent variance bias increases difference MSE variance grows Efficiency An estimator efficient estimator MSE MSE If unbiased estimators reduces var var authors describe efficient estimator unbiased 1US Census Bureau Income Climbs Poverty Stabilizes Uninsured Rate Increases Press release CB06 August http www ensus gov Press Release www releases ar hives ome_wealth html gsl_stats March DISTRIBUTIONS FOR DESCRIPTION estimator minimum variance unbiased estimators We test inequality page variance greater equal Cramer Rao lower bound If var equals CRLB know minimum variance BLUE Best Linear Unbiased Estimator var var linear unbiased estimators linear function unbiased Aymptotic unbiasedness Define n f x1 xn For example x1 x1 x2 x1 x2 x3 Then limn E n Clearly unbiasedness implies asymptotic unbiasedness Consistency plim n e fixed small o limn P n o Equivalently limn P n o2 One verify consistency Chebychev s inequality e g Casella Berger p In sense consistency asymptotic analog low MSE condition If MSE goes zero consistency follows necessarily vice versa However biased estimator high variance estimator things going inconsistent estimator waste You near infinite samples find different supposed pick Asymptotic efficiency var Cramer Rao lower bound This makes sense s asymptotic distribution finite mean variance consis tent EXPECTED VALUE Say given value x probability p x Then f x arbitrary function E f x x f x p x dx The p x dx integral statisticians measure calls weighting If p x constant x value x gets equal weighting value f x If p x1 twice large p x2 meaning twice likely observe x1 f x1 gets double weighting integral If vector data points x consisting n elements xi single observation equally likely p xi n The expected value sample familiar calculation E x xi n given information population Best Unbiased Esti mator true mean 2The term expected value implies mean humans actually expect occur But I gsl_stats March CHAPTER VARIANCE AND It is DISSECTIONS The variance discrete data familiar formula mean squared distance average Let x indicate mean data vector x best unbiased estimate variance sample var x n xi x Degrees freedom Rather calculating variance sample seek variance population sample estimate variance The best unbiased estimate variance pop ulation cvar x n X xi x We think sum divided n stead n Equation n random elements sum given mean x n elements nth element deterministi cally solved That n degrees freedom An online appendix book provides rigorous proof Equation unbiased estimator population variance As n n n esti mate variance based n n asymptotically unbiased estimators population variance The number degrees freedom df appear contexts book The df indicates number dimensions data possibly vary With additional information number independently drawn variables restrictions data Imagine variables normally di mensions added restriction x1 2x2 x3 Then defines plane happens orthogonal vector goes origin That adding restriction data points reduced dimensional sur face For sample variance restriction mean sample The square root variance called standard deviation It useful Normal distribu tion usually described terms standard deviation variance Outside context Normal vari ance far common The variance useful right familiar measure dis persion But decom posed ways depending situation provide information variance bias variation explained linear regression Since information extracted decomposi tion variance time time classical statistics worth going dis sections Recall basic algebra form x y expands x2 y2 2xy In special cases un sightly 2xy term eliminated merged term leav ing pleasing result x y x2 y2 million chance winning million dollar lottery states world I exactly dollars wealthier Further research pioneered Kahneman Tversky e g Kahneman et al found humans tend focus features probability distribution They consider events small probability zero probability manageable value e g reading p 1e p 1e Or assume likely state world occurs certainty Bear mind human readers papers interested definitions expectation mean gsl_stats March DISTRIBUTIONS FOR DESCRIPTION Throughout discussion x E x x constant given data set The expectation constant constant E x simply x E y2x expand n n y2i x x n n y x E y2 The breakdown variance equation var x E x x E x 2xx x E x2 E 2xx E x E x2 2E x E x E x2 E x Read var x expectation squared values minus square expected value This form simplifies transformations basic results type frequently appear probability homework questions Q7 Write function display var x E x2 E x E x2 E x input data use verify expressions equal columns data selected source hand Mean squared error The breakdown appears mean squared error Say biased estimate mean x true mean x define bias x x It turns MSE simple function true variance bias The value derived inserting x x expanding square MSE E x x E x x x x E x x 2E x x x x E x x var x bias x E x x bias x var x bias x In case middle term drops becauseE x x MSE breaks simply variance x plus bias squared gsl_stats March CHAPTER Within group group variance The breakdown variance common ANOVA estimations ANOVA short analysis variance arises data divided set groups Then total variance entire data set expressed group variance group variance Above x consisted homogeneous se quence elements xi n break subgroups xij j indicates group indicates elements group There mean x j group j simple mean nj elements group The unsubscripted x continues represent mean entire sample With notation hand similar breakdown given applied groups var x E x x n j nj xij x j x j x n j nj xij x j nj x j x nj xij x j x j x n j nj xij x j nj x j x n j nj var xj n j nj x j x The transition Equation works x j x constant given j nj xij x j x j x j Once unsightly middle term cancels left easily interpretable final equation In case element weighted mean group variances second weighted group variance group taken unit x j variance taken set group means The data metro db set gives average weekday passenger boardings sta tion Washington Metro subway system founding system November The system lines Blue Green Orange Red Yellow Listing breaks variance ridership Washington Metro line line variances Line query join riders lines tables The parens mean 3As Washington relevant detail post measurements May outside peak tourist season gsl_stats March DISTRIBUTIONS FOR DESCRIPTION include apop h void variance_breakdown char table char data char grouping apop_data aggregates apop_query_to_mixed_data mmw select var_pop s var avg s avg count s group s data data table grouping APOP_COL_T aggregates var vars APOP_COL_T aggregates avg means double total apop_query_to_float select var_pop s s data table doublemean_of_vars apop_vector_weighted_mean vars aggregates weights double var_of_means apop_vector_weighted_var means aggregates weights printf total variance g n total printf group variance g n mean_of_vars printf group variance g n var_of_means printf sum g n mean_of_vars var_of_means int main apop_db_open data metro db char joinedtab select riders riders line riders lines lines station riders station variance_breakdown joinedtab riders line Listing Decomposing variance group group Online source amongwithin comfortably inserted clause query line The query line pulls total variance total sum squares query lines gets group variances means Lines weighted mean variances weighted variance means Lines print data screen showing sub calculations add total variance Q7 Rewrite program use data wb db data set including lasses table break variance GDP capita class class variance Within group group variance interesting To ex ample Glaeser et al Equation break variance crime 4Using subquery like force SQL interpreter generate subtable query clearly inefficient Therefore functions like apop_anova wild run reate table query join data index table send table apop_anova function gsl_stats March CHAPTER city city variances find city variance orders magnitude larger city variance Returning Metro data group data year look group variation form group data line ask group variation include apop h int main apop_db_open data metro db char joinedtab select year riders line riders lines riders station lines station apop_data_show apop_anova joinedtab riders line year Listing Produce way ANOVA table breaking variance station passenger boardings year effects line effects interaction term residual Online source metroanova Listing produces ANOVA table spreadsheet like table giving group group variances The form table dates mid 1900s ANOVA basically complex thing matrix inverting computer tabular form facilitates calculation paper pencil desk calculator But conveys meaning entirely forgotten use pencil The rows output present group sum squares That aggregate data points given group mean variation remain With grouping1 grouping2 ways group data group grouping1 Green line Red line group grouping2 interaction groupby grouping1 grouping2 Green Line Red Line Green Line Red Line Using algebra like break total sum squares weighted sum squares grouping1 weighted sum squares grouping2 weighted sum squares grouping1 grouping2 weighted sum squares residual We compare weighted grouped sums residual sum listed F statistic ANOVA table As discussed chapter testing page F statistic taken indicate grouping explains variation explained comparable random group ing data The output example shows grouping year significant refined interaction grouping line year gsl_stats March DISTRIBUTIONS FOR DESCRIPTION grouping line significant meaning later studies justified focusing subway stations broken lines Q7 Most stations multiple lines station likeMetro Center included Blue Orange Red groups In fact Yellow line stations doesn t share lines You easily find online map WashingtonMetro verify This probably causes un derestimate importance line grouping How design grouping puts stations group It help implemen tation produce intermediate table presents desired grouping Does ANOVA new grouping table significance line grouping By changing second group code listing year NULL way ANOVA breaks total sum squares weighted sum squares grouping1 weighted sum squares residual The residual sum squares larger df residual larger case overall change F statistic great Regression variance Next consider OLS model detailed Section In case break ob served value estimated value plus error y yest o var y E yest o y E yest y E o2 2E yest y o It shown yesto OLSXo Xo o E y o y E o Thus 2E term zero left var y E yest y E o2 gsl_stats March CHAPTER Make following definitions SST total sum squares E y y var y SSR Regression sum squares E yest y SSE Sum squared errors E o2 Then expansion var y Equation written SST SSR SSE This popular breakdown variance relatively easy cal culate reasonable interpretation total variance variance explained regression plus unexplained error variance As elements appear page regard F test common coeffi cient determination indicator regression fits data It defined R2 SSR SST SSE SST You notice terminology sum squared components use F test matches ANOVA breakdowns coincidence cases portion data s variation explained model grouping regression portion unexplained model residual In cases use breakdown gauge model explains variation explained random grouping The exact details F test delayed chapter hypothesis testing COVARIANCE The population covariance 2xy n xi x yi y equivalent E xy E x E y Q Re apply variance expansion prove The variance special case x y As variance unbiased estimate sample covariance s2xy 2xy nn e x x y y divided n instead n Given vector variables x1 x2 xn typically want covariance combination This neatly expressed matrix gsl_stats March DISTRIBUTIONS FOR DESCRIPTION 1n 2n 2n1 n2 n diagonal elements variances e covariance xi diagonal terms symmetric sense 2ij ji j Correlation Cauchy Schwarz The correlation coefficient called Pearson correlation coefficient xy xy xy By statistic useful looking relations columns data summarized matrix like covariance matrix The cor relation matrix symmetric ones diagonal variable perfectly correlated The Cauchy Schwarz inequality puts bounds correlation coefficient That range indicates variable moves opposite direction indicates perfect sync The matrix correlations popular favorite getting basic descrip tive information data set produce apop_data_ orrelation The correlation matrix basis Cramer Rao lower bound page MORE MOMENTS Given continuous probability distribution data taken write expectation variance equation integral var f x E f x f x x f x f x p x dx 5It digression prove Cauchy Schwarz inequality Holder s inequality probability text Feller volume II p gsl_stats March CHAPTER Similarly higher powers skew f x x f x f x p x dx kurtosis f x x f x f x p x dx These integrals central moments f x They central subtracted mean function taking second fourth power Transformed moments Let S K fourth central moments given Some use standardized moment kurtosis equal K K K K author felt convenient Similarly S S skew These adjustments intended ease com parisons standard Normal acommodate differences scale The way know given source means refers skew kurtosis look definitions The GSL uses K engineers probably comparing data standard Normal Apophenia uses K corrections add complication situations outside Normal distri bution easy needed What information higher moments Section discuss powerful Central Limit Theorem says variable represents mean set independent identical draws N distri bution unknowns estimated data These parameters completely define distribution skew Normal zero kurtosis If kurto sis larger means assumption independent draws false observations interconnected One sees social networks stock markets systems independent agents observe imitate Positive skew indicates distribution upward leaning negative skew indicates downward lean Kurtosis typically plain English fat tails density tails distribution For example kurtosis N kurtosis Student s t distribution n degrees freedom greater decreases n converging page analysis An un normalized kurtosis known leptokurtic known platykurtic Figure mnemonic The caveats unbiased estimates sample versus population variance box page hold skew kurtosis calculating mean definitions leads biased estimate population skew kurtosis simple corrections produce unbiased estimate An online 6The central moment zero non central second moments difficult interpret basically ignored Since ambiguity authors refer useful moments nth moment n leave understood moment central non central gsl_stats March DISTRIBUTIONS FOR DESCRIPTION Figure Leptokurtic mesokurtic platykurtic illustration Gosset Biometrika Student p In notation time kurtosis variance squared appendix book offers facts central moments derives correction factors But cases population vs sample detail relevant small n Efron Tibshirani p state estimating variance n good estimating n highly esteemed precedent ignoring detail For higher moments sample population estimates converge quickly Coding Given vector Apophenia provides functions calculate e g apop_data set gather_your_data_here apop_data corr apop_data_correlation set APOP_COL set v1 APOP_COL set v2 double mean1 apop_vector_mean v1 double var1 apop_vector_var v1 double skew1 apop_vector_skew v1 double kurt1 apop_vector_kurtosis v1 double cov apop_vector_cov v1 v2 double cor apop_vector_correlation v1 v2 apop_data_show apop_data_summarize set The item code apop_matrix_summarize produces table summary statistics column data set gsl_stats March CHAPTER Your data aggregated line data represents multiple observa tions For example sampling efficiency improved sampling subpopula tions differently depending expected variance Sarndal et al For reasons data statistical agencies includes weightings This place details statistically sound means weighting data separate vector weights use apop_ve tor_ weighted_mean apop_ve tor_weighted_var et cetera use weights Or apop_data set s weights vector filled apop_data_summarizewill use Q7 Write query pulls number males females population density Census data data ensus db The query join geography demos tables county num ber exclude states national total return column table Write function void summarize_paired_data har q takes query produces column table outputs summary information columns including mean variance correlation coefficients Write main sends query function run program Is population density positively negatively correlated males female Write query pulls ratio median income time workers female median income time workers male population density Add line main send query summarizing function How new pair variables correlated Quantiles The mean variance misleading skewed data The option describing data set distribution likely ab Normal plot histogram data page A numeric option print quartiles quintiles deciles data For quartiles sort data display values data points way set The value minimum gsl_stats March DISTRIBUTIONS FOR DESCRIPTION data set value maximum value probably median For quintiles print values data points way set deciles print values percent Sorting data simple If apop_data set gsl_ve tor apop_data_sort my_data d gsl_vector_sort my_vector sort my_data place column d escending order sort vector place ascending order gsl_ve tor_get my_ve tor minimum data gsl_ve tor_get my_ve tor my_ve tor size maximum gsl_ve tor_get my_ve tor my_ve tor size median Alternatively function apop_ve tor_per entiles takes gsl_ve tor returns percentiles value data point way data It takes arguments data vector character describing rounding method u rounding d rounding averaging Since number elements data set rarely divisible position percentile points likely falls data points For example data set points tenth data point data set eleventh data point set tenth percentile If specify u eleventh data point specify d tenth data point specify simple average The standard definition median middle value data point data set odd number elements average data points closest middle data set number elements Thus function find median data set It finds percentiles averaging rule interpolation marks 50th percentile cleans returns value double find_median gsl_vector v double pctiles apop_vector_percentiles v double pctiles free pctiles return gsl_stats March CHAPTER Q7 Write function header double show_quantiles gsl_ve tor v har rounding_method int divisions passes v androunding_method apop_ve tor_per entiles prints ta ble selected quantiles screen For example divisions print quartiles divisions print quintiles divisions print deciles et cetera On page tabulated GDP capita countries world Use function print deciles country incomes data wb Q7 The trimean sum quartile quartile times median Tukey p It uses information distri bution median robust extreme values unlike mean Write function takes vector data points applies apop_ ve tor_per entiles internally returns trimean How trimean GDP capita compare mean median See page compares percentiles data percentiles assumed distribution test data drawn distribution Themost basic means describing data moments The basic moments produced skimmed data set simple cases need The variance decomposed smaller parts reveal ing information data set s variation arose The mean variance known information higher moments skew kurtosis It important know variables interrelate summarized correlation matrix There line function produce pieces formation Notably apop_data_summarize produces summary column data set You detailed numerical description data set s dis tribution quartiles quintiles use apop_ve tor_ entiles gsl_stats March DISTRIBUTIONS FOR DESCRIPTION SAMPLE DISTRIBUTIONS Here distributions observed variable They memorize statistics test Each story attached directly useful modeling For example think variable outcome n independent binary events variable modeled Binomial distribution estimate parameter distribution model variable test inclined Table presents table story distribution telling After catalog models I examples modeling The distribution The story Bernoulli A single success failure draw fixed p Binomial What odds getting x successes n Bernoulli draws fixed p Hypergeometric What odds getting x successes n Bernoulli draws p initially fixed draw ing replacement Normal Gaussian Binomial n j n xij n j Normal Lognormal If j n xij j Lognormal Multinomial n draws m possibilities probabilities p1 pm m pi Multivariate Normal Multinomial n Negative binomial How Bernoulli draws n successes Poisson Given events period events t pe riods Gamma The Negative Poisson given Poisson setup long n events Exponential A proportion remaining stock leaves period left time t Beta A versatile way describe odds p takes value Uniform No information upper lower bounds Table Every probability distribution tells story Common distributions statistical parameters opposed natural populations discussed Section gsl_stats March CHAPTER Bernoulli Poisson events The core system event times happens Some peo ple disease days rains days Events add continuous quantities cities chance rain given day chance populations high rates disease From variants instead asking successes n trials Binomial Poisson distributions ask trials expect n successes Negative binomial Gamma distri butions We look sampling replacement Hypergeometric We look case number trials goes infinity Normal aggregate trials product Lognormal In short surprisingly wide range situations described simple concept binary events aggregated ways The snowflake problem For controlled study typical physical sciences claim fixed probability p plausible social science experiments observation individual different geographic region assumption observations identical unacceptable snowflake unique It like fixed p fixed models simple appli cable situations frankly However building blocks produce descriptive models Section presents set linear regression models handle snowflake problem throw probability framework presented chapter But solve snowflake problem use simple probability models em bedding linear model inside distribution let pi differ element estimate pi linear combination element s characteristics Page covers examples possibilities provided models models Statistics estimators The catalog section includes typical items want distribution random number generator RNG produce data given distribution probability density function PDF cumulative density function CDF 7This case independent variable continuous When discrete PDF named probability mass function PMF CDF cumulative mass function CMF For result statement PDFs analogous result PMFs result statement CDFs analogous result CMFs gsl_stats March DISTRIBUTIONS FOR DESCRIPTION The catalog includes expected value variance distributions distinct means variances point key manner given data set mean statistic function data form f x Mean given model mean variance draw functions param eters e g given Binomial distribution parameters n p E x n p np Of course rarely know parameters left estimating data estimate p p function data set hand We return forth estimates data estimates model catalog models The details RNG use discussed Chapter RNG functions included catalog easy reference requires pointer agsl_rng named r THE BERNOULLI FAMILY The set distributions built narrative drawing pool events fixed probabil ity The commonly example flipping coin single event comes heads probability p But examples abound draw recipient sales pitch list people check purchased purchase pull single citizen population victim crime For event draw values zero known Bernoulli draw Bernoulli The Bernoulli distribution represents result Bernoulli draw meaning P x p p P x p p P x p Notice E x p p x zero P x p px p x x gsl_ran_bernoulli_pdf x p E x p p var x p p p RNG gsl_ran_bernoulli r p Binomial Now n Bernoulli draws observe n events The output count people dunned telemarketer agree purchase product crime victims fixed population The probability observing exactly k events p k Binomial n p Counting x successes n trials trivial For example ways successes occur trials gsl_stats March CHAPTER P x p n x p p p p P x p n x n n n n Figure Left The Binomial distribution n values p Right Bi nomial distribution p values n As n grows distribution approaches anN np p np p model underlying Bi nomial model treats equally The form distribution borrows counting technique combi natorics The notation n x indicates n choose x number unordered sets x elements pulled n objects The equation n x n x n x function gsl_sf_ hoose n x GSL s Special Functions sec tion For example exactly thirty successful trials ways 94e25 Combinatorics dictates shape curve There way list zeros ones ways list symmetrically zero In order counts zero ones This simple counting scheme produces bell curve Returning coin flipping p coin flipped times n p heads relatively high p heads p heads nil In fact n probability distribution approaches familiar Normal distribution mean np variance np p Figure Assuming telemarketer s probability selling equal expect plot months telemarketer results look like Normal distribution telemarketers successfully selling np victims exceptionally poorly The assumption telemarketer equally effective tested checking digression Normal distribution gsl_stats March DISTRIBUTIONS FOR DESCRIPTION P x p n n x px p n x gsl_ran_binomial_pdf x p n E x n p np var x n p np p RNG gsl_ran_binomial r p n If X Bernoulli p sum n independent draws n Xi Binomial n p As n Binomial n p Poisson np N np np p Since n known E x var x calculated data Equations oversolved system variables unknown p Thus test excess variance indicates interactions falsify observations iid independent identically distributed Bernoulli events A variant p n draws The statistic interest differs cal culated catalog easy transform information Say multiply elements set x k Then mean goes x xi n kx kxi n k xi n kx The variance goes x xi x n 2kx kxi kx n k2 xi x n k22k For example interested estimating p data Binomial type story Since E x np Binomial model estimate p E x n As variance let k paragraph n variance x n original variance var x np p times n2 gives var p p p n Hypergeometric Say pool N elements initially consisting s successes f N s failures So N s f Bernoulli probability entire pool p s N What odds x successes n draws replacement In case probability success changes draws The counting difficult resulting somewhat involved equation gsl_stats March CHAPTER P x s f n s x f n x N n gsl_ran_hypergeometric_pdf x s f n E x n s f ns N var x n s f n s N s N N n N RNG gsl_ran_hypergeometric r s f n As N drawing replacement equivalent Hypergeometric distribution approaches Binomial Multinomial The Binomial distribution based having series events states success failure sick heads tails et cetera But possible events like left right center Africa Eurasia Australia Americas The Multinomial distribution extends Bi nomial distribution cases The Binomial case expressed parameter p indicated success probability p failure probability p The Multinomial case requires k variables p1 pk k pi P x p n n x1 xk px11 p xk k gsl_ran_multinomial_pdf k p n E x1 x2 xn n p n p1 p2 pn var x n p n p1 p1 p1p2 p1pk p1p2 p2 p2 p2pk pkp1 pkp2 pk pk RNG gsl_ran_multinomial r draws k p There changes norm GSL s functions First p n arrays doubles size k If k p system normalizes gsl_stats March DISTRIBUTIONS FOR DESCRIPTION probabilities case Also RNGs draw number time draws K elements time bins array You verify k Binomial distribution Normal You know love bell curve aka Gaussian distribution It pic tured values Figure As Section explain detail set means generated iid draws Normal distribution That Draw K items population nondegenerate distribu tion x1 x2 xk The Normal approximation works best K large Write mean items x Repeat steps n times producing set x x x x n Then x Normal distribution Alternatively Binomial distribution produced bell curve n n Binomial distribution approaches Normal distribution P x exp x gsl_ran_gaussian_pdf x sigma mu E x var x x N y dy gsl_cdf_gaussian_P x mu sigma x N y dy gsl_cdf_gaussian_Q x mu sigma RNG gsl_ran_gaussian r sigma mu If X N Y N X Y N Because Normal symmetric X Y N Section p discusses Central Limit Theorem greater detail gsl_stats March CHAPTER P x x Figure The Normal distribution Multivariate Normal Just Normal distribution extension Bino mial Multivariate Normal extension Multi nomial Say data set X includes thousand observations seven variables X matrix Let mean vector length seven covariance variables seven seven matrix Then Multivariate Normal distribution fit data P X exp X X n det E X var X When X column reduces univariate Normal distribution Lognormal The Normal distribution apropos items sample mean set draws population xi s1 s2 sk k But data point product series iid samples x s1 s2 sk Then log x ln x ln s1 ln s2 ln sk log sum gsl_stats March DISTRIBUTIONS FOR DESCRIPTION P x x Figure The Lognormal distribution independent elements e n times mean Very broadly point data set produced summing iid draws Normally distributed point data set produced taking product iid draws log Normally distributed e lognormal distribution The section present example Figure shows Lognormal distributions A notational warning typical means expressing lognormal distribu tion refer mean Normal distribution replaced element x data set ex producing standard Normal distribution Be careful confuse mean variance data actually p x exp ln x x ran_lognormal_pdf x mu sigma E x e var x e2 e RNG ran_lognormal rng mu sigma gsl_stats March CHAPTER Negative binomial Say sequence Bernoulli draws How failures n successes If p percent cars illegally parked meter reader hopes write n parking tickets Negative binomial tells odds able stop n x cars The form based Gamma function z xz 1e xdx gsl_sf_gamma z You easily verify z z z Also generally z z positive integers Thus n x integers formulas based Gamma function reduce familiar factorial based counting formulas P x n p n x x n pn p x gsl_ran_negative_binomial_pdf x p n E x n p n p p var x n p n p p2 RNG gsl_ran_negative_binomial rng p n RATES A Poisson process like Bernoulli draw unit mea surement continuous typically measure time space It makes sense half hour half coin flip stories based Bernoulli draws modified slightly allow rate events hour applied half hour week Baltimore Maryland sees days precipitation year somewhat consistently spaced months But days rain snow single week The Poisson distribution answers question We count weeks rain week twice week et cetera The Exponential distribution answers question Turning want week rainfalls long wait The Gamma distribution answers question gsl_stats March DISTRIBUTIONS FOR DESCRIPTION P x x Figure The Poisson distribution Poisson Say independent events rainy day landmine bad data occur mean rate events span time space et cetera What probability x events single span We assuming events occur sufficiently rate rate applies different time periods rate day rate week rate hour See Figure P x e x x gsl_ran_poisson_pdf x lambda E x var x RNG gsl_ran_poisson r lambda As n Binomial n p Poisson np If X Poisson Y Poisson X Y independent X Y Poisson As Poisson N gsl_stats March CHAPTER Q7 Calculate Binomial distributed probability rainfalls seven days given probability rain day p Calculate Poisson distributed probability rainfalls seven days given day Gamma distribution The Gamma distribution named relies heavily Gamma function introduced page Along Beta distribution naming scheme great notational tragedies mathematics A better statistical context Negative Poisson relates Poisson distribution way Negative binomial relates Binomial If timing events follows Poisson distribution meaning events come rate period distribution tells long wait nth event occurs The form Gamma distribution shown parameter values Fig ure typically expressed terms shape parameter Poisson parameter Here summary function terms parameters P x n n n xn 1e x x gsl_ran_gamma_pdf x n theta P x n n n xn 1e x x E x n n n var x n k2 k x G y n dy gsl_cdf_gamma_P x theta x G y n dy gsl_cdf_gamma_Q n theta RNG gsl_ran_gamma r n theta gsl_stats March DISTRIBUTIONS FOR DESCRIPTION P x n x n n n n n Figure The Gamma distribution With n df Gamma distribution 2df distribution introduced page With n Gamma distribution Exponential distribution Exponential distribution The Gamma distribution found time n events occur consider time event occurs positive x0 positive x n Equation defining PDF Gamma distribution reduces simply e x If population items t e xdx percent event time zero time t If event causes item leave population minus percent population time t The form ex easy integrate gives percent left time t e t So story population members leave Poisson pro cess Common examples include stock unemployed workers find job period radioactive particles emanating block drug dosage remaining person s blood stream Figure shows examples Exponential distribution gsl_stats March CHAPTER P x x Figure The Exponential distribution Since exponent called Negative exponential distri bution P x e x gsl_ran_exponential_pdf x lambda E x var x x Exp dy gsl_cdf_exponential_P x lambda x Exp dy gsl_cdf_exponential_Q x lambda RNG gsl_ran_exponential r lambda gsl_stats March DISTRIBUTIONS FOR DESCRIPTION P x x Figure The Beta distribution DESCRIPTION Here distributions frequently model ing describe shape random variable Beta distribution Just Gamma distribution named Gamma function Beta distribution named Beta function parameters typically notated This book spell Beta Beta function use B Beta distribution The Beta function described following forms Beta x x dx gsl_sf_beta alpha beta The Beta distribution flexible way describe data inside range Figure shows different parameterizations lead left leaning right leaning concave convex curve page gsl_stats March CHAPTER P x Beta x x gsl_ran_beta_pdf x alpha beta E x var x x B y dy gsl_cdf_beta_P x alpha beta x B y dy gsl_cdf_beta_Q x alpha beta RNG gsl_ran_beta r alpha beta If distribution bimodal peaks zero If distribution unimodal As rises distribution leans rises distribution leans zero distribution symmetric If Uniform distribution The Beta distribution order statistics The order statistic set numbers x smallest number set second smallest largest order statistic max x Assume elements x drawn Uniform distribu tion Then th order statistic B distribution Q7 Write function takes gsl_rng integers b produces list b random numbers sorts returns ath order statistic Write function function times plot PDF returned data apop_plot_histogram It helps precede plot output Gnuplot set xrange range consistent Write main produces animation PDFs 100th order statistic set numbers Replace draw sort function draw B b distribution animate results gsl_stats March DISTRIBUTIONS FOR DESCRIPTION Uniform distribution What discussion distributions complete mention Uniform It represents belief value equally possible P x x x x gsl_ran_flat_pdf x alpha beta E x var x x U y dy x x x x gsl_cdf_flat_P x alpha beta x U y dy gsl_cdf_flat_Q x alpha beta RNG general gsl_ran_flat r alpha beta RNG gsl_rng_uniform r Probability theorists ages developed models dicate process follows certain guidelines data predictable form A single draw binary event fixed probability Bernoulli distribution wealth distributions derived An event occurs frequency period volume et cetera known Poisson process wealth distributions derived process If x mean set independent identically distributed draws nondegenerate distribution distribution x ap proaches Normal distribution This Central Limit Theorem The Beta distribution useful modeling variety variables restricted It unimodal bimodal lean direction simply match Uniform distribution gsl_stats March CHAPTER USING THE SAMPLE DISTRIBUTIONS Here examples use distributions described practical benefit LOOKING UP FIGURES If I draws Bernoulli event probability likelihood I successes Statistics textbooks include appendix listing tables common distribu tions tables effectively obsolete modern textbooks refer reader appropriate function stats package For long days grand tables code supplement includes normaltable code producing neatly formatted table CDFs set Normal distributions p value reported hypothesis tests minus listed value The code printed entirely boring tables produces provide nice way feel distributions Alternatively Apophenia s command line program apop_lookup look quick number GENERATING DATA FROM A DISTRIBUTION Each distribution neatly summarizes oft modeled story capsule simulation process build ing block larger simulation Listing gives quick initial example It based work originated Gibrat extended including Axtell Zipf s law distribution sizes cities firms agglomera tions tends Exponential type form In model comes agents growth rates assumed mean set iid random shocks Normally distributed First program produces set agents characteristic size stored agsl_ve tor The initialize function draws agent sizes Uniform distribution To requires gsl_rng main allocates apop_ rng_allo passes initialize See Chapter random number generators Each period firms grow Normally distributed rate grow function That grow function randomly draws g gsl_rng_gaussian reassigns firm size size size exp g The likely growth rate exp When g exp g g exp g gsl_stats March DISTRIBUTIONS FOR DESCRIPTION include apop h int agentct int periods int binct double pauselength gsl_rng r void initialize double setme setme gsl_rng_uniform r void grow double val val exp gsl_ran_gaussian r double estimate gsl_vector agentlist return apop_vector_mean agentlist int main gsl_vector agentlist gsl_vector_alloc agentct r apop_rng_alloc apop_vector_apply agentlist initialize int periods apop_plot_histogram agentlist binct NULL printf pause g n pauselength apop_vector_apply agentlist grow fprintf stderr mean g n estimate agentlist Listing A model Normally distributed growth Online source normalgrowth Also exp g exp g symmetry Normal distribution g g equal likelihood easy agent find good luck period countered comparable bad luck period leaving near started The output set Gnuplot commands use normalgrowth gnuplot With pause histogram output animation showing quick transition Uniform distribution steep Lognormal distribution agents fast approaching zero size handful size approach ing 8Here x axis firm size y axis number firms Typically Zipf type distributions displayed somewhat differently x axis rank firm 1st 2nd 3rd et cetera y axis size ranked firm Converting form left exercise reader Hint use gsl_ve tor_ sort gsl_stats March CHAPTER The step model estimation return pages It is output printed stderr aka screen pipe Gnuplot disturbed SIMULATION Fein et al found depressive patients responded combination Lithium monoamine oxidase inhibitor MAOI But types drug require careful monitoring Lithium overdoses common potentially damaging combination MAOIs chocolate fatal include apop h double find_lambda double half_life double lambda half_life log return gsl_cdf_exponential_Q lambda int main double li maoi int days gsl_matrix d gsl_matrix_alloc days double hourly_decay1 find_lambda hours lithium carbonate double hourly_decay2 find_lambda hours phenelzine size_t days li hourly_decay1 maoi hourly_decay2 li maoi APOP_MATRIX_ROW d onehour apop_vector_fill onehour li maoi maoi li printf plot maoi lines title Li maoi lines title MAOI maoi lines title MAOI Li pct n remove maoi apop_matrix_print d maoi Listing A simulation blood stream person taking drugs Online source maoi Listing simulates patient s blood stream follows regime Lithium carbonate average half life hours high variance MAOI named phenelzine average half life hours As story page drug leaves blood stream Poisson process drug remaining blood described Exponential distribution gsl_stats March DISTRIBUTIONS FOR DESCRIPTION A m o u n t L n d M A O I P e rc e n t M A O I L Day Li MAOI MAOI Li pct Figure The typical sawtooth pattern decay renewal The step convert half life commonly pharmacists parameter exponential distribution The find_lambda function Given gsl_ df_exponential_Q lambda answers question percentage given initial level remaining hour The main simulation simple hourly loop decrements drug blood stream calculated checks time patient meds records levels chart The ungainly printf statement end plots result Gnuplot save data needs reread data file times plot columns Figure shows density blood subject takes mg Lithium midnight day mg MAOI noon day For convenience scaling plot Lithium blood stream divided In later days subject s system reached dynamic equilibrium takes mg Li day loses mg day similarly mg MAOI The ratio MAOI Li jumps constantly range Q7 Derive verify find_lambda function correctly converts half life Exponential distribution s parameter gsl_stats March CHAPTER Q7 Let stocks employed unemployed Let half life employment e transition unemployment days half life unemployment e finding job weeks days Modify Lithium MAOI program model situation For period calculate loss employment unemployment stocks transfer appropriate number people stock What equilibrium unemployment rate FITTING EXISTING DATA The common goal book estimate parameters model data given data set find parameters distributions You parameter solved solved mean variance For example Equations de scribing parameters Binomial distribution system equations unknowns np np p It easy calculate estimates data plug estimates system equations find parameters distribution You verify equations n p This method moments estimation e g Greene pp 117ff To summarize method write parameter estimates functions mean variance skew kurtosis find estimates parame ters data use parameter estimates solve parameter estimates model But problems easily crop For example count observations find value n data set given n system equations equations unknown p The Poisson distribution similar simpler story single parameter equals different moments So data set shows use Apophenia doesn t fret issue uses maximum gsl_stats March DISTRIBUTIONS FOR DESCRIPTION likelihood estimator MLE MLE discussed fully Chapter For Uniform method moments doesn t work expression oversolved equations way solve However moments thought likely value given data x simply min x max x Most distributions apop_model associated apop_normal apop_gamma apop_uniform et cetera data set hand quickly estimate parameters apop_data d your_data apop_model norm apop_estimate d apop_normal apop_model beta apop_estimate d apop_beta apop_model_show norm apop_model_show beta apop_model_show apop_estimate d apop_gamma Q7 Listing produces data set Zipf distributed Add estimation estimate function fits Better run tournament First declare array models Lognormal Zipf Exponential Gamma Write loop esti mate model data fill array confidence levels based log likelihood tests Is tournament valid See notes multiple testing problem The method moments provides preview working model based estimations remainder book It took data produced estimate model parameters estimate statistic estimate model parameters produced data As reader noticed interactions data model param eters statistics create opportunities confusion Here notes bear mind The expected value variance measures data set model imposed function data E g E x The expected value variance measures model functions ideal parameters data set E g E x function Our estimate model parameters given data set function given data set known parameters hand For example Normal parameter model specification estimate gsl_stats March CHAPTER write function data Any variable hat like p notated function data p x We statistic like E x function data fact define statistic function data But models data free analogues statistics Given probability distribution P x expected value E f x x f x P x dx meaning integrate x E f x function The model lives taken understood context authors parameters understood context leaving expected value written asE f x expression function x BAYESIAN UPDATING The definition conditional probability based statement P A B P A B P B English like lihood A B occurring time equals likelihood A occurring given B times likelihood B occurs The said versing A B P A B P B A P A Equating complementary forms shunting P B gives common form Bayes s rule P A B P B A P A P B Now apply Say prior belief parameter distribution mean data set N let Pri We gather data set X express likelihood gathered data set given haphazard value P X Let B entire range values We use Bayes s rule produce posterior distribution Post X P X Pri P X So right hand prior belief s value expressed distribution likelihood function P X expressing odds observing data observed given parameter On left hand new distribution takes account fact observed data X In short equation data update beliefs distribution Pri Post The numerator relatively clear requires local information write P X form Post X P X Pri B B P X B Pri B dB gsl_stats March DISTRIBUTIONS FOR DESCRIPTION reveal denominator actually global information calculating requires covering entire range Local information easy global information hard pages ff Bayesian updating described form ignores global Post X P X Pri That posterior equals right hand times fixed denominator depend given value This compare ratios like Post X Post X given right conditions ratio running likelihood ratio tests discussed Chapter Computationally possibilities moving forward given problem determining global scale distribution First number conjugate distribution pairs shown produce output model matches prior form updated parameters In case apop_ update function simply returns given model new parameters example Chapter present computationally intensive method producing pos terior distribution analytic route closed e Monte Carlo Maximum Likelihood But apop_update black box takes models outputs updated conjugate form possible empiri cal distribution We draws output distribution plot use prior new updating procedure new data set comes et cetera An example Beta Binomial For assume likelihood tattoo constant individuals regard age gender drop clearly false assumption section multilevel modeling page We like know value likelihood That statistic interest p count people tattoos total number people sample Because weak knowledge p describe beliefs value distribution p small odds near zero reasonable chance The Beta distribution good way de scribe distribution positive inputs zero Let B indicate Beta distribution B Uniform distribution reasonably safe way express neutral prior belief p Alterna tively setting Pri p B weight p extremes raising second parameter little bring mode beliefs p See Figure gsl_stats March CHAPTER Given p distribution expected count tattooed individuals Binomial For individual p chance having tattoo simple Bernoulli draw The overall study makes n draws fits model un derlying Binomial distribution perfectly But know p paragraph begin taking p given That Binomial distribution describes P data p It happens Beta Binomial distributions conjugate This means given Pri p Beta distribution P data p Binomial distribu tion posterior Post p data Beta distribution like prior Tables conjugate pairs readily available online However parameters updated accommodate new information Let x number tattoos observed n subjects prior distribution B Then posterior B x n x distribution The discussion prior offered possibilities like But survey subjects count tattooed individuals dwarfs Therefore approximate posterior simply B x n x The catalog listed expected value Beta distribution With x n x reduces simply x n That expected posterior value p percentage people sample tattoos p Bayesian updating gave result exactly expect The variance Beta distribution Again N denominator basically disappears Filling x n x p p n Again Binomial distribution We B distribution small weak prior mean moderately sized data set entirely dwarfs beliefs expressed prior So point updating process First use stronger prior like effect posterior distribution updating data set Second system provides consistent mechanism combining multiple data sets The posterior distribution accounting data set form appropriate use prior distribution updated data set Thus Bayesian updating provides natural mechanism running metas tudies gsl_stats March DISTRIBUTIONS FOR DESCRIPTION Q7 Verify apop_update Beta prior Binomial likelihood function tattoo data produce estimated mean variance simple x n estimate Gather data column tattoos egohas tattoos coded yes calculate formul paragraphs What results assume stronger prior like B B NON PARAMETRIC DESCRIPTION Say data set like know distribution data drawn To point assumed form distribution Normal Binomial Poisson et cetera estimate parame ters distribution data But assuming simple parametric form describe distribution data drawn The simplest answer plain old histogram drawn data This sufficient But especially small data sets histogram dissatisfactory features If draws value value mean probability zero didn t luck drawing time Thus great deal nonparametric modeling consists finding ways smooth histogram based claim actual distribution lumpy data The histogram The histogram assumption free way describe like lihood distribution data drawn Simply lay row bins pile data point appropriate bin normalize bins sum desired plot Because common use togram plotting random draws discussion histogram production appear chapter random draws page The key free variable histogram bandwidth range x axis goes data point If bandwidth small histogram slices generally spiky data A large bandwidth oversmooths infinite bandwidth histogram bar particularly informative Formally bias variance trade extremes try bandwidths looks nice See Givens Hoeting ch extensive discussion question context data smoothing gsl_stats March CHAPTER Moving average The simplest means smoothing data moving average placing data point mean adjacent b data points b bandwidth You use histograms series For example movingavg online code supplement plots tem perature deviances shown data limate db moving average replaces data point mean deviation year window based apop_ve tor_moving_average function line line line line Figure A series density plots As h rises kernel density smooths fewer peaks Kernel smoothing The kernel density estimate based function f t X h n 1N t Xi h n h X1 X2 Xn R n data points observed N y Normal density function evaluated y h R bandwidth Thus overall curve sum set subcurves centered different data point Figure shows effect raising h shape set fitted curves When h small Normal distributions data point sharp spikes mode data point As h grows spikes spread merge sum subdistributions produces single bell curve See page 9The data male viewership TV specials Chwe gsl_stats March DISTRIBUTIONS FOR DESCRIPTION plots generated Silverman As usual simple form code produce default kernel density data set extensive form allows control Try Listing plots histogram precipitation figures kernel density smoothed version based N kernel Also try progression data s spikes smooth bell curve include apop h int main apop_db_open data climate db apop_data data apop_query_to_data select pcp precip apop_model h apop_estimate data apop_histogram apop_histogram_normalize h remove h remove k apop_histogram_print h h apop_model kernel apop_model_set_parameters apop_normal apop_model k apop_model_copy apop_kernel_density Apop_settings_add_group k apop_kernel_density NULL h kernel NULL apop_histogram_print k k printf plot h lines title data k lines title smoothed n Listing A histogram smoothing kernel densities Run smoothing gnuplot Online source smoothing Q7 Plot data tv set histogram bins smoothed version bin histogram moving average bandwidth bin histogram smoothed Normal x kernel density bin histogram smoothed Uniform x x kernel density gsl_stats March LINEAR PROJECTIONS Our weakness forbids considering entire universe makes cut slices Poincare p This chapter covers models sense data dimensions humans visualize The approach taken Section known prin cipal component analysis PCA find dimensional subspace best describes dimensional data flatten data dimensions The second approach Section provides structure The model la bels variable dependent variable claims linear combina tion independent variables This ordinary squares OLS model endless variants The remainder chapter looks OLS applied combination distributions prior chapter One way characterize projection approaches aim project N dimensional data best subspace significantly fewer N dimen sions different definitions best The standard OLS regression consists finding dimensional line minimizes sum squared distances data line PCA consists finding dimensions variance projection data dimensions maximized gsl_stats March LINEAR PROJECTIONS PRINCIPAL COMPONENT ANALYSIS PCA closely related factor analysis fields known spectral decomposition The phase calculating eigenvalues called singular value decomposition It purely descriptive method The idea want dimensions capture variance possible usually plot dimensions paper After plotting data markers certain observations find intuitive descriptions dimensions plotted data For example Poole Rosenthal projected votes cast Congressmen US Congresses found variance vote patterns explained dimensions One dimensions broadly de scribed fiscal issues social issues This method stands Poole Rosenthal look bills place scale data placed scales Shepard Cooper asked sighted subjects questions color words red orange principal component analysis data place words dimensions formed familiar color wheel They blind subjects found projection collapsed single dimension violet purple blue end scale yellow gold Thus data indicates blind subjects think colors univariate scale ranging dark colors bright colors It shown best n axes sense n eigenvectors data s covariance matrix n largest associated eigenvalues The programs discussed query variables US Census data population median age males females US state common wealth District Columbia Puerto Rico They factor analysis project original data space maximizes variance producing plot Figure The programs display eigenvectors screen They find eigenvector approximately variables given second term population describes variance data The X axis plot follows population2 The eigenvalues Y axis sided combination variable males female median age That said interpret Y axis Those familiar US geographywill observe states primarily covered cities extreme DC high 1They actually analysis intriguing maximum likelihood method eigenvector method Nonetheless end result interpretation 2As census California million Texas New York Florida et cetera gsl_stats March CHAPTER S e c o n d e ig e n v e c r First eigenvector AL AK AZ AR CACO CT DE DC FL GA HI ID ILIN IA KS KY LA ME MD MA MI MN MS MO MT NE NV NH NJ NM NY NC ND OH OK OR PA RI SC SD TN TX UT VT VA WA WV WI WY PR Figure States decomposed population X axis combination median age gender balance urban ness Y axis Y axis states far rural urban extreme Alaska Thus principal component analysis indicates plausibly interpret variation median age male female balance single variable representing state s urban rural balance Because interpreting meaning artificial axis subjective exercise stories possible example argue second axis instead represents state s East West location CODING IT As algorithms process coding straightforward involves number details This section com putation principal component analysis levels The goes steps calculating eigenvectors second single function The input output functions identical programs redundancy minimizing method implementing functions separate file Listing corresponding header simple repeated included online code supplement eigenbox h The query_data function self explanatory With clause sele t geo_ names row_names Apophenia use state names row names plain data These programs intended run pipe Gnuplot like eigeneasy gnuplot persist So want additional information Gnuplot gsl_stats March LINEAR PROJECTIONS include eigenbox h apop_data query_data apop_db_open data census db return apop_query_to_data select postcode row_names n m_per_100_f population 1e6 population median_age n geography income demos postcodes n income sumlevel n geography geo_id demos geo_id n income geo_name postcodes state n geography geo_id income geo_id void show_projection gsl_matrix pc_space apop_data data apop_opts output_type p apop_opts output_pipe stderr fprintf stderr The eigenvectors n apop_matrix_print pc_space NULL apop_data projected apop_dot data apop_matrix_to_data pc_space printf plot labels n apop_data_show projected Listing The tools including query method displaying labeled points Online source eigenbox understand need send different location Thus lines send stderr eigenvectors printed screen instead sent thestdout pipeline Gnuplot The plot command line labels meaning instead points letter postal codes seen Figure As actual math Listing shows step process The hard finding eigenvalues X X GSL saw coming gives thegsl_eigen_symm functions calculate eigenvectors symmetric matrix The find_eigens function shows use function The GSL polite allocate large vectors backs asks pass pre allocated workspaces needs things Thus findeigens function allocates workspace calls eigenfunction frees workspace It frees matrix eigenvalues calculated end matrix destroyed calculations referred To sure future tests way subje t work line sets pointer NULL 3There cheat apop_sv_de omposition function use apop_normalize_for_ svd xpx matrix function ensure row x x You erase line eigenhard input SVD specific normalization function apop_dot step look subtle shifts gsl_stats March CHAPTER include eigenbox h void find_eigens gsl_matrix subject gsl_vector eigenvals gsl_matrix eigenvecs gsl_eigen_symmv_workspace w gsl_eigen_symmv_alloc subject size1 gsl_eigen_symmv subject eigenvals eigenvecs w gsl_eigen_symmv_free w gsl_matrix_free subject subject NULL gsl_matrix pull_best_dims int ds int dims gsl_vector evals gsl_matrix evecs size_t indexes dims gsl_matrix pc_space gsl_matrix_alloc ds dims gsl_sort_vector_largest_index indexes dims evals dims APOP_MATRIX_COL evecs indexes temp_vector gsl_matrix_set_col pc_space temp_vector return pc_space int main int dims apop_data x query_data apop_data cp apop_data_copy x int ds x matrix size2 gsl_vector eigenvals gsl_vector_alloc ds gsl_matrix eigenvecs gsl_matrix_alloc ds ds apop_matrix_normalize x matrix c m apop_data xpx apop_dot x x find_eigens xpx matrix eigenvals eigenvecs gsl_matrix pc_space pull_best_dims ds dims eigenvals eigenvecs show_projection pc_space cp Listing The detailed version Online source eigenhard If space data rank eigenvectors dimensional data The pull_best_dimensions function allocates new matrix dims dimensions best eigenvectors cluded Again GSL saw coming provides gsl_sort_ ve tor_largest_index function returns array indices largest elements evals vector Thus indexes index largest eigenvalue indexes point vector values second largest eigenvector et cetera Given information easy loop lines copy columns set eigenvectors p _spa e matrix Finally principal component vectors calculated show_proje tion produces graphical representation result It projects data gsl_stats March LINEAR PROJECTIONS space simple dot product data p _spa e produces Gnuplottable output tricks discussed Given functions main routine declarations function calls implement procedure pull data calculate X X send result eigencalculator project data results include eigenbox h int main int dims apop_data x query_data apop_data cp apop_data_copy x apop_data pc_space apop_matrix_pca x matrix dims fprintf stderr total explained Lf n apop_sum pc_space vector show_projection pc_space matrix cp Listing The easy way let apop_sv_de omposition work Online source eigeneasy Listing presents program apop_matrix_p function singular value decomposition quick function That function simply normalization bookkeeping necessary use gsl_linalg_SV_ de omp function All left main function Listing query input data matrix SVD function display output results Q8 A matrix positive definite PD iff eigenvalues greater zero positive semidefinite PSD iff eigenvalues greater equal zero Similarly negative definite ND negative semidefinite NSD These multidimensional analogues positive negative For example x2 x R X X PSD X real elements An extremum f x maximum second derivative f x negative minimum f x positive f x maxi mum matrix second partial derivatives NSD minimum matrix second partials PSD s saddle point rep resenting maximum direction minimum false extremum gsl_stats March CHAPTER Q8 Write function matrix_is_definite takes matrix outputs single character P p N n indicate matrix types character x Write test function takes data matrix X favorite data set randomly generated matrix checks X X PSD Principal component analysis projects data dimensions dimensions display variance Given data matrix X process involves finding eigenvalues matrix X X associated largest eigenvalues projecting data space defined eigenvectors apop_matrix_p runs entire process efficiently lazy user OLS AND FRIENDS Assume variable interest y described linear combination explanatory variables columns X plus maybe Normally distributed error term o In short y X o vector parameters estimated This known ordinary squares OLS model reasons discussed introduction chapter To great extent OLS model null prior models default researchers use little information variables interrelate Like good null prior simple easy computer work flex ibly adapts minor digressions norm handles nonlinear subelements despite called linear regression exceptionally studied understood case y linear combination columns X OLS frequently solving snowflake problem chapter series clean univariate models stated outcome interest result series identical draws equal probability real world situations draw slightly different terminology OLS need control characteristics observation The term control analogy controlled experiments nuisance variables fixed metaphor appropriate adding column X representing control leads new projection new space gsl_stats March LINEAR PROJECTIONS completely change estimate original OLS parameters Later sections present options surmounting snowflake problem distributions chapter Because linear models studied documented book briefly skim This chapter puts OLS context page book model claims relationship X y pa rameters estimated data computational tools provide conveniences The chapter briefly covers OLS hypothesis testing Unlike models point OLS implicitly makes causal claim variables listed X because y values However true concept causality statistics The question statistical evidence causality valid tricky left volumes cover question detail For purposes reader merely note shift descriptive goal fitting distributions telling causal story A brief derivation Part OLS s charm easy instructive de rive OLS case We seek parameter estimate minimizes squared error o o y X o This smallest error term o orthogonal space X meaning X o If X o y X orthogonal reduce size o twiddling iota Proof After adding new equation y X X o X meaning new error term o X o non o X o X o o X o X X The term X X non negative scalar x x non negative real value x So way o non smaller o o X o But X o impossible The step proof X o zero way pick o non o o Q Prove Hint X o 4That regression results unstable trust single regression 5Economists especially dependent linear regression difficult controlled studies open economy Thus econometrics texts Maddala Kmenta Greene good place start exploring linear models 6See Perl book structural equation modeling The typical causal model directed acyclic graph representing causal relationships nodes data reject fail reject like model gsl_stats March CHAPTER true d h Projection So seek lead error X o Expanding o y X solve X y X X y X X X X 1X y This familiar form page Now consider projection matrix defined X P X X X 1X It named demonstrated XPv projects vector v space X Start XP X Writing reduces instantly X X X 1X X X So projection matrix projects X The expression XP o simplifies nicely X P o X X X 1X y X X X X 1X y X X X 1X y X X X 1X y X X X 1X X X X 1X y X X X 1X y X X X 1X y The projection matrix projects o space X o X orthogonal projected vector What projection matrix OLS Since X X 1X y X X X X 1X y XP y Thus OLS estimate y y X projection y space X y XPy And nutshell OLS model claims y projected space X finds parameters achieve squared error 7This called hat matrix shown links y y gsl_stats March LINEAR PROJECTIONS A sample projection At point narrative statistics textbooks include picture cloud points plane projected But Gnuplot setup let us plots data projected points hands feel include eigenbox h gsl_vector do_OLS apop_data set gsl_vector project apop_data d apop_model m apop_data d2 apop_data_copy d APOP_COL d2 ones gsl_vector_set_all ones return apop_dot d2 m parameters v vector int main apop_data d query_data apop_model m apop_estimate d apop_ols d vector project d m d vector do_OLS d d names rowct d names colct apop_data_print d projected FILE cmd fopen command gnuplot w fprintf cmd set view n splot projected points projected n Listing Code project data plane OLS Compile eigenbox earlier Online source proje tion Listing queries data plotted Figure OLS projec tion produces Gnuplot command file plot original projected data Ignore lines allow projection manually later Line OLS regression The apop_estimate function estimates pa rameters model data It takes data set model returns anapop_model structure parameters set The proje t function makes copy data set replaces dependent variable column ones producing sort data right hand OLS regression Line seven calculates X When data set printed line column X vector calculated second original y data fourth columns gsl_stats March CHAPTER non constant variables X Gnuplot awkwardness regards replotting data page Lines write data file row column names masked lines write line command file You run command line gnuplot ommand gnuplot screen plot The view set line basically head view plane data projected data probably green shifts projected value red It somewhat different picture PCA Listing Q Add label commands labels plot Spinning graph bit set view shows red points single plane position set view verify points match axes THE CATALOG Because OLS heavily associated hypothesis testing section plan ahead present estimates produced model expected value variance This gives need test hypotheses elements estimate OLS The model y X o n number observations y o n matrices k number parameters estimated X n k k Many results appear later assume column column ones If isn t need replace non constant column xi xi xi equivalence left exercise reader See actual format use constructing apop_data set Assumptions E o var oi constant 8If Gnuplot setup won t let spin plot pointing device modify Gnuplot command file print separate static graphs files set view commands try command like set view replot Gnuplot command prompt 9If like route normalizing column mean zero try apop_matrix_norm alize dataset m This normalize column calling normalize function need like APOP_COL your_data old gsl_ve tor_set_all old gsl_stats March LINEAR PROJECTIONS cov oi oj j Along assumption means n n covariance matrix observations errors 2I The columns X collinear e det X X X X exists n k Notice assume o Normal distribution assumption imposed chapter apply t tests When holds OLS X X X y E OLS var OLS X X Almost variance creates confusion A column data variance column errors o variance estimate OLS covariance matrix inclined estimate variance variance estimate The variance listed K K covariance matrix estimate OLS testing hypotheses later chapters It combination variances variance error term o columns data set covarianceX X variance OLS times inverse second INSTRUMENTAL VARIABLES The proofs gave guarantee calculate value X uncorre lated o y x Our hope estimate true model claiming y X o X uncorrelated o But case column X correlated error model way guarantees estimated error columns X correlated match model error column X correlated This creates major problems For example column data xi measured error actually observing xi oi This means error term equation 10If assumption barely met det X X zero small resulting estimates unstable meaning small changes X lead large changes X X parameter estimates This known problem multicollinearity The easiest solution simply exclude collinear variables regression principal component analysis find smaller set variables regress See e g Maddala pp suggestions gsl_stats March CHAPTER true error joined offset measurement error o o oi If true xi true o correlation xi oi o oi certainly As OLS estimate OLS X X 1X y taking step derivation X X OLS X y Also true defined satisfy y X o With simple manipulations find distance y X o X y X X X o X X OLS X X X o X X OLS X o OLS X X 1X o If X o distance OLS zero OLS consis tent estimator But X o correlated OLS correctly estimate Further precise measure right hand Equation don t know estimate positive negative large small Still column X affects element X X mismeasurement column throw estimate OLS coefficient column The solution replace erroneously measured column data strument variable correlated o correlated column data replace Let xi column measured error correlated o let z column alternate data instrument let Z original data setX column xi replaced z If cov z o following holds IV Z X Z y E IV var IV Z X 1Z Z X Z Whe det Z X small Z X variance large happens cov z xi We want variance estimator small possible brings usual rule searching instrument find variable measured significant error correlated possible original variable interest gsl_stats March LINEAR PROJECTIONS GLS Generalized Least Squares generalizes OLS allowing o o known matrix additional restrictions Note neatly plugging 2I estimator variance reduces equations OLS versions GLS X 1X X 1y E GLS var GLS X 1X But computational problem GLS written For data set million elements trillion elements case million zero A typical computer hold tens millions doubles memory simply writing matrix difficult let inverting Thus form wonderfully general use special form exploited computation tractable WEIGHTED LEAST SQUARES For example let diagonal matrix That er rors different observations uncorrelated error observation different This heteroskedastic data The classic example Econometrics differences income expect errors measurement consumption habits somebody earning year thousandth large measurement er rors consumption somebody earning million year It shown e g Kmenta optimum situation known observation use GLS equations set zero diagonal ith element 2i The GLS equations apply directly produceWeighted Least Squares estimates trick let us computation vector diagonal elements instead matrix Let vector ith element square root ith diagonal element For WLS ith element Now let y vector ith element ith element y times ith element X column wise product X That gsl_stats March CHAPTER void columnwise_product gsl_matrix data gsl_vector sqrt_sigma size_t data size2 Apop_matrix_col data v gsl_vector_mul v sqrt_sigma Then verify X X X X X y X y WLS X X X y E WLS var WLS X X Thus solve Weighted Least Squares elements writing matrix excess This method apop_wls use practice calculating X FITTING IT If data matrix apop_data set estimate OLS model line apop_estimate_show apop_estimate set apop_ols If data set non NULL weights vector replace apop_ols apop_wls If like control details estimation routine page format changing estimation settings online references detailed list options The apop_IV model requires settings setting treat ment model requires settings instruments element set Q8 In exercise page found relationship males female dependent variable density male female wage ratios independent variables Produce appropriate data set send apop_estimate your_data apop_ols check coefficients independent variables included gsl_stats March LINEAR PROJECTIONS If assumptions OLS fit need ready modify innards OLS estimation suit remainder section goes linear algebra layer abstraction steps estimate Listing extends example Listing page step linear algebra OLS Uncomment Line code link adding proje tiontwo o OBJECTS line makefile run produce new projection identical old include apop h gsl_vector do_OLS apop_data set apop_data d2 apop_data_copy set APOP_COL d2 firstcol apop_data y_copy apop_vector_to_data apop_vector_copy firstcol gsl_vector_set_all firstcol apop_data xpx apop_dot d2 d2 t gsl_matrix xpxinv apop_matrix_inverse xpx matrix X X apop_data second_part apop_dot apop_matrix_to_data xpxinv d2 t apop_data beta apop_dot second_part y_copy X X 1X y strcpy beta names title The OLS parameters apop_data_show beta apop_data projection_matrix apop_dot d2 second_part X X X 1X return apop_dot projection_matrix y_copy vector Listing The OLS procedure use projection spelled great detail Online source proje tiontwo Typically y values column data matrix step extract data separate vector Lines end apop_data set named y_ opy copies column input matrix second set named d2 copy input matrix column set Lines find X X 1X If like use debugger check value intermediate elements calculation Now X X 1X Lines single additional dot product find X X 1X y display result Line produces projection matrix X X X 1X given projection matrix dot product projects y gsl_stats March CHAPTER Q8 The GSL function solve equations type A C Householder transformations happens exactly form X X X y Given apop_data sets X X X y agsl_ve tor allocated beta line fill beta solution equation gsl_linalg_HH_solve xpx matrix xpy vector beta In practice necessary inverse solve OLS pa rameters covariancematrix X X But House holder method manages avoid explicitly finding inverse X X quicker situations like Cook s distance code page loop solves thousands OLS parameter sets Rewrite proje tiontwo use Householder transformation findbeta Check results alternate method match beta found inversion The Ordinary Least Squares model assumes dependent vari able affine projection Given sumptions likelihood maximizing parameter vector OLS X X X Y If I GLS X X X Y Depending value design number models In cases use apop_estimate But need coding processes simple question stringing lines lin ear algebra operations Chapter DISCRETE VARIABLES To point regression methods assuming y elements X continuous variables R If discrete variables need modifications There number distinctions First approaches handling columns discrete variables X different simpler approaches handling discrete values outcome variable y If y takes integer values worse zero t possibly Normally distributed o satisfy y X o There convolutions like stipulating observations x force o x o x force x o o non Normal makes mess gsl_stats March LINEAR PROJECTIONS hypothesis tests ll run later chapters More generally approach lacks grace requires cascade little fixes Greene p How proceed case depends type discrete variable hand A discrete binary variable takes exactly values male female case control rewritten simply zero Ordered discrete data typically count number children cars person owns Most qualitative categorizations lead multi valued unordered data For ex ample Metro station Red Blue Green Orange Yellow line voter align Red Blue Green Other party DUMMY VARIABLES For column zero data independent data X don t modify OLS change terpretation slightly taking zero observations baseline observations treatment group OLS coefficient zero dummy variable indicate effective treatment changing outcome Tests significance dummy variable shown equivalent ANOVA tests category s significance As extension categories ordered discrete like number chil dren don t jump x x induces shift size outcome jump x x linear model jump x x produce jump size If natural presume model violence reality But categories discrete unordered t use variable values implied linear relationship makes sense With Green Blue Red sense shift Green Blue exactly effect shift Blue Red shift Green Red exactly twice effect In case solution separate variable choices produce n coefficients n options The excluded option baseline like zero option binary case Here function apop_data_to_dummies saves day takes ith column data set outputs table n binary dummy variables example usage Given multiple categories produce interaction terms repre sent membership multiple categories e g control male control female 11If perfectly linear form implausible sensible transform input variable square x x gsl_stats March CHAPTER treatment male treatment female The easiest way sim ply create column columns mashed SQLite query sele t left right produces string leftright mySQL left right Then break column n dummy variables The underlying claim zero dummy variables y X k k indicates constant value gets added controls cases example But expect slopes differ cases controls cases equation y x11 x22 controls y x1 k x22 The way standard regression produce k case produce data set appropriate value x1 control zero case Then regression equivalent y x11 x1k x22 controls y x11 x22 cases desired include apop h apop_model dummies int slope_dummies apop_data d apop_query_to_mixed_data mmt select riders year line riders lines riders station lines station apop_data dummified apop_data_to_dummies d t slope_dummies Apop_col d yeardata int dummified matrix size2 Apop_col dummified c gsl_vector_mul c yeardata apop_data regressme apop_data_stack d dummified c apop_model apop_estimate regressme apop_ols apop_model_show return ifndef TESTING int main apop_db_open data metro db printf With constant dummies n dummies printf With slope dummies n dummies endif Listing Two regressions dummy variables Online source dummies Listing runs simple regression ridership station Washington Metro constant year set dummy variables Green Orange gsl_stats March LINEAR PROJECTIONS Red Yellow lines meaning Blue line baseline The query line pulls numeric columns ridership year text column Line converts text column zero apop_data set consisting dummy variables If ask debugger display dummified data set row single column Line stacks dummies right numeric data point data set right form regressed line Again worth asking debugger data set final regressable form We test claim tests equally likely page ifndef wrapper lines let read file testing pro gram Here wrapper innocuous The slope_dummies switch determines dummies constant dummies slope dummies For constant dummies need zeros ones apop_data_to_dummies gives need For slope dummies need replace current year column column vector multiplication achieves lines PROBIT AND LOGIT Now question discrete outcome vari ables For example person buys house makes house purchasing binary variable Say value house linear sum value components extra bedroom adds dollars nearby highly rated school adds square meter worth thousand dollars et cetera That write linear model total value U cost bedrooms school quality square meters o typically normalized o usual error term To phrase model briefly U x o But leaving model standard linear form add rule person buys iff U Thus input set characteristics x consumers houses weightings output binary variable person acted buyer consumed geyser erupted et cetera From details decision model depend outcome binary multivalued unordered ordered Logit choices The alternative logit model logistic model sumes o Gumbel distribution The Gumbel known Type I 12Real estate economists hedonic pricing model implementation typically differs respects logit model discussed See Cropper et al comparison gsl_stats March CHAPTER Extreme Value distribution meaning expresses distribution statis tic max x appropriate conditions McFadden won Nobel prize partly showing assumptions including eccentric error term reduce following simple form P x exp x1 P x exp x1 exp x1 There normalized exp x0 Probit The probit model similar setup logit o N The model rephrased slightly Say U x deterministic prob ability consuming given utility random To describe setup let U x x o introduction minus sign discussed detail let probability agent acts P x U x N y dy N y indicates Normal PDF y integral CDF U x As ornery form especially CDF Normal engineers error function affectionately erf simplify particularly easy calculate This partly explains prevalence logit form existing journal articles issue maximum likelihood estimation cheap erf involved With k different options logit model generalizes neatly case multiple values multinomial logit P B k exp xi P k B exp xx k exp xi k B set choices x chosen k different vector coefficients option k You normalized given options zero form reduces binomial form Ordered multinomial probit Presume underlying continuous vari able Y X true Yi Y Yi Y A1 Yi A1 Y A2 That 13The generalization clean Apophenia doesn t include separate binary multinomial logit models apop_logit model counts options acts accordingly gsl_stats March LINEAR PROJECTIONS P Yi A1 X N y dy P Yi A2 X A1 X N y dy P Yi A2 X N y dy Rather finding single cutoff acting acting estimate cutoffs choosing zero items estimating apop_ multinomial_probit return usual vector parameters plus vector A1 A2 Ak k options Parameter overload Very reasonable models produce overwhelming num ber parameters For multinomial logit model k options n elements observation x n k param eters estimated Your goals dictate parameters focus meaning If sole goal prove A causes B univariate test coeffi cient A significant look tests If want predict comparable people act future need model s estimate odds individual select choice predicted likely selection Perhaps want catalog average probabilities apop_data_summarize outmodel expe ted Or want know dynamic shifts happens B A rises Economists marginal change virtually field interest questions This closely related question parameter A statistically significant chapter max imum likelihood estimates variance measure confidence hypothesis tests literally inverse second derivative measure change outcome variable given change income variable For linear model marginal change trivial calculate proposed y x meaning y xi That change xi leads change y Interpreting marginal changes models difficult For probit cutoff draws induce choice option zero lead option x As x grows marginal expansion element x cutoff falls slightly set draws gsl_stats March CHAPTER lead option grows This fits intiution characterized elements linear sum x increasing proclivity choose option For ordered probit multiple cutoffs A marginal change haps raise cutoff options lowering odds choosing time raise cutoff options raising odds choosing See Greene p estimation program s documentation making productive use estimated model IIA Return multinomial logit formula Equation consider ratio probability events E1 E2 P E1 x B P E2 x B exp x P y B exp x y exp x P y B exp x y exp x1 exp x2 The key point simple result ratio depend set options The term property independence irrelevant alternatives IIA The term derives claim choice option way depends options The standard example commuters choosing red bus blue bus train red bus blue bus identical paint job We expect ratio P red P blue let P red P train P blue P train x If train service P red P blue train riders pick blue bus pick red blue bus knocked commission everybody riding likely switch red bus blue bus P red P train x In short train option irrelevant choice red blue bus blue bus option irrelevant choice train red bus In code supplement find data ele tion db includes small selection survey data United States s election National Election Studies The survey asked respondents opinion candidates including candidates Vice President scale database find person respondent preferred 14Duverger s law tells past post system United States tends lead stable party system strategic considerations vote We data question survey included online supplement asked respondent expects vote totals Gore Bush everybody The favorite thermometer score avoids strategic issues involved vote choice produces broader range preferences Gore Bush Nader Cheney et cetera Q Write query complete count candidate survey data gsl_stats March LINEAR PROJECTIONS The IIA condition dictates logit estimates relative odds choosing candidates won t change candidates enter exit intuitively expect sort shift The examples suggest level model people choose category bus train Republican Democrat Green having chosen category select single item red bus blue bus Buchanan McCain Such model weakens IIA choice s entry exit affect choice category These hierarchies class item type multilevel model section overview multilevel modeling possibilities including discussion nested logit The theoretical IIA property irrefutable room error lines algebra But theoretical models pushed produce mathematically clean results relevant application model real data Does IIA strong influence real world logit estimates We test IIA running unconstrained logit logit estima tion restricted option missing fits likelihood ratio LR form appear page variants test exist literature Cheng Long built number artificial data sets designed agents choices conforming IIA designed IIA property They found LR tests ineffective unconstrained constrained odds ratios differ demonstrating IIA times IIA sufficiently reliable relationship underlying data IIA property parameter estimates demonstrated IIA Fry Harris broad method slightly optimistic results encountered glitches problemswith power results sensitive option removed The probit model matches logit save type bell curve describes error term expects parameters predicted odds similar In fact Amemiya points logit parameters typically match probit parameters scaled factor That option Li 6Pi Yet logit parameters IIA property probit parameters This hard reconcile accept IIA property logit theoretically absolute empirically weak For discrete explanatory variables use standard family OLS models adding dummy variables The standard linear form X plugged parent function like Probit s comparison X Normal distribution logit s comparison Gumbel distribution generate models discrete choice gsl_stats March CHAPTER The Logit model property ratio odds selecting choices e g p A p B depend choices exist The computational pleasantries OLS demonstrates longer applicable usually need maximum likelihood search find best parameters MULTILEVEL MODELING Retake snowflake problem page models point assumed observation independently identically distributed relative frequently case One way break observations fall clusters fami lies classrooms geographical region A regression simply includes fami ly classroom region dummy variable asserts observation iid relative outcome rises falls fixed depending group membership But distribution errors family different family unobserved characteristics classroom students likely different unobserved characteristics A better alternative model estimation group separately At level subgroup unmodeled conditions likely constant group members Then group estimated parameters fit model observational unit group The type multilevel model distinct subclusters like detail derivation parameters For example feel propensity consume good Normally dis tributed know likelihood consumption based number factors We saw model probit asserted likeli hood consuming simply CDF Normal distribution parameter parameter form X The logit asserted probability consuming exp exp form X These examples multilevel models There parent model typ ically primary interest instead estimating data estimate results subsidiary models To point broadly seen types model simple distributions like Normal Poisson models asserting extensive causal structure like OLS family Either form parent multilevel model form child gsl_stats March LINEAR PROJECTIONS Clustering A classroom model assume distribution outcomes classroom estimate class assert linear form outcome variable 1i 2i Unemployment models typically modeled Poisson processes estimate region link estimates assert s Normal distribution include apop h void with_means apop_data d2 apop_query_to_data select avg riders year riders lines riders station lines station group lines line year apop_model_show apop_estimate d2 apop_ols void by_lines apop_data lines apop_query_to_text select distinct line lines int linecount lines textsize apop_data parameters apop_data_alloc linecount int linecount char color lines text apop_data d apop_query_to_data select riders year riders lines n riders station lines station lines line s color apop_model m apop_estimate d apop_ols APOP_ROW parameters r gsl_vector_memcpy r m parameters vector apop_data_show parameters apop_data s apop_data_summarize parameters apop_data_show s int main apop_db_open data metro db printf Regression parent Normal child n n with_means printf Normal parent regression child n n by_lines Listing Two model estimations regression parent Normal parent Online source ridership Listing estimates models change ridersip theWashingtonMetro time You saw models data set form dummy variable regressions page Here multilevel approach regression parent distribution child way gsl_stats March CHAPTER Having SQL hand pays immensely comes clustering easy select data group label certain value calculate aggregates group clause It apparent example data points SQL tables indexed grouping aggregation faster sifting table line line The model regression parent distribution child estimate Normal model Metro line year regress set s estimated Of course finding trivial s mean standard SQL calculate output query line set statistics set Normal distribution estimations Then regression estimates parameters regression statistics The second model distribution parent regression child run separate gression Metro line find mean variance OLS parameters Line queries list colors Blue line Red line lines loop runs separate regression color writes results parameters matrix Then line finds set parameters The estimations tell different stories produce different estimates slope ridership time Notably Green line added relatively unused sta tions 1990s means slope Green line s ridership time different lines This clear second case run separate regression line drowned data set includes line Notice size parent model s data set changes different spec ifications model second Thus model greater confidence slope different zero Gelman Hill point test parameters parent model means n small clusters effect parent n clusters problem Clusters small n large natural variation child parameter estimates reflected quality estimation parent parameters But test claims child parameters need concern directly quality child parameters 15In second specification data points negative slope positive slope Nonethe mean times n giving confidence mean significant gsl_stats March LINEAR PROJECTIONS Q8 To better handle differs lines overall regression draw graphs total ridership year average ridership year total ridership line average ridership line The difference total average ridership based fact number stations constant time produce crosstab data progression new station additions The plots line best written loop based ginning line Listing How graphs advise statistics calculated dummy multilevel models An example multi level logit We break process choosing pres idential candidate steps voter de cides choose Democrat Republican Green candidate chooses offered candidates chosen party The probability se lecting candidate thusP candidate P candidate party P party We need logit estimations Democratic candidates Republican candidates party choice The IIA property continue hold party parties change candidates shift P party proportional odds choosing given Democrat ver sus choosing given Republican shift Listing logit estmations way multilevel model Once SQL saves day multilevel estimations In case explanatory variables won t change model model fixed logit function outcome variable change Thus function takes varying tail query appends fixed list explanatory variables pull consistent data The tails queries main straightforward candidates Democrats Republicans party names Out overwhelming data logit function displays screen mean odds selecting option 16There Green candidate Ralph Nader P Nader P Green gsl_stats March CHAPTER include apop h apop_model logit char querytail apop_data d apop_query_to_mixed_data mmmmmt select age gender illegal_immigrants aid_to_blacks s querytail apop_text_to_factors d apop_model m apop_estimate d apop_logit apop_data ev apop_expected_value d m apop_data_show apop_data_summarize ev return m int main apop_db_open data nes db logit favorite choices logit favorite choices c party p p person c favorite p party R logit favorite choices c party p p person c favorite p party D logit p party choices c party p p person c favorite Listing Logit models grouping candidates party Online source andidates Q8 The example complete probability choosing candidate given party probability choosing party product Fill remainder code finding predicted odds candidate observation You count person chose candidate model says person likely pick Did multilevel logit better job picking candidates standard level logit The simple concept nesting models akin McFadden s nested logit McFadden In fact aren t possibilities Gelman Hill offer Describing snowflakes Now consider multilevel model model pa rameters inputs models This typical form want use stories Chapter like Negative Binomial model Poisson want parameter vary according observation conditions As probit model section fits description agent cutoff based linear model agent s characteristics e cutoff person xi cutoff fed parent Normal distribution gsl_stats March LINEAR PROJECTIONS Because Apophenia s model handling functions work model log likelihood specified process writing log likelihood model supremely simple Find linear estimates parameters observation probably usingapop_dot input matrix input parameters Use stock log_likelihood function find log likelihood outcome data given parameter estimates step Listing implements process The step building model basically consists reusing existing building blocks The log likelihood function simply steps find X run loop estimate separate Poisson model row based ith outcome variable ith Poisson parameter The main program declares model pulls data runs data new model standard OLS model include apop h double pplc_ll apop_data d apop_model child_params apop_data lambdas apop_dot d child_params parameters apop_data smallset apop_data_alloc double ll size_t d vector size double lambda apop_data_get lambdas apop_model pp apop_model_set_parameters apop_poisson lambda apop_data_set smallset apop_data_get d ll pp log_likelihood smallset pp return ll int main apop_model pplc Poisson parent linear child log_likelihood pplc_ll prep apop_probit prep apop_db_open data tattoo db apop_data d apop_query_to_data select tattoos ct tattoos ct tattoos year birth yr tattoos number days drank alcohol days booze tattoos yr ct booze notnull apop_data d2 apop_data_copy d apop_model_show apop_estimate d pplc apop_model_show apop_estimate d2 apop_ols Listing A multilevel model Online source probitlevels gsl_stats March CHAPTER In case interpret parameters similar manner discussion probit logit cases The parent parameter calculated X shift xi leads shift Thus checking param eters significantly different zero directly compare relative magnitudes parameters relative effect shift inputs Statistics subjective field At point seen range mod els saw distribution models Chapter linear models earlier chapter embed model form infinite number richer models Plus simulation agent based models standing nested different type model The Metro data modeled different ways depending count male female ratio counties ways Which model choose A model subjective description situation situations afford mul tiple perspectives This rare advice statistics textbook creative We living 1970s anymore tools tailor model perceptions world instead forcing beliefs fit computationally simple model Try models reasonable Having different perspectives situation raises odds come away true correct understanding situation We objective tools disposal selecting model Chapter demonstrate means testing disparate models better fit data For example running code Listing reveals likelihood alternative model higher log likelihood OLS model You use test page test difference likelihood truly significant We create models data estimate parameters model generated estimation model One common use clustering develop model individual groups states classrooms families discuss patterns set groups country school community We use multiple levels solve snowflake problem spec ifying different probability event observation retaining simple basic model events For example probit based plugging output standard linear form Normal CDF gsl_stats March HYPOTHESIS TESTING WITH THE CLT I m looking patterns static They start sense longer I m Gibbard The purpose descriptive statistics data The purpose hypothesis testing data don t Say took samples population maybe height individuals mean sample measurements cm If sums right indisputable certain fact But mean height population drew data set To guess answer question need assumptions data set relates population drawn Statisticians followed number threads data don t Each starts data set assumptions environ ment data generation method concludes output distribution compared data Here list common assumptions It impossible comprehensive categories overlap offers reasonable lay discussion following chapters Classical methods Claim data produced process allows application Central Limit Theorem gsl_stats March CHAPTER Maximum likelihood estimation Write likelihood function given data parameter pairing find likely parameter given data Bayesian analysis Claim distribution expressing prior beliefs parame ters likelihood function data hand combine produce posterior distribution parameters Resampling methods Claim random draws data comparable random draws population bootstrap principle generate dis tribution random draws data Kernel smoothingmethods Claim histogram existing data lumpy version true distribution smooth data produce output distribution All approaches discussed remainder book This chapter focus making inferences population use Central Limit Theorem CLT The CLT describes distribution sample mean x works regardless form underlying data That matter true distribution data distribution sample mean specific form long n large For relatively small n methods inference Monte Carlo methods discussed Chapter preferable Metadata Metadata data data Any statistic function data definition metadata Be careful confuse characteristics data metadata example variance mean smaller variance base data Like hypothesis tests Central Limit Theorem primar ily concerned distribution base data set distribution mean data set The CLT gives basis Normal distribution produce variants based Nor mal The square Normally dis tributed variable x Chi squared distribution writ x2 read statistic distributed Chi squared degree free dom Dividing Normal distribu tion transformed distribution produces distribution t distri bution ratio s produces F distribution Because rooted CLT statements true regardless vagaries lying population statistics describe Having found means describing distribution unobservable popula tion parameter Section look number simple tests They direct applications distributions given names like t test test F test The remainder chapter applies building blocks complex struc tures test hypotheses elaborate statistics For example inde pendent statistics So squared distance gsl_stats March HYPOTHESIS TESTING WITH THE CLT histogram segment hypothesized distribution tal distance thousand segments hypothesized distribution total distance test aggregate claim data histogram close distribution THE CENTRAL LIMIT THEOREM The CLT key piece magic chapter Make series n independent identically distributed draws x1 x2 xn fixed underlying population The underlying population nondegenerate distribution Let mean sequence draws x true mean overall population Then n n x N That matter underlying population distribution mean series draws approach Normal distribution Put way repeated procedure drew k independent data sets population plotted histogram x1 xk eventually familiar bell curve Because distribution x CLT embodies stage procedure produce means series k sets draws metadata base distribution seek distribution metadata k means data Listing demonstrates exactly stage procedure works worth understanding detail On line panel Figure data program makes draws It near bell curve The inner loop make_draws function j indexed loop takes t draws CDF adds total When total divided t line loop mean t draws distribution The outer loop indexed loop records draw t means Line plots distribution set means This double loop base CLT reflected assumptions data sets Say draw t data points data set 1By nondegenerate I mean outcome positive probability If data items value mean samples matter slice However sample takes distinct values CLT eventually provide bell curve You verify modifying code Listing There theoretical distributions infinite variance because problems CLT course issue finite data sets gsl_stats March CHAPTER include apop h int drawct double data gsl_vector make_draws int ct gsl_rng r double total gsl_vector gsl_vector_alloc drawct int drawct total int j j ct j total data gsl_rng_uniform_int r sizeof data sizeof data gsl_vector_set total ct return int main gsl_rng r apop_rng_alloc int ct ct ct printf set title Mean draws n ct gsl_vector o make_draws ct r apop_plot_histogram o NULL gsl_vector_free o printf pause n Listing Take mean increasing number draws The distribution means approaches Normal distribution Online source ltdemo claiming Normally distributed We presume Normally distributed constant multitude events affected data point sort haphazard way That individual data point went process like inner loop lines absorbing large number random shocks After little shocks gathered single data point line The main function intended CLT works best data point mean draws base distribution Line repeatedly calls make_draws At t make_draws makes draws distribution The produces data points mean draws data point mean draws The program dumps plots histograms STDOUT run program ltdemo gnuplot gsl_stats March HYPOTHESIS TESTING WITH THE CLT Figure shows frames output program The frame animation simply set spikes representing base data The second frame data point mean draws series humps draws large numbers large numbers small In frame combinations possible humps As frames progress humps merge gether form familiar bell curve This telling counting story page explained Binomial distribution approaches bell curve F q u en cy x Means draw F q u en cy x Means draws F q u en cy x Means draws F q u en cy x Means draws Figure Sample outputs CLT demo Listing gsl_stats March CHAPTER Finally notice x axes snapshots original data plotted scale fourth frame goes So distribution x approach bell curve approaches narrow bell curve Q9 Modify line try different base distributions system draw Thanks creative use sizeof line don t need specify size array But footnote page bad form Deleting elements produces es pecially interesting patterns What sort data sets lead quickly bell curve data sets require averaging thousands elements achieving decent approximation Normal Equation story formally data set xwith n elements true mean variance n x n approaches aN distribution From regularity nature derive distributions follow Variants variance There confusion use n worth quick review For data set distribution variance notated The formula variance data set n xi n makes sense symbol square included For Normal distribution square root variance known stan dard deviation describe width distribution For exam ple Normal distribution plus minus standard deviations mean Outside Normal distribution rarely Let data set x variance Then variance mean x var x n standard deviation x n Once important bear mind dealing data metadata The Central Limit Theorem states observation xi mean draws iid distribution n distribution xi follows Equation That overall mean square root vari ance set xi s xi n approaches N distribution 2If shift x axis bothers ask Gnuplot hold constant scale adding line likeprintf set xrange n main gsl_stats March HYPOTHESIS TESTING WITH THE CLT MEET THE GAUSSIAN FAMILY With exception Normal distributions distinct distributions Section The distributions typically describe data observed real world The distributions aimed describing metadata means variances model parameters NORMAL The Normal distribution presented page de scribe parameters derived The big problem Normal distribution depends unknown It depends frequently testing claim fixed value assume derive Thus trickery section involves combining distributions ways unknown s cancel DISTRIBUTION The square variable distributionN distri bution degree freedom sum n independent distributed variables n degrees freedom Figure shows distribution different degrees freedom If X N X2 If Xi N n X21 X2n 2n If Xi 2n E Xi n The summation property immensely useful sums vari ables contend The common case sample variance sum squares Being sum squares Normally distributed elements easy Snedecor Cochran p n x x 2n The numerator estimate sample variance times n use test sample variance equals given establish confidence interval estimate variance But useful describing variance estimates The sample variance 2n 2n given n data points mean actually calculated data meaning effectively sum n variables plus longer stochastic constant For degrees freedom sidebar page It worth mentioning origin distribution common form Pear son Taylor expansion errors Multinomial gsl_stats March CHAPTER P x d f x df df df Figure The distribution degrees freedom Being sum squares greater zero As elements added sum distribution lopsided approaches Normal distribution distribution form k1 k2 k3 ki s appropriate constants He found satisfactory precision term series The ANOVA family tests based approximation tests claim data random draws fit story Multino mial distribution page sum distributions leads distribution STUDENT S t DISTRIBUTION Let x vector data error terms regression Then x n tn x x n scalar It look approximation Normal replacing To form t distribution came consider dividing CLT equation Equation x n N gsl_stats March HYPOTHESIS TESTING WITH THE CLT P x d f x df df df Figure The t distribution degrees freedom For degree freedom distribution especially heavy tails variance infinite df grows distribution approaches aN 2n n Then x n x n The key stumbling block unknown value cancels numera tor denominator This work genius Mr Student calculate exact shape distribution straightforward manipulation Normal tables Some t distributions degrees freedom pictured Figure The t1 distribution e n called Cauchy distribution As n tn distribution approaches N distribution 3Student actually Mr William Sealy Gosset published t distribution based work employee Guinness Brewery gsl_stats March CHAPTER P x d f d f x df df df df Figure The F distribution pairs numerator denominator degrees freedom F DISTRIBUTION Instead ratio anN ra tio distributed variables The s denominators cancel leaving distribution calculated tables This derivation definition F distribution 2m m n n F m n See Figure sample F distributions Also consider square t distribution The numerator tn distribution Normal distribution square denominator square root 2n distributed variable square n Thus square tn distributed variable F1 n distribution The F distribution allows construct tests comparing distributed vari able numerator distributed variable denominator variables sum arbitrary number elements We use F distribution construct comparisons relatively com plex statistics LOOKUP TABLES There things cover distribution finding values PDF values CDF occasionally values inverse CDF For single quick number command line program apop_lookup For getting value PDFs gsl_stats March HYPOTHESIS TESTING WITH THE CLT given point inside program headers GSL s PDF lookup functions double gsl_ran_gaussian_pdf double x double sigma double gsl_ran_tdist_pdf double x double df double gsl_ran_chisq_pdf double x double df double gsl_ran_fdist_pdf double x double df1 double df2 The prefix gsl_ran indicates functions random number generation module lude gsl gsl_randist h Random number gener ation delayed Chapter The mean Normal function fixed zero modifyX accordingly e g X drawn N ask GSL gsl_ran_gaussian_pdf X The common distribution calculation found tables statis tics texts calculating CDF point The P functions calculate CDF point e X f y dy Q functions calculate CDF point e X f y dy These sum express area terms whichever function clearest Here list functions double gsl_cdf_gaussian_P double x double sigma double gsl_cdf_tdist_P double x double df double gsl_cdf_chisq_P double x double df double gsl_cdf_fdist_P double x double df1 double df2 plus P replaced Q These test hypotheses context claims like If shaky hypothesis tests section But versed traditional notation hypothesis tests notice overuse letter P easily lead confusion The gsl_ df_gaussian_P function gives known p value tailed test mean zero gsl_ df_gaussian_Q function gives infrequently q value hypothesis Put way find mean Normally distributed data standard deviations zero reject tailed hypothesis mean equal zero confidence1 gsl_ df_gaussian_P gsl_ df_gaussian_Q For hypothesis reversed Here table strives clarify function goes confidence null rejected gsl_stats March CHAPTER goes p value H0 H0 confidence gsl_ _P gsl_ _Q p value gsl_ _Q gsl_ _P For centered tailed test p value takes form GSL_MIN gsl_ran_gaussian_P mu sigma gsl_ran_gaussian_Q mu sigma equivalently gsl_ran_gaussian_Q fabs mu sigma The confidence fail reject tailed null minus In direction want know need reject hypothesis certainty For example value risk oriented regulator want know worst day loss bank expect month To formalize question value point CDF Assuming Normal distribution profit loss bank report value risk gsl_ df_gaussian_Pinv Here requisite function declarations double gsl_cdf_gaussian_Pinv double p double sigma double gsl_cdf_tdist_Pinv double p double df double gsl_cdf_chisq_Pinv double p double df double gsl_cdf_fdist_Pinv double p double df1 double df2 plus Pinvs replaced Qinvs Q9 The power test likelihood successfully rejecting null hy pothesis effect data null rejected page When designing experiment need estimate power given design decide gather samples million I expect mean Normally distributed data Given assumptions n reject null hypothesis confidence What Normal approximation assumption deemed inapplicable data taken t distributed 4This assumption false Securities typically leptokurtic fat tailed returns page gsl_stats March HYPOTHESIS TESTING WITH THE CLT The square Normal distribution distribution Both distributions rely unknown variance We guess confidence intervals mere approxima tions The ratio Normal square root transformed dis tribution t distribution By taking ratio form distributions unknown s cancel valid confidence region constructed finite data The ratio distributions F distribution Again un known s cancel TESTING A HYPOTHESIS The chapter point discussed certain manners gathering data aggre gating statistic taking mean taking sum squares lead certain known distributions Thus statistic produced man ner evaluate confidence claim statistic true For example mean data set transformed having t distribution assuming CLT holds Similarly difference means data sets precise probability placed claims difference means CLAIMING A FIXED MEAN This test called z test foot note The claim mean column data equal H The procedure test Find mean data Given variance data set 2d estimate standard deviation mean m d n For tailed test find percentage tn distribution H n e gsl_ df_tdist_Q fabs H m n Report p value For tailed test report twice calculated number p value Q9 Can reject claim H0 The typical Puerto Rican county poverty rate Use county level info poverty_p t_all column ome table data ensus db data set Given US Census Bureau defines poverty cost living main United States interpret result gsl_stats March CHAPTER Consider parameters OLS regression recall takes form X X 1X y This complex expression simple mean element vector shown simple t distribution Thus standard test regression parameter sig nificantly different zero variant test t distributed scalar The details given section regression tests COMPARING THE MEAN OF TWO SETS Among common simplest questions sets observations process Chapter page showed t test test claim You encouraged reread section eye test procedure Reporting confidence There tradition reporting p value test greater ar tificial threshold p p But Gigerenzer cites Fisher stating scientific worker fixed level significance year year circumstances jects hypotheses gives mind particular case light evidence ideas Based observation better form list actual confidence calculated allow reader decide given value provides small great confidence results The error bars Chapter provide convenient way The paired t test common vari ant standard t test Say data paired sense element set element second set related way means element ai corresponding element bi difference ai bi makes real world sense For example look student scores test fore set lessons scores students lessons Then looking t distribution data comparing t distribution 5There standardized naming scheme tests A test basically consists components context statistic calculated distribution statistic compared Thus tests names like paired data test component sum squares test component F test component There correct way procedure encouraged avoid approach possible First distributions Normal t F real world applica tions means approach gives names myriad tests Two people talking running chi squared test find talking entirely different contexts statistics There odd anomaly naming customs Normal distribution calling statistic compared Normal normal statistic Gaussian statistic typically called z statistic There z distribution z test half log F distribution longer commonly F slightly convenient Finally distribution use depends context data statistic t distribution small n approaches Normal n easily find situation looking statistic called t test Normal distribution tables looking z statistic t distribution tables gsl_stats March HYPOTHESIS TESTING WITH THE CLT data look vector differences ai bi find con fidence zero falls appropriate t distribution This generally powerful test meaning likely reject null hypothesis difference vectors paired t test generally preferable unpaired version applicable Apophenia provides apop_paired_t_test function run test appropriate BASED TESTS One quick application distribution testing variance takes certain value We posit denominator Equation fixed number check tables given degrees freedom This relatively rare use distribution A common use advantage summation property combine individual statistics elaborate tests Any time statistics form observed expected expected observed expected Normally distributed use Pearson s Taylor series approximation piece statistics form test There examples form section ANOVA goodness fit testing F BASED TESTS Because squaring goes distributed statistic x x indistinguishable difficult test tailed claims form b We use t test tailed claim single variable F test combination multiple variables Let H0 claim Q c This surprisingly versatile hypothesis For example vector elements Q c Then H0 Or Q c gives H0 Or want test H0 Then let Q c In ANOVA terminology hypothesis linear combination coefficients known contrast To test hypotheses simply stack hypothesis row Q c gsl_stats March CHAPTER Any linear affine hypothesis having parameter fit form Define q number constraints rows Q n sample size k number parameters estimated As let X represent X normalized column mean zero column constant vector Now H0 true estimated OLS Q N c 2Q X X 1Q construct distributed linear com bination square q standard Normals Q c Q X X 1Q Q c 2q Alternatively testing value variance regression error o N o o 2n k As divide scaled versions Equation Equation statistic F distribution unknown element n k q Q c Q X X 1Q Q c o o Fq n k If read far know code operations Equation But fortunately Apophenia includes function calculate To need feed function estimate apop_ data set indicating set contrasts wish test vector element c matrix element Q As Equation row input matrix represents hypothesis test equality constraints use vector matrix pair like double line apop_data constr apop_line_to_data line 6For method form variance Q variance Section Q See e g Amemiya gsl_stats March HYPOTHESIS TESTING WITH THE CLT The final vector matrix form constraint matches form con straints pp case constraints equalities Listing presents example It runs regression males female data page link code code The constraint condition The apop_F_test function takes set regression results F test commonly closely married OLS type regressions include eigenbox h int main double line apop_data constr apop_line_to_data line apop_data d query_data apop_model est apop_estimate d apop_ols apop_model_show est apop_data_show apop_f_test est constr Listing Run F test run regression Online source ftest Here useful simplification Let R2 coefficient determination defined n number data points k number parameters including parameter constant term F statistic based Q I c Then shown n k R2 k R2 Q9 Verify identity Equation Equation defini tions page yest X estimated value y SSR yest y SSE o o R2 SSR SSE gsl_stats March CHAPTER Statistical custom based availability computational shortcuts F statistic Equation appears default output regression packages It decide particular test statistic relevant situation dealing custom report Apo phenia facilitates hypothesis test assuming default send NULL variables apop_F_test estimate NULL Q9 Verify identity Equation running linear regression data set produced exercise page passing apop_ model produced apop_F_test find F statistic Apohenia s R2 finding function find SSE SSR The simplest hypothesis test parameters model t test It claims mean data set different given value A special case claim mean data sets differ The test allows comparison linear combinations allegedly Normal parameters But squared pa rameter test asymmetric tailed hypotheses The F test provides generality test tailed tailed hypotheses claims contrasts simulta neously true ANOVA ANOVA contraction analysis variance catch term variety methods aim decompose data set s variance subcategories Given variables groups variation groups groups Can variation dependent variable attributed primarily independent variables The descriptive portion ANOVA techniques covered pages This section covers hypothesis testing ANOVA You want run metroanova appeared Listing page It produces ANOVA table includes sums squared er rors ratio At point recognize sum squared errors having distribution assuming errors Normally distributed df weighted ratio sums squared errors F distributed 7Scheffe parodies singular focus form calling F test book gsl_stats March HYPOTHESIS TESTING WITH THE CLT Thus traditional ANOVA table includes F test testing claim group variation larger group variation meaning grouping explains random variation data Independence The crosstab represents form grouping rows divide observations categories grouping columns divide observations categories Are groupings independent To concrete example array events people chose left right Left Right Up Down Is incidence Up Down correlated incidence Left Right independent Draws boxes follow Multinomial dis tribution Up Down Bernoulli draw probabilities pU pD Left Right separate independent Bernoulli draw probabilities pL pR expected value Up Left EUL npUpL similarly EDL EUR EDR Notating actual incidence Up Left OUL use fact page reasonable approximation errors Multinomial distribution observed variance expected value OUL EUL EUL Similarly cells sum OUL EUL EUL OUR EUR EUR ODL EDL EDL ODR EDR EDR This expression degree freedom horizontal set elements mean df similarly vertical set df df df If rows columns df Listing calculates long way twice short way The al _ hi_squared function calculates Equation one_ hi_sq function calculate individual term Finally main gathers data calls functions After calls apop_test_anova_independen e work line The distribution means series Binomial draws approach Normal n situations n closer For case Fisher gsl_stats March CHAPTER include apop h double one_chi_sq apop_data d int row int col int n APOP_ROW d row vr APOP_COL d col vc double rowexp apop_vector_sum vr n double colexp apop_vector_sum vc n double observed apop_data_get d row col double expected n rowexp colexp return gsl_pow_2 observed expected expected double calc_chi_squared apop_data d double total int n apop_matrix_sum d matrix int row row d matrix size1 row int col col d matrix size2 col total one_chi_sq d row col n return total int main double dataline apop_data data apop_line_to_data dataline double stat chisq stat calc_chi_squared data chisq gsl_cdf_chisq_Q stat data matrix size1 data matrix size2 printf chi squared statistic g p Chi squared g n stat chisq apop_data_show apop_test_anova_independence data apop_data_show apop_test_fisher_exact data Listing Pearson s test Fisher s Exact test Online source fisher calculated probability given table direct combinatorial com putation The equations Fisher exact test mess story null hypothesis Up Down Left Right independent calculation trivial apop_test_fisher_exa t line Listing Scaling How calculation affected replicated count data set k counts O UL kOUL E UL kEUL Then O UL E UL E UL k OUL EUL EUL That scaling data set k scales test statistic k For data set exists k null hypothesis rejected gsl_stats March HYPOTHESIS TESTING WITH THE CLT Across data sets scale easily different statistical significance easier achieve set larger scale Generally tenuous sert data set test statistic 96th percentile distribution diverges independence data set test statistic 9th percentile Use test establish data rejects null hypothesis use methods simple covariance establish magni tude difference For comparison prior tests involving mean sensitive scale No tably consider ratio Central Limit Theorem based element data vector x scaled k mean var n kx kx n kx kx n2 x x x x All equal ratio mean n written n affected scale x number elements data set way statistic affected rescaling REGRESSION In prior chapter linear regression model purely descriptive purposes find best way project y X If add assumption o Normally distributed test hypotheses parameter estimates Given assumption shown coefficients independent variables vector t distribution confidence element differs constant calculated e g Greene p The covariance matrix OLS X X variance regression s error term o vector errors n data points k regressors including constant column o o n k provides unbiased estimate The estimated variance diagonal element estimated variance second diagonal element As typical test statistic data count degrees freedom data points minus constraints specifically n data points k regression parameters including attached constant column df n k 8The form variance error term analogizes directly basic variable unbiased estimate variance Pn xi x n First setup OLS guarantees o oi o oi o o matches numerator basic formula The denominator cases degrees freedom example k regressors n data points n k degrees freedom gsl_stats March CHAPTER Given estimated variance constant c write test statistic c check statistic tn k distribution test claim c This test bears close resemblance test mean data set t distributed scalar statistic presented page If joint hypotheses contrasts elements directly apply discussion F tests use estimated mean covariance matrix n k degrees freedom Comparison ANOVA If regression consists dummy vari ables shown process equiv alent ANOVA style categorization methods Q9 Alaska famously low females features distin guish lower states Create dummy variable Alaska state regress males female Alaska dummy population density constant course Are independent variables significant Q9 Run independence test table row categories Alaska Alaska column categories males females Hint need combine population males fe males columns produce count region sum gions How test differ compare percent male female total count males females region What changes story underlying test version better represents hypothesis OLS friends real advantages testing crosstab approaches like ANOVA First readily handles continuous variables ANOVA handle approximation rough categories Second allows comparison vast number variables ANOVAs typically comparing independent variables dependent OLS regression project dependent variable space literally hundreds independent variables In fact run regression basically guaranteed number variables significant The multiple testing problem Freedman showed dangers data snooping ran domly generating set columns random numbers He set column dependent variable ex 9Data snooping called data mining term lost meaning refer categorization techniques classification trees gsl_stats March HYPOTHESIS TESTING WITH THE CLT plained potential regressors Using simple exploratory technique culled potential explanatory variables variables He ran variable regression found variables significant p value better significant p better Other tests regression indicated good fit But data pure noise construction Recall paragraph book goals statistical analysis directly conflict If researcher spends time looking descriptive statistics data committing informal data snoop ing akin Freedman s initial exploratory regression biases chances rejecting null favor But folly researcher check data outliers quirks running regression embark producing entirely new data set regression What correct balance Statistics answer statisti cians Those descriptive oriented camp impor tance good graphical displays viewing data way possible testing oriented camp believe pre test searching simply asking apophenia Here spin issue people testing exactly hypothesis tend develop affection reluctant reject pet hypoth esis Thus research conducted humans improve multiple hypotheses simultaneously competing Chamberlin p explains Love long represented blind true sonal realm measurably true intellectual realm The mo ment offered original explanation phenomenonwhich satisfactory moment affection intellectual child springs existence explanation grows definite theory parental affections cluster intellectual offspring grows dear holds seemingly tentative lovingly tentative The mind lingers pleasure facts fall happily embrace theory feels natural coldness refrac tory There springs unconscious pressing facts fit theory The search facts observation phenomena interpretation dominated affection favored theory appears author advocate overwhelmingly established The theory rapidly rises ruling position investigation observation interpretation controlled directed From unduly favored child readily master leads author whithersoever His solution test multiple hypotheses simultaneously The inves gsl_stats March CHAPTER tigator parent family hypotheses parental relation forbidden fasten affections unduly He points maintaining multiple hypotheses allows complex explana tions outcome partly caused factor partly partly After Nature compelled conform exactly hypothesis Apophenia s model object makes easy test mix diverse hypotheses Chamberlin s suggestion methods comparing models later chapters But number models grows odds failing reject model purely chance grows There hard fast rule determining correct number models test bear mind tension multiple goals balance struck Correcting multiple testing Moving informally poking data consider case experiment s basic de sign involves systematic fixed series tests like running separate test genetic marker list million This known multiple test ing problem simple means correcting Say number drawn draw p probability p Then probability draw greater p p probability n independent draws greater p p n reversed probability n independent draws p p n Thus probability n tests fixed p value indicate positive result p n For example p n likelihood rejecting null We instead fix value like reverse equation find p value individual tests lead rejection nulls likelihood A line algebra p n For n need set p value individual tests In example testing n genetic markers desired overall p value individual tests 129e There wonderfully simple approximation expression let p n For example approximation second 5e Both approximations true value gsl_stats March HYPOTHESIS TESTING WITH THE CLT Thus simple rule known Bonferroni correction multiple tests divide desired overall p value number tests appropriate individual p values The correction standard biometrics virtually unknown social sciences When reading papers pages regressions cor rections multiple testing easily apply equation head multiplying reported individual p value number tests comparing larger figure usual significance levels Because know expected mean covariances regres sion parameters OLS IV WLS models tested individually standard t test tested group F test When running battery tests based regression oth erwise use Bonferroni correction create stringent sig nificance level The common form calculating stringent p value simply divide test p value number tests GOODNESS OF FIT This section present ways test claims form empirical distribution data matches certain theoretical distribution For example want check errors regression reasonably close Normal distribution The visually appealing way compare distributions Q Q plot stands quantile quantile plot The x y coordinate plotted x1 percentile data y1 percentile distribution testing second x2 second percentile data y2 second percentile ideal distribution et cetera To extent data fits ideal distribution points draw x y line digressions line stand The half Listing presents example displaying plot check whe ther precipitation Normally distributed It gathers data usual apop_ query_to_data manner estimates closest fitting Normal distribution plots percentiles data percentiles estimated dis tribution As Figure shows data set closely fits Normal distribution extremes tail bit elongated extreme tail bit Q9 Modify Listing test temperature log temperature Normally distributed Would distribution fit better gsl_stats March CHAPTER D ta p er ce n ti le s Normal percentiles Figure Percentiles precipitation y axis plotted percentiles Normal distri bution x axis include apop h int main apop_db_open data climate db apop_data precip apop_query_to_data select PCP precip apop_model est apop_estimate precip apop_normal Apop_col_t precip PCP v apop_plot_qq v est outfile gnuplot double var apop_vector_var v double skew apop_vector_skew v pow var double kurt apop_vector_kurtosis v gsl_pow_2 var double statistic v size gsl_pow_2 skew gsl_pow_2 kurt printf The skew g normalized kurosis g reject null data Normal g confidence n skew kurt gsl_cdf_chisq_P statistic Listing Pull data estimate Normal best fits data plot data ideal distribution The output presented Figure Online source qqplot HIGHER MOMENTS A slightly rigorous alternative means testing Nor mality check higher moments Normal distri bution Bowman Shenton Kmenta pp There general chi squared goodness fit test distribution gsl_stats March HYPOTHESIS TESTING WITH THE CLT A Normal distribution parameters mean standard de viation Normal distribution defined therefrom Notably moment zero fourth moment We need calculate distribution statistics The skew kurtosis mean iid process recall definitions page sum divided n square Let s moment data divided let fourth moment divided Then Ls n s2 distribution degree freedom Lk n Some prefer test simultaneously Lsk n s2 distribution degrees freedom The second half Listing translates formula code Given Q Q plot surprise test soundly fails reject null hypothesis data Normally distributed Another alternative keeping theme book bootstrap variance kurtosis let find confidence interval state percentage certainty kurtosis suggestion practice page CHI SQUARED GOODNESS OF FIT TEST Say histogram vec tor points claim drawn histogram It nice test confidence claim holds goodness fit test Mathematically simple We k bins histograms h0 holds histogram draws allegedly h1 holds data Then 10Recall page scaling matters test histograms representing PDFs sum definition histogram representing density population size nwill bins summing n definition That means statistics test PDFs test distribution counts different null likely rejected distribution counts So investigating histogram careful testing right hypothesis claims distribution population typically best represented test PDFs counts n gsl_stats March CHAPTER k h0 bins h1 bins h0 bins k You recognize form matching observed expected expected form ANOVA tests earlier chapter Q9 On page plotted leading digit arbitrary data set saw sloped sharply Now use chi squared goodness fit test formally check data set fits Equation Write function produce vector elements count elements slot equal number data points given leading digit Don t forget vectors count zero want slot represent leading digit rescale final vector PMF e elements sum Equation isn t PMF sum values isn t Thus need total mass rescale calculations Benford s equation accordingly Once element vectors equal mass di rectly apply Expression find statistic run test include apop h int main apop_db_open data climate db apop_data precip apop_query_to_data select PCP precip apop_model est apop_estimate precip apop_normal gsl_rng r apop_rng_alloc apop_model datahist apop_estimate precip apop_histogram apop_model modelhist apop_histogram_model_reset datahist est 1e6 r apop_data_show apop_histograms_test_goodness_of_fit datahist modelhist Listing The precipitation data test Online source goodfit Listing tests precipitation data Normally distributed goodness fit test Lines repeat query estimation qqplot page gsl_stats March HYPOTHESIS TESTING WITH THE CLT Line turns input data histogram Notice uses apop_ estimate form models histogram means expressing model You t goodness fit test histograms bins match sense range bin histogram exactly matches range corresponding bin second The easiest way ensure histograms match generate second histogram template apop_histogram_model_reset If wanted compare vectors test line use apop_histogram_ve tor_reset The new histogram gets filled random draws model means need apop_histogram_model_reset number draws 1e6 gsl_rng provide random numbers The use gsl_rng covered page By line histograms representing data model sync Thus simple matter send histograms apop_histo grams_test_goodness_of_fit calculate statistic Expression KOLMOGOROV S METHOD Kolmogorov suggested considering steps histogram Poisson process developed test based parametrization Conover Given histograms produced mentioned methods apop_test_kolmogorov finds maximum difference CMFs find probability CMF occur histograms base data set Because test uses ordering slices PMF Chi squared test test generally higher power Kolmogorov s test serves test Normality compare data set s CDF Normal distribution Q9 Is GDP capita log normally distributed Pull log GDP capita data data wb db data set Create histogram e estimate apop_histogram model Fit Normal distribution use create matching histogram apop_histogram_model_reset Send histograms apop_test_kolmogorov How test results compare produced apop_ histograms_test_goodness_of_fit gsl_stats March CHAPTER Q9 How precipitation Figure gave strong suggestion data Normally distributed modify code example formally test hypothesis data set drawn Normal distribution Kolmogorov Smirnov method The Q Q quantile quantile plot gives quick visual impression distribution data matches theoretical distribution We test claim data set Normally distributed fact skew kurtosis Normal distribution fixed given mean variance More generally compare distributions dividing bins comparing square deviation dis tribution test The Kolmogorov Smirnov test offers method compar ing distributions typically power Chi squared method gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Since fabric universe perfect work wise Creator takes place universe rule maximum minimum appear Leonhard Euler Whether divine design human preference problems involving search optima To point models closed form solutions optimal parameters nice computational shortcut finding hunt directly There variety routines find optima Apophenia provides consistent end apop_maximum_likelihood function Given distribution p value input p x local information need evaluate function point write value However optimum function global information meaning need know value distribution possible input x x order know optimum located This chapter begin simple mathematical theory maximum likelihood estimation MLE confront engineering question find global optimum function gathering local information small number points Once prerequisites place remainder chapter covers MLEs practice description testing 1Letter Pierre Louis Moreau de Maupertuis c cited Kline p gsl_stats March CHAPTER LOG LIKELIHOOD AND FRIENDS To point met probability distri butions PDFs listed Sections This section takes probability distribution P X given produces variant derivative functions prove easier work Also reminder list notation book page Let x1 x2 independent identical draws iid distribution The independence assumption means joint probability product individual probabilities P x1 x2 P x1 P x2 The assumption identical distributions e draws dis tribution P allows write neatly P x1 x2 P xi A probability function gives probability d data given parameters likelihood function probability d specified set parameters given observed data The philosophical implica tions distinction discussed There basic transformations likelihood function appear repeatedly worth getting know Define log likelihood function asLL ln P x x score derivative respect S lnP lnP n LL LL n information matrix negation derivative score respect I S 2LL 2LL n1 2LL 1n 2LL 2n gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION An example Bernoulli Say draws Bernoulli distribution recall page means draw probability p zero probability p case draws ones zeros The likelihood drawing ones p5 likelihood drawing zeros p putting independence assumption likelihood arbitrary value p given data set x P x p x p p The log likelihood LL x p ln p ln p The score case dimensional vector S x p p p information value matrix I x p p2 p Both intuitively number reasons discussed makes sense focus likely value p value maximizesP x p given observed data x Since log monotonic transformation p maximizes P x p maximizes LL x p Recall basic calculus necessary condition smooth function f x maximum df dx case LL x p S x p Setting Equation zero solving p gives p But zero derivative indicate maximum minimum tell look second derivative If second derivative negative derivative zero derivative negative func tion beginning dip downward At point function maxi mum minimum Since information matrix defined negation score s derivative check maximum minimum verifying information value positive p I In dimensions analog information matrix positive definite exercise page To summarize given p P LL S I It easy check values p verify produce lower probabilities observing given data gsl_stats March CHAPTER Q10 Verify statements Generate data set matrix element includes ones zeros order Put data set global variable Produce output model apop_estimate your_data apop_ bernoulli Display output model apop_model_show check prob ability e Write function takes apop_data struct holding vector item returns log likelihood parameter given global data apop_log_likelihood Send function plotafun tion routine page range verify log likelihood highest parameter An example Probit Recall Probit model page It specified agent act iff utility greater Normally distributed error term That act probability P x x N y dy whereN y indicates standard Normal PDF y integral CDF x Reversing let xA set x s led action xN set led non action Then likelihood given data set P x x A P xAi N P xNi LL x x A ln P xAi N ln P xNi By way logit Equation page tells story simpli fies significantly gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION LL x x A x A ln exi term counts act second includes body Q Verify log likelihood Equation Unlike binomial example find optimum log like lihood function probit logit derivative quick algebra We instead need use search method discussed later chapter A DIGRESSION THE PHILOSOPHY OF NOTATION The probability function frequentist interpre tation fixed distribution story fixed parameter million draws x occur P x percent time The likelihood function interpretation assume data produced model fixed happen ignorant There mysterious pool s drawn probability Thus probability x given model cases objectively verifiable fact likelihood given x model subjective construct facilitates types comparison s The integral x e fixed x P x dx The integral likelihood function Ronald Aylmer Fisher famed eugenicist statistician techniques ap pear book vehement keeping clear distinction I n I proposed term likelihood view fact respect parameter probability obey laws probability time bears problem rational choice possible values parameter relation similar probability bears problem predicting events games chance Whereas relation psychological judgment likelihood resemblance probability concepts wholly distinct Fisher p See Pawitan interpretation likelihood functions But simple practical matter probability x given fixed parameter P x likelihood given fixed data x P x At computer point writing separate functions p x beta andlikelihood beta x single function serve purposes Just fix x produce likelihood function fix probability distribution values x gsl_stats March CHAPTER We choices notation lead confusion The use distinct function names probability likelihood P x L x typical clarifies philosophical differences leaves reader recognize numerically identical functions x The second option use single function clarifies computational aspects leaves reader ponder philosophical I am plications single function produces objective probability distribution viewed way subjective likelihood function viewed way Because book centrally focused computation takes second approach listing probability likelihood P x form MORE ON LL S AND I The log likelihood function number di vine properties add making log likelihood preferable plain likelihood cases wait score First exponentiation distributions Sections ln P easier deal equivalent P purposes notably found maximum found maximum Also consider calculating iid likelihood function given thousand data points The probability observing data set form P xi Since P xi product typically order As discussed page number delicate computer readily deal Taking logs value pi negative number e g ln ln product sum ln P xi ln P xi Thus log likelihood typical thousand point data set order instead robust manageable You saw example different scales point sample Bernoulli ex ample p LL Analytically maximum log likelihood function useful reasons names Cramer Rao lower bound Neyman Pearson lemma It begins useful property score 2There consistent means describing subjective probability accommodate ways slicing P x The subjectivist approach closely related Bayesian approach takes views P x existing minds matter slice need physical interpretation The axiomatic approach led Ramsey Savage de Finetti posits rules lead consistent subjective beliefs followed places constraints probability likelihood Again probability likelihood accepted subjective beliefs reason distinguish notationally 3All proofs case scalar Proofs multidimensional case analogous gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Theorem If P x satisfies certain regularity conditions described footnote statistic f x Ex S f Ex f That score sort derivative machine expected value score times statistic equivalent derivative expected value statistic Finding optimum requires finding point derivative zero second derivative negative theorem gives easy trick finding derivatives The pages trick When reading theorem worth recalling sleight notation page f x function data Ex f x x produced certain model parameters function parameters Proof The expected value score times statistic Ex S f x S f x P x dx x ln P x f x P x dx x P x P x f x P x dx x f x P x dx x f x P x dx Ex f x require involved notation 4Equation proof uses claim R f dP d dx d d R f Pdx If t reverse integral derivative like applies The common explanation switch valid case exponential family definition exponential family covered book rest assured applies Normal Gamma Beta Binomial Poisson et cetera distribution Uniform But applies generally need uniform convergence PDF parameters given limit Casella Berger Section Roughly satisfied PDF value deriva tive finite For prefer exponential family story note PDF approximated arbitrarily closely sum exponential family distributions Barron Sheu distribution fails arbitrarily close distribution works For example Uniform distribution fails infinite slope end distribution steep slope 1e steep slope 1e works fine gsl_stats March CHAPTER The sequence events consisted substituting definition score Equation substituting familiar form derivative log Equation canceling pair P x s At point simple weighting P x introduced page replaced weight ing dP x Before x1 twice likely x2 e P x1 2P x2 f x1 double weighting Now slope P x x1 twice steep slope x2 f x1 gets double weighting The final steps state right conditions integral f x measure based dP x equivalent derivative integral f x measure based P x Equation switched integral derivative assumptions theorem s footnote Equation recognized integral expectation given probability density Corollary E S Proof Let statistic f x trivial function f x Then Theorem tells E S E Q10 Verify expected value score zero distribu tions given Chapter Exponential page Hint need calculate integral expected value score range zero infinity integration parts help Lemma The information equality var S E S S E I Proof The half comes fact var S E S S E S E S saw E S For second half write E I expansion S P x P x usual rules taking derivative ratio gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION E ln P x E P x P x E P x 2P x P x P x E 2P x P x S S E S S Q Prove final step showing E 2P x P x Hint use lessons proof Theorem write expectation integral switch integral derivatives The information equality computationally convenient place variance hard directly compute square derivative probably calculate For culmination sequence need Cauchy Schwarz inequality appeared page It said correlation variables ranges That f g cov f g var g var f cov f g var g var f cov f g var g var f Lemma The Cramer Rao lower bound Let f x statistic assume distribution P x meets cri teria prior results Then Ex f x E I var f x gsl_stats March CHAPTER The proof consists simply transforming Cauchy Schwarz inequality lemmas Let g Equation score equation expands Ex f x S Ex f x Ex S var S var f x The left hand components simplified results Ex f x S f x Theorem Corollary said E S zero second half numerator disappears The information equality states var S E I Applying gives Cramer Rao lower bound Further MLEs number properties let tailor CRLB Let statistic f x maximum likelihood estimate parameter MLE x MLEs biased finite data sets shown asymptotically un biased This means n sufficiently large E MLE x Therefore MLE x numerator left hand Equation It proven maximum likelihood estimators actually achieve CRLB meaning case inequality Equation equality The information matrix additive If data set gives I1 gives I2 produce Itotal I1 I2 For set iid draws draw information e expected information matrix property distribution draw data total information n data points nI The end result following form use easily calculate covariance matrix MLE parameter estimates var MLE x nEx I Equation makes MLEs cornerstone statistics Given MLEs achieve CRLB large n asymptotically efficient want test parameter estimates find t F test relatively 5See Casella Berger pp formal proofs statements section gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION easy computation finding variances need run test For models simulations especially want know outcome sensitive exact value parameter information matrix gives sensitivity measure parameter HOW TO EVALUATE A TEST A hypothesis test fooled ways hypoth esis true test rejects hypoth esis false test fails reject Evaluation vocab Here vocabulary terms stats class right tested Likelihood Type I error rejecting null true Likelihood Type II error accepting null false Power likelihood rejecting false null Unbiased values parame ter I e likely accept null false true Consistent power n There balance struck tween errors rises falls But tests born equal If hypothesis chance true coin flip test heads accept tails reject gives chance Type I error chance Type II error sit uations tests errors significantly lower By measure better tests coin flipping test A big help distinguishing Type I Type II error minus Type II error rate surprisingly descriptive power To high power tele scope star slightly different things like stars galaxies planets But low power lens like human eye looks like little dot Similarly high power test detect distinctions low power test fails reject null hypothesis difference Or cynical journals limited interest publishing null results high power test increases odds able publish results As imagine researchers concerned maximizing power THE NEYMAN PEARSON LEMMA TheNeyman Pearson lemma Neyman Pear son 1928a b states likelihood ratio test minimum possible Type II error maximum power test given level After establishing fact select Type I error level confident best Type II errors likelihood ratio test 6Of course run tests Chapter derived CLT need sure CLT holds maximum likelihood estimate statistic question This problem small data sets large data sets simulations based millions runs concern gsl_stats March CHAPTER Likelihood ratios Say cost test correctly accepts rejects hy pothesis zero cost Type I error CI cost Type II error CII Then sensible reject H0 iff expected cost rejecting expected cost rejecting That reject H0 iff CIP H0 x CIIP H1 x We translate cost minimization rule ratio likelihoods Recall Bayes s rule page P A B P B A P A P B To apply Bayes s rule rejection test set A H0 B x P A B P H0 x P B A P x H0 similarly H1 Then CIP H0 x CIIP H1 x CI P x H0 P H0 P x CII P x H1 P H1 P x c P x H1 P x H0 Inequality rejection rule Inequality uses Bayes s rule insert likelihood functions Inequality cross division canceling theP x s defining critical value c CIP H1 CIIP H0 e doesn t depend x If tell shape P H1 P H0 number I value c Inequality true probability The test gather data calculate likelihood ratio right hand Inequality reject H0 iff inequality true The Neyman Pearson lemma states test best sense Type I error fixed LR test minimizes probability Type II error So design test like fixing value comfortable custom says use calculating likelihood functions assured best Type II errors Most standard tests expressed likelihood ratio form Type II errors pretty mentioned considered taken care 7Alternatively ratio costs CI CII I value c Thus draw relation relative costs choice 8For proof e g Amemiya pp 9Every test Type I Type II error thanks Neyman Pearson Lemma describe test Type I level phrases like test p value The introductory chapter Hunter Schmidt excellent essay description severely misleading The extreme case test fail reject null Type I error rate null hypothesis false wrong time gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION For sufficiently specified model find probability given data set produced model If data set consists independent identically distributed el ements likelihood product term data point For computational analytic reasons log likelihood easier work product sum The parameters maximize likelihood function model identically maximize log likelihood minimum vari ance unbiased estimators The variance known quan tity given Cramer Rao lower bound Type I Type II errors complementary goes generally goes However given Type I error level differ ent tests different Type II error levels The Neyman Pearson lemma guarantees likelihood ratio test minimum prob ability Type II error fixed Type I error level DESCRIPTION MAXIMUM LIKELIHOOD ESTIMATORS Apophenia provides function find model s optimum apop_maximum_like lihood function It pro vides standardized interface types optimization routines different approaches finding optima You provide log likelihood function unable provide derivatives maximiza tion routines find Since Cramer Rao lower bound tells variance likely parameter apop_maximum_likelihood return parametrized apop_model variances useful informa tion This section gives overview standard optimization methods choose raise odds find optimum functions You wondering need know details Isn t finding opti mum likelihood function solved problem The answer decidedly Most standard optimization algorithms built work smooth closed form globally concave function finding value x maximizes f x x2 If function meets conditions odds good default optimization routine stats package choosing work fine But produces likelihood value model simulation model likelihood function howwell model matches real world data set A dynamic program gsl_stats March CHAPTER ming problem model The consumer choosing goods end Chapter model If model stochastic element multiple equilibria fails fulfill expectation simple globally concave function need tailor method settings problem hand include apop h double sin_square apop_data data apop_model m double x apop_data_get m parameters return sin x gsl_pow_2 x apop_model sin_sq_model sin x times x p sin_square Listing A model optimized Online source sinsq x si n x x x si n x x x si n x x 3e 3e 2e 2e 1e 1e 5e 5e e e e e e e e e e x si n x x Figure Top row The simplex method conjugate gradients row simulated annealing root search Online source lo almax_print 10The problem finding optimum broad large sections optimization research book mention For example broad active field combinatorial optimization covers questions like optimal ordering n elements given optimization function problem n options See e g Papadimitriou Steiglitz combinatorial optimization methods gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION include apop h apop_model sin_sq_model void do_search int m char double p double result Apop_settings_add sin_sq_model apop_mle starting_pt p Apop_settings_add sin_sq_model apop_mle method m apop_model apop_maximum_likelihood NULL sin_sq_model result gsl_vector_get parameters vector printf The s algorithm found g n result int main apop_opts verbose Apop_settings_add_group sin_sq_model apop_mle sin_sq_model do_search APOP_SIMPLEX_NM N M Simplex do_search APOP_CG_FR F R Conjugate gradient do_search APOP_SIMAN Simulated annealing do_search APOP_RF_NEWTON Root finding Listing Using apop_maximum_likelihood types method solve maximum Compile sinsq Online source lo almax Listing presents simple model consisting equation x2 sin x This simple equation infinite number local modes As x value function modes rises proportion x2 global maximum Figure shows attempts search function gives good picture shape curve On lines function defined Because system oriented data analysis function takes data set apop_model holding parame ters In case data set simply ignored Line declares new model information MLE function need like number parameters probability func tion Listing optimization ways The apop_model struct hold array groups settings Line adds model group ofMLE appropriate settings That group includes things like starting point method tolerance details gsl_stats March CHAPTER Lines change starting_pt method elements model s MLE settings group p m respectively All real work line calls maximization The subsequent lines interrogate output The main routine optimization different methods By inserting Apop_settings_add sin_sq_model apop_mle tra e_path outfile line system writes points tests search optimum produce plots like Figure You disparate styles meth ods simplex conjugate gradient methods somewhat similar case simulated annealing exhaustive clearly shape curve root search barely leaves zero deciding close Chapter demonstrate method producing picture function random walk meantime simulated annealing tra e_path pro vide pretty effective way graph complex unfamiliar function When run program asking different methods gives different answers leads important lesson trust sin gle optimization search By redoing optimization different points dif ferent methods better idea optimum found local peak optimum entire function See tips restarting optimizations METHODS OF FINDING OPTIMA Here problem want find maximum f x time evaluate function derivative limited number points For example f x complex simulation takes parameters runs hour spitting value like answer week Here methods currently supported Apophenia They basi cally standardization routines provided GSL In turn GSL s choice optimization routines bears close resemblance recom mended Press et al reference thorough discussion algorithms work broad advice picking algorithm Also Avriel provides thorough mathematician oriented overview optimization andGill et al provides practical modeler oriented overview topics gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Simplex method Nelder Mead APOP_SIMPLEX_NM For d dimensional search method drawsa polygon d corners step shifts corner polygon small est function value new location polygon contract smaller size Eventually polygon shrink maximum When average distance polygon midpoint d corners toleran e algorithm returns midpoint This method doesn t require derivatives Conjugate gradient Including Polak Ribiere APOP_CG_PR Fletcher Reeves APOP_CG_FR Broyden Fletcher Goldfarb Shanno APOP_CG_BFGS Begin picking starting point direction find minimum single dimension relatively simple dimensional minimization proce dure like Newton s Method Now new point start conjugate gradient method picks new single line search Given direction vector d1 vector d2 orthogonal iff d 1d2 Colloquially orthogonal vectors right angles After optimiza tion search alongd1 makes intuitive sense dimensional search orthogonal vector However situations search strategy optimal second direction typically right angles Instead conjugate gradient satisfies d 1Ad2 matrix A Orthog onality special case A For quadratic functions form f x x Ax bx search conjugate gradients find optimum steps dimensions A However function probably approximates quadratic form different quadratic form point meaning approximations points A1 A2 It neces sary actually calculate A point quality search depends close quadratic form function quadratic effective method Polak Ribiere Fletcher Reeves differ choice method build gradient prior Press et al recommend Polak Ribiere The BFGS method slightly different maintains running best guess Hessian uses updating gradients However 11Formally argue means conjugate gradient method I class gsl_stats March CHAPTER general rules underlying assumptions hold closer function fixed function smooth derivatives better BFGS Here pseudocode algorithms The variables teletype settings tuned calling routine lines Listing For routines set verbose progress search screen Start p0 starting_pt gradient vector g0 While pi gi tolerance pi gi Pick new candidate point pc pi gi step_size gi If pc pi pi pc Else pi minimum point line pi pc toleran e12 Find gi method specific equation We guaranteed pi pi followed gradient uphill If function continues uphill direction pc point function path goes uphill downhill point pi pc The step calculate gi step differs methods In cases requires knowing derivatives function current point If model dlog_likelihood function system use numerical approximation Root finding Including Newton s method APOP_RF_NEWTON Hybrid method APOP_RF_HYBRID Hybrid method internal scaling APOP_RF_HYBRID_NOSCALE A root search finds optimum function searching point derivative function maximized zero Notice t distinguish maxima minima likelihood functions minima FR PR methods routines bear close resemblance share pseudocode 12Do confuse tolerance algorithms promise example promise Taylor expansion tolerance true value parameter MLE estimate MLE There way guarantee thing Instead tolerance indicates internal measure change algorithms f small indicate convergence 13In fact worth making sure models divergent likelihood likelihood increases Divergent likelihoods probably sign misspecified model gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Let f x function root seek e g score let f matrix derivatives e g information matrix Then step Newton s method follows xi xi f xi f xi You familiar dimensional version begins point x1 follows tangent coordinate x1 f x1 x axis equation x2 x1 f x1 f x1 generalizes multidimensional version The hybrid edition algorithm imposes region current point yond algorithm venture If tangent sends search infinity algorithm follows tangent far edge trust region The basic hybrid algorithm uses trust region different scale di mension takes extra gradient calculating evaluations trust region useless I shall defined functions scaling version simply uses standard Euclidian distance current proposed point The algorithm repeats function zero sought e g score value toleran e Simulated annealing APOP_SIMAN A controlled randomwalk As methods system tries new point better switches Initially system allowed large jumps iteration jumps smaller eventually converging Also decreasing probability new point likely chosen One reason allowing jumps likely parameters situations multiple local optima Early random walk system readily jump neighborhood optimum later fine tune way optimum Other motivations transition rules elucidated chapter Monte Carlo methods Here algorithm greater detail setting names appropriate places Start temp t_initial Let x0 starting_point While temp t_min Repeat following iters_fixed_T times Draw new point xt step_size units away xt Draw random number r If f xt f xt Jump new point xt xt Else r exp xt xt k temp Jump new point xt xt Else remain old point xt xt Cool Let temp temp mu_t gsl_stats March CHAPTER Unlike methods number points tested simulated anneal ing run dependent function specification reachest_min steps exactly steps If know model globally convex standard probability functions method overkill model complex interaction simulated annealing bet It use derivatives derivatives exist I shall behaved appropriate available analytically computation methods use derivative information converge faster If model stochastic methods build picture space tably conjugate gradient methods fooled early bad draws Sim ulated annealing memoryless sense inputs deci sion jump current point candidate A unrepresentative draws send search wrong direction draws search eventually meander direction gone Global v local optima As saw case x2 sin x methods guarantee optimum found global optimum way computer global knowledge function f x x One option restart search variety starting points hope multiple peaks different starting points rise different peaks The simulated annealing algorithm deals multiple peaks search easily jump neighborhood peak In fact number steps simulated annealing algorithm algo rithm shown converge global optimum probability How calculating infinite number steps tends unreasonable time need select time confidence trade appropriate situation Q10 Lines Listing set key parameters method starting point Try values Which better job jumping larger modes Bonus rewrite program command line switches getopt exercise batch file Q10 The e on101 models Chapter provide relatively rare situation optimization analytic values Hopefully simulations special case true This fine opportunity try methods values delta step size tolerance method et cetera Do extreme prices preferences create problems optimization settings gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION RESTARTING To reiterate recommendation trust single opti mization search But number ways order multiple searches You start large step size wide tolerance search jumps space quickly restart smaller step size tolerance hone result Along similar vein different search methods different stopping criteria switching simplex algorithm conjugate gradient algorithm example lead better optimum If suspect multiple local optima try different starting points different extremes space randomly generated points If running constrained optimization constraints binds odd interactions penalty function optimized Try series optimizations penalty declining zero optimum gets closer boundary The apop_estimate_restart function help run sequences opti mizations online reference details Given appropriate apop_data set apop_model apop_ maximum_likelihood function apply number maximum searching techniques find optimal parameters No computational method guarantee global optimum computer gather local information function Restarting search different locations help establish unique optimum find multiple optima You try methods sequence apop_estimate_ restart You use restarting technique coarse search neighborhood optimum finer search beginning coarse search ended MISSING DATA Say data set complete NaN observation column When run gression NaN s propagate wind NaN s parameter estimates How fill missing data We turn MLE problem seek likely values fill NaN s based model data generated gsl_stats March CHAPTER Infant mortality Missingness Income Missingness Infant mortality Figure Two different flows causation At left non ignorable missingness value infant mortality statistic determines infant mortality data missing At right MAR low income causes high infant mortality causes infant mortality data missing online source mar dot But need distinguish types missingness Data miss ing completely random MCAR incidence missing data uncor related variable data set This truly haphazard error somebody tripped meter s power cord surveyors drunk The because missing data data set Data missing random MAR incidence missing data col umn uncorrelated existing data column condition observed data columns For example poor countries tend bad demographic data incidence missing infant mortality rate correlated low GNP capita Once conditioned GNP capita reason expect missingness correlated infant mortality The because missing values column data set column As right hand diagram Figure flow causation infant mortality missingness Conversely governments embarrassed high infant mortality rates Statistics bureaux orders measure rate release measure falls certain threshold In case incidence missing data directly related value missing data The because missing data value data This known missing random MNAR non ignorable missingness problem implies bias definition There methods dealing censored non ignorable missingness discussed sources Greene For discus sion types missing data Allison gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Listwise deletion One option dealing NaN s MCAR listwise deletion The idea supremely simple row missing data variable throw entire row This conceptually simple impose additional structure model data executed line code apop_data nan_free_data apop_data_listwise_delete dirty_data Alternatively page syntax listwise deletion SQL But listwise deletion isn t appropriate As worst case situation survey questions everybody filled exactly By listwise deletion throw entire data set But listwise deletion data set going shorter meaning lose information data MAR MCAR throwing observations missing data means biasing information variables In example throw countries missing infant mortality data throwing countries low GDP capita ML imputation This maximum likelihood estimation comes Dempster et al Let missing data existing data holes X usual Then goal find likely value The step specify model data allegedly generated evaluate likelihood given The norm completed data set Multivariate Normal distribution meaning n columns data distributed n dimensional bell curve mean vector covariance matrix However sense data number forms But given parametrized distribution search data points likely occurred In data orruption db database find Transparency International s Corruption Perceptions Index Because index depends dozen hard gather measures missing data points List ing goes entire process filling data points pulling data database reducing estimable subset listwise deletion fitting Multivariate Normal subset filling NaN s data set maximum likelihood It run slowly filling eighty NaN s means search dimensional space For missing data probably better finding means dividing data set incre mentally filling blanks You encouraged look results decide plausible gsl_stats March CHAPTER include apop h int main apop_db_open data corruption db apop_data corrupt apop_db_to_crosstab cpi country year score apop_data clean apop_data_listwise_delete corrupt apop_model mlv apop_estimate clean apop_multivariate_normal apop_ml_imputation corrupt mlv apop_crosstab_to_db corrupt cpi_clean country year score Listing Filling NaN s Multivariate Normal Online source orrupt For example use data Yugoslavia Is Multivariate Normal appropriate model data formed On lengthy surveys people successfully fill entire form In worst case questions subjects answered Listwise deletion throw subject In case best bet pair wise deletion calculate mean vector ignoring NaN s covariance pair columns removing observations NaN variables Pairwise deletion introduce odd biases covariance matrix resort TESTING WITH LIKELIHOODS In order test hypothesis statistic need means describing theoretical distribution When statistic MLE standard means interlocking approximations Taylor expansion Normal approximation This convenient Normal approximation proves innocuous settings Taylor expansion involves cast characters dealing point LL S I USING THE INFORMATION MATRIX The Cramer Rao lower bound gives value variance parameters inverse expected information matrix Given variance parameter t F tests It easier approximation The expected information matrix expectation possible parameters means property model set parameters Conversely estimated gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION information matrix derivative score likely values parameters We expect different different parameter estimates Efron Hinkley found applications consider inverse estimated information matrix preferable estimator vari ance expected information matrix For exponential family distributions identical optimum From computational perspective certainly preferable use estimated information local property optimum global property entire parameter space Simulated annealing decent job sampling entire space methods way meaning need execute second round data gathering variances Apophenia s maximum likelihood estimation returns covariance matrix constructed estimated information matrix The covariance matrix provides estimate stability individual pa rameter allows test hypotheses individual parameters tests model likelihood ratio methods discussed However number approximations point Basically applying t test assuming million draws parameter s MLE generated fewmillion draws new data asymptotically Normally distributed We encountered assumption earlier testing parameters linear regression assume errors Normally distributed So caveats apply means generating data sets test Normality use methods discussed Chapter bootstrap distribution working consulting firm assume Normality holds There sample code section know run t test given statistic s mean estimated variance USING LIKELIHOOD RATIOS We use likelihood ratio LR tests compare models For example claim model like constraint test constraint actually binding ratio likelihood constraint likelihood Or t decide OLS model probit model ratio likelihood models tell confidence likely 14In Klemens I discuss length utility variance MLE gauge simulation s parameters volatile effect outcome little effect gsl_stats March CHAPTER A loose derivation As intimated Neyman Pearson lemma ratio likelihoods good way test hypothesis Given ratio likelihoods P1 P2 log difference ln P1 P2 LL1 LL2 Now consider Taylor expansion log likelihood function The Taylor expansion commonmeans approximating function f x series derivatives evaluated certain point For example second degree Taylor expansion sevenwould f x f x f x 2f o o error term The approximation exactly correct x decreasingly precise meaning o gets larger values seven In case log likelihood expanded Taylor expansion LL LL LL LL o As definitions beginning chapter derivative sec ond term score second derivative term I When optimum score zero Also norm Taylor expansions assume o Then expansion simplifies LL LL I Typically likelihood ratio test involves ratio unrestricted model model constraint imposed Let LLc constrained log likeli hood repeat Equation constrained log likelihood LLc LLc Ic Now hypothesis constraint binding constrained unconstrained optimizations find value Then LL LLc 2LLc 2LL 2I 2Ic I Ic The second equation follows having value constrained unconstrained optimizations means LL LLc But haven t said distribution LL LLc Consider case Normal distribution fixed free param gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION eter mean Score S x xi Q Verify finding LL x probability distribution page The right hand Equation takes familiar mean like form CLT based Normally distributed Since E I E S S Normally distributed statistic squared distribution Expression distribution And fact holds Normal likelihood function Pawi tan p Say exists transformation function t x P x t x Normally distributed Then P x t x Pc x t x P x Pc x Instead canceling transformation cancel log likelihood step LL x t x LLc x t x LL x LLc x Either way Expression transformation means untransformed version So provided likelihood function sufficiently behaved t x exist don t worry deriving This specific case invariance principle like lihood functions broadly says transformations likelihood function change information embedded function This use likelihood ratio tests Neyman Pearson lemma recommended We find log likelihood model unconstrained constrained forms times difference look result tables The LR test constraint case As typical likelihood ratio test involves ratio unrestricted restricted model null hypothesis constraint binding Let P log plain likelihood overall model Pc likelihood model K restrictions K parameters fixed zero In context discussion ln P Pc ln P ln Pc 2K In modeling terms unrestricted model models discussed earlier apop_probit apop_normal apop_ols gsl_stats March CHAPTER OLS parameters OLS X X 1Xy shown identical max imum likelihood estimate include eigenbox h double linear_constraint apop_data d apop_model m apop_data constr apop_line_to_data double return apop_linear_constraint m parameters vector constr void show_chi_squared_test apop_model unconstrained apop_model constrained int constraints double statistic unconstrained llikelihood constrained llikelihood double confidence gsl_cdf_chisq_P statistic constraints printf The Chi squared statistic g reject null non binding constraint g confidence n statistic confidence int main apop_data d query_data apop_model unconstr apop_estimate d apop_ols apop_model_show unconstr Apop_settings_add_group apop_ols apop_mle apop_ols Apop_settings_add apop_ols apop_mle starting_pt unconstr parameters vector data Apop_settings_add apop_ols apop_mle use_score Apop_settings_add apop_ols apop_mle step_size 1e apop_ols estimate NULL apop_ols constraint linear_constraint apop_model constr apop_estimate d apop_ols printf New parameters n apop_vector_show constr parameters vector show_chi_squared_test unconstr constr Listing Comparing different models likelihood ratios OLS model OLS model constraint logit model Online source lrtest Listing presents unconstrained constrained optimization It uses query page produces data set outcome variable males females independent variables population median age The question coefficient median age significant rephrased constrain median age coefficient zero significant effect log likelihood The unconstrained optimization line ordinary squares model finds MLE gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION Lines mangle base OLS model constrained model estimated maximum likelihood By setting estimate element NULL estimation line uses default method maximum likelihood estimation The constraint function line uses apop_linear_ onstraint function test input vector satisfies constraint expressed apop_ data set type introduced page In case constraint unconstrained OLS estimation finds binding constraint Line uses new syntactic trick anonymous structures The type parens form double looks like type cast basically acts way declaring data braces nameless array doubles The line equivalent lines code main routine ftest program page double tempvar apop_line_to_data tempvar But away packing line bothering temp variable When conjunction designated initializers anonymous structs convey lot information line code unreadable mess depending sthetic preferences By commenting constraint setting line uncon strained model estimated maximum likelihood verify OLS parameters MLE parameters identical You recognize function line direct translation Expression It test claim constraint binding rejects null confidence t test associated linear regression rejected null parameter zero The statistic line LL LLc positive opti mum constrained optimization found unconstrained model unconstrained model potentially find higher likelihood If term negative sign unconstrained optimization far true optimum restart new method new starting point tighter tolerances tweaks Be sure compare results test results F test page The LR test non nested case The form test nested models restricted form un der hypothesis nonbinding constraint find estimate conceivably arrive log likelihood If case gsl_stats March CHAPTER cancellation Taylor expansion Equation happen In case Cox Vuong statistic distribution ln P1 P2 E ln P1 P2 n N The denominator simply square root sample size The numerator LL1 LL2 familiar point The expected value problematic global value log likeli hoods conceivably arrive probability weighted integral LL1 LL2 entire space s Alternatively assume zero That easiest test run Expression null hypothesis difference expected value logs define TESTING include dummies c void show_normal_test apop_model unconstrained apop_model constrained int n double statistic unconstrained llikelihood constrained llikelihood sqrt n double confidence gsl_cdf_gaussian_P fabs statistic tailed printf The Normal statistic g reject null difference models g confidence n statistic confidence int main apop_db_open data metro db apop_model m0 dummies apop_model m1 dummies show_normal_test m0 m1 m0 data matrix size1 Listing Compare twoMetro ridership models page Online source lrnonnest Listing reads code OLS estimations Washington Metro ridersiop page zero dummy dummy year s slope If flip dummies file main function wrapped preprocessor statement ifndef TESTING Because TESTING defined main function file passed Therefore line read dummies directly ending mains gsl_stats March MAXIMUM LIKELIHOOD ESTIMATION The main function simply estimates models calls show_ normal_test function translation Expression null hypothesis E LLOLS LLlogit Remember number approximations underly nested non nested LR tests In nested case generally considered innocuous rarely verified mentioned For non nested probit logit models log likelihoods behave somewhat similar manner n reason able apply non nested statistic But radically different models like OLS model versus agent based model approximations start strain You directly compare log likelihoods test statistic sense scale difference decide statistics tell disparate models gsl_stats March MONTE CARLO Monte Carlo Italian Spanish Mount Carl city Monaco famous casinos glamorous associations Reno Atlantic City Monte Carlo methods randomization taking existing data mak ing random transformations learn But process volves randomness outcome mere whim fates At roulette table single player come ahead millions suckers testing luck casinos find bet favor reliable method making money Similarly single random transformation data doubt produce distorted impression reapplying thousands millions times present increasingly accurate picture underlying data This chapter look basics random number generation It discuss general process describing parameters distribution parameters data set distribution Monte Carlo techniques As special case bootstrapping method getting variance data rights able variance Nonparametric methods return chapter shuffling redrawing data feel odds hypothesized event occurred write hypothesis tests based resampling data gsl_stats March MONTE CARLO gsl_rng apop_rng_alloc int seed static int first_use first_use first_use gsl_rng_env_setup gsl_rng setme gsl_rng_alloc gsl_rng_taus2 gsl_rng_set setme seed return setme Listing Allocating initializing random number generator RANDOM NUMBER GENERATION We need stream random num bers programming need replicable stream random numbers There places need replication The debugging don t want segfault trying track appear dis appear run The second reporting results colleague asks arrived numbers able reproduce exactly Of course stream numbers time creates possibility getting lucky draw lucky mean number things The compromise use collection deterministic streams numbers apparent pattern stream numbers indexed value seed The GSL implements process Listing shows innards apop_rng_allo function Apo phenia library initialize gsl_rng In cases function takes integer sets random number generation RNG environment produce new numbers Tausworth routine Fans RNG methods check GSL documentation setting gsl_rng alternative algorithms On function calls gsl_rng_env_setup function work internal magic GSL Listings example function 1Is valid replicable stream seemingly random numbers random Because RNGs arguably random prefer term pseudorandom number generator PRNG describe This question rooted philosophical question book delve difference perceived randomness given level information true randomness See e g Good pp 2Formally RNG produces stream eventually cycles beginning The seed simply specifies cycle begin But cycle long little loss thinking seed producing separate stream If initialize RNG stream value point cycle follow lock step This avoided need sequence streams better simple list seeds like gsl_stats March CHAPTER RANDOM NUMBER DISTRIBUTIONS Now random number gener ator functions use draw favorite distributions Input RNG allocated plus appropriate parameters GSL transform RNG necessary double gsl_ran_bernoulli gsl_rng r double p double gsl_ran_beta gsl_rng r double double b double gsl_ran_binomial gsl_rng r double p int n double gsl_ran_chisq gsl_rng r double df double gsl_ran_fdist gsl_rng r double df1 double df2 double gsl_ran_gaussian gsl_rng r double sigma double gsl_ran_tdist gsl_rng r double df double gsl_ran_flat gsl_rng r double double b double gsl_rng_uniform gsl_rng r The flat distribution Uniform A B distribution The Uniform distribution gets options function gsl_rng_uniform r The Gaussian draw assumes mean zero intend draw e g N use gsl_ran_gaussian r The apop_model struct includes draw method works like func tions random draws allows standardization exotic models like histogram example Listing page An example Beta distribution The Beta distribution wonderful sorts modeling describe wide range probability functions variable For example saw prior page But parameters difficult interpret mean variance Thus Apophenia provides convenience function apop_beta_from_mean_var takes returns appropriate Beta distribution corresponding values As know variance Uniform exactly means Beta distribution variance greater close perverse things happen computationally The mean function positive density iff x If send apop_beta_ from_mean_var values outside bounds function return GSL_NAN What Beta distribution look like Listing sets RNG makes million draws Beta distribution plots result 3More trivia Uniform symmetric skew zero It is kurtosis gsl_stats March MONTE CARLO include apop h int main int draws 1e7 int bins doublemu try double sigmasq try gsl_rng r apop_rng_alloc apop_data d apop_data_alloc draws apop_model m apop_beta_from_mean_var mu sigmasq int draws apop_draw apop_data_ptr d r m Apop_settings_add_group apop_histogram apop_histogram d bins apop_histogram_normalize apop_histogram apop_histogram_plot apop_histogram NULL Listing Building picture distribution random draws Online source drawbeta P x x P x x Figure The flexible Beta distribution Run drawbeta gnuplot Most code simply names constants real action occurs line apop_beta_from_mean_var takes returns apop_model representing Beta distribution appropriate parameters In line data set d filled random draws model The apop_ draw function takes pointer double argument puts value location based RNG model sent second argu ments Thus need use apop_data_ptr gsl_ve tor_ptr gsl_ matrix_ptr apop_draw pointer right location The slightly awkward pointer syntax means copying reallocation necessary slow drawing millions numbers The final lines code generic apop_histogram model set use gsl_stats March CHAPTER data d given number bins normalize histogram produced total density plot result The output example 1e7 draws left Figure contrast case pictured right r A r2 A 2r Figure A circle inscribed square The ratio area circle area square Online source square ir le gnuplot Q11 As Figure circle inscribed inside square ratio area circle square Thus randomly draw points square expect fall circle Estimate random draws square For zero 1e8 Draw xi Uniform distribution draw yi Uniform distribution Determine xi yi falls unit circle meaning x2i y equivalent x2i y2i Every draws display proportion draws inside circle times How close estimate come Hint clearer display fabs M_PI pi_estimate instead esti mate gsl_stats March MONTE CARLO DRAWING FROM YOUR OWN DATA Another possibility drawing fa mous distributions data theoretically approximates draw actual data If data vector draw random index return value index Let r appropriately initialized gsl_rng let your_data gsl_ve tor like draws Then following liner single draw gsl_vector_get your_data gsl_rng_uniform_int r your_data size Drawing histograms There reasons data togram form rough probability mass function like ones plotting data Chapter describing data Chapter goodness fit tests Chapter output apop_update function Here draw produce artificial data sets include apop h gsl_rng r void plot_draws apop_model m char outfile int draws 2e3 apop_data d apop_data_alloc draws size_t draws apop_draw apop_data_ptr d r m apop_model h3 apop_estimate d apop_histogram apop_histogram_print h3 outfile int main r apop_rng_alloc apop_db_open data wb db apop_data pops apop_query_to_data select population p pop p apop_model h apop_estimate pops apop_histogram apop_histogram_print h hist plot_draws apop_estimate pops apop_lognormal lognormal_fit printf set xrange 2e5 set yrange n plot hist boxes lognormal_fit lines n Listing Draw actual histogram country populations exponential distribution closely fits Online source drawfrompop For example like generate communities populations distributed way countries world distributed Listing gsl_stats March CHAPTER ways The simplest approach simply generate histogram world populations draws histogram Line creates filled histogram filling un parametrized base apop_ histogram model list populations In cases simply data job The World Bank data set lists countries simulation produces millions communities repetition numbers produce odd effects The solution presented estimate Lognormal distribution draw ideal distribution Line model fit sends output apop_estimate func tion plot_draws function makes multitude draws ideal distribution plots You result smoother zero entry bins real world data Lines fill column zero d data line turns data togram The easiest way data sets plot write separate files lines files Gnuplot script lines Seeding time There situations fixed seed want want different numbers time run program The easiest solution seed time function The standard library function time NULL return number seconds elapsed beginning roughly UNIX C composed As I write time returns good way tell time There number functions turn hours minutes months reference C s standard library GNU C Library documentation details But illegible form provides perfect seed RNG new second Listing time shows sample program produces draws RNG seeded time This industrial strength random number gener ation patterns conceivably slip For example extreme case try compiling time run continuously she shall In Bourne family she shall probably POSIX system try true time You numbers dozen times clock ticks stream repeated times You command prompt gsl_stats March MONTE CARLO include time h include apop h int main gsl_rng r apop_rng_alloc time NULL int printf 3g t gsl_rng_uniform r printf n Listing Seeding RNG time Online source time ctrl c If multiple processors run simulation runs start second replicating Finally hope debug program need write time started replicate runs break I assume database open column table named runs The time long integer GNU standard library printf tag li long int right_now time NULL apop_query insert runs li right_now gsl_rng r apop_rng_alloc right_now Caveats aside want variety time program runs seeding time works fine The standard C RNG If GSL available standard C library cludes rand function random draws ansrand function set seed E g include stdlib h srand printf One draw U g rand RAND_MAX The GSL s RNGs preferable reasons First gsl_ran_max r typ ically greater RAND_MAX giving greater variation precision Second C language standard specifies rand function works meaning machines different streams random numbers seed Finally rand gives entire program exactly stream numbers initialize gsl_rngs independent For example agent simulation RNG run simulation gsl_stats March CHAPTER agent added removed probably breakpoint GDB guar anteed variation agent RNG shifts Here sample code clarify setup initialized typedef struct agent long int agent_number gsl_rng r agent void init_agent agent initme int agent_no initme agent_number agent_no initme r apop_rng_init agent_no Random number generators produce deterministic sequence val ues This good thing debug program replicate results Change stream initializing different seed Given random number generator use draw common distribution histogram data set DESCRIPTION FINDING STATISTICS FOR A DISTRIBUTION For statistic distribution pairs ex ists closed form solution statistic kurtosis N vari ance Binomial distribution np p et cetera You recourse Slutsky theorem says given estimate r statistic continuous function f f r valid estimate f Thus sums products means variances easy calculate However find situations need global value like mean variance closed form means calculating value Even closed form theorem begins limit n holds evidence theorem falling flat dozen data points One way calculate expected value statistic f given probability distri bution p numeric integral entire domain distribution For resolution slices write loop sum gsl_stats March MONTE CARLO E f p f p f p f p f p This effective details hammered distribution domain integrate You decide fine resolution bar ring tricks resolution new calculation modification prior calculations If like approach GSL includes set numerical integration functions Another approach evaluate f values randomly drawn distribu tion Just Listing produced nice picture Beta distribution taking random draws decent number random draws produce good es timate desired statistic overall distribution Values definition appear proportion likelihood p takes care There cutoff tails distribution assumed away You incre mentally monitor E f random draws getting extra time An example kurtosis t distribution You probably know t distribu tion like Normal distribu tion fatter tails probably fatter tails The kurtosis vector easy calculate apop_ve tor_kurtosis By taking million draws t distribution produce vector values cover distribution find kurtosis vector Listing shows program execute procedure The main function sets header output table Table calls one_df df The loop lines draws storing vector v Once vector filled line calculates partially normalized kurtosis That calculates raw kurtosis variance squared box page endless debate best express kurtosis The closed form formula partially normalized kurtosis t distribution df degrees freedom 3df df For df kurtosis undefined variance undefined Cauchy distribution e t distribution df At df finite monotonically decreases df continues upwards gsl_stats March CHAPTER include apop h void one_df int df gsl_rng r long int runct 1e6 gsl_vector v gsl_vector_alloc runct runct gsl_vector_set v gsl_ran_tdist r df printf t g df apop_vector_kurtosis v gsl_pow_2 apop_vector_var v df printf t g df df printf n gsl_vector_free v int main int df df_max gsl_rng r apop_rng_alloc printf df t k est t k actual n df df df_max df one_df df r Listing Monte Carlo calculation kurtoses t distribution family Online source tdistkurtosis Table shows excerpt simulation output true kur tosis The exact format variance estimate kurtosis given Q use methods find falls df df variance kurtosis estimate infinite shrinks df grows Correspondingly bootstrap estimates kurtosis unreliable df consistent df dozen You check running program different seeds e g replacing zero seed line code time NULL By making random draws model statements global properties model improve accuracy number draws The variance Monte Carlo estimation parameter tends mirror variance underlying parameter The maximum likelihood estimator parameter achieves Cramer Rao lower bound variance Monte Carlo estimate larger significantly gsl_stats March MONTE CARLO df k est k analytic Table The fatness tails t distributions df INFERENCE FINDING STATISTICS FOR A PARAMETER The t distribution example random draws closed form distribution order pro duce estimate function distribu tion parameters Conversely want estimate function data variance mean data set v ar X We data set random draws fromX produce values statistic Xr whereXr represents ran dom draw X estimate variance draws This known bootstrap method estimating variance BOOTSTRAPPING THE STANDARD ERROR The core bootstrap simple algorithm Repeat following m times Let X n elements randomly drawn data replacement Write statistic s X Find standard error m values X This algorithm bears resemblance steps demonstrated CLT demo Listing page draw m iid samples find statistic like mean look distribution statistics underlying data So X mean like statistic involving sum gsl_stats March CHAPTER n CLT applies directly artificial statistic approaches Normal distribution Thus makes sense apply usual Normal distribution based test test hypotheses true value include oneboot h int main int rep_ct gsl_rng r apop_rng_alloc apop_db_open data census db gsl_vector base_data apop_query_to_vector select in_per_capita income sumlevel double RI apop_query_to_float select in_per_capita income sumlevel geo_id2 gsl_vector boot_sample gsl_vector_alloc base_data size gsl_vector replications gsl_vector_alloc rep_ct int rep_ct one_boot base_data r boot_sample gsl_vector_set replications apop_mean boot_sample double stderror sqrt apop_var replications doublemean apop_mean replications printf mean g standard error g RI mean stderr g p value g n mean stderror RI mean stderror gsl_cdf_gaussian_Q fabs RI mean stderror Listing Bootstrapping standard error variance state incomes capita Online source databoot include oneboot h void one_boot gsl_vector base_data gsl_rng r gsl_vector boot_sample int boot_sample size gsl_vector_set boot_sample gsl_vector_get base_data gsl_rng_uniform_int r base_data size Listing A function produce bootstrap draw Online source oneboot Listings shows algorithm executed code It tests hypothesis Rhode Island s income capita different mean Lines Listing introductory material queries pull req uisite data For Rhode Island scalar test rest country vector numbers state common wealth district territory data gsl_stats March MONTE CARLO Lines main loop repeatedm times pseudocode makes draws finds mean draws Listing function single bootstrap draw scripts The one_boot function draws replacement simply requires repeatedly calling gsl_rng_uniform_int random index writing value index data vector Lines Listing find mean standard error returned data lines run standard hypothesis test comparing mean Normal distribution scalar Recall discussion page standard deviation data set square root variance standard deviation mean data set n In case interested distribution X distribution E X s use instead n Q11 Rewrite databoot calculate standard error base_data directly bootstrapping test hypothesis standard error estimate bootstrapped version Do need test calculated value n include apop h int main int draws boots doublemu sigma gsl_rng r apop_rng_alloc gsl_vector d gsl_vector_alloc draws apop_model m apop_model_set_parameters apop_normal mu sigma apop_data boot_stats apop_data_alloc boots apop_name_add boot_stats names mu c apop_name_add boot_stats names sigma c int boots int j j draws j apop_draw gsl_vector_ptr d j r m apop_data_set boot_stats apop_vector_mean d apop_data_set boot_stats sqrt apop_vector_var d apop_data_show apop_data_covariance boot_stats printf Actual n var mu g n gsl_pow_2 sigma draws printf var sigma g n gsl_pow_2 sigma draws Listing Estimate covariance matrix Normal distribution Online source normalboot gsl_stats March CHAPTER A Normal example Say want know covariance matrix esti mates Normal distribution parameters We look find n 2n require effort looking books So instead write program like Listing generate covariance Running program computational method comes reasonably close analytic value The introductory material line allocates vector bootstrap sam ples data set holding mean standard deviation bootstrap sample Instead drawing permutations fixed data set version program produces artificial data vector draws Normal distribution lines lines write statistics data set In case producing statistics line finds covariance matrix scalar variance Q11 On page I mentioned test claim kurtosis Normal distribution equals bootstrap Modify normalboot test hypothesis Where program currently finds means variances samples find sample s kurtosis Then run t test claim mean vector kurtoses To test hypothesis model parameter need estimate parameter s variance If analytic way find variance multi ple draws data calculate parameter find variance artificial set parameters The Central Limit Theorem tells artificial parameter set approach behaved Normal distribution 4E g Kmenta p That text provides covariance matrix parameters The variance variance n gsl_stats March MONTE CARLO DRAWING A DISTRIBUTION To point searching global parameters distribution data set output single number small matrix What want find entire distribution We disposal standard RNGs draw Normal Uniform distribu tions section present techniques transform draws draws distribution hand For reasons random draws preferable brute force numeric integration ef ficient produce picture distribution random draws grid search draws focus likely points tails cut arbitrary limit desired level precision reached computation And remember produces nonnegative univariate measure read subjective likelihood function For example real world data distribution firm sizes The model page gives artificial distribution firm sizes given input parameters standard error currently hard coded model s growth function Given run find distance d actual distribution artificial Notice function produces new output distribution fore new distance actual The likely value produces smallest distance write L d example Then use methods chapter find likely parameters draw picture likelihood function given input parameters calculate variance given subjective likelihood function test hypotheses given subjective likelihood IMPORTANCE SAMPLING There means making draws distribution form draws Train Chapter catalogs For example importance sampling means producing draws new function known function reference Let f function interest let g known distribution like Normal Uniform We want use stock Normal Uniform distribution RNGs produce RNG arbitrary function For million Draw new point xi reference distribution g Give point weighting f xi g xi Bin data points histogram weighting point appropriately 5Recall invariance principle page basic transformations likelihood function don t change data don t fret exact form subjective likelihood function results d d2 d example gsl_stats March CHAPTER At end histogram valid representation f making draws graphing function calculating global infor mation What reference distribution use Theoretically engi neering considerations advise pick function reasonably close trying draw want high likelihood parts g match high likelihood parts f So f generally bell curve use Normal distribution focused near zero use Lognormal Exponential truly information fall Uniform rough exploratory search let better choice MARKOV CHAIN MONTE CARLO Markov Chain Monte Carlo iterative al gorithm starts state rule defining likelihood transitioning given state initial state Markov Chain In context state simply possible value parameter s estimated By jumping parameter value proportion likelihood view probability density parameters The exact probability jumping point given specific form presented shortly This primarily Bayesian updating let review setup method The analysis begins prior distribution expressing current beliefs parameters Pprior likelihood function PL X expressing likelihood observation given value parameters The goal combine form posterior page Bayes s Rule tells Ppost X PL X Pprior B B PL X B Pprior dB The numerator easy calculate denominator global value meaning know certainty evaluating numerator infinite number points The Metropolis Hastings algorithm type MCMC offers solution The gist start arbitrary point draw new candidate point prior distribution If candidate point likely current point according PL jump candidate point likely jump After burn period record values The histogram recorded values histogram posterior distribution To precise pseudocode description algorithm gsl_stats March MONTE CARLO Begin setting arbitrary starting value For million Draw new proposed point p Pprior If PL X p PL X p Draw value u U If u PL X p PL X p If record Report histogram posterior distribution As evident description algorithm primarily prior posterior distribution To jump new point new chosen prior happens probability Pprior new selected likelihood roughly proportional PL x new recorded draw proportional Pprior new PL x new This rough intuitive argument rigorous prove points recorded valid representation posterior Gelman et al Why draw prior multiply likelihood directly going business conditionally jumping It matter effi ciency The likely elements posterior likely contribute large integral denominator updating equation estimate integral efficiently biasing draws likely posterior values jumping scheme achieves fewer draws naive draw prior scheme You seen concrete example MCMC simulated annealing algorithm Chapter It jumps point point decision rule like The proposal distribution basically improper uniform distribution point likely likelihood function probability distribution optimum simulated annealing seeking The difference simulated annealing algorithm makes smaller smaller jumps combined fact MCMC designed tend likely points eventually settle maximum value point throw away prior values For Bayesian updating algorithm jumps fixed prior tend likely values posterior 000th jump likely long use point found produce output distribution At computer function execute algorithm apop_update This func tion takes parametrized prior model unparametrized likelihood data outputs parametrized posterior model gsl_stats March CHAPTER The output model types As Beta Binomial example page known conjugate distributions posterior form prior distribution updated parameters For example prior named Gamma distribution apop_gamma model named likelihood namedExponential distribution apop_ exponential named output apop_update closed form Gamma distribution appropriately updated parameters But tables closed form conjugates offer applicable system fall MCMC output apop_histogram model results jumps Q11 Check close MCMC algorithm comes closed form model Pick arbitrary parameter Exponential distribution I Gen erate thousand random data points Pick arbitrary values Gamma distribution parameters prior Estimate posterior sending distributions randomly generated data prior apop_update Make copy apop_exponential model change apop_update won t find conjugate distribution table Re send apop_update Use goodness fit test find howwell output distributions match Q11 The output updating process distribution prior new updating step In theory distributions repeatedly updated new data approach putting probability mass true parameter value Continuing exercise Regenerate new set random data points Exponential distribution I Send new data closed form posterior prior exercise Exponential model updating routine Plot output Once generate update plot routine working loop generate animation beginning prior continuing dozen updates Repeat MCMC estimated posteriors gsl_stats March MONTE CARLO NON PARAMETRIC TESTING Section presented procedure testing claim data set con sisted drawing bootstrap data set regenerating statistic draws statistic CLT data But test hypotheses simply making draws data checking characteristics bothering produce statistic use CLT To concrete example testing parametric assumptions consider permutation test Say draw red cards black cards crooked deck The hypothesis mean value red cards drew equals mean black cards counting face cards like You red cards pile left black cards pile right calculate xred red xblack black assume xblack xred t distribution run traditional t test compare means But red equals black fact cards drew black red irrelevant value xleft xright That shuffled stacks black red cards dealt cards left right xleft xright appreciably different If deal thousand shuffled pairs piles draw distribution values xleft xright If xred xblack looks far center distribution reject claim color cards irrelevant What benefit shuffling redealing run t test compare groups On theoretical level method relies assumption bootstrap principle assumptions underlying CLT Instead assuming theoretical distribution shuffle redraw produce distribution outcome values matches true sample data assumption red black rely bootstrap principle learned sample representative population sample drawn On practical level closer match data provides real benefits Lehmann Stein showed permutation test powerful likely fail reject equality hypothesis means actually different In pseudocode test procedure 6The test attributed Chung Fraser From p The computation took day programming minutes machine time gsl_stats March CHAPTER xred xblack d joined test control vectors Allocate million element vector v For million Draw replacement d vector size xred L Put elements d second vector R vi L R percentage vi Reject claim confidence Fail reject claim confidence Q11 Baum et al sampled genetic material large number cases bipolar disorder controls confirmed bipo lar disorder To save taxpayer money pooled samples form groups listed file data genes code supplement Each line file lists percent pool given SNP single nucleotide polymorphism given marker For SNPs canwe reject hypothesis difference cases controls Write program read data database use algorithm test reject hypothesis difference marker frequency given SNP finally write main runs test SNPs database The data cut SNPs base Bonferroni correction tests Other nonparametric tests tend follow similar theme given null hypothesis bins equiprobable develop odds observed data occurred See Conover book length list tests including Kolmogorov s method page book TESTING FOR BIMODALITY As finale book Listing shows use kernel densities test multimodality This volves generating series kernel density estimates data original data thousand bootstrap estimates nonparametric test hypothesis distribution fewer n modes value n Recall kernel density estimate page based form f t x h n 1N t xi h n h x vector n data points observed N y Normal PDF evaluated y h R bandwidth Let k number modes gsl_stats March MONTE CARLO Figure p showed h rises spike point spreads merges spikes k falls eventually entire data set subsumed single peaked curve k Thus monotonic relationship h number modes density function induced h Silverman offers bootstrap method testing null hypothesis form distribution k modes Let h0 smallest value h f t X h0 k modes null hypothesis h0 consistent data We draw bootstrap samples data x x write f t x h0 f t x h0 count modes func tions Silverman shows thanks monotonic relationship h number modes percentage bootstrap distributions k modes matches likelihood f t x h0 consistent data If ject hypothesis need lower value h modes explain data Listing shows code produce figures test data set television program ratings bimodality Since draws bootstrap samples link oneboot page The ountmodes function makes scans range data The generates histogram point x axis piles value point Normal distribution centered data point The result histogram like pictured Figure page produced Gnuplot animation program write kernelplot file The second pass checks modes simply asking point higher points left right h k For index ktab intended smallest value h produces fewer modes The fill_kmap function calls ountmodes find number modes produced range values h The function runs largest h smallest value h written slot smallest value produces mode count k p Now smallest bandwidth h produces given k actual data boot bootstrapping one_boot p uses straight forward loop produce sample ountmodes function applied artificial sample If mode count larger num ber modes original success hypothesis The function 7Also loop function demonstrates pleasant trick scanning wide range instead stepping fixed increment steps percentages quickly scans hundreds looks lower end range detail gsl_stats March CHAPTER returns percent successes report p value hypothesis distribution k modes In parts output k battery tests finds low confidence k modes high confidence k modes If tests run value h inconsistent smallest level smoothing possible case tests k k parameters little For small k output monotonic shows confident data set modes reasonably confident include oneboot h double modect_scale modect_min modect_max h_min h_max max_k boot_iterations pauselength char outfile kernelplot void plot apop_data d FILE f fprintf f plot lines n apop_data_print d NULL fprintf f e npause g n pauselength int countmodes gsl_vector data double h FILE plothere int len modect_max modect_min modect_scale apop_data ddd apop_data_calloc len double sum modect_min size_t j j ddd matrix size1 j sum size_t k k data size k sum gsl_ran_gaussian_pdf gsl_vector_get data k h data size h apop_data_set ddd j apop_data_set ddd j sum modect_scale intmodect len apop_data_get ddd apop_data_get ddd apop_data_get ddd apop_data_get ddd modect plothere plot ddd plothere apop_data_free ddd return modect void fill_kmap gsl_vector data FILE f double ktab double h h_max h h_min h gsl_stats March MONTE CARLO int val countmodes data h f val max_k ktab val h double boot gsl_vector data double h0 int modect_target gsl_rng r double over_ct gsl_vector boots gsl_vector_alloc data size int boot_iterations one_boot data r boots countmodes boots h0 NULL modect_target over_ct gsl_vector_free boots return over_ct boot_iterations apop_data produce_p_table gsl_vector data double ktab gsl_rng r apop_data ptab apop_data_alloc max_k apop_name_add ptab names Mode c apop_name_add ptab names Likelihood c int max_k apop_data_set ptab apop_data_set ptab boot data ktab r return ptab void setvars gsl_vector data rescale based data doublem1 gsl_vector_max data doublem2 gsl_vector_min data modect_scale m1 m2 modect_min m2 m1 m2 modect_max m1 m1 m2 int main APOP_COL apop_text_to_data data tv data setvars data FILE f fopen outfile w apop_opts output_type p apop_opts output_pipe f double ktab calloc max_k sizeof double fill_kmap data f ktab fclose f gsl_rng r apop_rng_alloc apop_data_show produce_p_table data ktab r Listing Silverman s kernel density test bimodality Online source bimodality gsl_stats March CHAPTER Another way speed process bimodality clump data summing Normal distributions If points require calculations Normal PDF point real line If clumped calculate Normal PDF times run times fast Q11 Write function group nearby data points single point given weight use weight element apop_data struc ture record Rewrite ountmodes function use clumped weighted data set How clumping need results degrade significantly Via resampling test certain hypotheses assuming parametric forms like t distribution The typical kernel density estimate consists specifying distribu tion point data set combining form global distribution The resulting distribution ways faithful data As bandwidth sub distributions grows overall distribu tion smoother One test hypotheses multimodality kernel densities finding smallest level smoothing necessary achieve n modality bootstrapping odds level smooth ing produce n modality gsl_stats March A ENVIRONMENTS AND MAKEFILES Since C standard product single corporation foundation differ ent C development environments minor potentially maddening differences The problem decent compiler useful libraries available given system basically surmounted missing ask package manager install Internet But question package manager Some systems like libraries usr lo al like opt locations makes sense eccentric Windows operating system The solution set environment variables specify compiler find elements needs produce program These variables maintained operating system specified POSIX standard A ENVIRONMENT VARIABLES If type env command prompt list environment vari ables set system Here sampling prompt She shall bin bash USER klemens 1Later members Windows family operating systems claim POSIX compliance meaning works Windows command prompt require downloading Microsoft s Interix package However Cygwin system prompt basically keeps set environment variables I assume Cygwin s she shall Windows gsl_stats March APPENDIX A LD_LIBRARY_PATH usr local lib PATH home klemens tech bin usr local bin usr bin bin usr games sbin HOME home klemens Every time program spawns new subprogram environment variables duplicated passed child program Notably start program she shall prompt environment variables saw typedenv passed program The env command works grep page For example find she shall try env grep She shall Setting Now know she shall syntax setting envi ronment variables differs slightly she shall she shall You probably bash variant Bourne she shall In systems set environment variables she shall s export command export USER Stephen Some shells picky spacing complain space equals sign Others accept syntax require line version USER Stephen export USER This form clarifies process setting environment veriable bash con sists setting local variable passed child programs moving set local variables set environment variables If she shall programming loss setting variables environment The family shells C she shall bears vague passing resemblance C programming language In sh use setenv setenv USER Stephen 2bash Bourne she shall overhaul she shall Stephen Bourne wrote UNIX gsl_stats March ENVIRONMENTS AND MAKEFILES Getting In shells she shall like programs value vari able putting dollar sign variable For example echo USER print current value USER variable screen To useful example export PATH PATH HOME bin extend PATH replacing variable copy plus directory end For lengthy path listed command result following new path PATH home klemens tech bin usr local bin usr bin bin usr games sbin home klemens bin Setting good Every time she shall starts reads number configuration files she shall s manual page man bash man sh details But read file home directory begins dot ends r bashr shr It POSIX custom file begins dot hidden meaning usual directory listing ls files However explicitly ask hidden files ls ls d GUI file browser s hidden option certainly find number r files home directory The she shall s r files plain text generally look like sort thing type command line If anexport setenv command file commands read ev ery time she shall starts variables set appropriately work The file auto execute edit To effects edit ei ther exit enter she shall use sour e bashr sour e shr explicitly ask she shall read file 3By way making backups home directory worth verifying hidden files backed gsl_stats March APPENDIX A GETTING AND SETTING FROM C It easy read write environment vari ables POSIX system easy read write C Listing A shows getenv function return string holding value given environment variable setenv set given environment variable line USER given value Stephen The argument setenv specifies existing environment variable overwritten There useful environment handling func tions standard C library reference details include stdlib h environment handling include stdio h int main printf You s n getenv USER setenv USER Stephen printf But s n getenv USER Listing A Getting setting environment variables Online source env Why safe run program Listing A overwrite important environment variable Because child process program change copy environment variables It overwrite variables environment parent process she shall Overriding basic security precau tion requires great deal cooperation parent process But setenv useless program starts programs function likesystem popen inherit current set environment variables When she shall opened program passed environment exactly manner The operating system maintains set variables describing cur rent overall environment user s current work ing directory These environment variables passed child programs They set read command line pro gram gsl_stats March ENVIRONMENTS AND MAKEFILES A PATHS On problem getting C compiler find libraries When type command g she shall locates program hard drive loads memory But given executable maybe bin opt Program Files she shall know look It uses environment variable named PATH lists set direc tories separated colons non Cygwin Windows systems separated semicolons You saw number examples variable run env grep PATH check path When type command she shall checks directory path program requested If finds executable right she shall runs program moves element path tries If makes way entire path finding program gives sends familiar command found error message The current directory typically path programs ls found searching path The solution explicit path like run_me extend current directory path export PATH PATH Recall adding line bashr shr path ex tended time log There paths relevant compilation C program include path library path The procedure she shall s executable path lude file e g lines Listing A prepro cessor steps directory listed include path checking quested header file It uses file matches given gives header found error When linker needs function structure library searches library path manner But unlike executable path ways directory appear include library path 4Many consider putting current directory path security risk e g Frisch p security lingo allows current directory attack If current directory head path malicious user script named ls common directory script contains command rm rf HOME When switch location try directory listing she shall instead finds malicious script home directory erased Putting current directory end path provides slightly protection malicious script instead named common typo mr But given today s POSIX computers person close associates adding path real risk situations 5Java users notice close resemblance paths CLASSPATH environment variable effectively include library path gsl_stats March APPENDIX A There default include path default library path built compiler linker This typically includes usr lude usr lo al lude include path usr lib usr lo al lib libpath The environment variables INCLUDEPATH LIBPATH path These names different depending linker g uses ld looks LD_LIBRARY_PATH Mac systems looks DYLD_ LIBRARY_PATH On HP UX systems try SHLIB_PATH Or generally try manld man find right environment variable system You add paths compiler s command line The flag I usr lo al lude add directory include path flag L usr lo al lib add directory library path Also libraries shared context means linked compilation actually called execution Thus execute newly complied program missing library error fix need add directory libpath environment variable Searching So know syntax adding directory PATHs directory add Say know need add header TLA library file like tla h The command find dir tla hwill search dir subdirectory dir file given Searching entire hierarchy beginning root directory find tla h typically takes human noticeable time try typical candidates dir usr opt lo al Mac OS X sw Compiled libraries different names different systems best bet search files beginning given string e g find dir tla Assembling compiler command line There source code tells system libraries linked libraries listed command line First list non standard directories search capital L flag need lower case l flag library called Order matters specifying libraries Begin dependent library object file continue dependent The Apophenia library depends SQLite3 GSL library depends BLAS library like GSL s companion CBLAS If program object file named sour e o gsl_stats March ENVIRONMENTS AND MAKEFILES want output program named run_me need specify like gcc source o lapophenia lgsl lcblas lsqlite3 gcc source o lapophenia lsqlite3 lgsl lcblas The Sqlite3 library GSL CBLAS mutually independent come The rest ordering compilation work You need specify location libraries path ex tension come command looks like gcc L usr local lib source o lapophenia lgsl lcblas lsqlite3 This immense typing fortunately section offers program designed save labor Many systems including she shall preprocessor linker search specified directory path requested items The default path preprocessor linker extended environment variables instructions compiler s com mand line A MAKE The program provides convenient system specify flags warnings debugging symbols include paths libraries link knows details You write makefile describes source files needed compilation flags compiler needs system assembles elaborate command line required invoke compiler Many programs gdb vi EMACS let run leaving program Because assembling command line hand time consuming error prone strongly encouraged makefile program Fortunately worked flags paths compile program makefile probably work program system So effort writing makefile pays reuse copies new program The discussion based Listing A makefile program named 6With linkers order matter If case system consider lucky try preserve ordering ease potential transition system gsl_stats March APPENDIX Arun_me files header file It far removed sample makefile online code supplement OBJECTS file1 o file2 o PROGNAME run_me CFLAGS g Wall Werror std gnu99 LINKFLAGS L usr local lib lgsl lgslcblas lsqlite COMPILE gcc CFLAGS c o executable OBJECTS gcc CFLAGS OBJECTS LINKFLAGS o PROGNAME file1 o file1 c my_headers h COMPILE file2 o file2 c my_headers h COMPILE run executable PROGNAME PARGS Listing A A sample makefile program source files The program types operation expanding variable names following dependencies Variable expansion Variable names makefile lookmuch like environment vari ables behave like They slightly easier set lines use VAR value When referencing use dollar sign parens line replace CFLAGS g Wall Werror std gnu99 All environment variables available way makefile include definitions like INCLUDEPATH INCLUDEPATH HOME include As environment variables variables makefiles customarily caps constants program good form group file ease reference adjustment The variables line special variables indicate target file build respectively brings discussion dependency handling gsl_stats March ENVIRONMENTS AND MAKEFILES Dependencies The remainder file series target dependency pairs actions A target produced Here targets object files executable page makefile targets PDFs graphics The dependency source containing data produce target An object file depends C source code defining functions structures executable depends object files linked PDF document depends underlying text In cases dependency data wrote target computer generated The lines colons line target dependency descriptions single item colon target items colon files targets target depends After target line instructions building target de pendencies For simple C program instructions line typically g The makefile page shows involved target build scripts span lines An important annoyance lines describing building process indented tabs spaces If text editor replaces tabs set spaces fail Having described format makefile let process system type command line The target makefile default target begins specify target command line So looks target specification line seven powers variable expansion sees run_me depends file1 o file2 o Let assume time program built object file exists Then search rule build digresses later targets There executes command specified COMPILE lines Having created subsidiary targets returns original target runs command link object files Let modify file1 files The program starts tree targets sees needs check file1 o file2 o Checking dependencies forfile1 o sees file date dependency newer time stamp So file1 o recompiled run_me recreated The system knows file2 o need recompiled bother In larger project recompilation project seconds minutes save time typing 7The odds good trl V let insert odd character text E g trl V tab insert tab converting spaces 8What time stamp file broken copied computer different time zone mis stamped The tou h command e g tou h update time stamp file current time If want recompile file1 o tou h file1 gsl_stats March APPENDIX A Compile run Almost fail step successful compile run program The makefile gives option ing run dependency lines depends exe utable dependency line You specify targets command line simply type system assumes target file line type command run use instructions run target halts error program fails compile system stops let us changes compiles correctly goes run program It considers compilation warnings errors successful heed warnings The solution add Werror CFLAFS command line tell compiler treat warnings errors There easy way pass switches command line information frommake s command line program The makefile uses hack defining environment variable PARGS On command line exportPARGS b gnuplot persist makewill run program b flag pipe output gnuplot If find PARGS hack hackish run program command line usual Recall C syntax b b eval uated true Similarly command line command run_me run halt errors continue run_me successful Again command line compilation execution coupled shells ability repeat command line try arrow fast means recompiling executing gsl_stats March ENVIRONMENTS AND MAKEFILES QA Take sample programs wrote Chapter create direc tory Create new directory copy file Copy sample makefile online sample code Modify PROGNAME OBJECTS line correspond project If errors system find certain headers need modify INCLUDEPATH line If errors linker symbols found need change LINKFLAGS line Type verify commands prints object executable files ex pected Now makefile works machine copy use programs minimal changes probably changing PROGNAME OBJECTS Themakefile summarizes method compiling program system At head makefile place variables describe flags needed compiler linker The makefile specifies files depend files need updating updated gsl_stats March B TEXT PROCESSING If lucky data set exactly format needs read stats package Apophenia Graphviz et cetera By definition rarely lucky appendix explain mod ify text file conform input standard The book far included great deal writing new text files printf family process modifying existing files That modifying files place simply frustrating Something simple replac ing requires rewriting entire file point Thus appendix look means modifying text existing pro grams tedium modifying files It is goal narrowly defined rest text convert data file input format required stats package command file language Gnuplot Graphviz family tool SQL As rest analysis able execute procedures fully automated manner This doubt receive revised data set week corrections data writing script produces audit trail colleagues use check work The primary technique covered appendix parsing regular expres sions specify certain regularity numbers followed letters gsl_stats March TEXT PROCESSING program searches regular expression text modifies instructions A secondary theme POSIX water metaphor Data flows stream location burbling data files piping filters like sed stream editor Eventually stream data reaches final destination graphic database file holding clean filtered data Along way effectively search files As projects large desire tools find use buggy_variable program reference author documents Regular expression tools help After discussion assembles pipeline small parts ap pendix cover simple searching introduction regular expressions The final section applies tools produce scripts reformat text files format appropriate data handling systems B She shall SCRIPTS As Gnuplot SQLite s command line utility operate she shall command line typing commands writing script having she shall read script Thus try commands command line work desired cut paste script permanent record Your script reformat text files begin perl sed commands reformat data file C program process file finally Gnuplot display graph The quickest way script executable use sour e command Given list commands text file mys ript execute usingsour e mys ript QB Write short script compile execute hello_world On line script compilation command exercise page second line execute Then run script command line 1If going run script times want directly executable This place POSIX tutorial hmod mys ript hmod x mys ript trick Having script executable script current directory aka she shall mys ript Why need See discussion paths Appendix A gsl_stats March APPENDIX B She shall symbol Meaning C equivalent send output file fopen filename w fprintf append output file fopen filename fprintf read output file fopen filename r fgets redirect output program popen progname w fprintf popen progname r fgets input line script exercise reader usefgets Table B Redirection command prompt C REDIRECTION Each program standard input stream standard output stream named stdin stdout plus stream typically error messages stderr This section cover means bending redirecting streams section cover tools join filter streams text By default stdin stdout keyboard screen respectively For ex ample sed reads stdin writes stdout command sed n p simply prints screen typed If try means simply sed repeats type she shall prompt sed n p Hello Hello How How Stop imitating Stop imitating ctrl D Now shifting streams she shall symbols listed Table B A clause form filename tells system write gsl_stats March TEXT PROCESSING screen file Thus command sed n p outfile command line print screen type hit trl D exit written outfile As variant outfile append file overwriting outfile A tells system input keyboard file Thus sed n p input_file dump contents input_file screen QB Use sed n p redirection input output copy file She shall expansion POSIX compliant shells use single ticks double ticks delimit strings behave dif ferently The single tick form expand VAR value VAR environment variable treat special Conversely double tick form makes changes called program sees string Single ticks tend easier use regular expressions preponderance special characters Notably basically impossible inside double ticks need form like start string pausestring ontinue Your final redirection option pipe The pipe connects stdout program stdin Question exponents end On page code supplement find program getopt prints exponents stdout Below grep search input certain pattern ex ample grep myfile search myfile lines ending Thus redirect output getopt file input file togrep getopt powers_of_four grep powers_of_four The pipe streamlines directly linking stdout program andstdin second bypassing need file commands getopt grep This line prints filtered output getopt screen point pattern exponents eminently clear Now seen wondering Appending input sense instead form allows small gsl_stats March APPENDIX B scripts normally separate file directly command line Here sample usage gnuplot end script set term postscript set redirect_test ps plot sin x end script The end s riptmarker string EOF popular choice indi cates script end Everything line concluding string line sent Gnuplot typed directly stderr There stream stdin stdout stderr intended print error messages diagonstics screen whenstdout dumping data file There reasons redirect stream program producing errors scroll faster read In sh use use redirect stdout stderr given destination e g errors txt In bash write stderr file use For example errors txt dump compilation errors errors txt file Streams C C UNIX co evolved methods redi recting inputs outputs easy C implementation As saw pages 166ff use fprintf send data file program You noticed close similarity opening writing file opening writing pipe FILE f fopen write_to_me w FILE g popen gnuplot w fprintf f The waves roll n fprintf g set label The waves roll set yrange plot fclose f pclose g If code program named water fopen fprintf instruc tions analogous water write_to_me The popen fprintf pair analogous water gnuplot gsl_stats March TEXT PROCESSING This book spent little time subject reading data stdin look fs anf favorite C reference changing w s r s code flow data FILE program analogous towater write_to_me gnuplot water QB Modify getopt filter output grep Popen grep writing Modify printf statement fprintf statement The standard pipes defined C s standard library popened stream That fprintf stdout Data forth oming n fprintf stderr danger n write stdout andstderr expected What difference fprintf stdout data printf data There use whichever form convenient situation Finally system function simply runs argument run command line Most POSIX machines come small program named fortune prints fortune cookie type sayings Most t eat consume dozen time Listing B shows program auto mates process loop The program use printf lude stdio h C program printing child programs include stdlib h int main int system fortune system echo system sleep Listing B Print fortunes Online source fortunate There differences system command popen command First fact way rest program accesssystem s input output popen allows input output 2This program longer needs system lines summarized system fortune e ho sleep Notice apop_system convenience function allows command printf style placeholders e g apop_system p s s from_file to_file gsl_stats March APPENDIX B program But system stop program wait child program finish popen open pipe step program So command finish continue use system command run background wait data available use popen All programs standard inputs outputs default keyboard screen Both stdin stdout redirected stdin read file infile form stdout write file outfile form Use outfile overwrite outfile append Programs send output programs pipe You redirection inside C fopen effect command line s infile outfile popen open pipe B SOME TOOLS FOR SCRIPTING Depending system type thousand commands command prompt This section points especially useful purposes dealing data text format summarized Table B All olumn egrep perl POSIX standard expect scripts based standard commands portable The basic file handling functions mkdir ls p listed reference discussed If familiar encouraged refer abundance basic UNIX POSIX tutorials online print e g Peek et al Most commands ways redundant methods book use ut pull column data read data database sele t column use w count data points use sele t ount Generally command line work good pure text manipulation quick questions e g I thousands data points I expecting need write program query sort numeric analysis answer detailed questions data e g rows duplicates There join command database style joins files However hard use clean numeric codes For example try join merge data wb pop data wb gdp 4apop_query_to_double sele t ount data apop_query_to_double sele t ount sele t distin t data gsl_stats March TEXT PROCESSING Basic file handlingmkdir rmdir Make remove directory d Change new current directoryls List directory contents p mv Copy rename filerm Remove file Reading List file concatenate multiple fileshead tail List lines fileless Interactively browse file olumn Display file columns aligned Writingsort Sort ut paste Modify files columns linesnl Put line number head lineuniq Delete duplicate lines Informationman Manual pages commandsw Count words lines charactersdiff Find differences files Regular expressionssed Stream editor add delete lines search replacegrep egrep Search file stringperl All search replace Table B Some commands useful handling text files gsl_stats March APPENDIX Bman What hmod Ask man hmod What e ho com mand Listing B Try man e ho The manual pages list basic function program switches They good way learn use POSIX system definitive reference command line details The simplest thing text file print Instead opening file text editor simply type atfile_to_read command prompt quick look The use named concatenation Given files b b writes immediately b These paging programs useful viewing longer files The POSIX standard named files screenful text displays page text banner screen readingmore The successor provides features paging These programs read stdin try combi nations like sort data_file head tail head myfilewill print lines myfile tail myfile print These good getting quick idea large file Also tail f myfile follow myfile showing lines updating new lines added This provide reassurance slow running program working sed The stream editor discussed detail suffices know replicate head tail The command sed n 3p a_file print lines a_file The line indicated sed n p a_file print second lines file sort This self descriptive command The data wb pop file code supplement sorted population sort data wb pop output file alphabetical order Sort useful switches sort ignoring blanks second nth column data reverse order et cetera See mansort details gsl_stats March TEXT PROCESSING QB Sorting data wb pop like sorts header lines countries Write command calls sed delete headers pipes output sort ut paste Almost commands operate rows ut paste operate columns Specify delimiter d flag field s f flag column column zero To list countries World Bank data try ut d f data wb pop like population numbers use ut d f data wb pop paste puts second input right input vertical version QB Use command string exercise sort population GDP data new files ut second file include column numbers Then use paste sorted_pop sorted_gdp ombined_data produce file types data Verify lines data aligned correctly e Albania column list GDP Algeria w This program short word count count words lines characters The default print w w w l w counts When writing text document use word count usual w wreport tex If data file number data points probably w ldata header need subtract lines This program especially useful tandem grep discussed length How lines data wb pop file missing data WB indicates grep data wb pop output lines string appears w l count lines input piping grep data wb pop w l count lines s nl This command puts line numbers beginning line Remember SQLite puts rowid row table invisible appears explicitly sele ted But want quickly counter column data set way gsl_stats March APPENDIX B olumn This command POSIX standard common GNU BSD based systems Text data easy read computer human rarely Say data file tab delimited field ex actly tab columns Name tab Age Rumpelstiltskin tab Dopey tab This read spreadsheet perfectly displayed screen output messy Rumpelstiltskin s tab aligned Dopey s The olumn command addresses exactly problem splitting columns inserting spaces screen human consumption For example olumn s t data wb pop format data wb pop file columns splitting delimiters The column include header lines know sed pipe headerless data olumn diff diff f1 f2 print lines differ files This quickly overwhelming files significantly different long day modifying files versions file marginally different diff help sort By end chap ter case opportunities use diff appear EMACS vim IDEs modes run diff simultaneously display differences versions file To use running program want clean code little Save original backup run backup pro gram writing output out1 Then clean code fit run cleaned code writing out2 If damage diff out1 out2 return uniq This program removes duplicate lines file uniq dirty lean write version input file successive duplicate lines deleted For example given input b b c c gsl_stats March TEXT PROCESSING output omit line leave remainder file intact If order matter sort your_file uniq ensure identical lines sequential uniq easily able clean Alternatively read file database use sele t distin t your_table B REGULAR EXPRESSIONS The remainder appendix cover pro grams parse regular expressions Regular expressions comprise language provides compact means summarizing text pattern Knowledge regexes comes handy wide variety contexts including standard viewers like fair number online search tools text editors like vi EMACS Entire books written seemingly small subject comprehensive Friedl This chapter use grep sed Perl trivial use regular expression parsing substitution command line Other systems C Python require regex compilation step usage extra step script program makes line commands difficult Standard syntax lack thereof Unfortunately variants regex syntax programs process author program felt need sort tweak standard Broadly basic regular expression BRE syntax derives fromed text editor output available computers line printer Most POSIX utilities sed grep awk use basic regex syntax There barely modified variant known extended regular expression ERE syntax available egrep switch utilities New features like special meaning characters like evolved host BRE programs place BRE syntax characters match actual text special operators preceded backslash e g EREs characters indicate special operations start special operator indicates plain plus sign characters The scripting language Perl introduced somewhat expanded syntax regular expressions significantly extensions modifications post Perl programs adopted Perl compatible regular expressions PCREs This chapter covers Perl GNU grep sed exist writing encouraged check manual pages online references subtle shifts backslashes regular expression systems gsl_stats March APPENDIX B THE BASIC SEARCH The simplest search literal string text Say C program searching use vari able var You use command line program grep search var files The syntax simple grep var search files current directory ending string var print result command line This great deal In olden times programmers phone book simple text file number line When needed Jones s number run grep Jones my_phone_book find If need know Venezuela s GDP grep find time grepVenezuela data wb gdp QB Use grep search c files uses printf The C n option e g grep C outputs n context lines match repeat search context lines eachprintf Bracket expressions Now start add special characters regex lan guage An expression square brackets represents single character single character number choices For example expression fs match f s Thus expression fs printf match fprintf sprintf printf grep fs printf find uses C files More bits bracket ex pression syntax You use ranges A Z searches English capital letters A Za z searches English capital lowercase letters matches digit You match character given list putting item bracketed list Thus fs matches single character f s There common sets characters named POSIX utili ties For example digit search numbers Notice digit character class digits digit bracket expres sion matching single digit See Table B partial list These character sets locale dependent For example U A Z common letter language computer speaks element alpha 5The comes old ed command ll meet ed later global regular expression printing g p Many acronym general regular expression parser 6If searching character don t bracket Conversely need literal dash indicate range To find lines carats dashes try grep myfile gsl_stats March TEXT PROCESSING POSIX Perl English digit d Numeric characters alpha Alphabetic characters upper Uppercase alphabetic characters lower Lowercase alphabetic characters alnum w Alphanumeric characters blank Space tab characters spa e s Any white space including space tab t newline n pun t Punctuation characters print Printable characters including white space graph Printable characters excluding white space D W S Not numeric chars alphanumeric chars white space Table B Some useful regular expression character sets chars main regex meaning meaning brackets Bracket expression named group Beginning line don t match following chars Any character plain period A plain dash range e g Table B Some characters different meanings inside outside brackets All combined expression For example eE digit match digit minus plus letter e Or named variableopt grep opt spits comment lines options try grepopt Brackets bit disorienting syntax visually offset group elements bracket matter long bracketed expression result represents exactly character text Also demonstrated Table B syntax inside brackets syntax outside brackets gsl_stats March APPENDIX B Regular expressions vehemently case sensitive grep perl sear h_me turn instances Perl The option use command line switch search case grep perl sear h_me The second option oc casionally useful control use bracket grep Pp erl sear h_me Alternatives Recall Apophenia includes printing functions asapop_data_print apop_matrix_print Say like examples Alternatives form A B written ba sic regex notation A B Notice analogizes nicely C s A B form This flavors regular expression diverge backslashes start creep Plain grep uses BREs needs backslashes parens pipe A B When run grep E grep uses EREs Most systems egrep command equivalent grep E Also recall difference single ticks double ticks com mand line discussed box page single tick form treat backslashes special manner example find single literal period instead All said commands search apop_data_print apop_matrix_print grep apop_ data matrix _print c grep apop_ data matrix _print c grep E apop_ data matrix _print c egrep apop_ data matrix _print c A special symbols You won t care certain segment A dot matches character star repeat prior atom Therefore grep apop _print c find line apop followed eventually followed by_print Once symbols mean different things different contexts In regex dot represents character star represents repetition 7Officially egrep obsolete longer POSIX standard gsl_stats March TEXT PROCESSING command line wildcard matching referred globbing dot dot star represents character The caret dollar sign indicate beginning end line respectively grep int match lines int right beginning line grep match open braces end line A single space tab characters meaning dot match In grep atom W match single space tab atom w match white space Since frequently unknown bunch tabs spaces head code files better way search int declarations grep W int c The quickest way search command line grep The syntax grep regular expression files_to_sear h Without special characters grep simply searches line matches given text To find instance fixme file grep fixme filename A bracketed set indicates single character The set include single characters Pp erl matches bothPerl perl maybe want case insensitive search grep The set include ranges A Za z match standard English letter The set exclude elements P S find character P Q R S Express alternatives form A B Different systems different rules special charac ter The ERE PCRE form alternation egrep perl A B BRE form grep A B A single dot represents character including single space tab grep understands W mean white space character w mean non white space character 8It difficult search lines Perl m option m A reliable universal option simply remove newlines turning input stream single long line The easiest way form like tr n infile grep tr translates elements set case newline elements second pipe gsl_stats March APPENDIX B REPLACING grep wonderful searching purely read program If want change find need use com plex program perl sed stream editor sed easily handle data filtering needs perl adds useful features traditional regular expression parsing syntax If find regexes frequently want know handled Perl compatible regex scripting system like Python Ruby Perl These programs complement C nicely provide built facilities string handling decid edly C s strongest suit Perl sed syntax sed certainly installed system POSIX standard perl certainly installed modern system utilities use If installed install package manager Both programs generally expect use command file command directly command line e command For example perl e print Hello world n run Perl command print Hello world n sed operates input file start emulating grep sed n e regex p file_to_search Sed likes regexes slashes p short print command regex p means print line matches regex Sed s default print input line print command repeat lines match regex leads massive redundancy The n command sets sed print asked matching lines print Generally sed command parts lines work single letter indicating command optional information command Above specification lines work regular expression meaning command p operate lines match regex 9As saw sed examples section redirection e flag optional form hurts gsl_stats March TEXT PROCESSING You replace regex fixed set lines mean lines In context means line file p print lines You print given set lines range so1 p print lines Sed commandsp Print s Substitute d Delete Insert Append The box presents single letter sed commands dis cussed sed fewmore relevant work The command following single letter command vary depending command As examples blank p command Search replace syntax The basic syntax search replace s repla e withme g The slashes distinguish command s text search repla e text replacement modi fiers g box If file scan use programs filters perl p e s replace g file_to_modify modified_file sed e s replace g file_to_modify modified_file Why g Back regexes real time editing files sense fix instance expression line Thus default modify instance specify replacing second instance match s instance s instances s g global This valid syntax occasionally useful filtering data file want g option Perl s p switch opposite sed s n switch As opposed sed Perl s default print pattern matches means p switch pass line match regex With p non matching lines appear lines match appropri ate substitutions want In case sed right thing default need n switch 10The bash she shall command history purposes result absolutely perverse behavior double ticks single ticks essential To print lines beginning example use sed n p infile 11A convenient trick merits foontote use delimiter slash separate parts search replace For example s repla e g works equally This primarily useful search replace text slashes slash delimiter means need escape slashes text s usr lo al usr g different delimiter means slashes longer special s usr lo al usr g work fine The norm slashes text use gsl_stats March APPENDIX B Since probably want search replace line file preceding single letter command s But want replacerepla e lines beginning specify sed command s repla e g In case sed taking regular expres sions searches vertically file lines match When finds line searches horizontally line repla e QB Say database manager accept pipe delimited text wants commas instead Write script replace pipe thedata lassroom file comma Then modify script replace pipes data leaving pipe comments unchanged Another alternative replace place switch perl p bak e s replace g files_to_modify sed bak e s replace g files_to_modify With option Perl sed write substitutions screen file directly original file The bak extension tells Perl andsed backup modification bak extension You use extension nice e g produce backup file named test named test If specify suffix backup copy edits It decide sufficiently confident backups QB Create file named about_me line reading I teapot Use perl sed transform teapot kettle toaster Verify change diff You welcome include multiple commands line way In perl separate semicolon C In sed perl simply specify additional e commands executed order use pipe perl pi bak e s replace g s replace g files_to_modify perl pi bak e s replace g e s replace g files_to_modify sed bak e s replace g e s replace g files_to_modify 12The switch standard sed works GNU sed probably Bear mind concerned script s portability 13If editing file place difficult sed By writing new file switching original finished So filtering 1GB file place backup 1GB free space hard drive disk error gsl_stats March TEXT PROCESSING perl p e s replace g modify_me perl p e s replace g modified_version sed s replace g modify_me sed s replace g modified_version Replacing modifications Parentheses delimit conditional seg ments store sections later use For example file reads There monkeys play copyeditor feels sentence written imprecise You forgot number monkeys tempted use s monkeys Callimi ogoeldii g lose number monkeys However segment search term parentheses use sed Perl refer replacement Thus correct command lines replacing unknown number monkeys sed e s monkeys Callimico goeldii g monkey_file sed r e s monkeys Callimico goeldii g monkey_file perl p e s monkeys Callimico goeldii g monkey_file The 1will replaced found parentheses sed normally uses BREs GNU version sed uses EREs given r flag If multiple substitutions need higher numbers Say like formalize sentence monkeys fightingthe lab owners Do multiple parens sed e s monkeys fighting lab owners Callimico goeldii fighting Homo sapiens g monkey_file Repetition Say values data suspect replaced NaN The search s NaN g won t work replaced NaN NaN count separate matches Or values suspect The search s NaN g won t work replaced NaN We need means speci fying repititions precisely Here symbols match repetitions element optional gsl_stats March APPENDIX B atom appears zero times atom appears times GNU grep sed atom appears zero times GNU grep sed To replace integers file perl pi e s 1NaN g search_me sed s 1NaN g search_me sed r s 1NaN g search_me This mess precise mess legible The match non numeric character ginning line The second matches digits The match non numeric character end line Thus precisely communicated want integer Since search string sets parens output string refer It repeats verbatim replaces second NaN desired Structured regexes Chapter presented number techniques writing code legible easy debug Break large chunks code subfunctions Debug subfunctions incrementally Write tests verify components act All applies regular expressions You break expresssions variables she shall substitute expression For example illegible regexes somewhat clarified named subex pressions This version bash sh 14You replace variables she shall form like digits This useful vari able merge following text search exponential numbers certain form digits e digits gsl_stats March TEXT PROCESSING notdigit digits replacecmd s notdigit digits notdigit 1NaN g sed replacecmd search_me The e ho command useful testing purposes It dumps text stdout makes line tests relatively easy For example echo sed replacecmd echo sed replacecmd echo sed replacecmd QB Modify search include optional decimal arbitrary length Write test file test modification works correctly B ADDING AND DELETING Recall format sed command print line page find_me p This con sists location command With slashes location lines match find_me explicitly specify line number range lines 7p print line seven p print line input file or1 p print entire file You use line addressing schemes d delete line For example sed d infile print infile line QB Use d produce version data lassroom comments moved You use insert given line s append given line A examples Add pause Gnuplot data block sed e e pause pauselength plotfile Put text plot head file sed e plot plotfile Pretend missing data exist sed e NaN d plotfile By way want sed print Hello world inserting line ignoring rest file sed n e Hello world any_random_file gsl_stats March APPENDIX B Perl things easily inside Perl script inserting deleting lines command line pleasant sed QB Refer exercise page read text file produced Graphviz readable output file That exercise read text apop_ data set wrote output sensible pulling classes database But given text file data lassroom code supplement modify directly Graphviz s format Write sed script Delete header line Replace pipe Replace number n noden Add header page Add end brace line Pipe output neato check processing produced correctly neato readable file right graph QB Turn data lassroom file SQL script create table Delete header line Add header reate table lass ego nominee Bonus points write search replace converts existing header form Replace pair numbers n m SQL command insertinto lass n m For added speed begin ommit wrapper entire file Pipe output sqlite3 verify edits correctly created populated table A reader recommends following inserting line perl n e print print pause pauselength e For adding line file perl p e print plot n For deleting line perl n e print NaN gsl_stats March TEXT PROCESSING Your best bet command line search replace perl sed Syntax perl pi s repla e g data_file orsed s repla e g data_file If set parens search portion refer Perl s replace portion sed s replace The let repeat previous character bracketed expression zero zero times respec tively B MORE EXAMPLES Now syntax regexes applica tions come fast easy Quoting unquoting Although dot match character conve nient want Say looking expressions quotes It translates regular expression meaning character repeated zero times enclosed quotes But consider line bit se ondbit You meant matches instead bit se ond bit extreme quotes What meant want quotes That use blank like acceptable Say program produced data quotes reading program like having quotes Then fix problem perl pi e s g data_file Getting rid commas Some data sources improve human readability separat ing data commas example data wb gdp reports USA s GDP millions dollars Unfortunately program reads commas field delimiters human readability conve nience ruins computer readability But commas text entirely valid want remove commas numbers We searching number comma number pattern replacing numbers Here sed command line versions process gsl_stats March APPENDIX B sed s g fixme txt sed r s g fixme txt Suspicious non printing characters Some sources like odd non printing characters data Since don t print hard spot produce odd effects like columns tables names like pop OE tion It hard problem diagnose easy problem fix Since print matches printing characters print matches non printing characters following command replaces characters sed s print g fixme txt Blocks delimiters Some programs output multiple spaces approximate tabs programs expect input whitespace delimiter read multiple spaces long series blank fields But easy merge whitespace finding instance blanks e spaces tabs row replacing single space sed s blank g datafile You use strategy reducing block delimiters appropriate ass Alternatively sequence repeated delimiters need merging mark missing data data file pipe delimited data producer s way saying NaN If input program trouble need insert NaN s This easy s NaN g But catch need run substitution twice regular expression parsers lap matches We expect input invoke replacements form NaN NaN regular expression parser match pipes leav ing single later matches end NaN output By running replacement twice guarantee pipe finds pair sed datafile e s NaN g e s NaN g 16The solution root problem avoid whitespace delimiter I recommend pipe delimiter virtually human text data gsl_stats March TEXT PROCESSING Text database The apop_text_to_db command line program corre sponding C function input stdin Thus end streams directly dump data SQLite database For POSIX programs typically file input traditional way indicate sending data stdin instead text file use file For example work exercise convert data SQL commands way apop_text_to_ db sed d data classroom apop_text_to_db friends classes dbgnuplot Database plot Apophenia includes command line program apop_plot_ query takes query outputs Gnuplottable file It provides extra power H option bin data histogram plotting use functions var SQLite support But instances unnecessary SQLite read query file command line pipe Gnu plot read formatted file pipe As saw Chapter turning column numbers matrix Gnuplottable file simply requires puttingplot data If query file queryfile se quence sqlite3 separator data db queryfile sed set key nplot gnuplot persist The separator clause necessary Gnuplot like pipes delimiter Of course option available sqlite3 add e s sed command QB Write single command line plot yearly index surface temper ature anomalies year temp columns temp table thedata limate db database 17Even unnecessary program knows read lines matching comments But example little boring apop_text_to_db data lassroom friends lasses db gsl_stats March APPENDIX B UNIX versus Windows end line If file long line breaks funny characters interspersed L s place found crossfire long standing war lines end In style manual typewriters starting new line actually consists operations moving hori zontally beginning line carriage return CR moving vertically line line feed LF The ASCII character CR ctrl M appears screen single character M ASCII character LF L The designers AT T UNIX decided sufficient end line LF L designers Microsoft DOS decided line end CR LF pair ML When open DOS file POSIX system recognize L end line consider M garbage leaves file When open POSIX file Windows t find ML pairs lines end As frustration programs auto correct line endings meaning file look OK text editor fall apart stats package Recall page r CR n LF Going DOS UNIX means removing single CR line going UNIX DOS means adding CR line simple sed commands Convert DOS file UNIX sed s r dos_file Convert UNIX file DOS sed s r unix_file Some systems dos2unix unix2dos commands missing commands basically run single line sed 18Typing M produce CR M single special character confusingly represented screen characters In shells ctrl V means insert character exactly I type sequence ctrl V ctrl M insert single CR character appears screen M ctrl V ctrl L similarly produce LF 19Perhaps ask package manager dosutils package gsl_stats March C GLOSSARY See list notation symbols page Acronyms ANSI American National Standards Institute ASCII American Standard Code Information Interchange ANOVA analysis variance p BLAS Basic Linear Algebra System BLUE best linear unbiased estimator p BRE basic regular expression p CDF cumulative density function p CMF cumulativemass function p CLT central limit theorem p df degree s freedom ERE extended regular expression p erf error function p GCC GNU Compiler Collection p GDP gross domestic product GLS generalized squares p GNU GNU s Not UNIX GSL GNU Scientific Library p GUI graphical user interface IDE integrated development environ ment IEC International Electrotechnical Commission IEEE Institute Electrical Elec tronics Engineers IIA independence irrelevant alterna tives p gsl_stats March APPENDIX C iid independent identically dis tributed p ISO International Standards Organiza tion IV instrumental variable p LR likelihood ratio p MAR missing random p MCAR missing completely random p MCMC markov chain Monte Carlo p ML maximum likelihood MLE maximum likelihood estima tion estimate p MNAR missing random p MSE mean squared error p OLS ordinary squares p PCA principal component analysis p PCRE Perl compatible regular expres sion p PDF probability density function p PDF portable document format PRNG pseudorandom number genera tor p PMF probability mass function p RNG random number generator p SSE sum squared errors p SSR sum squared residuals p SST total sum squares p SQL Structured Query Language p SVD singular value decomposition p TLA letter acronym UNIX acronym main glos sary WLS weighted squares p Terms affine projection A linear projection expressed matrix T x transformed xT But projection maps An affine projection adds constant transforming x xT k transforms nonzero value p ANOVA The analysis variance body statistical methods analyzing measurements assumed structure yi x1i1 x2i2 xpip ei n coefficients xji integers usually Scheffe p apophenia The human tendency patterns static p array A sequence elements type An array text characters called string p gsl_stats March GLOSSARY arguments The inputs function p assertion A statement program test errors The statement evaluate true program halts message failed assertion p bandwidth Most distribution smoothing techniques including kernel den sity estimates gather data points fixed range point evalu ated The span data points gathered bandwidth For cases like Normal kernel density estimator tails span term indicate bandwidth gets larger far reaching data effect p Bernoulli draw A single draw fixed process produces probability p zero probability p p bias The distance expected value estimator s true value E See unbiased statistic p binary tree A set structures similar linked list structure con sists data pointers left structure right structure You typically head tree arbitrary element quickly data organized linked list p BLUE The Estimator Linear function Unbiased Best sense var var linear unbiased estimators p bootstrap Repeated sampling replacement population produces sequence artificial samples produce sequence iid statistics The Central Limit Theorem applies find expected value variance statistic entire data set set iid draws statistic The implies samples data learn data bit like pulling oneself bootstraps See jackknife bootstrap principle p bootstrap principle The claim samples data sample properties matching samples population p address When calling function sending function copy input variable s location opposed value p value When calling function sending function copy input variable s value p gsl_stats March APPENDIX C Cauchy Schwarz inequality Given correlation coefficient vectors x y xy holds 2xy p Central Limit Theorem Given set means mean set n iid draws data set set means approach Normal distribution n p central moments Given data vector x mean x kth central moment f n x x f x f x k In continuous case x distribution p x kth central moment f x f x f x k p x dx In cases central moment zero noncentral moment The second known variance skew fourth kurtosis p closed form expression An expression x2 y3 written ing line notation manipulated usual algebraic rules This contrast function algorithm empirical distribution described long code listing histogram data set compiler A non interactive program e g g translate code human readable source file computer readable object file The compiler closely intertwined preprocessor linker point preprocessor compiler linker amalgam usually called compiler Compare inter preter p conjugate distributions A prior likelihood pair prior updated likelihood posterior form prior updated parameters For example given Beta distribution prior Binomial likelihood function posterior Beta distribution Unrelated conjugate gradient methods p consistent estimator An estimator x consistent constant c limn P x c o o That sample size grows value x converges probability single value c p consistent test A test consistent power n p contrast A hypothesis linear combination coefficients like p correlation coefficient Given square roots covariance variances xy x y correlation coefficient xy xyxx p gsl_stats March GLOSSARY covariance For data vectors x y 2xy 1n xi x yi y p Cramer Rao lower bound The elements covariancematrix estimate parameter vector equal greater limit constant given PDF Equation page For MLE CRLB reduces nI I information matrix p crosstab A dimensional table row represents values vari able y column represents values variable x row column entry provides summary statistic subset data y given row value x given column value See page exam ple p cumulative density function The integral PDF It is value given point indicates likelihood draw distribution equal given point Since PDF non negative CDF monotoni cally nondecreasing At CDF zero CDF p cumulative mass function The integral PMF That CDF dis tribution discrete values p data mining Formerly synonym data snooping current usage meth ods categorizing observations small set typically nested bins generating trees separating hyperplanes data snooping Before formally testing hypothesis trying series preliminary tests select form final test Such behavior taint inferential statistics statistic parameter test different distribution statistic favorable parameter tests p debugger A standalone program runs program allowing user halt program point view stack frames query program value variable point program s execution p declaration A line code indicates type variable function p degrees freedom The number dimensions data set varies If n data points independent df n restrictions reduce data s dimensionality df n You think df number independent pieces information p dependency A statement makefile indicating file depends object file depends source file When depended file changes dependent file need produced p gsl_stats March APPENDIX C descriptive statistics The half probability statistics aimed filtering useful patterns world overwhelming information The half inferential statistics p dummy variable A variable takes discrete values usually zero indicate category observation lies p efficiency A parameter estimate comes close possible achieving Cramer Rao lower bound small variance possible dubbed efficient estimate p error function The CDF Normal distribution p environment variable A set variables maintained system passed child programs They typically set she shall s export setenv command p expected value The noncentral moment aka mean average p frame A collection function variables scope function p GCC The GNU Compiler Collection reads source files variety languages produces object files accordingly This book uses ability read compile C code p Generalized Least Squares The Ordinary Least Squaresmodel assumes covariance matrix observations 2I e scalar times identity matrix A GLS model model conforms OLS assumptions allows different form p globbing The limited regular expression parsing provided she shall ex panding list file names ending Uses entirely different syntax standard regular expression parsers p graph A set nodes connected edges The edges directional forming directional graph Not confused plot p grid search Divide space inputs function grid write value function point grid Such exhaustive walk space picture function graphing packages find optimum function However resort purposes search random draw methods Chapters efficient precise p gsl_stats March GLOSSARY hat matrix Please projection matrix p header file A C file consisting entirely declarations type definitions By lude ing multiple C files variables functions types declared header file defined file p Hessian The matrix second derivatives function evaluated given point Given log likelihood function LL negation Hessian informa tion matrix p heteroskedasticity When errors associated different observations different variances observations consumption rates poor wealthy This violates assumption OLS produce inef ficient estimates weighted squares solves problem p identically distributed A situation process produce elements data set considered identical For example data points drawn Poisson distribution individuals randomly sampled population p identity matrix A square matrix non diagonal element zero diagonal element It is size typically determined context typically notated I There infinite number identity matrices matrix matrix matrix custom refer identity matrix iff If The following statements equivalent A B A iff B A B A defined B B defined A iid Independent identically distributed These conditions Cen tral Limit Theorem See independent draws identically distributed p importance sampling A means making draws easy draw dis tribution draws difficult distribution p independence irrelevant alternatives The ratio likelihood choice A ing selected likelihood choice B selected depend options available adding deleting choices C D E change ratio p independent draws Two events x1 x2 draws data set independent P x1 x2 probability x1 x2 equal P x1 P x2 p gsl_stats March APPENDIX C inferential statistics The half probability statistics aimed fighting apophenia The half descriptive statistics p information matrix The negation derivative Score Put differently given log likelihood function LL information matrix negation Hessian matrix See Cramer Rao lower bound p instrumental variable If variable measured error OLS pa rameter estimate based variable biased An instrumental variable replacement variable highly correlated measured error variable A variant OLS instrumental variable produce unbiased parameter estimates p interaction An element model contends x1 x2 causes outcome combination x1 x2 simultaneously x1 x2 x1 x2 This typically represented OLS regressions simply multiplying form new variable x3 x1 x2 p interpreter A program translate code human readable language computer s object code binary format The user inputs individual commands typically interpreter produces executes appropriate machine code line Gnuplot sqlite3 command line program interpreters Compare compiler jackknife A relative bootstrap A subsample formed removing element estimating j1 subsample formed replacing element removing second estimating j2 et cetera The multitude jn s formed estimate variance overall parameter estimate p join Combining database tables form typically including columns second There usually column join e g table names heights second table names weights joined matching names tables p kernel density estimate A method smoothing data set centering standard PDF like Normal point Summing sub PDFs produces smooth overall PDF p kurtosis The fourth central moment p lexicographic order Words dictionary sorted letter completely ignoring Then words beginning A sorted gsl_stats March GLOSSARY second letter Those beginning letters aandblom aard wolf aasvogel sorted letter Thus lexicographic ordering sorts characteristic breaks ties second characteristic breaks ties p library A set functions variables perform tasks related specific task numeric methods linked list handling The library basically object file slightly different format typically kept library path p likelihood function The likelihood P X X probability d parameters given observed dataX This contrast probability data set given fixed parameters P X See page discussion p likelihood ratio test A test based statistic form P1 P2 This times logged LL1 LL2 Many tests surface fit form shown equivalent LR test p linked list A set structures structure holds data pointer structure list One traverse list following pointer head element element following element s pointer element et cetera p linker A program takes set libraries object files outputs executable program p Manhattanmetric Given distances dimensions dx x1 x2 dy y1 y2 standard Euclidian metric combines find straight line distance d2x d y The Manhattan metric simply adds distance dimension dx dy This distance travel going East West streets North South streets p A program keeps track dependencies runs commands specified makefile needed files date dependencies change Usually produce executables source files change p macro A rule transform strings text fixed pattern For example preprocessor replace occurrence GSL_MIN b b b p metadata Data data For example pointer data location base data statistic data summarizing transforming base data p gsl_stats March APPENDIX C mean squared error Given estimate named MSE E This shown equivalent var bias2 p memory leak If lose address space allocated space remains reserved impossible use Thus sys tem s usable memory effectively smaller p missing random Data variable MAR incidence missing data points unrelated existing data variable given variables Generally means external because caused value causes values missing p missing completely random Data variable MCAR cor relation incidence missing data data set That because missingness entirely external haphazard p missing random Data variable MNAR correlation incidence missing data missing data s value That missingness caused data s value p Monte Carlo method Generating information distribution pa rameter estimates repeatedly making random draws distribution p multicollinearity Given data set X consisting columns x1 x2 columns xi xj highly correlated determinant X X near zero value inverse X X unstable As result OLS family estimates reliable p noncentral moment Given data vector x mean x kth noncentral mo ment n x x x k In continuous case x distribution p x kth noncentral moment f x f x kp x dx The noncentral moment anybody cares aka mean p non ignorable missingness See missing random p non parametric A test model non parametric rely claim statistics parameters question textbook distribution t Nor mal Bernoulli et cetera However non parametric models parameters tune non parametric tests based statistic characteristics determined null pointer A special pointer defined point p gsl_stats March GLOSSARY object A structure typically implemented stru t plus supporting func tions facilitate use structure gsl_ve tor plus gsl_ ve tor_add gsl_ve tor_ddot functions object file A computer readable file listing variables functions types defined file Object files executables linking Bears relation objects object oriented programming p order statistic The value given position sorted data largest number set second largest number median smallest number et cetera Ordinary Least Squares A model fully specified page contends dependent variable linear combination number independent variables plus error term overflow error When value variable large type unpredictable things occur For example systems MAX_INT MAX_INT The IEEE standard specifies float double variable overflows set special pattern indicating infinity See underflow error p path A list directories computer search files Most shells PATH environment variable search executable programs Similarly preprocessor searches header files e g lude stdlib h directories INCLUDEPATH environment variable extended I flag compiler command line The linker searches libraries include libpath extensions specified L compiler flag p pipe A connection directly redirects output program input In she shall pipe formed putting programs C formed popen function p pivot table See crosstab p plot A graphic axes function values marked relative axes Not confused graph p pointer A variable holding location piece data p POSIX The Portable Operating System Interface standard By mid 1980s multitude variants theUNIX operating system appeared Institute Elec trical Electronics Engineers convened panel write standard pro grams written flavor UNIX easily ported flavor Santa Cruz Operation s UNIX International Business Machines AIX Hewlett gsl_stats March APPENDIX C Packard s HP UX Linux Sun s Solaris members Microsoft s Windows family comply standard power The likelihood rejecting false null That significant effect odds test detect This minus likelihood Type II error p prime numbers Prime numbers left taken pat terns away Haddon p p principal component analysis A projection data X basis space con sisting n eigenvalues X X number desirable properties p probability density function The total area PDF given range equal probability draw distribution fall range The PDF nonnegative E g familiar bell curve Normal distribution Compare cumulative density function p probability mass function The distribution probabilities given discrete value drawn I e PDF distribution discrete values p projection matrix XP X X X 1X XPv equals projection v column space X p profiler A program executes programs determines time spent program s functions It find bottlenecks slow running program p pseudorandom number generator A function produces deterministic se quence numbers pattern Initializing PRNG different seed produces different streams numbers p query Any command database Typically command uses sele t keyword request data database query non question command command create new table drop index et cetera p random number generator See pseudorandom number generator p regular expressions A string describe patterns text num bers followed letter p gsl_stats March GLOSSARY scope The section code able refer variable For variables declared outside function scope file declaration vari ables declared inside block scope declaration inside block p score Given log likelihood function ln P score vector deriva tives S ln P p seed The value pseudorandom number generator initialized p segfault An affectionate abbreviation segmentation fault p segmentation fault An error program attempts access computer s memory allocated program If reading unauthorized memory security hole writing unauthorized memory destroy data create system instability Therefore system catches segfaults halts program immediately occur p she shall A program primary purpose facilitate running programs When log text driven systems immediately she shall s input prompt Most shells include facilities setting variables writing scripts p singular value decomposition Given m n data matrix X typically m n find n eigenvectors associated n largest eigenval ues This step principal component analysis SVD currently practiced includes number techniques transform eigenvectors appropriate p skew The central moment indication distribution leans left right mean p source code The human readable version program It converted object code computer execute stack Each function runs frame When program starts begins establishing main frame main calls function function s frame thought laid main frame Similarly subsequent functions pending frames pile form stack frames When stack program terminates p 20This assuming X X rank gsl_stats March APPENDIX C standard deviation The square root variance variable notated If variable Normally distributed usually compare point s distance mean For distributions Normal bell shaped limited descriptive utility See standard error variance p standard error An abbreviation standard deviation error p statistic A function takes data input mean x vari ance error term regression X y OLS parameter X X 1X y p string An array characters Because string array handled pointer type operations functions print string like plain text represents p structure A set variables intended collectively represent object person comprising e g height weight bird compris ing e g type pointers offspring p Structured Query Language A standard language writing database queries p switches As physical machinery switches options affect program s operation They usually set command line usually marked dash like x p trimean quartile times median quartile Tukey p p threading On modern computers processor s execute multiple chains commands For example data independent events simultaneously processed processors In case single thread program instructions split multiple threads gathered program completes p type The class data variable intended represent integer character structure amalgamation subtypes p type casting Converting variable type p Type I error Rejecting null true p Type II error Accepting null false See power p gsl_stats March GLOSSARY unbiased statistic The expected value statistic equals true population value E p unbiased estimator Let test s Type I error let Type II error A test unbiased values parameter I e likely accept null false true p underflow error Occurs value variable smaller smallest number system represent For example current system finite precision arithmetic simply zero See overflow error p UNIX An operating system developed Bell Labs Many UNIX like op erating system plural Unices UNIX properly refers code written Bell Labs evolved code owned Santa Cruz Operation Others correctly called POSIX compliant The stand pun predecessor operating system Multics variance The second central moment usually notated p Weighted Least Squares A type GLS method different observations given different weights The weights reason producing representative survey sample method heteroskedastic data p gsl_stats March gsl_stats March BIBLIOGRAPHY Abelson Harold Sussman Gerald Jay Sussman Julie Structure Interpretation Computer Programs MIT Press Albee Edward The American Dream Zoo Story Signet Allison Paul D Missing Data Quantitative Applications Social Sciences Sage Publications Amemiya Takeshi Qualitative Response Models A Survey Journal Economic Literature Amemiya Takeshi Introduction Statistics Econometrics Harvard University Press Avriel Mordecai Nonlinear Programming Analysis Methods Dover Press Axtell Robert Firm Sizes Facts Formulae Fables Fantasies Center Social Economic Dynamics Working Papers Feb Barron Andrew R Sheu Chyong Hwa Approximation Density Func tions Sequences Exponential Families The Annals Statistics Baum AE Akula N Cabanero M Cardona I Corona W Klemens B Schulze TG Cichon S Rietschel M Nathen MM Georgi A Schumacher J Schwarz M Jamra R Abou Hofels S Propping P Satagopan J Consortium NIMHGe netics Initiative Bipolar Disorder Detera Wadleigh SD Hardy J McMahon gsl_stats March BIBLIOGRAPHY FJ A genome wide association study implicates diacylglycerol kinase eta DGKH genes etiology bipolar disorder Molecular Psychiatry Benford Frank The Law Anomalous Numbers Proceedings American Philosophical Society Bowman K O Shenton L R Omnibus Test Contours Departures Normality Based b1 b2 Biometrika Casella George Berger Roger L Statistical Inference Duxbury Press Chamberlin Thomas Chrowder TheMethod Multiple Working Hypothe ses Science Cheng Simon Long J Scott Testing IIA Multinomial Logit Model Sociological Methods Research Chung J H Fraser D A S Randomization Tests Multivariate Two Sample Problem Journal American Statistical Association Chwe Michael Suk Young Rational Ritual Culture Coordination Common Knowledge Princeton University Press Cleveland William S McGill Robert Graphical Perception Graphi cal Methods Analyzing Scientific Data Science Codd Edgar F A Relational Model Data Large Shared Data Banks Communications ACM Conover W J Practical Nonparametric Statistics 2nd edn Wiley Cook R Dennis Detection Influential Observations Linear Regression Technometrics Cox D R Further Results Tests Separate Families Hypotheses Journal Royal Statistical Society Series B Methodological Cropper Maureen L Deck Leland Kishor Nalin McConnell Kenneth E Valuing Product Attributes Using Single Market Data A Comparison Hedonic Discrete Choice Approaches Review Economics Statistics Dempster A P Laird N M Rubin D B Maximum Likelihood Incomplete Data EM Algorithm Journal Royal Statistical Society Series B Methodological gsl_stats March BIBLIOGRAPHY Efron Bradley Hinkley David V Assessing Accuracy Max imum Likelihood Estimator Observed Versus Expected Fisher Information Biometrika Efron Bradley Tibshirani Robert J An Introduction Bootstrap Monographs Statistics Probability Chapman Hall Eliason Scott R Maximum Likelihood Estimation Logic Practice Quantitative Applications Social Sciences Sage Publications Epstein Joshua M Axtell Robert Growing Artificial Societies Social Science Bottom Up Brookings Institution Press MIT Press Fein Sidney Paz Victoria Rao Nyapati LaGrassa Joseph The Combi nation Lithium Carbonate anMAOI Refractory Depressions American Journal Psychiatry Feller William An Introduction Probability Theory It is Applications Wiley Fisher R A Two New Properties Mathematical Likelihood Proceedings Royal Society London Series A Containing Papers Mathematical Physical Character Fisher Ronald Aylmer On Interpretation Contingency Ta bles Calculation P Journal Royal Statistical Society Fisher Ronald Aylmer Statistical Methods Scientific Inference Oliver Boyd Freedman David A A Note Screening Regression Equations The Amer ican Statistician Friedl Jeffrey E F Mastering Regular Expressions 2nd edn O Reilly Media Frisch leen Essential System Administration O Reilly Associates Fry Tim R L Harris Mark N A Monte Carlo Study Tests Independence Irrelevant Alternatives Property Transportation Research Part B Methodological Gardner Martin Wheels Life Other Mathematical Amusements W H Freeman Gelman Andrew Hill Jennifer Data Analysis Using Regression Multilevel Hierarchical Models Cambridge University Press gsl_stats March BIBLIOGRAPHY Gelman Andrew Carlin John B Stern Hal S Rubin Donald B Bayesian Data Analysis 2nd edn Chapman Hall Texts Statistical Science Chapman Hall CRC Gentle James E Elements Computational Statistics Statistics Com puting Springer Gentleman Robert Ihaka Ross Lexical Scope Statistical Comput ing Journal Computational Graphical Statistics Gibbard Ben Lightness Barsuk Records In Death Cab Cutie Transat lanticism Gibrat Robert Les Inegalites Economiques Applications Aux Inegalites des Richesses la Concentration des Entreprises aux Populations des Villes aux Statistiques des Familles etc d une Loi Nouvelle la Loi de L effet Propor tionnel Librarie du Recueil Sirey Gigerenzer Gerd Mindless Statistics The Journal Socio Economics Gill Philip E Murray Waler Wright Margaret H Practical Optimiza tion Academic Press Givens Geof H Hoeting Jennifer A Computational Statistics Wiley Series Probability Statistics Wiley Glaeser Edward L Sacerdote Bruce Scheinkman Jose A Crime Social Interactions The Quarterly Journal Economics Goldberg David What Every Computer Scientist Should Know Floating point Arithmetic ACM Computing Surveys Gonick Larry Smith Woollcott Cartoon Guide Statistics Collins Good Irving John Random Thoughts Randomness PSA Proceed ings Biennial Meeting Philosophy Science Association Gough Brian ed GNU Scientific Library Reference Manual 2nd edn Network Theory Ltd Greene William H Econometric Analysis 2nd edn Prentice Hall Haddon Mark The Curious Incident Dog Night time Vintage Huber Peter J Languages Statistics Data Analysis Joural Com putational Graphical Statistics Huff Darrell Geis Irving How Lie With Statistics W W Norton Company gsl_stats March BIBLIOGRAPHY Hunter John E Schmidt Frank L Methods Meta Analysis Correcting Error Bias Research Findings 2nd edn Sage Publications Internal Revenue Service Federal Tax Rate Schedules Department Treasury Kahneman Daniel Slovic Paul Tversky Amos Judgement Under Un certainty Heuristics Biases Cambridge University Press Karlquist A ed Spatial Interaction Theory Residential Location North Holland Kernighan Brian W Pike Rob The Practice Programming Addison Wesley Professional Kernighan Brian W Ritchie Dennis M The C Programming Language 2nd edn Prentice Hall PTR Klemens Ben Finding Optimal Agent based Models Brookings Center Social Economic Dynamics Working Paper Kline Morris Mathematics The Loss Certainty Oxford University Press Kmenta Jan Elements Econometrics 2nd edn Macmillan Publishing Company Knuth Donald Ervin The Art Computer Programming 3rd edn Vol Fundamental Algorithms Addison Wesley Kolmogorov Andrey Nikolaevich Sulla determinazione empirica di una legge di distributione Giornale dell Istituto Italiano degli Attuari Laumann Anne E Derick Amy J Tattoos body piercings United States A National Data Set Journal American Academy Der matologists Lehmann E L Stein C On Theory Some Non Parametric Hy potheses The Annals Mathematical Statistics Maddala G S Econometrics McGraw Hill McFadden Daniel Conditional Logit Analysis Qualitative Choice Be havior In Zarembka Chap pages McFadden Daniel Modelling Choice Residential Location In Karlquist Pages Nabokov Vladimir Pale Fire G P Putnams s Sons gsl_stats March BIBLIOGRAPHY National Election Studies The National Election Study dataset University Michigan Center Political Studies Newman James R ed The World Mathematics Simon Schuster Neyman J Pearson E S 1928a On Use Interpretation Certain Test Criteria Purposes Statistical Inference Part I Biometrika 20A Neyman J Pearson E S 1928b On Use Interpretation Certain Test Criteria Purposes Statistical Inference Part II Biometrika 20A Orwell George Secker Warburg Papadimitriou Christos H Steiglitz Kenneth Combinatorial Optimiza tion Algorithms Complexity Dover Paulos John Allen Innumeracy Mathematical Illiteracy Conse quences Hill Wang Pawitan Yudi In All Likelihood Statistical Modeling Inference Using Likelihood Oxford University Press Pearson Karl On Criterion given System Deviations Probable Case Correlated System Variables Such That Can Reasonably Supposed Have Arisen Random Sampling London Edinburgh Dublin Philosophical Magazine Journal Science July Reprinted Pearson pp Pearson Karl Karl Pearson s Early Statistical Papers Cambridge Peek Jerry Todino Gonguet Grace Strang John Learning UNIX Operating System 5th edn O Reilly Associates Perl Judea Causality Cambridge University Press Pierce John R An Introduction Information Theory Symbols Signals Noise Dover Poincare Henri Chance In Newman translated George Bruce Halsted Pages Polhill J Gary Izquierdo Luis R Gotts Nicholas M The Ghost Model Other Effects Floating Point Arithmetic Journal Artificial Societies Social Simulation Poole Keith T Rosenthal Howard A Spatial Model Legislative Roll Call Analysis American Journal Political Science gsl_stats March BIBLIOGRAPHY Press William H Flannery Brian P Teukolsky Saul A Vetterling William T Numerical Recipes C The Art Scientific Computing Cambridge University Press Price Roger Stern Leonard Mad Libs Price Stern Sloan Rumi Jelaluddin The Essential Rumi Penguin Translated Coleman Barks Sarndal Carl Erik Swensson Bengt Wretman Jan Model Assisted Sur vey Sampling Springer Series Statistics Springer Verlag Scheffe Henry The Analysis Variance Wiley Shepard Roger N Cooper Lynn A Representation Colors Blind Color Blind Normally Sighted Psychological Science Silverman B W Some Aspects Spline Smoothing Approach Non Parametric Regression Curve Fitting Journal Royal Statistical Society Series B Methodological Silverman Bernard W Using Kernel Density Estimates Investigate Mul timodality Journal Royal Statistical Society Series B Methodological Smith Thomas M Reynolds Richard W A Global Merged Land Air Sea Surface Temperature Reconstruction Based Historical Observations Journal Climate Snedecor George W Cochran Willian G Statistical Methods 6th edn Iowa State University Press Stallman Richard M Pesch Roland H Shebs Stan Debugging GDB The GNU Source level Debugger Free Software Foundation Stravinsky Igor Poetics Music Form Six Lessons The Charles Eliot Norton Lectures Harvard University Press Stroustrup Bjarne The C Programming Language Addison Wesley Student Errors Routine Analysis Biometrika Thomson William AGuide Young Economist Writing Speaking Effectively Economics MIT Press Train Kenneth E Discrete Choice Methods Simulation Cambridge University Press Tukey John W Exploratory Data Analysis Addison Wesley gsl_stats March BIBLIOGRAPHY Vuong Quang H Likelihood Ratio Tests Model Selection Non Nested Hypotheses Econometrica Wolfram Stephen The Mathematica Book 5th edn Wolfram Media Zarembka P ed Frontiers Econometrics Academic Press gsl_stats March INDEX C C C C C C filename define ifndef ifndef lude lude C C caret regex brackets negation regex brackets head line C 200x Abelson et al xi affine projection agent based modeling Agents assigning RNGs Albee xii Allison Amemiya Amemiya analysis variance animation anonymous structures ANOVA comparison OLS description testing ANSI 419apop_ F_test 312IV 278anova 226array_to_matrix 125array_to_ve tor 125beta_from_mean_var rosstab_to_db data_allo 120data_ opy 122data_ orrelation 231data_fill 310data_get 121data_listwise_ delete 347data_mem py 125data_print 126data_ptr 121data_ptr 359data_set 121data_show 231data_sort 233data_sta k 282data_summarize 232data_to_dummies 283data 232db_merge_table 102db_merge 102db_rng_init 84db_to_ rosstab gsl_stats March INDEXdet_and_inv 134dot 267draw 359estimate_restart 345estimate 323exponential 374gamma 374histograms_test_ goodness_of_fit 323histogram_model_ reset 323histogram_ve tor_ reset 323histogram 323ja kknife_ ov 131line_to_data 353line_to_matrix 125linear_ onstraint 151linear_ onstraint 353logit 284lookup 304matrix_ _all 119matrix_ 119matrix_determinant 134matrix_inverse 134matrix_map_all_sum 119matrix_map 100matrix_normalize 274matrix_p 269matrix_print 168matrix_summarize 231matrix_to_data 125maximum_likelihood 340merge_dbs 98model ff multinomial_probit 285normalize_for_svd 267ols 279open_db 98opts apop_opts specific optionspaired_t_test 309plot_histogram 358plot_histogram 172plot_qq 320plot_query 417query_to_data 127query_to_float 99query_to_matrix 127query_to_mixed_data 104query_to_text 121query_to_ve tor 127query 127rng_allo 357system 397t_test 110table_exists 108test_anova_ independen e 313test_fisher_exa t 314test_kolmogorov 323text_to_data 125text_to_db 417update 374ve tor_ 119ve tor_apply 119ve tor_ orrelation 231ve tor_distan e 150ve tor_exp 83ve tor_grid_ distan e 150ve tor_kurtosis 365ve tor_log10 174ve tor_log 117ve tor_map 119ve tor_mean 231ve tor_moving_ average 262ve tor_per entiles 233ve tor_print 126ve tor_skew 231ve tor_to_data 125ve tor_to_matrix 125ve tor_var 231ve tor_weighted_ mean 232ve tor_weighted_var 232wls 278zipf 149APOP_COL 142APOP_DB_ENGINE 106Apop_matrix_row 114apop_opts db_name_ olumn 120db_nan 108output_append 167output_pipe 169output_type 126thread_ ount 119APOP_ROW 142APOP_SUBMATRIX apophenia arbitrary precision 138arg 206argp arguments 420argv 206Arithmeti ex eption ore dumped gsl_stats March INDEX arithmetic C array arrays ASCII 419asprintf 111assert assertion assignment asymptotic efficiency 221atof 208atoi 208atol Avriel 340awk POSIX Axtell aymptotic unbiasedness bandwidth bar chart Barron Sheu bash 382bash POSIX Baum et al Bayes s rule Bayesian Bayesian updating 372begin SQL Benford s law Benford Bernoulli Bernoulli distribution 358gsl_ran_ bernoulli _pdf Bernoulli draw Beta distribution 358gsl_ df_beta_ P Q 249gsl_ran_beta _pdf Beta function 249between SQL BFGS algorithm Broyden Fletcher Goldfarb Shanno Conjugate gradient algorithm bias binary tree Binomial distribution 358gsl_ran_binomial _ pdf birthday paradox BLAS block scope BLUE Bonferroni correction Boolean expressions bootstrap bootstrap principle Bourne she shall Bowman Shenton bracket expressions regexes BRE breaking ctrl c Broyden Fletcher Goldfarb Shanno Conjugate gradient algorithm buffer C keywords har onst 65double 135do 21extern 51float 135for 118free 58ifndef 354if 211in lude 385int 29long 29stati 357stru t 60typedef 191union 121void 36while C she shall C address value allo carriage return ase SQL Casella Berger ast SQL casting type casting POSIX Cauchy distribution Cauchy Schwarz inequality causality d POSIX CDF cumulative density function cellular automaton Central Limit Theorem central moments Chamberlin har C Chebychev s inequality gsl_stats March INDEX Cheng Long distribution 358gsl_ df_ hisq_P 305gsl_ df_ hisq_Pinv test goodness fit scaling hmod POSIX choosegsl_sf_ hoose Chung Fraser Chwe Cleveland McGill closed form expression CLT clustering CMF cumulative mass function Codd coefficient determination color olumn POSIX command line utilities combinatorial optimization Command line arguments command line programs POSIX commands commenting comments C Gnuplot SQL ommit SQL comparative statics compilation compiler conditional probability conditionals configuration files conjugate distributions conjugate gradient Conover consistent estimator consistent test onst C constrained optimization contour plot contrast Conway John Cook s distance Cook correlation coefficient counting words covariance Cox p POSIX Cramer Rao Inequality Cramer Rao lower bound reate SQL Cropper et al crosstab sh redirecting stderr sh POSIX cumulative density function cumulative mass function current directory attack ut POSIX CVS subversion data conditioning format data mining data snooping data structures de Finetti debugger debugging decile quantile declaration functions gsl_matrix gsl_ve tor pointers types variables degrees freedom 423delete SQL Dempster et al dependency dereferencing 43des SQL descriptive statistics designated initializers df 419diff POSIX discards qualifiers pointer target type discrete data 123distin t 81do C 23dos2unix POSIX dot files dot product dot graphing program 182double C 135doxygen POSIX 185drop SQL dummy variable e 136ed POSIX efficiency Efron Hinkley Efron Tibshirani 231egrep POSIX eigenvectors Einstein Albert Electric Fence Eliason xi gsl_stats March INDEX 447else C 21EMACS POSIX 403env POSIX environment variable Epstein Axtell ERE erf error function Euler s constant 136every Gnuplot 175ex ept SQL excess variance 239exit expected value Exponential distribution 247gsl_ df_ exponential_ P Q 247gsl_ran_ exponential _pdf exponential family 349export POSIX 424extern C Extreme Value distribution F distribution 358gsl_ df_fdist_P F test 309apop_F_test factor analysis 265f lose Fein et al Feller Fermat s polygonal number theorem 47fflush 169fgets Fibonacci sequence files hidden 383find POSIX Fisher exact test Fisher RA likelihood Fisher Fisher Fisher Flat distribution Uniform distribution flat distribution Fletcher Reeves Conjugate gradient algorithm 341float C 135fopen 394for C FORTRAN 10fortune POSIX 397fprintf frame 424free C Freedman frequentist Friedl Frisch 385from SQL Fry Harris outer join function pointers 190g_key_file_get Game Life Gamma distribution 331gsl_ df_gamma_ P Q 246gsl_ran_gamma _pdf Gamma function Gardner Gaussian distribution Normal distribution GCC 424g 216g POSIX 214gdb debugginggdb POSIX GDP Gelman Hill xi Gelman et al Generalized Least Squares Gentleman Ihaka 145getenv 384gets Gibbard Gibrat GIF Gigerenzer Gill et al Givens Hoeting Glaeser et al Glib global information global variables initializing globbing GLS Generalized Least Squares GNU GNU Scientific Library Gnuplot comments 160gnuplot POSIX comments Gnuplot keywordsevery 175plot 160replot 170reset 168splot Gnuplot settingskey 165out 159pm3d 165term 159title 164xlabel 164xti s 165ylabel 164yti s Goldberg golden ratio gsl_stats March INDEX Gonick Smith xiii Good goodness fit Gough graph graphing Gnuplot Graphviz flowcharts nodes Graphviz Greene 346grep POSIX 408grep egrep POSIX grid search 424group GSL GNU Scientific Librarygsl_ blas_ddot df_ df_beta_ P Q df_exponential_ P Q df_flat_ P Q df_gamma_ P Q df_gaussian_ P Q df_lognormal_ P Q df_negative_ binomial_ P Q 244linalg_HH_solve 134linalg_SV_de omp 269matrix_add_ onstant 117matrix_add 117matrix_allo 114matrix_div_elements 117matrix_free 114matrix_get 116matrix_mem py matrix_mul_elements 117matrix_ptr 359matrix_row 142matrix_s ale 117matrix_set_all 114matrix_set_ old 114matrix_set_row 114matrix_set 116matrix_sub 117matrix_transpose_ mem py 149matrix 140pow_2 132pow_int 132ran_ 358ran_bernoulli _pdf 237ran_beta _pdf 249ran_binomial _pdf 238ran_exponential _ pdf 247ran_flat _pdf 251ran_gamma _pdf 246ran_gaussian _pdf 241ran_ hypergeometri _ pdf 239ran_lognormal _pdf 243ran_max 363ran_multinomial _ pdf 240ran_negative_ binomial _pdf 244ran_negative_ binomial 244ran_poisson _pdf rng_env_setup 357rng_uniform_int 369rng_uniform 358rng 357sf_beta 249sf_ hoose 238sf_gamma 244sort_ve tor_ largest_index 268stats_varian e 230ve tor_add_ onstant 117ve tor_add 117ve tor_div 117ve tor_free 114ve tor_get 116ve tor_mem py 125ve tor_mul 117ve tor_ptr 359ve tor_s ale 117ve tor_set 116ve tor_sort 233ve tor_sub 117ve tor 142GSL_IS_EVEN 19GSL_IS_ODD 19GSL_MAX 212GSL_MIN 212GSL_NAN 135GSL_NEGINF 135GSL_POSINF GUI Guinness Brewery Gumbel distribution Haddon half life halting ctrl c hash table hat matrix 424having SQL 82head POSIX header file aggregation gsl_stats March INDEX variables hedonic pricing model help command line switches man Gnuplot Hessian heteroskedasticity hidden files hierarchical model multilevel model Hipp D Richard histograms drawing plotting testing Householder solver transformations How Lie Statistics Huber Huff Geis Hunter Schmidt Hybrid method Hypergeometric distribution 239gsl_ran_hypergeo metri _pdf IDE identical draws identically distributed identity matrix IEC IEC floating point standard IEEE IEEE floating point standard 135if C iff 425ifndef C IIA iid importance sampling imputation maximum likelihood 347in SQL 79in lude C include path incrementing independence irrelevant alternatives independent draws 425index SQL indices internal representation inferential statistics 425INFINITY infinity information equality information matrix initializers designated 32insert SQL instrumental variable 426int C interaction Internal Revenue Service interpreter 426interse t SQL invariance principle 351isfinite 135isinf 135isnan ISO IV instrumental variables jackknife jittering join command line program database Kahneman et al kernel density estimate Kernighan Pike Kernighan Ritchie 210key Gnuplot key files key value Klemens Kline Kmenta Knuth Donald Knuth Kolmogorov kurtosis LATEX lattices Laumann Derick layers abstraction 5ldexp leading digit squares Ordinary Least Squares left outer join legend plot Lehmann Stein leptokurtic 306less POSIX lexicographic order libraries library Life Game 178like likelihood function philosophical implications likelihood ratio ff gsl_stats March INDEX likelihood ratio test 427limit SQL line feed line numbers linked list linker listwise deletion 347LOAD SQL local information log likelihood function log plots 174log10 logical expressions logistic model logit model logit nested Lognormal distributiongsl_ran_ lognormal _pdf lognormal distribution 242long C 29long doubleprintf format specifier love blindness LR 420ls POSIX macro Maddala 275main 427make POSIX 391mallo ff 214MALLOC_CHECK_ 214man POSIX Manhattan metric MAR marginal change Markov Chain Monte Carlo math library 52math h matrices determinants dot product inversion views max GSL_MAX maximum likelihood tracing path maximum likelihood estimation ff MCAR McFadden McFadden MCMC mean squared error median 234mem py 124memmove memory debugger memory leak mesokurtic metadata database metastudies method moments Metropolis Hastings min GSL_MIN missing random missing completely random missing data missing random 428mkdir POSIX ML MLE maximum likelihood estimation MNAR modulo moment generating functions xii moments Monte Carlo method 428more POSIX MSE multicollinearity multilevel model ff multilevel models Multinomial distribution multinomial distributiongsl_ran_ multinomial _pdf multinomial logit multiple testing problem Multivariate Normal distribution multivariate Normal distribution 242mv POSIX mySQL 106mysqlshow Nabokov naming functions 115NAN number National Election Studies Negative binomial distribution 244gsl_ df_negative_ binomial_ P Q 244gsl_ran_negative_ binomial _pdf negative definite Negative exponential distribution negative semidefinite Nelder Mead simplex algorithm nested logit network analysis networks graphing Newton s method Neyman Pearson 1928a gsl_stats March INDEX Neyman Pearson 1928b Neyman Pearson lemma 350nl POSIX non ignorable missingness non parametric noncentral moment Normal distribution 331gsl_ df_gaussian_ P Q 241gsl_ran_gaussian _ pdf 241gsl_ df_gaussian_P variance normality tests number 135null SQL null pointer object object file object oriented programming 121offset OLS Ordinary Least Squares optimization constrained order SQL order statistic order statistics Ordinary Least Squares decomposing variance Orwell 42out Gnuplot outer join SQL outliers spotting overflow error pairwise deletion Papadimitriou Steiglitz parameter files partitioned matrices 122paste POSIX path Paulos Pawitan xi PCA 420p lose PCRE PDF probability density function Pearson correlation coefficient Pearson Peek et al penalty function percentile quantileperl POSIX Perl permutation test Pierce pipe pivot table platykurtic plot 429plot Gnuplot plotting Gnuplotpm3d Gnuplot PMF probability mass function Poincare pointer ff declaration function null Poisson distribution 331gsl_ran_poisson _ pdf Polak Ribiere Conjugate gradient algorithm Polhill et al Poole Rosenthal 265popen positive definite positive semidefinite POSIX POSIX commandsEMACS 403awk 403bash d hmod olumn p sh ut 401diff 402dos2unix 418doxygen 185ed 404egrep 406env 382export 424find 386fortune 397g 214gdb 387gnuplot 417grep egrep 399grep 408head 400less 403ls 399make 391man 400mkdir 399more 400mv gsl_stats March INDEXnl 401paste 401perl 418ps2pdf 160rmdir 399rm 399sed 418setenv 424sort 400tail 400tou h 389tr 407uniq 402unix2dos 418vim 402vi 403w posterior distribution Postscript 188pow power precision numerical preprocessor Press et al xiii Price Stern prime numbers principal component analysis 430printf printing printf prisoner s dilemma PRNG probability density function probability mass function probit probit model profiler programs POSIX commands projection matrix 430ps2pdf POSIX pseudorandom number generator Python Q Q plot quantile query R2 Ramsey 330rand 363random SQL random number generator random numbers SQL ranks 253reallo regression regular expressions ff bracket expressions case sensitivity white space 407replot Gnuplot 170reset Gnuplot revision control subversion right outer join 106rint 33rm POSIX 399rmdir POSIX RNG 420round rounding 33rowid 401rowid SQL Ruby Rumi Sarndal et al sample codeMakefile tex 188Rtimefisher 9agentgrid gnuplot 161amongwithin argv 207arrayflo k 197bdayfns 35bdaystru t 32bimodality 378birds 195birthday allbyadd allbyval andidates ltdemo ooks orrupt 348databoot 368drawbeta 359drawfrompop 361dummies 282e on101 analyti 154e on101 153e on101 main 155eigenbox 267eigeneasy 269eigenhard 268env 384errorbars 173fisher 314flow 22fortunate 397ftest 311fuzz 140getopt 209getstring 204gkeys 205glib onfig 205goodfit 322ja kiteration 132jitter 175latti e 171life 179listflo k 199lo almax 339lrnonnest 354lrtest 352maoi 254markov 129metroanova gsl_stats March INDEX 453multipli ationtable 114newols 146normalboot 369normalgrowth 253normallr 151normaltable 252notanumber 135oneboot 368pipeplot 168plotafun tion 191powersoftwo 138primes 62primes2 63probitlevels 293proje tion 273proje tiontwo 279qqplot 320ridership 289selfexe ute 170simpleshell 40sinsq 338smoothing 263squares 59statedummies 112taxes 118tdistkurtosis 366time 363timefisher 9treeflo k 201ttest 111ttest long 110wbtodata sample distributions Sampling artificial population Savage 330s anf Scheffe scientific notation scope global score 431sed POSIX seed segfault segmentation fault segmentation fault 431sele t SQL ffsetenv 384setenv POSIX settings apop_models she shall Shepard Cooper 265SIGINT Silverman Silverman simulated annealing singular value decomposition 431sizeof skew Slutsky theorem Smith Reynolds Snedecor Cochran snowflake problem 288snprintf 67sort POSIX sortingsort_ve tor_ largest_index database output gsl_ve tors andapop_datas source code spectral decomposition 265splot Gnuplot 161sprintf SQL comments SQL keywordsLOAD 107begin ase ast ommit reate 84delete 86des 83drop 86ex ept 94from 77having 82index 90insert 86interse t 94in 79limit 83null 105order 83outer join 106random 84rowid 85sele t ffunion 94union 94update 87where SQLite 75sqlite_master 86sqrt 52srand SSE SSR SST stack Stallman et al standard deviation standard error 432stati C static variables initializing statistic statistics packages rants 11stderr 396stdin 394stdout gsl_stats March INDEX stopping ctrl c Stravinsky 123str mp 68str py stride string strings ffglib library 193strlen 66strn 66strn py Stroustrup 42stru t C structural equation modeling structure stru t anonymous Structured Query Language Student Student s t distribution Student subjectivist subqueries subversion surface plots SVD switches syntax error 19system t distribution t distribution 365gsl_ df_tdist_P 305gsl_ df_tdist_Pinv t test 309apop_paired_t_test 308apop_t_test 308tail POSIX Taylor expansion 350term Gnuplot test functions TEX Thomson threading 432time 362title Gnuplot TLA 420tou h POSIX 389tr POSIX Train transition matrix transposition seegsl_matrix_ transpose_mem py dot products tree binary tree trimean Tukey type type casting Type I error Type II error 432typedef C unbiased estimator unbiased statistic underflow error Uniform distribution 358gsl_ df_flat_ P Q 251gsl_ran_flat _pdf 251union C 121union SQL 94union SQL 94uniq POSIX United States America national debt deficit UNIX 433unix2dos POSIX 418update SQL utility maximization utils 193va uum value risk variance 433vi POSIX views 128vim POSIX 402void C void pointers Vuong 354w POSIX Weighted Least Squares 433where SQL 78whi h 168while C William S Gosset Windows WLS Wolfram word count 401xlabel Gnuplot 164xti s Gnuplot 165ylabel Gnuplot 164yti s Gnuplot z distribution z test Zipf distribution Zipf s law\n",
            "6 []\n",
            "6 Dropbox SzeliskiBookDraft_20210930 pdf Simplify life\n",
            "7 []\n",
            "7 Introduction Machine Learning Fall Amnon Shashua School Computer Science Engineering The Hebrew University Jerusalem Jerusalem Israel ar X iv v1 cs L G A pr Contents Bayesian Decision Theory page Independence Constraints Example Coin Toss Example Gaussian Density Estimation Incremental Bayes Classifier Bayes Classifier class Normal Distributions Maximum Likelihood Maximum Entropy Duality ML Empirical Distribution Relative Entropy Maximum Entropy Duality ML MaxEnt EM Algorithm ML Mixture Distributions The EM Algorithm General EM d Data Back Coins Example Gaussian Mixture Application Examples Gaussian Mixture Clustering Multinomial Mixture bag words Application Support Vector Machines Kernel Functions Large Margin Classifier Quadratic Linear Programming The Support Vector Machine The Kernel Trick The Homogeneous Polynomial Kernel The non homogeneous Polynomial Kernel The RBF Kernel Classifying New Instances iii iv Contents Spectral Analysis I PCA LDA CCA PCA Statistical Perspective Maximizing Variance Output Coordinates Decorrelation Diagonalization Covariance Matrix PCA Optimal Reconstruction The Case n m Kernel PCA Fisher s LDA Basic Idea Fisher s LDA General Derivation Fisher s LDA class LDA versus SVM Canonical Correlation Analysis Spectral Analysis II Clustering K means Algorithm Clustering Matrix Formulation K means Min Cut Spectral Clustering Ratio Cuts Normalized Cuts Ratio Cuts Normalized Cuts The Formal PAC Learning Model The Formal Model The Rectangle Learning Problem Learnability Finite Concept Classes The Realizable Case The Unrealizable Case The VC Dimension The VC Dimension The Relation VC dimension PAC Learning The Double Sampling Theorem A Polynomial Bound Sample Size m PAC Learning Optimality SVM Revisited Appendix Bibliography Bayesian Decision Theory During lectures looking inference training data problem random process modeled joint probability distribu tion input measurements output class labels variables In general estimating underlying distribution daunting unwieldy task number constraints tricks trade speak certain conditions task manageable fairly effective To things simple assume discrete world e values random variables finite number values Consider example random variables X taking k possible values x1 xk H taking values h1 h2 The values X stand Body Mass Index BMI measurement weight height2 person H stands possibilities h1 standing person weight h2 possibility person normal weight Given BMI measurement like estimate probability person weight The joint probability P X H dimensional array way array 2k entries cells Each training example xi hj falls cells P X xi H hj P xi hj holds ratio number hits cell j total number training examples assuming training data arrive d As result ij P xi hj The projections array vertical horizontal axes sum ming columns rows called marginalization produces P hj P xi hj sum j th row probability P H hj e probability person weight measurement called priors Likewise P xi j P xi hj probability P X xi probability receiving BMI measurement begin called evidence Note Bayesian Decision Theory h1 h2 x1 x2 x3 x4 x5 Fig Joint probability P X H X ranges discrete values H values Each entry contains number hits cell xi hj The joint probability P xi hj number hits divided total number hits See text details definition j P hj P xi In Fig P h1 P h2 higher prior probability person weight normal weight Also P x3 highest meaning encounter BMI x3 highest prob ability The conditional probability P hj xi P xi hj P xi ratio tween number hits cell j number hits th column e probability outcome H hj given measure ment X xi In Fig P h2 x3 Note j P hj xi j P xi hj P xi P xi j P xi hj P xi P xi Likewise conditional probability P xi hj P xi hj P hj number hits cell j normalized number hits j th row represents probability receiving BMI xi given class label H hj weight person In Fig P x3 h2 probability receiving BMI x3 given person known normal weight Note P xi hj The Bayes formula arises P xi hj P hj P xi hj P hj xi P xi P hj xi P xi hj P hj P xi The left hand P hj xi called posterior probability P xi hj called class conditional likelihood The Bayes formula provides way estimate posterior probability prior evidence class likelihood It useful cases natural compute collect data class likelihood simple compute directly Bayesian Decision Theory posterior For example given measurement like estimate probability measurement came tossing pair dice spinning roulette table If x measurement h1 stands pair dice h2 roulette natural compute class conditional P pair dice P roulette Computing posterior directly difficult As example consider medical diagnosis Once known patient suffers disease hj natural evaluate probabilities P xi hj emerging symptoms xi As result inference problems natural use class conditionals basic building blocks use Bayes formula invert obtain posteriors The Bayes rule lead unintuitive results particu lar known base rate fallacy shows nonuniform prior influence mapping likelihoods posteriors On intuitive basis people tend ignore priors equate likelihoods posteriors The follow ing example typical consider Cancer test kit problem following features given subject Cancer C probability test kit producing positive decision P C means P C probability kit producing neg ative decision given subject healthy H P H means P H The prior probability Cancer population P C These numbers appear glance reasonable e probability test kit produce correct indication given subject Cancer What actually interested probability subject Cancer given test kit generated positive decision e P C Using Bayes rule P C P C P C P P C P C P C P C P H P H means chance subject Cancer given test kit produced positive response means poor performance If draw posteriors P h1 x P h2 x probability distribution array Fig P h1 x P h2 x values X smaller value x3 x4 Therefore decision minimize probability misclassification This example adopted Yishai Mansour s class notes Machine Learning Bayesian Decision Theory choose class maximal posterior h argmax j P hj x known Maximal A Posteriori MAP decision principle Since P x simply normalization factor MAP principle equivalent h argmax j P x hj P hj In case information prior P h known known prior uniform obtain Maximum Likelihood ML principle h argmax j P x hj The MAP principle particular case general principle known proper Bayes loss incorporated decision process Let l hi hj loss incurred deciding class hi fact hj correct class For example loss function l hi hj j j The squares loss function l hi hj hi hj typically outcomes vectors high dimensional space class labels We define expected risk R hi x j l hi hj P hj x The proper Bayes decision policy minimize expected risk h argmin j R hj x The MAP policy arises case l hi hj loss function R hi x j P hj x P hi x Thus argmin j R hj x argmax j P hj x Independence Constraints Independence Constraints At point pause ask obtained Clearly inference problem captured joint probability distribution need formulas How obtain necessary data fill probability distribution array begin Clearly additional simplifying constraints task practical size kind arrays exponential number variables There families simplifying constraints literature statistical independence constraints parametric form class likelihood P xi hj inference density estimation problem structural assumptions latent hidden variables graphical models Today focus simplifying constraints statistical independence properties Consider random variables X Y The variables statistically independent X Y P X Y P X meaning information value Y add X The independence condition equivalent constraint P X Y P X P Y This easily proven X Y P X Y P X Y P Y P X P Y On hand P X Y P X P Y P X Y P X Y P Y P X P Y P Y P X Let values X range x1 xk values Y range y1 yl The associated k l way array P X xi Y yj repre sented outer product P xi yj P xi P yj vectors P X P x1 P xk P Y P y1 P yl In words way array viewed matrix rank determined k l minus sum vector parameters kl minus parameters Likewise X1 X2 Xn n statistically independent random vari ables Xi ranges ki discrete distinct values n way array P X1 Xn P X1 P Xn outer product n vectors determined k1 kn minus n parameters instead k1k2 kn minus parameters Viewed tensor joint probabil I bit simplifying things ignoring fact entries array non negative This means additional non linear constraints effectively reduce number parameters stays exponential Bayesian Decision Theory ity rank tensor The main point statistical independence assumption reduced representation multivariate joint distribution exponential linear size Since variables typically divided measurement variables output class variable H general H1 Hl useful intro duce weaker form independence known conditional indepen dence Variables X Y conditionally independent given H denoted X Y H iff P X Y H P X H meaning given H value Y add information X This equivalent condition P X Y H P X H P Y H The proof goes follows If P X Y H P X H P X Y H P X Y H P H P X Y H P Y H P H P X Y H P Y H P H P H P X H P Y H If P X Y H P X H P Y H P X Y H P X Y H P Y H P X Y H P Y H P X H Consider example Joe Mo live opposite sides city Joe goes work train Mo car Let X event Joe late work Y event Mo late work Clearly X Y independent factors For example train strike because Joe late strike extra traffic people car instead train causing Mo pate Therefore variable H standing event train strike decouple X Y From computational standpoint conditional independence assump tion similar effect unconditional independence Let X range k distinct value Y range r distinct values H range s distinct values Then P X Y H way array size k r s Given X Y H means P X Y H hi way slice way array H axis represented outer product vectors P X H hi P Y H hi As result way array represented s k r parameters instead skr Likewise X1 Xn H n way array P X1 Xn H hi slice H axis n array P X1 Xn H represented outer product n vectors e k1 kn n parameters Independence Constraints Example Coin Toss We use ML principle estimate bias coin Let X random variable taking value H hypothesis taking real value standing coin s bias If coin s bias q P X H q q P X H q q We receive m d examples x1 xm xi We wish determine value q Given x1 xm H ML problem solve q argmax q P x1 xm H q m P xi q argmax q logP xi q Let m stand number instances e xi m Therefore ML problem q argmax q log q n log q Taking partial derivative respect q setting zero q log q n log q q n q produces result q n Example Gaussian Density Estimation So far considered constraints induced conditional independent state ments random variables means reduce space time complexity multivariate distribution array Another approach assume form parametric form governing entries array popular assumption Gaussian distribution P X1 Xn N E mean vector covariance matrix E The parameters density function denoted E vector x Rn P x n E exp x E x Assume given d sample k points S x1 xk xi Rn like find Bayes optimal argmax P S Bayesian Decision Theory maximizing likelihood assuming priors P equal maximum likelihood MAP produce result Because sample drawn d assume P S k P xi Let L logP S logP xi Log monotonously increasing argmax L The parameter estimation recovered taking derivatives respect e L We L log E k n log xi E xi We start simple scenario E 2I e covariances zero variances equal Thus E 2I E 2n After substitution removal items depend L nk log xi The partial derivative respect L xi obtain k k xi The partial derivative respect L nk xi obtain kn k xi Note reason dividing n fact 2n k k xi n j 2j n Incremental Bayes Classifier In general case E rank symmetric matrix derivative eqn respect L E xi E rank obtain k xi For derivative respect E note auxiliary items E E E E E trace AE E 1AE Using fact x y trace xy transform z E 1z trace zz E vector z Given E symmetric E trace zz E E 1zz E Substituting z x obtain L E kE E xi xi E obtain E k k xi xi Incremental Bayes Classifier Consider application conditional dependence Bayes incremental rule Suppose processed n examplesX n X1 Xn computed P H X n We given new measurement X wish compute update posterior P H X n X We use chain rule P X Y Z P X Y Z P Y Z P Z X Y P X Y P Y P Z Y P Y P Z X Y P X Y P Z Y obtain P H X n X P X X n H P H X n P X X n conditional independence P X X n H P X H The term P X X n expanded follows based rule P X1 Xn P X1 X2 Xn P X2 X3 Xn P Xn Xn P Xn Bayesian Decision Theory P X X n P X X n H hi P H hi P X n P X H hi P X n H hi P H hi P X n P X H hi P H hi X n After substitution obtain P H hi X n X P X H hi P H hi X n j P X H hj P H hj X n The old posterior P H X n prior updated formula Consider following example We coin fair biased Head probability Let H h1 event coin fair H h2 coin biased We start prior probabilities P h1 P h2 higher initial belief coin fair Suppose coin toss Head e X1 Then P h1 x1 P x1 h1 P h1 P x1 P h2 x1 Our posterior belief coin fair gone Head toss Assume measurement X2 P h1 x1 x2 P x2 h1 P h1 x1 normalization P h2 x1 x2 belief coin fair continues Head tosses Bayes Classifier class Normal Distributions For topic lecture consider class inference problem We encountered problem course context SVM LDA In Bayes framework H h1 h2 denotes class member variable possible outcomes MAP decision policy calls adopted Ron Rivest s class notes Bayes Classifier class Normal Distributions making decision based data x h argmax h1 h2 P h1 x P h2 x words class h1 chosen P h1 x P h2 x The decision surface function x described P h1 x P h2 x The questions ask Bayes optimal decision sur face like assume classes normally distributed different means covariance matrix What condition equal priors P h1 P h2 decision surface hyperplane hyperplane produced LDA Claim If P h1 P h2 P x h1 N E P x h1 N E Bayes optimal decision surface hyperplane w x w E In words decision surface described x E E Proof The decision surface described P h1 x P h2 x equivalent statement ratio posteriors equivalently log ratio zero Bayes formula obtain log P x h1 P h1 P x h2 P h2 log P x h1 P x h2 In words decision surface described logP x h1 logP x h2 x E x x E x After expanding terms obtain eqn Maximum Likelihood Maximum Entropy Duality In previous lecture defined principle Maximum Likelihood ML suppose random variables X1 Xn form random sample discrete distribution joint probability distribution P x x x1 xn vector sample parameter parameter space discrete set values class membership When P x considered function called likelihood function The ML principle select value maxi mizes likelihood function observations training set x1 xm If observations sampled d common valid assump tion ML principle maximize argmax m P xi argmax log m P xi argmax m logP xi product nature problem convenient maximize log likelihood We closer look today ML principle introducing key element known relative entropy measure distributions ML Empirical Distribution The ML principle states empirical distribution d sequence examples closest possible terms relative entropy defined later true distribution To statement clear let X set symbols a1 let P probability belonging parametric family parameter drawing symbol X Let x1 xm sequence symbols drawn d according P The occurrence frequency f measures number draws symbol ML Empirical Distribution f xi let empirical distribution defined P X f f f f m f The joint probability P x1 xm equal product P xi according definitions equal P x1 xm m p xi X P f The ML principle equivalent optimization problem max P Q X P f Q q Rn q qi denote set n dimensional probability vectors probability simplex Let pi stand P ai fi stand f ai Since argmaxxz x argmaxx ln z x given ln p fi fi ln pi solution problem found setting partial derivative Lagrangian zero L p n fi ln pi pi ipi Lagrange multiplier associated equality constraint pi Lagrange multipliers associated inequality constraints pi We complementary slackness condition sets pi After setting partial derivative respect pi zero pi fi Assume fi n Then complementary slackness pi We left result pi fi Following constraint p1 obtain fi As result obtain P P In case fi use convention ln continuity arrive pi We arrived following theorem Theorem The empirical distribution estimate P unique Maximum Maximum Likelihood Maximum Entropy Duality Likelihood estimate probability model Q occurrence frequency f This like obvious result actually runs deep result holds particular non intuitive glance distance mea sure non negative vectors Let dist f p distance measure vectors The result states P argmin p dist f p s t p pi family distance measures dist It turns distance measure known relative entropy satisfies ML result stated Relative Entropy The relative entropy RE measure D x y non negative vec tors x y Rn defined D x y n xi ln xi yi xi yi In definition use convention ln based con tinuity ln y x ln x When x y probability vectors e belong Q D x y xi ln xi yi known Kullback Leibler divergence The RE measure distance metric symmetric D x y D y x satisfy triangle inequality Nevertheless interesting properties fundamental measure statistical inference The relative entropy non negative zero x y This comes log sum inequality xi ln xi yi xi ln xi yi Thus D x y xi ln xi yi xi yi x ln x y x y exactly picture bit complex Csiszar s measures dist p f P fi pi fi satisfy eqn provided exponential However dist f p parameters positions switched relative entropy satisfy eqn regardless order parameters p f Maximum Entropy Duality ML MaxEnt But ln b b b iff ln b b follows inequality ln x x x holds x x We state following theorem Theorem Let f occurrence frequency training sample P Q ML estimate iff P argmin p D f p s t p pi Proof D f p fi ln pi fi ln fi fi argmin p D f p argmax p fi ln pi argmax p ln p fi There related interesting points First proof Thm observe non negativity constraint p need enforced long f holds definition closest p f constraint pi come non negative Second fact closest point p f comes scaling f definition empirical distribution P arises relative entropy measure For example squares distance measure f p result scaling f In words looking projection vector f probability simplex e intersection hyperplane x non negative orthant x Under relative entropy projection simply scaling f need enforce non negativity Under sqaures projection hyper plane x non negative orthant Fig illustration So relative entropy special regard provides ML estimate simplifies optimization process noticeable handle latent class model lecture Maximum Entropy Duality ML MaxEnt The relative entropy measure symmetric expect different comes optimization minxD x y compared minyD x y The lat The fact non negativity comes free apply class distribution models This point refined lecture Maximum Likelihood Maximum Entropy Duality f p p Fig Projection non neagtaive vector f hyperplane xi Under relative entropy projection P scaling f lives probability simplex Under squares projection p2 lives outside probability simplex e negative coordinates ter e minP QD P0 P P0 empirical evidence Q model provides ML estimation For example lecture consider Q set low rank joint distributions called latent class model ML relative entropy minimization solution found Let H p pi ln pi denote entropy function With regard minxD x y state following observation Claim argmin p Q D p n argmax p Q H p Proof D p n pi ln pi pi ln n ln n H p follows condition pi In words closest distribution uniform achieved maxi mizing entropy To interesting need add constraints Consider linear constraint p ipi To concrete con Maximum Entropy Duality ML MaxEnt sider die faces thrown times wish estimate probabilities p1 p6 given average ipi Say average expect unbiased die The Laplace s prin ciple insufficient reasoning calls assuming uniformity additional information controversial assumption cases In words information pi pi choose uniform distribution reason choose distribution Thus employing Laplace s principle average likely distribution uniform What This kind problem stated optimization problem max p H p s t pi ipi We constraints aid Lagrange multipliers arrive result pi exp expi Note exponential pi non negativity comes free Following constraint pi exp exp obtain pi Z expi Z function normalization factor needs set later There special uniform distribution seeking probability vector p close possible prior probability p0 constraints min p D p p0 s t pi ipi result pi Z p0i exp We consider adding linear constraints p form fijpi bj j k The result pi Z p0i exp Pk j jfij Probability distributions form called Gibbs Distributions In Any measure class dist p p0 P p0i pi p0i minimized linear constraints satisfy result pi provided exponential Maximum Likelihood Maximum Entropy Duality practical applications linear constraints p arise average information system temperature fluid pi probabilities particles moving velocities rainfall data general environmental data pi represent probability finding animal colonies discrete locations 3D map A constraint form fijpi bj states expectation Ep fj equal empirical distribution E P fj P uniform given input Let P p Rn p pi Ep fj Ep fj j k Q q Rn q Gibbs distribution We consider looking ML solution parameters k Gibbs distribution min q Q D p q p uniform minD p q replaced max ln qi D n x ln n lnxi As turns MaxEnt ML duals intersection sets P Q contains single point solves problems Theorem The following equivalent MaxEnt q argminp PD p p0 ML q argminq QD p q q P Q In practice duality theorem recover parameters Gibbs distribution ML route second line theorem algorithm known iterative scaling algorithm EM Algorithm ML Mixture Distributions In Lecture saw Maximum Likelihood ML principle d data achieved minimizing relative entropy model Q occurrence frequency training data Specifically let x1 xm d xi X d d tupple symbols taken alphabet X having n different letters a1 Let P empirical joint distribu tion e array d dimensions axis n entries e entry P i1 id ij n represents normalized co occurrence d tupe ai1 aid training set x1 xm We wish find joint distribution P d array belongs model family distributions Q closest possible P relative entropy P argmin P Q D P P In lecture focus model distributions Q represents mixtures simple distributions H known latent class models A latent class model arises joint probability P X1 Xd observe e P generated observing samples x1 xm fact marginal P X1 Xd Y Y hidden latent random variable k different discrete values k Then P X1 Xd k j P X1 Xd Y j P Y j The idea given value hidden variable H problem recovering model P X1 Xd Y j belongs family joint distributions H relatively simple problem To idea clearer consider following example Assume coins The coin probability heads equal p second coin probability heads equal q At trial choose toss coin EM Algorithm ML Mixture Distributions probability coin probability Once coin chosen tossed times producing observation x We given set observations D x1 xm observation xi triplet coin tosses coin Given D construct empirical distribution P array defined P i1 i2 i3 m xi i1 i2 i3 m Let yi random variable associated observation xi yi xi generated coin yi xi generated coin If knew values yi task simply estimate separate Bernoulli distributions separating triplets generated coin generated coin Since yi known marginal P x x1 x2 x3 P x x1 x2 x3 y P y P x x1 x2 x3 y P y pni p ni qni q ni x1 x2 x3 triplet coin toss ni number heads triplet tosses In words likelihood P x triplet tosses x x1 x2 x3 linear combination mixture Bernoulli distributions Let H stand Bernoulli distributions H u d u n ui u d stands outer product u Rn d times e n way array indexed i1 id ij n value equal ui1 uid The model family Q mixture Bernoulli distributions Q k j jPj j j Pj H specifically coin toss example Q p p q q p q We entries P Q minimizes D P P set Q determined parameters p q For coin toss The EM Algorithm General example looks like argmin p q D P p p q q argmax p q i1 i2 i3 P i1i2i3 log pni123 p ni123 qni123 q ni123 ni123 i1 i2 i3 Trying work algorithm minimizing unknown parameters p q somewhat unpleasant families distributions H log sum present optimization function turn sum log task easier We able turn problem succession problems H single problem Q j jH Another point worth attention non negativity output variables simply minimizing relative entropy measure constraints class model Q guarantee non negative solution As shall breaking problem successions problems H non negativity free feature The technique turning log sum sum log finding ML solution mixture model known Expectation Maximization EM algorithm introduced Dempster Laird Rubin It based ideas introduce auxiliary variables ii use Jensen s inequality The EM Algorithm General Let D x1 xm represent training data xi X taken instance space X leave unspecified For leave matters general possible specifically independence assumptions data generation process The ML problem find setting parameters maximizes likelihood P x1 xm wish maximize P D parameters equivalent maximizing log likelihood argmax logP D log y P D y y represents hidden variables We denote L logP D EM Algorithm ML Mixture Distributions Let q y D arbitrary distribution hidden variables y con ditioned parameters input sample D e y q y D We define lower bound L follows L log y P D y log y q y D P D y q y D y q y D log P D y q y D Q q The inequality comes Jensen s inequality log j jaj j j log aj j j What obtained auxiliary function Q q satisfying L Q q distributions q y D The maximization Q q proceeds interleaving variables q separately ascend set variables At t iteration fix current value t t th iteration maximize Q q t q maximize Q q t q t argmax q Q q t t argmax Q q t The strategy EM algorithm maximize lower bound Q q hope ascend lower bound function ascend respect L The claim guarantees ascend Q generate ascend L Claim Jordan Bishop The optimal q y D t step P y D t Proof We Q P y D t t L t proves claim L Q q q best q distribution The EM Algorithm General hope find makes lower bound meet L t Q P y D t t y P y D t log P D y t P y D t y P y D t log P y D t P D t P y D t logP D t y P y D t L t The proof provides validity approach ascending lower bound Q q point t functions coincide e lower bound function t equal L t continue ascend Q guaranteed ascend L convergence guaranteed It shown omitted point convergence stationary point L shown originally C F Jeff Wu years EM introduced fairly general conditions The second step maximizing t argmax y P y D t logP D y This defines EM algorithm Often Expectation step described taking expectation Ey P y D t logP D y followed Maximization step finding maximizes expectation term EM algorithm Eqn describes principle algorithm general making assumptions statistical relationship data points hidden variable problem presented eqn unwieldy We reduce eqn manageable making d assumption This detailed following section manner deriving EM adapted Jordan Bishop s book notes EM Algorithm ML Mixture Distributions EM d Data The EM optimization presented eqn simplified assume data points hidden variable values d P D n P xi P D y n P xi yi P y D n P yi xi For yi y yi P y D y1 yn yi P y1 x1 P yn xn yi yi P yi xi yj P yj xj Substituting simplifications eqn obtain t argmax k j m P yi j xi t logP xi yi j yi k Back Coins Example We apply EM scheme running example mixture Bernoulli distributions We wish compute Q t y P y D t logP D y n j P yi j xi t logP xi yi j Back Coins Example maximize Q respect p q Q n P yi xi logP xi yi P yi n P yi xi logP xi yi P yi log p ni p ni log qni q ni stands t P yi xi The values known o po qo given previous iteration The Bayes formula compute P yi xi P xi yi P yi P xi op ni o po ni op ni o po ni o qnio qo ni We wish compute maxp q Q The partial derivative respect Q obtain update formula given k n The partial derivative respect p Q p ini p ni p obtain update formula p ni Likewise update rule q q ni To conclude start initial guess values p q com pute values update iteratively values p q end iteration new values computed EM Algorithm ML Mixture Distributions Gaussian Mixture The Gaussian mixture model assumes P x x Rd linear combination Gaussian distributions P x k j P x y j P y j P x y j d 2dj exp x cj j Normally distributed mean cj covariance matrix 2j I Let D x1 xm d sample data wish solve mean covariances individual Gaussians factors mixing coefficients j P y j In order clear parameters located write P x j instead P x y j j cj 2j mean variance j th factor We denote collection mixing coefficients j j j k Let w j auxiliary variables point xi factor y j standing w j P yi j xi The EM step eqn t argmax k j m w j t log jP xi j s t j j Note constraint j j The update formula w j use Bayes formula w j t P yi j t P xi yi j t P xi t Zi t j P xi t Zi scaling factor j w j The update formula j cj j follow taking partial derivatives eqn setting zero Taking partial derivatives respect Application Examples j cj j obtain update rules j m m w j cj iw j m w j ixi 2j d iw j m w j xi cj In words observations xi weighted w j Gaussian fitted k times factor Application Examples Gaussian Mixture Clustering The Gaussian mixture model classically clustering applications In clustering application receives sample points x1 xm point resides Rd The task learner case unsupervised learning group m points k sets Let yi k m stands required labeling The clustering solution assignment values y1 ym according clustering criteria In Gaussian mixture model points clustered arise Gaussian distribution The EM algorithm provides proba bilistic assignment P yi j xi denoted w j Multinomial Mixture bag words Application The multinomial mixture coins example toyed typically representing count data representing text documents high dimensional vectors A vector representation text document asso ciates word fixed vocabulary coordinate entry vector The value entry represents number times particular word appeared document If ignore order words ap peared count frequency set documents d1 dm set words w1 wn jointly represented co occurence n m matrix G Gij contains number times word wi appeared doc ument dj If scale G ij Gij distribution P w d This kind representation set documents called bag words EM Algorithm ML Mixture Distributions For purposes search filtering desired reveal additional infor mation words documents topic document belongs topics word associated This similar clustering task documents associated topic clustered This achieved considering topics value latent variable y P w d y P w d y P y y P w y P d y P y assumption w d y e words documents conditionally independent given topic The conditional independent assumption gives rise multinomial mixture model To specific ley y k denote k possible topics let j P y j note j j latent class model P w d k j jP w y j P d y j Note P w y j vector denote uj Rn P d y j vector denote vj Rm The term P w y j P d y j stands outer product ujv j vectors e rank n m matrix The Maximum Likelihood estimation problem find vectors u1 uk v1 vk scalars k empirical distribution represented unit scaled matrix G close possible relative entropy measure low rank matrix j jujv j subject constraints non negativity j j uj vj unit scaled uj vj Let xi w d stand th example q example pair word document w n index word alphabet d m index document The EM algorithm involves following optimization step t argmax q k j P yi j xi t logP xi yi j argmax q k j w t ij log juj w vj d s t uj vj An update rule ujr r th entry uj derived derivative Application Examples Lagrangian ujr q w t ij log uj w ujr ujr N r log ujr w r w t ij ujr N r w r w t ij ujr N r stands frequency word wr documents d1 dm Note N r result summing r th row G vector N N n marginal P w d P w d Given constraint uj obtain update rule ujr N r w r w t ij n s 1N s w sw t ij Update rules remaining unknowns similarly derived Once EM converged w r w ij probability word wr belong j th topic d sw ij probability s th document comes j th topic Support Vector Machines Kernel Functions In lecture begin exploration class hyperplane separation problem We given training set instances xi Rn m class labels yi e training set positive negative examples We wish find hyperplane direction w Rn offset scalar b w xi b positive examples w xi b negative examples means margins yi w xi b positive Assuming hyperplane exists clearly unique We need introduce constraint find sensible solution infinitley possible hyperplanes separate training data Another issue framework limited sense real world classification problems somewhat unlikely exist linear separating function begin We need find way extend framework include non linear decision boundaries reasonable cost These issues focus lecture Regarding issue separating hyper plane assuming training data linearly separable question need ask solutions best generalization properties In words goal constructing learning machine necessarily perfect training data training data merely sample instance space necessarily representative sample simply sample Therefore sample training data necessarily guarantee imply entire instance space The goal constructing learning machine maximize performance test data instances haven t seen turn means Large Margin Classifier Quadratic Linear Programming wish generalize good classification performance training set entire instance space A related issue generalization distribution generate training data unknown Unlike statistical inference material far time attempt estimate distribution The reason derive optimal learning algorithms bypass need estimating distributions explained later course PAC learning introduced For focus algorithmic aspect learning problem The idea consider subset C hyperplanes fixed margin margin defined distance closest training point hyperplane min yi w xi b w The Support Vector Machine SVM introduced Vapnik colleagues seeks separating hyperplane simultaneously min imizes empirical error maximizes margin The idea maximiz ing margin intuitively appealing decision boundary lies close training instances likely generalize learning machine susceptible small perturbations instance vectors A formal motivation approach deferred PAC learning material introduce later course Large Margin Classifier Quadratic Linear Programming We like set linear separating hyperplane optimiza tion problem consistent training data maximizes margin induce separating hyperplane possible consistent hyperplanes Formally speaking distance point x hyperplane defined w x b w w Since allowed scale parameters w b note w x b w x b set distance boundary points hyperplane w w scaling w b point s smallest margin closest hyperplane normalized w x b margin simply w w Fig Note argmaxw2 w w equivalent argmaxw2 w w Support Vector Machines Kernel Functions turn equivalent argminw w w Since positive points negative points farther away boundary points separability constraints w x b x positive instance w x b x negative instance Both separability constraints combined y w x b Taken defined following optimization problem min w b w w subject yi w xi b m This type optimization problem quadratic criteria function linear inequalities known literature Quadratic Linear Pro gramming QP type problem This particular QP requires training data linearly separable condition unrealistic We relax condi tion introducing concept soft margin separability holds approximately error min w b w w l subject yi w xi b m Where pre defined weighting factor The non negative vari ables allow data points miss classified creating approx imate separation Specifically xi positive instance yi soft constraint w xi b original constraint xi boundary point laying away half space assigned positive instances When point xi reside inside margin half space assigned negative instances Likewise xi negative instance yi soft constraint w xi b Large Margin Classifier Quadratic Linear Programming bw maximize margin w j Fig Separating hyperplane w b maximal margin The boundary points associated non vanishing Lagrange multipliers margin errors associated criteria function encourages small number margin errors The criterion function penalizes L1 norm non vanishing overall system seek solution possible margin errors Fig Typically possible L1 norm preferable L2 norm overly weighs high magnitude outliers cases dominate energy function Another note strictly speaking right thing penalize margin errors based L0 norm e number non zero entries drop balancing parameter This matter far away point hyperplane matters point classified correctly definition empirical error Lecture The problem optimization problem longer convex non convex problems notoriously difficult solve Moreover class convex optimization problems described Eqn solved polynomial time complexity So far described problem formulation solved provide solution sensible generalization properties Although proceed shelf QLP solver pursue dual problem The dual form highlight key properties approach enable extend framework handle non linear Support Vector Machines Kernel Functions decision surfaces little cost In appendix brief tour basic principles associated constrained optimization Karush Kuhn Tucker KKT theorem dual form Those recommended read moving section The Support Vector Machine We return primal problem eqn representing maximal margin separating hyperplane margin errors min w b w w l subject yi w xi b m We derive Lagrangian Dual problem By new key property emerge facilitated fact criteria func tion note equality constraints need involves inner products training instance vectors xi This prop erty form key mapping original input space dimension n higher dimensional space allowing non linear decision surfaces separating training data Note particular problem strong duality conditions satisfied criteria function inequality constraints form convex set The Lagrangian takes following form L w b w w m m yi w xi b m Recall min w b L w b Since minimum obtained vanishing partial derivatives Lagrangian respect w b step evaluate The Support Vector Machine constraints substitute L obtain L w w iyixi L b iyi L From constraint obtain w iyixi w de scribed linear combination subset training instances The reason instances participate linear superposition KKT conditions yi w xi b e instance xi classified correctly boundary point conversely yi w xi b e xi boundary point xi margin error note margin error instance value smallest possible required reach equality constraint criteria function penalizes large values The boundary points margin errors called support vectors w defined support vectors The constraint equivalent constraint l Also note e point xi margin error point KKT conditions As result Therefore based values following classifications point xi margin margin error points xi margin error point point xi margin Substituting results constraints Lagrangian L obtain dual problem max m m j ijyiyjxi xj subject m m yii The criterion function written compact manner Support Vector Machines Kernel Functions follows Let M l l matrix entries Mij yiyjxi xj M vector vector m transpose row vector Note M positive definite e x Mx vectors x property important later The key feature dual problem simpler primal fact isn t primal equality constraints elegant feel key feature problem completely described inner products training instances xi m This fact shown crucial ingredient called kernel trick computation inner products high dimensional spaces simple functions defined pairs training instances The Kernel Trick We ended dual formulation SVM problem noticed input data vectors xi represented Gram matrix M In words inner products input vectors play role dual mulation explicit use xi function xi inner products This observation suggests use known kernel trick replace inner products non linear functions The common principle kernel methods construct nonlinear vari ants linear algorithms substituting inner products nonlinear kernel functions Under certain conditions process interpreted map ping original measurement vectors called input space higher dimensional space possibly infinitely high commonly referred feature space Mathematically kernel approach defined follows let x1 xl vectors input space Rn con sider mapping x Rn F F inner product space The kernel trick calculate inner product F kernel function k Rn Rn R k xi xj xi xj avoiding explicit mappings evaluation Common choices kernel selection include d th order polynomial kernels k xi xj x xj d Gaussian RBF kernels k xi xj exp xi xj If algorithm restated input vectors appear terms inner products substitute inner products kernel function The resulting kernel algorithm interpreted running original algorithm space F mapped objects x We know M dual form positive semi definite M The Kernel Trick written M Q Q Q y1x1 ylxl Therefore x Mx Qx choices x means eigenvalues M non negative If entries M replaced yiyjk xi xj condition enforce function k positive definite kernel function A positive definite function defined set vectors x1 xq values q matrix K entries Kij k xi xj positive semi definite Formally conditions admissible kernels k known Mercer s conditions summarized Theorem Mercer s Conditions Let k x y symmetric contin uous The following conditions equivalent k x y ii x y x y uniformly converg ing series ii satisfying x x dx x y k x y x y dxdy iii xi q q matrix Kij k xi xj positive semi definite Perhaps non obvious condition No allows feature map infinitely coordinates vector Hilbert space For example shall kernel exp xi xj inner product vectors infinitely coordinates We consider number popular kernels The Homogeneous Polynomial Kernel Let x y Rk define k x y x y d d natural number Then corresponding feature map x k d d O kd coordinates value x d n1 nk xn11 x nk k ni P ni d d n1 nk d n1 nk multinomial coefficient number ways distribute d balls k bins j th bin hold exactly nj balls x1 xk d ni P ni d d n1 nk xn11 x nk k Support Vector Machines Kernel Functions The dimension vector space x x Rk measured following combinatorial problem arrangements k partitions placed d items answer k d k k d d O kd For example k d gives x y x21y 2x1x2y1y2 x 2y x y x x21 x 2x1x2 The non homogeneous Polynomial Kernel The feature map x contains monomials power lesser equal d e ni d This acheived increasing dimension k nk fill gap k ni d d Therefore dimension x x Rk k d d We x y d x1y1 xkyk d ni Pk ni d d n1 nk xn11 y n1 x nk y nk nk 2nk Therefore entries vector x values x d n1 nk xn11 x nk k nk ni Pk ni d For example k d gives x y x21y 2x1x2y1y2 x 2y 2x1y1 2x2y2 x y x x21 x 2x1x2 2x1 2x2 In example mapping fromR2 toR6 hyperplanes w x b inR6 correspond conics R2 w21 x w x2 2w1w2 x1x2 2w1 x1 2w2 x2 b Assume like find separating conic Parabola Hyperbola Ellipse function line R2 The discussion far suggests construct Gram matrix M dual form d polynomial kernel k x y x y parameter choosing The extra effort need invest negligible simply replace occurrence x xj x xj The Kernel Trick The RBF Kernel The function k x y e x y known Radial Basis Function RBF kernel function infinite expansion Without loss generality let e x y e x 2e y 2ex y j x y j j e x 2e y j e x 22j j j e y 2j j j x y j j P ni j e x 2j j j j n1 nk xn11 x nk k e y 2j j j j n1 nk yn11 y nk k From entries feature map x x e x 22j j j j n1 nk xn11 x nk k j Pk ni j Classifying New Instances By adopting kernel k fact mapping x x proceed solve w b QLP solver The QLP solution dual form yield solution Lagrange multipliers m We saw eqn express w function mapped examples w iyi xi Rather explicitly representing w task prohibitly expensive general dimension feature space polynomial mapping k d d store support vectors input vectors corresponding use evaluation test examples f x sign w x b sign iyi xi x b sign iyik xi x b Support Vector Machines Kernel Functions We kernel trick enabled look non linear separating surface making implicit mapping input space higher di mensional feature space dual form SVM formulation change required way Gram matrix constructed The price paid convenience carry support vectors time classification f x A couple notes worthwhile point The constant b recovered support vectors Say x positive support vector margin error e Then w x b b recovered The second note number support vectors typically number training examples empirically Thus computational load evaluation f x relatively high Approximations proposed literature looking reduced number support vectors necessarily aligned training set scope course The kernel trick gained popularity introduction SVM taken life applied principal component analysis PCA ridge regression canonical correlation analysis CCA QR factorization list goes We meet kernel trick later Spectral Analysis I PCA LDA CCA In lecture following focus spectral methods learning Today focus dimensionality reduction Principle Component Analysis PCA multi class learning Linear Discriminant Analysis LDA Canonical Correlation Analysis CCA In lecture focus spectral clustering methods Dimensionality reduction appears dimension input vector large imagine pixels image example coordi nate measurements highly inter dependent imagine redun dancy present neighboring pixels image High dimensional data impose computational efficiency challenges translate poor generalization abilities learning engine lectures PAC A di mensionality reduction viewed feature extraction process takes input large feature set original measurements creates smaller number new features fed learning engine In lecture focus feature extraction specific constrained stanpoint We looking mixing linear combina tion input coordinates obtain linear projection Rn Rq q n In wish reduce redundancy preserving possible variance data From sta tistical standpoint achieved transforming new set variables called principal components uncorrelated retain variation present original coordinates For example image processing application input images highly dundant neighboring pixel values highly correlated The purpose feature extraction transform input image vector output components redundancy possible Form geometric standpoint achieved finding closest squares sense Spectral Analysis I PCA LDA CCA linear q dimensional susbspace m sample points S The new sub space lower dimensional best approximation sample S These equivalent perspectives data compression dimensionality reduc tion form central idea principal component analysis PCA probably oldest going Pearson best known techniques multivariate analysis statistics The computation PCA simple definition straightforward wide variety different applications number different derivations number different terminologies especially outside statistical literature basis number variations basic technique We extend variance preserving approach data representation labeled data sets We describe linear classifier approach sepa rating hyperplane form point view looking hyperplane data projected separation maximized dis tance class means maximal data class compact variance spread minimized The solution produced like PCA spectral analysis data This approach goes Fisher s Linear Discriminant Analysis LDA What common PCA LDA use spectral ma trix analysis e eigenvalues eigenvectors matrices representing subspaces data ii techniques produce optimal results normally distributed data easy imple ment There large variety uses spectral analysis statistical learning literature including spectral clustering Multi Dimensional Scaling MDS data modeling general Another point note time course type data distribution plays role analysis techniques defined distribution optimal Gaussian distribution We describe non linear extension PCA known Kernel PCA focus PCA analysis couple vantage points PCA optimal reconstruction dimension reduction e data compression ii PCA redundancy reduction decorrelation output components PCA Statistical Perspective Let x1 xm Rn sample data S vectors Rn arranged columns matrix A It convenient assume data centered e xi If data centered center computing mean vector m xi replace original data PCA Statistical Perspective sample new sample xi In statistical sense coordinates vector x Rn considered random variables row matrix A sample values particular random variable drawn unknown probability distribution associated row position We wish find vectors u1 uq arranged columns matrix U q min n m new feature measurements y U x result linear combinations u x u q x original feature measurements x certain desirable properties The idea property seek new coordinates y statistical inde pendence e P y1 yq P y1 P yq mean removed redundancy original data x best possible manner This goal ask linear transformation instead ask weaker property hold pairwise co variance cov yi yj vanishes e covariance matrix new coordinates diagonal A diagonal covariance insures redundancy moval good statistical independence However data Normally distributed P x N mean covariance transformation diagonalizes covariance matrix guarantees statistical independence Among transformations de correlate data seek maximizes spread variance sample data projected new axes vectors Maximizing Variance Output Coordinates The property like maximize projection sample data new axes spread possible To start analysis assume q e n components input vector x reduced single output component y u x We looking single vector u Rn direction maximizes variance output component y Formally looking unit vector u maximizes u xi Appendix A basic statistical definitions note E y u xi u xi In words projected points axis represented vector u spread possible squares sense In vector notation optimization problem takes following form max u u A subject u u The Lagrangian problem L u u AA u u u Spectral Analysis I PCA LDA CCA By taking partial derivative L u obtain following necessary condition Appendix B AA u u tells u eigenvector n n symmetric positive definite matrix AA There n eigenvectors associated AA easily convince looking associated maximal eigenvalue substitute u instead AA u criterion function u AA u obtain u u eigenvalues positive AA positive definite optimum obtained maximal eigenvalue The leading eigenvector u AA called principal axis data sample represented columns matrix A y u x called principal component data sample For convenience denote u1 u leading eigenvector eigenvalue AA Next look y2 u x uncorrelated y1 u x maximum variance u3 uq Two random variables uncorrelated covariance vanishes By definition covariance Appendix A obtain Cov y1y2 u xi u xi u xix u2 u AA u2 u AA u1 1u u2 We use condition u u2 specify zero correlation y1 y2 The functional optimized max u2 u A subject u u2 u u2 Lagrangian L u2 u AA u2 u u2 u u2 By taking partial derivative respect u2 obtain necessary condition AA u2 u2 u1 Multiply equation u1 left u AA u2 u u2 u u1 noting u AA u2 u u2 obtain As result obtain AA u2 u2 PCA Statistical Perspective u2 form eigenvalue eigenvector pair AA As large possible remaining spectral de composition By induction shown remaining principal vectors u3 uq decreasing order eigenvactors AA vari ance th principal component yi u x Taken PCA solution following optimization problem max u1 uq u A subject u ui u uj j q It useful later write optimization function concise manner follows Let U n q matrix columns ui D diag q q q diagonal matrix q Then U U I AA U UD Using fact trace xy x y trace AB trace BA trace A B trace A trace B convert u A trace U AA U follows u AA ui trace A uiu A trace A uiu A trace A UU A trace U AA U Thus PCA solution following optimization function max U Rn q trace U AA U subject U U I The solution saw U u1 uq consists decreasing order eigenvectors AA At optimum trace U AA U equal trace D equal sum eigenvalues q It worthwhile noting q n UU U U I PCA transform change basis Rn known Karhunen Loeve transform To conclude PCA transform looks q orthogonal direction vectors called principal axes projection input sample vectors principal directions maximal spread equivalently variance output coordinates y U x maximal The principal directions leading respect descending eigenvalues q eigen vectors matrix AA When q n principal directions form basis Rn property de correlating data maximizing variance coordinates sample input vectors Spectral Analysis I PCA LDA CCA Decorrelation Diagonalization Covariance Matrix In previous section saw PCA generates new coordinate system y U x coordinates y1 yq x new system uncorre lated This means covariance matrix principle components diagonal In section explore perspective detail The covariance matrix x sample data x1 xm zero mean m xix m AA matrix AA derived scaled version co variance sample data Appendix A The scale factor m unimportant process eigenvectors unit norm scale AA produce set eigenvectors The diagonal entries covariance matrix x represent corre lation measure statistical dependence th j th com ponent vectors e entries input vectors x The existence correlations components features input signal sign redundancy point view transforming input representation redundant like find transformation y U x output representation y associ ated diagonal covariance matrix y e components y uncorrelated Formally y m yiy m U AA U wish find n q matrix U AA U diagonal If addition require variance output coordinates maximized e trace U AA U maximal need constrain length column vectors U e set ui unique solution U columns orthonormal defined q eigenvectors covariance matrix x This exactly optimization problem defined eqn We PCA decorrelates input data Decorrelation statistical independence thing If coordinates statistically independent covariance matrix diagonal follow uncorrelated variables statistically independent covariance measure dependence In fact covariance measure pairwise dependency However fact uncorrelated xy P x P y x x y y p x y P x P y x x y y p x p y P x x x p x P y y y p y PCA Optimal Reconstruction variables statistically independent multivariate normal distribution Gaussian In words sample data x drawn probability distribution p x Gaussian form PCA transforms sample data statistically independent set variables y U x The details explained Recall multivariate normal distribution random variables x x1 xn defined p x N p x n e x x Also recall linear combination variables produces normal distribution N U U U y y y y y y x U x U x U x U x U xU choose U y U U diagonal matrix y diag n We case p x n e P xi written product univariate normal distributions pxi xi p x n 2i e xi n pxi xi proves assertion decorrelated normally distributed variables statistically independent PCA Optimal Reconstruction A different equivalent perspective PCA transformation optimal reconstruction squares sense dimension reduction We given sample data x1 xm looking small number orthonormal principal vectors u1 uq q min n k define q dimensional linear subspace Rn best approximate original input vectors squares sense In words projection x sample points xi q dimensional subspace minimize xi x possible q dimensional subspaces Rn Let U subspace spanned principal vectors columns U let P n n projection matrix mapping point x Rn projection x U From definition projection vector x x Spectral Analysis I PCA LDA CCA orthogonal subspace U Let y y1 yq coordinates x respect principal vectors e Uy x Then orthogonality x Uy Uw vectors w Rn Since true w U Uy U x Therefore y U U 1U x result projection matrix P P U U U 1U satisfying Px x In case columns U orthonormal U U I P UU We ready describe optimization problem U wish find orthonormal set principal vectors U U I xi UU xi minimized Note xi UU xi A UU A 2F B F j b ij square Frobenious norm matrix The optimal reconstruction problem min U A UU A 2F subject U U I We argmin U A UU A 2F argmax U trace U AA U shows optimal reconstruction problem solved PCA recall Eqn From identity B 2F trace BB A UU A 2F trace A UU A A UU A Expanding right hand gives trace A UU A A UU A trace AA trace AA UU trace UU AA trace UU AA UU The second term equal commutativity trace equal 4th term commutativity trace U U I Taken A UU A 2F trace AA trace U AA U To conclude proven taking q eigenvectors AA obtain linear subspace close possible squares sense original sample data Hence PCA viewed vehi cle optimal reconstruction dimension reduction The optimization The Case n m problem solution leading q eigenvectors AA described eqn max U Rn q trace U AA U subject U U I The Case n m Consider situation n dimension input vectors rela tively large compared number sample vectors m For example con sider input vectors representing sized images faces e n m In words looking small number face templates known eigenfaces approximate original set face images In case AA large num ber non vanishing eigenvalues higher Given eigendecomposition process O computational burden high However possible perform eigendecomposition A A matrix instead shown Let columns ofQ q m eigenvectors A A e A AQ QD D diagonal containing corresponding eigenvalues After pre multiplying sides A obtain AA AQ AQ D conclude AQ contains q eigenvectors un normalized AA We U AQD U U D 2Q A AQD D 2DD I fact Q A AQ D Note eigenvalues A A AA AA AQD AQD D Kernel PCA We case n m described previous section step fur ther consider large values n practically uncomputable situation results mapping original input vectors high dimensional space x Rn F dim F n For example x representing d th order monomials coordinates x e dim F n d d exponential d The mappings interest paired non linear kernel function k x x x x Performing PCA A x1 xm equivalent finding Spectral Analysis I PCA LDA CCA non linear surface Rn nature non linearity depends choice best approximates original sample data x1 xm The problem AA computable A A computable A A ij k xi xj From previous section U AQD AV contains q eigen vectors AA Q D computable Since A computable represent U explicitly project new vector x principal directions u1 uq obtain principal components e output vector y U x follows y U x V A x V k x1 x k xm x Given principal components entries y U x x measure example distance x projection x UU x Uy linear subspace spanned u1 uq need explicitly compute principal axes ui follows x x x x x x x x k x x y U Uy x UU x k x x y y 2y y k x x y Fisher s LDA Basic Idea We extend variance preserving approach data representation labeled data sets We focus class sets look separating hyperplane f x w x b x belongs class f x x belongs second class f x In statistical literature type function called linear discriminant function The decision boundary given set points satisfying f x hyperplane Fisher s Linear Discriminant Analysis LDA variance preserving approach finding linear discriminant function Fisher s LDA Basic Idea Fig Linear discriminant analysis based class centers sufficient Seeking projection maximizes distance projected centers prefer horizontal axis vertical classes overlap horizontal axis The projected distance vertical axis smaller classes better separated The conclusion sample variance classes taken consideration We introduce popular statistical technique called Canon ical Correlation Analysis CCA learning mapping input output vectors notion angle subspaces What common techniques PCA LDA CCA use spectral matrix analysis e eigenvalues eigenvectors matrices representing subspaces data These tech niques produce optimal results normally distributed data easy implement There large variety uses spectral analysis statistical learning literature including spectral clustering Multi Di mensional Scaling MDS data modeling general To appreciate general idea Fisher s LDA consider Fig Let centers classes denoted respectively A linear discriminant function projection 1D subspace classes separated 1D subspace The obvious step kind analysis sure projected centers separated possible We easily direction 1D subspace proportional follows w w w w w w The right hand term maximized w As illustrated Spectral Analysis I PCA LDA CCA Fig type consideration sufficient capture separability projected subspace spread variance data points centers play important role For example horizontal axis figure separates centers better vertical axis hand worse job separating classes way data points spread centers The argument favor separating centers work data points living hyper sphere centers sufficient The basic idea Fisher s LDA consider sample covariance matrix individual classes centers following way The optimal 1D projection maximizes variance projected centers minimizes variance projected data points class separately Mathematically idea implemented maximizes following ratio max w s21 s s21 scaled variance projected points class s21 xi C1 x likewise s22 xi C2 x x w w xi b We formalize approach derive solution We begin general description multiclass problem sample data points belong q different classes later focus case q Fisher s LDA General Derivation Let sample data points S members q classes C1 Cq number points belonging class Ci denoted li total number training set l li Let j denote center class Ci denote center complete training set S j lj bfxi Cj xi l xi S xi Fisher s LDA General Derivation Let Aj matrix associated class Cj columns consists mean shifted data points Aj x1 j xlj j xi Cj Then lj AjA j covariance matrix associated class Cj Let Sw w stands sum class covariance matrices Sw q lj AjA j From discussion previous section w w Sww wish minimize To note xi Cj x j xi Cj w xi j w w w AjA j w Let B matrix holding class centers B q let Sb q BB b stands From discussion w w Sbw wish maximize Taken wish maximize ratio called Rayleigh s quotient max w J w w Sbw w Sww The necessary condition optimality J w Sbw w Sww Sww w Sbw w Sww From obtain generalized eigensystem Sbw J w Sww That w leading eigenvector S 1w Sb assuming Sw invertible The general case finding q axes involves finding leading general ized eigenvectors Sb Sw derivation scope lecture Note S 1w Sb symmetric real value solution complication pursue course Instead focus class q setting Spectral Analysis I PCA LDA CCA Fisher s LDA class The general derivation simplified classes The covariance matrix BB rank matrix BB As result BB w vector direction Therefore solution w eqn w S 1w The decision boundary w x x S 1w S 1w This decision boundary surface course consider Bayseian inference It shown decision boundary Maximum Likelihood solution case classes normally distributed means covariance matrix Sw LDA versus SVM Both LDA SVM search called optimal linear discriminant function difference The heart matter lies definition constitutes sufficient compact representation data In LDA assumption class represented mean vector spread e covariance matrix This true normally distributed data true general This means expect LDA produce optimal discriminant linear function classes normally distributed With SVM hand assumption data distributed Instead emerging result data represented subset data points lie boundary classes called support vectors Rather making parametric assumption data captured e mean covariance theory shows data captured special subset points The tools result naturally complex quadratic linear programming versus spectral matrix analysis advantage optimality guaranteed making assumptions distribution data e distribution free It shown SVM LDA produce result class data normally distributed Canonical Correlation Analysis Canonical Correlation Analysis CCA technique learning mapping f x y x Rk y Rs notion subspace similarity extension inner product vectors training set xi yi n Such mapping y point Rk opposed discrete set labels referred regression opposed classification Like PCA LDA approach look projection axes projection input output vectors axes satisfy certain requirements like PCA LDA tools matrix spectral analysis It convenient stack vectors rows input matrix A output matrix B Let A n k matrix rows x x n B n s matrix rows y y n Consider vectors u Rk v Rs project input output data producing Au x u x nu Bv The requirement like place projection axes Au Bv words Au Bv maximal The requirement projection input points u axis similar projection output points v axis If extend notion multiple axes u1 uq necessarily orthogonal v1 vq q min k s requirement new coordinates input points projected subspace spanned u vectors similar new coordinates output points projected subspace spanned v vectors In words wish find q dimensional subspaces Rk Rs sets projected points aligned possible CCA goes step makes assumption input output relationship solely determined relation angles column spaces A B In words particular columns A important important space UA spanned columns Since g Au point UA linear combination columns A h Bv point UB g h cosine angle cos axes provided normalize vectors g h If continue line reasoning recursively obtain set angles q called principal angles subspaces uniquely defined cos j maxg UA max h UB g h Spectral Analysis I PCA LDA CCA subject g g h h h hi g gi j As result obtain following optimization function axes u v max u v u A Bv s t Au Bv To solve problem perform QR factorization A B A QR factorization matrix A Grahm Schmidt process resulting orthonormal set vectors arranged columns matrix QA column space equal column space A matrix RA contains coefficients linear combination columns QA A QARA Since orthoganilzation unique Grahm Schmidt process perfroms orthogonalization RA upper diagonal matrix Likewise let B QBRB Because column spaces A QA u exists u Au QAu Our optimization problem max u v u Q AQBv s t u v The solution problem u v leading singular vectors Q AQB The singular value decomposition SVD matrix E decomposition E UDV columns U leading eigen vectors EE rows V leading eigenvectors E E D diagonal matrix entries corresponding square eigen values note eigenvalues EE E E The SVD decomposition property q leading eigenvectors UDV closest squares sense rank q matrix E Therefore let U DV SVD Q AQB q eigenvectors Then sought axes U u1 uq simply R A U likewise axes V v1 vq equal R B V The axes called canon ical vectors vectors gi Aui mutually orthogonal called variates The concept principal angles Jordan Hotelling introduce recursive definition Given new vector x Rk resulting vector y found solving linear system U x V y assumption new basis coordinates x y similar To conclude relationship A B captured creating similar variates e creating subspaces dimension q projec tions input vectors output vectors similar coordinates Canonical Correlation Analysis The process obtaining q dimensional subspaces performing QR factorization A B followed SVD Here spectral analysis input output data matrices plays pivoting role input output association Spectral Analysis II Clustering In previous lecture ended formulation max Gm k trace G KG s t G G I showed solution G leading eigenvectors symmetric pos itive semi definite matrix K When K AA sample covariance matrix A x1 xm xi Rn eigenvectors form basis k dimensional subspace Rn closest L2 norm sense sample points xi The axes called principal axes g1 gk preserve variance original data sense projection data points g1 maximum variance projection g2 maximum variance vectors orthogonal g1 etc The spectral decomposition sample covariance matrix way compress data means linear super position original coordinates y G x We ended ratio formulation max w w S1w w S2w S1 S2 scatter matrices defined w S1w variance class centers wish maximize w S2w sum class variance want minimize The solution w generalized eigenvector S1w S2w maximal In lecture additional applications search leading eigenvectors plays pivotal solution So far seen spectral analysis relates PCA LDA today fo cus classic Data Clustering problem partitioning set points x1 xm k classes e generating output indicator variables y1 ym yi k We begin K means algorithm clustering optimization criteria relates K means Algorithm Clustering grapth theoretic approaches like Min Cut Ratio Cut Normalized Cuts spectral decomposition K means Algorithm Clustering The K means formulation originally introduced assumes clusters defined distance points class centers In words goal clustering find k mean vectors c1 ck provide cluster assignment yi k point xi set The K means algorithm based interleaving approach cluster assignments yi established given centers centers computed given assignments The optimization criterion follows min y1 ym c1 ck k j yi j xi cj Assume c1 ck given previous iteration yi argmin j xi cj assume y1 ym cluster assignments given set S m S j S xj argmin c j S xj c In words given estimated centers current round new assignments computed closest center point xi given updated assignments new centers estimated taking mean cluster Since step guaranteed reduce optimiza tion energy process converge local optimum The drawback K means algorithm quality local optimum strongly depends initial guess centers assignments If start wild guess centers fairly unlikely process converge good local minimum e close global optimum An alternative approach define approximate simpler problem closed form solution obtained computing eigenvectors matrix The global optimum K means NP Complete problem mentioned briefly section Next rewrite K means optimization criterion matrix form relates spectral formulation eqn Spectral Analysis II Clustering Matrix Formulation K means We rewrite eqn follows Instead carrying class variables yi define class sets k n j n j The K means optimization criterion seeks centers class sets min k c1 ck k j j xi cj Let lj j following expansion squared norm dropping x xi end equivalent problem min k c1 ck k j ljc j cj k j j x cj Next substitute cj definition lj j xj obtain new equivalent formulation centers cj eliminated form considera tion min k k j lj r s j x r xs conveniently written maximization problem max k k j lj r s j x r xs Since resulting formulation involves inner products replaced xi xi eqn mapping chosen xi xj replaced non linear function xi xj known kernel trick discussed previous lectures Having ability map input vectors high dimensional space K means applied provides flexibility increases chances getting good clustering global K means solution local optimum depends initial conditions bad The RBF kernel popular context xi xj e xi xj pre determined parameter Note xi xj interpreted loosely probability xi xj clustered Let Kij xi xj making K m m symmetric positive semi definite matrix referred affinity matrix Let F n n matrix entries Fij lr j r class r Fij K means Algorithm Clustering In words sort points xi according cluster membership F block diagonal matrix blocks F1 Fk Fr lr lr lr block s scaled lr Then Eqn written terms K follows max F n j KijFij trace KF In order form optimization problem need represent structure F terms constraints Let G n k column scaled indicator matrix Gij lj j e xi belongs j th class Gij Let g1 gk columns G easily verified grgr diag Fr F j gjg j GG Since trace AB trace BA write eqn terms G max G trace G KG conditions G need spell We start necessary conditions Clearly G non negative entries Because point belongs exactly cluster G Gij j G Gii li G G I Furthermore rows columns F GG sum e F1 F means F doubly stochastic translates constraint GG G We necessary conditions G G ii G G I iii GG The claim asserts sufficient conditions Claim The feasibility set matrices G satisfy conditions G GG G G I form Gij lj xi j Proof From G g r gs GirGis e G single non vanishing element row It convenient assume points sorted according class membership columns G non vanishing entries consecutive order let lj number non vanishing entries column gj Let uj vector lj entries holding non vanishing entries gj Then doubly stochastic constraint GG results uj uj j k Multiplying sides yields uj lj uj lj Spectral Analysis II Clustering This completes equivalence matrix formulation max G Rm k trace G KG s t G G G I GG original K means formulation eqn We obtained optimization criteria eqn addi tional constraints G non negative GG doubly stochastic The constraint G G I comes requirement point assigned class The doubly stochastic constraint comes class balancing requirement expand Min Cut We arrive eqn graph theoretic perspective We start representing graph Min Cut problem matrix form follows A convenient way represent data clustered undirected graph edge weights V m vertex set E V V edge set E R positive weight function Vertices graph correspond data points xi edges represent neighborhood relationships edge weights represent similarity affinity pairs linked vertices The weight adjacency matrix K holds weights Kij j j E Kij A cut graph defined disjoint sets A B V A B V sum edge weights connecting sets cut A B A j BKij measure dissimilarity sets The Min Cut problem find minimal weight cut graph solved polynomial time Max Network Flow solution The following claim associates algebraic conditions G indicator matrix Claim The feasibility set matrices G satisfy conditions G G1 G G D diagonal matrix D form Gij xi j Proof Let G g1 gk From G g r gs GirGis e G single non vanishing element row From G1 single non vanishing entry row value In case classes k function tr G KG equal j Kij j Kij Therefore maxG tr G KG equivalent Spectral Clustering Ratio Cuts Normalized Cuts minimizing cut j Kij As result Min Cut problem equivalent solving optimization problem max G Rm tr G KG s t G G1 G G diag We close eqn difference G orthogonal instead orthonormal doubly stochasitc constraint replaced G1 The difference bridged considering balancing requirement Min Cut produce unbalanced partition set vertices large contains spurious set vertices having small number edges larger set This undesirable outcome context clustering Consider balancing constraint G m k makes strict requirement k clusters equal number points We relax balancing constraint slightly combining balancing constraint G1 single constraint GG m k e GG scaled doubly stochastic Note conditions GG m k G G D result D m k I Thus propose relaxed balanced hard clustering scheme max G tr G KG s t G GG m k G G m k I The scale m k global scale dropped affecting resulting solution Min Cut relaxed balancing requirement eqn saw equivalent K means max G tr G KG s t G GG G G I Spectral Clustering Ratio Cuts Normalized Cuts We saw doubly stochastic constraint bal ancing desire A relaxation balancing desire perform optimization steps replace affinity matrix K closest chosen error measure doubly stochastic matrix K ii find solution problem max G Rm k tr G K G s t G G G I GG come close K tr G K G tr K GG K doubly stochastic GG come close satisfying doubly stochastic constraint motivation step approach Moreover drop non negativity constraint G Note non negativity constraint crucial physical interpretation Spectral Analysis II Clustering G k clusters possible interpretation shall As result left spectral decomposition problem eqn max G Rm k tr G K G s t G G I columns G leading eigenvectors K We refer step normalization process popular normalizations literature leading Ratio Cuts Normalized Cuts Ratio Cuts Let D diag K1 diagonal matrix containing row sums K The Ratio Cuts normalization look K closest doubly stochastic matrix K minimizing L1 norm turns K K D I Claim ratio cut Let K symmetric positive semi definite values range The closest doubly stochastic matrix K L1 error norm K K D I Proof Let r minF K F s t F1 F F Since K F K F matrix F r K F D1 D I Let F K D I K K D I D I The Laplacian matrix graph D K If v eigenvector Laplacian D K eigenvalue v eigenvector K K D I eigenvalue D K smallest eigenvector v Laplacian largest K second smallest eigenvector Laplacian ratio cut result corresponds second largest eigenvector K Because eigenvectors orthogonal second eigenvector positive negative entries inner product zero sign entries second eigenvector determines class membership Spectral Clustering Ratio Cuts Normalized Cuts Ratio Cuts second smallest eigenvector Laplacian D K approximation Hall 70s Min Cut formulation Let z Rm determine class membership xi xj clustered zi zj similar values This leads following optimization problem min z j zi zj 2Kij s t z z The criterion function equal z D K z derivative Lagrangian z D K z z z respect z gives rise necessary condition D K z z Ratio Cut scheme follows Normalized Cuts Normalized Cuts looks closest doubly stochastic matrix K relative entropy error measure defined RE x y xi ln xi yi yi xi We encounter relative entropy measure detail later course We K form K diagonal matrix Claim The closest doubly stochastic matrix F relative entropy error measure given non negative symmetric matrix K e min imizes min F RE F K s t F F F F1 F form F K unique diagonal matrix Proof The Lagrangian problem L ij fij ln fij kij ij kij ij fij j fij j j fij The derivative respect fij L fij ln fij ln kij j obtain fij e iejkij Spectral Analysis II Clustering Let D1 diag e1 en D2 diag e1 en F D1KD2 Since F F K symmetric D1 D2 Next diagonal matrix found iterative process K replaced D 2KD D defined diag K1 Claim For non negative symmetric matrix K iterating process K t D 2K t D D diag K t converges doubly stochastic matrix The proof based showing permanent increases monotonically e perm K t perm K t Because permanent bounded process converge permanent change conver gence point resulting matrix doubly stochastic The resulting doubly stochastic matrix closest K relative entropy Normalized Cuts takes result iteration replacing K K D 2KD followed spectral decomposition case k classes partitioning information found second leading eigenvector K like Ratio Cuts different K Thus K manner closest doubly stochastic matrix K fairly close iteration dominant process Normalized Cuts second leading eigenvector ofK D 2KD approximation balanced Min Cut described Deriving principles proceeds follows Let sum V1 V2 sumi V1 j V2Kij defined subsets necessarily disjoint vertices The normalized cuts measures cut cost fraction total edge connections nodes graph Ncuts A B cut A B sum A V cut A B sum B V A minimal Ncut partition longer favor small isolated points cut value likely large percentage total connections small set vertices A related measureNassoc A B defined Nassoc A B sum A A sum A V sum B B sum B V reflects tightly average nodes group connected Given cut A B sum A V sum A A easily verify Spectral Clustering Ratio Cuts Normalized Cuts Ncuts A B Nassoc A B optimal bi partition represented maximizingNassoc A V A The Nassoc naturally extends k classes partitions follows Let k disjoint sets jj V Nassoc k k j sum j j sum j V We rewrite Nassoc matrix form establish equivalence eqn Let G g1 gk gj sum j V 1s indicating membership j th class Note g j Kgj sum j j sum j V trace G KG Nassoc k Note g Dgi sum V r dr G DG I Let G D1 2G G G I trace G D 2KD 2G Nassoc k Taken maximizing Nassoc equivalent max G Rm k trace G K G s t G G G I K D 2KD Note exactly K means matrix setup eqn doubly stochastic constraint relaxed replacement K K The constraint G dropped resulting solution G k leading eigenvectors K We arrived seemingly different paths eqn drop constraint G end closed form solution consisting k leading eigenvectors K When k classes easily verify partitioning information fully contained second eigenvector Let v1 v2 leading eigenvectors K Clearly v D1 eigenvector eigenvalue D 2KD D1 D 2K1 D1 In fact largest eigenvalue left exercise v1 D1 Since K symmetric v v1 v2 contains positive negative entries interpreted indicating class membership positive class negative The case k treated embedding known Multi Dimensional Scaling coordinating points xi rows G In Spectral Analysis II Clustering words th row G representation xi Rk Under ideal con ditions K block diagonal distance clusters infinity rows associated points clustered identical e n original points mapped k points Rk In practice performs iterative K means embedded space The Formal PAC Learning Model We far algorithms explicitly estimate underlying dis tribution data Bayesian methods EM algorithms sense optimal underlying distribution Gaussian PCA LDA We encountered algorithm SVM sumptions underlying distribution instead tied accuracy margin training data In lecture remainder course address issue accuracy generalization formal manner Because learner receives finite training sample learning function training set perform badly new input instances What like establish certain guarantees accuracy learner measured instance space training set We use guarantees better understand large margin principle SVM context generalization In remainder lecture refer following notations class learning functions denoted C A learning functions referred concept hypothesis A target function ct C function zero error input instances function exist The Formal Model In learning situations interest like assume learner receives m examples sampled fixed unknown distri bution D learner best training set order achieve accuracy confidence objectives The Probably Approximate Correct PAC model known formal model introduced The Formal PAC Learning Model Valient provides probabilistic setting formalizes notions accuracy confidence The PAC model makes following statistical assumption We assume learner receives set S m instances x1 xm X sampled randomly independently according distribution D X In words random training set S length m distributed according product probability distribution Dm The distribution D unknown obtain useful results simply assuming D fixed need attempt recover D learning process To recap following assumptions D unkown ii D fixed learning process iii example instances sampled independently Identically Independently Distributed d We distinguish realizable case target concept ct x known exist unrealizable case guar antee In realizable case training examples Z xi ct xi m D defined X yi Y given ct xi In unrealizable case Z xi yi D distribution X Y element pair X Y We define meant error induced concept function h x In realizable case given function h C error h defined respect distribution D err h probD x ct x h x x X ind ct x h x D x dx ind F indication function returns proposition F true The function err h probability instance x sampled according D labeled incorrectly h x Let parameter given learner specifying accuracy learning process e like achieve err h Note err ct In addition define confidence parameter given learner defines probability err h prob err h equivalently prob err h In words learner supposed meet accuracy criteria allowed deviate small probability Finally learning The Formal Model algorithm supposed efficient running time polynomial ln n size concept target function ct measured number bits necessary describing example We algorithm L learns concept family C formal sense PAC learnable ct C distribution D instance space X algorithm L generates efficiently concept function h C probability err h The inclusion confidence value unnatural What desire learner demonstrate consistent performance regardless training sample Z In words learner produces hypothesis h accuracy threshold e err h training sample Z We like accuracy formance hold training samples sampled distribution Dm requirement difficult satisfy formal model allows failures e situations err h training samples Z long failures rare frequency occurrence controlled parameter small like In unrealizable case function h C err h need define mean best learning algorithm achieve Opt C min h C err h best concept class C functions map X Y Given desired accuracy confidence values learner seeks hypothesis h C prob err h Opt C We ready formalize discussion introduce defi nition formal learning model Anthony Bartlett pp Definition Formal Model Let C concept class functions map set X Y A learning algorithm L function L m xi yi mi C set training examples C following property given integer m0 m m0 probability distribution D X Y Z training set length m The Formal PAC Learning Model drawn randomly according product probability distribution Dm probability hypothesis h L Z C output L err h Opt C We C learnable PAC learnable learning algorithm C There points emphasize The sample size m0 suf ficient sample size PAC learning C L allowed vary Decreasing value makes learning problem difficult turn larger sample size required Note m0 depend distribution D sufficient sample size given work distribution D provided D fixed learning experience training later testing This point crucial property formal model sufficient sample size allowed vary distribution D need information distribution der set sample complexity bounds adversary supplying training set control rate convergence L solution solution proven optimal arbitrarily slow suitable choice D What makes formal model work distribution invariant manner critically depends fact interesting learning scenarios concept class C complex For example later lecture finite concept class C learnable sample complexity realizable case m ln C In lecture consider concept classes infinite size despite fact class infinite low complexity Before illustrate concepts example useful measure empirical error known sample error err h defined proportion examples Z h mistake err h m h xi ct xi replace ct xi yi unrealizable case The situation bounding true error err h minimizing sample error err h conve nient later The Rectangle Learning Problem The Rectangle Learning Problem As illustration learnability consider problem introduced Kearns Vazirani learning axes aligned rectangle positive negative examples We problem PAC learnable find m0 In rectangle learning game given training set consisting points 2D plane positive negative label The positive examples sampled inside target rectangle parallel main axes R negative examples sampled outside R Given m examples sampled d according distribution D learner supposed generate approximate rectangle R consistent training set assuming R exists satisfies accuracy confidence constraints We need decide learning strategy Since solution R uniquely defined given training set Z need add constraints guarantee unique solution We choose R axes aligned concept gives tightest fit positive examples e smallest area axes aligned rectangle contains positive examples If positive examples given R We assume Z contains non collinear positive examples order avoid complications associated infinitesimal area rectangles Note chosen strategies middle ground tightest fit positive examples tightest fit negative examples forth Defining strategy necessary analysis type strategy critical We define error err R concept R generated learning strategy We note strategy defined R R R tightest fit solution consistent sample data positive example outside R training set We define weight w E region E plane w E x E D x dx e probability random point sampled according distri bution D fall region Therefore error associated concept R err R w R R The Formal PAC Learning Model Fig Given tightest fit positive examples strategy R R The strip T1 weight strip T defined upper strip covering area R R wish bound error w R R probability seeing m examples We divide region R R strips T T Fig overlap corners We estimate prob w T noting overlaps regions makes estimates pessimistic truly counting overlapping regions twice making lean conservative estimations Consider upper strip T If w T We interested quantifying probability case Assume w T define strip T1 starts upper axis R stretches extent w T1 Clearly T1 T We w T iff T1 T Furthermore Claim T1 T iff x1 xm T1 Proof If xi T1 label positive T1 R But label positive given learning strategy fitting tightest rectangle positive examples xi R Since T1 R follows xi T1 We w T iff point T1 appears sample S x1 xm T1 intersects R T T1 The probability point sampled according distribution D fall outside T1 Given independence assumption examples Learnability Finite Concept Classes drawn d prob x1 xm T1 prob w T m Repeating analysis regions T T T union bound P A B P A P B come conclusion probability strips R R weight greater m In words prob err L m We expression convenient manipulation inequality e x x recall n n e follows z z e taking power rz r obtain z r erz set r z x m 4e m obtain bound m ln To conclude assuming learner adopts tightest fit positive ex amples strategy given m0 ln training examples order find axes aligned rectangle R assert probability error associated R e probability m th point classified incorrectly We form analysis applies distri bution D assumption independence draw Also sample size m behaves sense desires higher level accuracy smaller higher level confidence smaller sample size grows accordingly The growth m linear linear ln Learnability Finite Concept Classes In previous section illustrated concept learnability particular simple example We focus applying learnability model general family learning examples We consider family learning problems finite concept classes C For example conjunction learning problem boolean formulas n literals contains 3n hypotheses variable appear conjunction appears negated We The Formal PAC Learning Model shown n lower bound number mistakes worst case analysis line algorithm achieve With definitions formal model learnability perform accuracy sample complexity analysis apply learning problem finite concept classes This introduced Valiant In realizable case C algorithm L returns hypothesis h C consistent training set Z learning algorithm C In words finite concept class learnable learning algorithms simply need generate consistent hypotheses The sample complexity m0 associated choice shown equal ln C In unrealizable case algorithm L generates hypothesis h C minimizes empirical error error obtained Z learning algorithm C The sample complexity shown equal ln C We derive cases The Realizable Case Let h C consistent hypothesis training set Z know hypothesis exists particular h ct target concept generating Z suppose err h prob x D h x ct x Then probability respect product distribution Dm h agrees ct random sample length m m Using inequality saw e x x prob err h h xi ct xi m m e m We wish bound error uniformly e err h concepts h C This requires evaluation prob max h C err h h xi ct xi m There C functions h Union Bound probability function C error larger consistent Learnability Finite Concept Classes ct random sample length m C e m prob h err h h xi ct xi m h err h prob h xi ct xi m h err h e m C e m For positive probability provided m ln C This derivation summarized following theorem Anthony Bartlett pp Theorem Let C finite set functions X Y Let L algorithm m ct C Z training sample xi ct xi m hypothesis h L Z satisfies h xi ct xi Then L learning algorithm C realizable case sample complexity m0 ln C The Unrealizable Case In realizable case algorithm simply needs generate consistent hypothesize considered learning algorithm formal sense In unrealizable situation target function ct exist algorithm minimizes empirical error e algorithm L generates h L Z having minimal sample error err L Z min h C err h learning algorithm C assuming finite C This particularly useful property given true errors functions C un known It natural use sample errors err h estimates performance L The fact given large sample training set Z sam ple error err h close true error err h somewhat restatement law large numbers probability theory For ex ample toss coin times relative frequency heads approaches true probability head rate determined law The Formal PAC Learning Model large numbers We bound probability difference empirical error true error h exceeds Hoeffding s inequality Claim Let h function X Y Then prob err h err h 2e 2m probability distribution D positive integer m Proof This straightforward application Hoeffding s inequality Bernoulli variables Hoeffding s inequality says Let X set D proba bility distribution X f1 fm real valued functions fi X ai bi X interval real line ai bi Then prob m m fi xi Ex D f x 2e 2m2P bi ai Ex D f x m m fi x D x dx In case fi xi iff h xi yi ai bi Therefore m fi xi err h err h Ex D f x The Hoeffding bound need What given hypothesis h C empirical error close true error high probability Recall goal minimize err h possible h C access err h If guarantee close h C minimizing err h h C approximately minimize err h Put formally order ensure L learns class C prob max h C err h err h In words need empirical errors converge high probability true errors uniformly C m If guaranteed high probability h C err h err h err h So algorithm L running training set Z returns h L Z minimizes empirical error err L Z err L Z min h err h Opt C Learnability Finite Concept Classes needed order L learns C Thus left prove following claim Claim prob max h C err h err h C e 2m Proof We use union bound Finding maximum C equivalent taking union events prob max h C err h err h prob h C Z err h err h union bound Claim h C prob err h err h C 2e 2m Finally given C e 2m obtain sample complexity m0 ln C This discussion summarized following theorem Anthony Bartlett pp Theorem Let C finite set functions X Y Let L algorithm m training set Z xi yi m hypothesis L Z satisfies err L Z min h C err h Then L learning algorithm C sample complexity m0 ln C Note main difference realizable case Theorem larger The realizable case requires smaller training set estimating random quantity smaller variance data need The VC Dimension The result PAC model known formal learning model concept class C PAC learnable learning strategy simply consist gathering sufficiently large training sample S size m mo given accuracy confidence parameters finds hypothesis h C consistent S The learning algorithm guaranteed bounded error err h probability The error measurement includes data seen training phase This state affair holds slight modifications sam ple complexity bounds consistent hypothesis unreal izable case In case learner simply needs minimize empirical error err h sample training data S m sufficiently large learner guaranteed err h Opt C probability The measure Opt C defined minimal err g g C Note realizable case Opt C The property bounding true error err h minimizing sample error err h convenient The fundamental question conditions type generalization property applies We saw previ ous lecture satisfactorily answer provided cardinality concept space bounded e C happens Boolean concept space example In lecture proven mo O ln C sufficient guaranteeing learning model formal sense e generalization property described In lecture follows goals mind First generalize result finite concept class cardinality infinite car The VC Dimension dinality note bound meaningful C Can learn formal sense non trivial infinite concept class saw example PAC learnable infinite concept class class axes aligned rectangles In order answer question need general measure concept class complexity replace cardinality term C sample complexity bound mo It tempting assume number parameters fully describe concepts C serve measure fact needs powerful measure called Vapnik Chervonenkis VC dimension Our second goal pave way provide theoretical foundation large margin principle algorithm SVM derived Lecture The VC Dimension The basic principle VC dimension measure C infinite cardinality restriction application concepts C finite sample S finite outcome This outcome typically governed exponential growth size m sample S The point growth stops exponential complexity concept class C exhausted manner speaking We assume C concept class instance space X infinite We assume concept class maps stances X e input instances mapped positive negative labels A training sample S drawn d according fixed unknown distribution D S consists m instances x1 xm In notations try reserve c C denote target concept h C denote concept We begin following definition Definition C S h x1 h xm h C set vectors m C S set members m dimensional Boolean vectors induced functions C These members called dichotomies behaviors S induced realized C If C makes realization C S 2m members An equivalent description collection subsets S C S h S h C h C makes partition S sets positive The VC Dimension negative points The set C S contains subsets S positive points S h A realization provide m m 2m We use descriptions C S collection subsets S set vectors interchangeably Definition If C S 2m S considered shattered C In words S shattered C C realizes possible dichotomies S Consider example finite concept class C c1 c4 applied instance vectors results x1 x2 x3 c1 c2 c3 c4 Then C x1 shattered C x1 x3 shattered C x2 x3 shattered With definitions ready describe measure concept class complexity Definition VC dimension The VC dimension C noted V Cdim C cardinality d largest set S shattered C If sets S arbi trarily large shattered C V Cdim C V Cdim C max d S d C S 2d The VC dimension class functions C point d samples S cardinality S d longer shattered C As long C shatters S manifests richness sense obtain S possible results dichotomies Once ceases hold e S d means C exhausted richness complexity An infinite VC dimension means C maintains richness sample sizes Therefore VC dimension combinatorial measure function class complexity Before consider number examples geometric concept classes VC dimension important clarify lower upper bounds existential universal quantifiers definition VC dimension The VC dimension d exists sample S d The VC Dimension shattered C mean samples size d shattered C Conversely order VC dimension d sample size d shattered Naturally proving upper bound difficult proving lower bound VC dimension The following examples shown hand waiving style meant form rigorous proofs stated bounds shown illustrative purposes Intervals real line The concept class C governed param eters closed interval A concept class tag input instance x positive x negative The VC dimension select sample points x1 x2 positioned open interval We need values realize possible dichotomies This clearly possible place interval inter section interval x1 x2 null producing fully include x1 x2 producing partially intersect x1 x2 x1 x2 excluded producing remaining dichotomies To VC dimension need sample points x1 x2 x3 line shattered It sufficient dichotomies realizable labeling realizable interval x1 x3 labeled positive definition interval fully include interval x1 x3 x1 x2 x3 x2 labeled positive Thus V Cdim C Axes aligned rectangles plane We seen concept class previous lecture point plane labeled positive lies axes aligned rectangle The concept class C governed parameters The VC dimension consider configuration input points arranged cross pattern recall need sample S shattered We place rectangles concepts class C dichotomies realized example placing rectangle include vertical pair points exclude horizontal pair points induce labeling It important note case configurations points shattered prove lower bound sufficient existence single shattered set points To VC dimension need prove set points The VC Dimension shattered For set points point internal e extreme left right point If label internal point negative remaining points positive axes aligned rectangle concept cold realize labeling external points labeled positive fully concept rectangle internal point included rectangle labeled positive Separating hyperplanes Consider linear half spaces plane The lower bound VC dimension non collinear points R2 shattered e possible labelings points realized placing separating line appropriately By having points line realize dichotomies placing line points realize 4th The remaining dichotomies realized sign flip previous cases To upper bound need set points shattered We consider cases points form convex region e lie convex hull defined points ii points define convex hull 4th point internal In case labeling positive diagonal pair negative pair realized separating line In second case labeling positive hull points negative interior point realize Thus VC dimension general VC dimension separating hyperplanes Rn n Union finite number intervals line This example concept class infinite VC dimension For sample points line place sufficient number intervals realize labeling The examples far simple wrong impression correlation number parameters required describe concepts class VC dimension As counter example consider parameter concept class C sign sin x infinite VC dimension set m points line realize possible labelings choosing The Relation VC dimension PAC Learning sufficiently large value serves frequency sync function appropriate phase We conclude section following claim Theorem The VC dimension finite concept class C bounded V Cdim C log2 C Proof V Cdim C d exists 2d functions C function induces labeling 2d labelings Thus C 2d follows d log2 C The Relation VC dimension PAC Learning We saw VC dimension combinatorial measure concept class complexity like replace cardinality term sample complexity bound The result interest VC dimension concept class infinite class PAC learnable Theorem Concept class C V Cdim C learnable formal sense Proof Assume contrary C PAC learnable Let L learning algorithm m number training examples required learn concept class accuracy That seeing m training examples learner generates concept h satisfies p err h Since VC dimension infinite exist sample set S 2m instances shattered C Since formal model PAC applies training sample use set S follows We define probability distribution instance space X uniform S probability 2m zero Because S shattered target concept possible choose target concept c following manner prob ct xi xi S words labels ct xi determined coin flip The learner L selects d sample m instances S structure The VC Dimension D means S S outputs consistent hypothesis h C The probability error xi S prob ct xi h xi The reason S shattered C e select target concept labeling S 2m examples select labels m points seen learner arbitrarily flipping coin Regardless h probability mistake The expectation error h E err h m 2m m 2m This 2m points sample according D points zero probability error half zero h consistent training set S error remaining half Thus average error Note E err h choice based sample size m For sample size m follow construction generate learning problem learner produces consistent hypothesis expectation error The result E err h possible accuracy confidence values set probability err h probability err h Taking worst case come average error E err h We arrived contradiction C PAC learnable We obtain bound growth S C sample size S m larger VC dimension V Cdim C d concept class We need definitions Definition Growth function C m max S C S m The measure C m maximum number dichotomies induced C samples size m As long m d C m 2m The question happens growth pattern C m m d We growth polynomial fact crucial learnability C The Relation VC dimension PAC Learning Definition For natural numbers m d following definition d m d m d m d m By induction m d possible prove following Theorem d m d m Proof induction m d For details pp For m d d m 2m For m d derive polynomial upper bound follows d m d d m d d m m m d m m d m m ed From obtain d m d d m ed Dividing sides d m d yields d m ed m d d em d d O md We need result ready present main result lecture Theorem Sauer s lemma If V Cdim C d m C m d m Proof By induction d m For details pp Taken fairly interesting characterization combinatorial measure complexity concept class C scales sample size m When VC dimension C infinite growth exponential e C m 2m values m On hand concept class bounded VC dimension V Cdim C d growth pattern undergoes discontinuity exponential polynomial growth C m 2m m d em d d m d The VC Dimension As direct result observation m d larger d entropy smaller m Recall informa tion theoretic perspective entropy random variable Z discrete values z1 zn probabilities pi n defined H Z n pi log2 pi I pi log2 pi measure information e large pi small meaning information occurrence unlikely event vanishes event certain pi The entropy expectation information Entropy maximal uniform distribution H Z log2 n The entropy information theory context viewed number bits required coding z1 zn In coding theory shown entropy distribution provides lower bound average length possible encoding uniquely decodable code fro symbol goes symbol When distribution uniform need maximal number bits e compress data In case concept class C VC dimension d m d possible dichotomies realized need m bits 2m dichotomies representing outcomes sample However m d small fraction 2m dichotomies realized distribution outcomes highly non uniform need bits coding outcomes sample The technical results follow formal way expressing rigorous manner simple truth If possible compress possible learn The crucial point learnability direct consequence phase transition exponential polynomial growth number dichotomies realized concept class In lecture continue prove double sampling orem derives sample size complexity function VC dimension The Double Sampling Theorem In lecture use measure VC dimension combina torial measure concept class complexity bound sample size com plexity A Polynomial Bound Sample Size m PAC Learning In section follow material presented Kearns Vazirani pp prove following Theorem Double Sampling Let C concept class VC di mension d Let L algorithm given set S m labeled examples xi c xi sampled d according fixed unknown distribution D instance space X concept c C produces output concept h C consistent S Then L learning algorithm formal sense provided sample size obeys m c0 log d log constant c0 The idea proof build approximate concept space includes concepts arranged distance approximate concepts h target concept c distance de fined weight region X conflict target concept To formalize story need definitions Unless specified c C denotes target concept h C denotes concept The Double Sampling Theorem Definition c h h c x c x h x c h region instance space concepts agree error region The probability x c h equal definition err h Definition c h c h C c h c h C err h c set error regions concept h C concepts The error regions respect target concept The set c c set error regions weight exceeds Recall weight defined probability point sampled according D hit region It important later evaluate VC dimension c Unlike C looking VC dimension class function VC dimension set regions space Recall definition C S previous lecture equivalent definitions based set vectors representing labeling instances S induced concept The second equivalent definition based set subsets S induced concept concept divides sample points S positive negative labeled points So far convenient work definition evaluating VC dimension c useful consider second definition c S r S r c collection subsets S induced intersections regions c An intersection S region r defined subset points S fall r We easily VC dimensions C c equal Lemma V Cdim C V Cdim c Proof elements C S c S susbsets S need S cardinality sets equal C S c S To sufficient element s C S unique corresponding element c S Let A Polynomial Bound Sample Size m PAC Learning c S subset S induced target concept c The set s subset S realized concept h points S labeled positive h Therefore set s c S subset S containing points hit region h c element c S Since mapping C S c S Definition net For sample set S net c region c hit point S r c S r In words S hits error regions c weight exceeds S net Consider example concept class intervals line A concept defined interval points inside interval positive outside negative Given c C target concept h C concept error region h c union intervals I1 consists points x h c I2 interval points x c h Assume distribution D uniform sake example prob x I I length interval I As result err h I1 I2 The sample set S x k k contains sample points increments Therefore interval larger hit point S definition S net It important note S forms net guaranteed err h Let h C consistent hypothesis S returned learning algorithm L Becuase h consistent h c c hit S recall h c error region respect target concept c h consistent agrees c S S hit h c Since S forms net c h c c recall definition S hits error regions weight larger As result error region h c weight smaller means err h The conclusion bound probability random sam ple S form net c bounded probability concept h consistent S err h This goal proof double sampling theorem prove The Double Sampling Theorem Proof following Kearns Vazirani pp Let S1 ran dom sample size m sampled d according unknown distribution D let A event S1 form net c From preceding discussion goal upper bound probability A occur e prob A If A occurs e S1 net definition region r c hit S1 S1 r Note r h c concept h consistent S1 At point space possibilities infinite probability fail hit h c m random examples m Thus probability fail hit h c c bounded c m help fact c infinite The idea proof turn finite space sample follows Let S2 random sample size m We select m S1 S2 guarantee high probability S2 hit r times In fact wish S2 hit r m2 probability prob S2 r m prob S2 r m We use Chernoff bound lower tail obtain bound right hand term Recall m Bernoulli trials coin tosses Z1 Zm expectation E Zi p consider random variable Z Z1 Zm expectation E Z note pm prob Z e Considering sampling m examples form S2 Bernoulli trials m probability example hit r We obtain prob S2 r m e m happens m ln O To summarize obtained far calculated probability S2 hit r times given r fixed previous sampling e given S1 form net To formalize let B denote combined event S1 form event S2 hits r m times Then shown m O prob B A A Polynomial Bound Sample Size m PAC Learning From calculate prob B prob B prob B A prob A prob A means original goal bounding prob A equivalent finding bound prob B prob A prob B The crucial point new goal analyze probability event B need consider finite number possibilities consider regions c S1 S2 r S1 S2 r c This occurrence event B equivalent saying r c S1 S2 r m e region r hit m times S1 r This c S1 S2 contains subsets S1 S2 realized intersections regions c Thus infinite number regions finite number subsets We wish analyze following probability prob r c S1 S2 r m S1 r Let S S1 S2 random sample 2m note sampling d equivalent sampling S1 S2 separately r satisfying r m fixed Consider random partitioning S S1 S2 consider problem estimating probability S1 r This problem equivalent following combinatorial question 2m balls colored Red Blue exaclty l m Red balls We divide 2m balls groups equal size S1 S2 interested bounding probability l balls fall S2 probability S1 r This turn equivalent dividing 2m uncolored balls S1 S2 groups randomly choose l balls colored Red analyze probability Red balls fall S2 This probability exactly m l 2m l l m 2m l 2l m This probability evaluated fixed S r Thus probability occurs r c S satisfying r m prob B calculated summing possible fixed r applying The Double Sampling Theorem union bound prob Zi prob Zi prob B c S m c S m C S m m d d m follows m O log d log Few comments worthwhile point It possible upper bound sample complexity m tight showing lower bound m d pp ii The treatment holds unrealizable case target con cept c C slight modifications bound In context learning algorithm L simply minimize sample empiri cal error err h defined err h m h xi yi xi S The generalization double sampling theorem Derroye states empirical errors converge uniformly true errors prob max h C err h err h 4e m2 d d m follows m O log d log Taken arrived fairly remarkable result Despite fact distribution D training sample S drawn unknown known fixed learner simply needs minimize empirical error If sample size m large learner guar anteed minimized true errors accuracy confidence parameters define sample size complexity Equivalently Opt C err h m Not convergence independent D rate con vergence independent matter optimal h Optimality SVM Revisited located The important ar bitrarily slow convergence rate maliciously choosing D The beauty results D effect simply needs choose sample size large accuracy confidence VC dimension concept class learned Optimality SVM Revisited In Lecture discussed large margin principle finding optimal separating hyperplane It natural ask PAC theory pre sented far explains maximal margin hyperplane optimal regard formal sense learning e generalization empirical errors true errors We saw previous section sample com plexity m d depends VC dimension concept class n hyperplanes Rn Thus natural question certainly arise gain employing kernel trick For fixed m mapping input instance space X dimension n higher exponentially higher feature space simply mean compro mising accuracy confidence learner VC dimension equal instance space dimension plus Given fixed sample size m best learner minimize empirical error time try minimize VC dimension d concept class The smaller d fixed m higher accuracy confidence learning algorithm Likewise smaller d fixed accuracy confidence values smaller sample size required There possible ways decrease d First decrease dimen sion n instance space X This amounts feature selection find subset coordinates relevant learning task r perform dimensionality reduction PCA example A second approach maximize margin Let margin associated separating hyperplane h e consistent sample S Let input vectors x X bounded norm x R It shown VC dimension concept class C hyperplanes margin C min R2 n Thus margin small VC dimension remains n As margin gets larger comes point R2 n result VC dimension decreases Moreover mapping instance space X higher dimension feature space change VC dimension The Double Sampling Theorem long margin remains It expected margin scale scale rapidly scaling dimension image space feature space To conclude maximizing margin minimizing empirical er ror advantageous decreases VC dimension concept class causes accuracy confidence values learner largely immune dimension scaling employing kernel trick Appendix Appendix A0 Variance Covariance etc Let X Y random variables let f x y function x X y Y let p x y probability event x y occurring The expectation E f x y defined E f x y x X y Y f x y p x y The mean variance covariance defined x E X x y xp x y y E Y x y yp x y 2x V ar X E x x x y x x 2p x y 2y V ar Y E y y x y y y 2p x y xy Cov XY E x x y y x y x x y y p x y In vector matrix notation let x represent n random variables ofX1 Xn e x x1 xn instance vector p x probability instance occurrence Then mean vector covariance matrix E defined x X1 Xn xp x E x x x p x Note covariance matrix E linear superposition rank matrices x x coefficients p x The diagonal E containes variances variables x1 xn For uniform distribution sample data S consisting m points let A x1 xm matrix columns consist points centered mean m xi The sample covariance matrix E m AA A0 Derivatives Matrix Operations Scalar Functions Vector A0 Derivatives Matrix Operations Scalar Functions Vector The important examples scalar function vector x linear form x quadratic form x Ax square matrix A d x dx d x Ax dx Ax x A dx dx Ax x A dx x A A dx derivative d x Ax rule products d f g df g f dg g Ax f x noting d Ax Adx Thus d dx x d dx x Ax x A A If A symmetric d dx x Ax 2Ax A0 Primer Constrained Optimization A0 Equality Constraints Lagrange Multipliers Consider general optimization equality constraints gives rise notion Lagrange multipliers min x f x subject h x f Rn R h Rn Rk h vector function h1 hk Rn R We want derive necessary sufficient constraint point xo local minimum subject k equality constraints h x Assume xo regular point meaning gradient vectors hj x linearly independent Note h xo k n matrix null space matrix null h xo y h xo y defines tangent plane point xo We following fundamental theorem f xo null h xo words vectors y spanning tangent plane point xo perpendicular gradient f xo Appendix The sketch proof follows Let x t t smooth curve surface h x e h x t Let xo x y d dt x tangent curve xo From definition tangency vector y lives null h xo e y hj x j k Since xo x local extremum f x d dt f x t t f xi dxi dt t f xo y As corollary basic theorem gradient vector f xo span h1 xo hk xo e f xo k hi xo coefficients called Lagrange Multipliers expression f x ihi x called Lagrangian optimization problem A0 Inequality Constraints KKT conditions Consider general constrained optimization inequality con straints called non linear programming min x f x subject h x g x g Rn Rs We assume optimal solution xo regular point following meaning Let J set indices j gj xo xo regular point gradient vectors hi xo gj xo k j J linearly independent A basic result prove Karush Kuhn Tucker KKT theorem Let xo local minimum problem suppose xo regular point Then exist k s A0 Primer Constrained Optimization zy zy y z yz G nRx xfxg Fig A0 Geometric interpreatation Duality text f xo k hi xo s j j gj xo s j jgj xo Note condition jgj xo equivalent condition jgj xo g xo sum vanish term vanishes turn implies j gj xo The expression L x f x k ihi x s j jgj x Lagrangian problem associated condition jgj xo called KKT condition The remaining concepts need duality Lagrangian Dual problem Appendix A0 The Langrangian Dual Problem The optimization problem called Primal problem The La grangian Dual problem defined max subject min x f x ihi x j jgj x Note assume value values rigorous replaced min inf The basic result weak duality theorem Let x feasible solution primal e h x g x let feasible solution dual problem e f x The proof immediate min y f y ihi y j jgj y f x ihi x j jgj x f x inequality follows h x j jgj x g x As corollary theorem min x f x h x g x max The basic result strong duality theorem specifies con ditions inequality equality Let f g convex functions let h affine e h x Ax b A k n matrix min x f x h x g x max The strong duality theorem allows solve primal problem dualizing solving dual problem instead exactly return solving primal problem When A0 Primer Constrained Optimization zy y z G optimal primal optimal dual Fig A0 An example duality gap arising non convexity text convexity conditions hold obtain min x f x h x g x max means optimal solution dual problem provides lower bound primal problem situation called duality gap Taken duality theorem summarizes discussion far Theorem Duality Theorem In order x optimal Primal solution optimal Dual solution necessary sufficient x Primal feasible ii j gj x iii x argminxL x We end section geometric interpretation duality A0 Geometric Interpretation Duality For clarity consider primal problem single inequality con straint min f x g x g Rn R Consider set G y z y g x z f x y z plane The set G image Rn g f map Fig A0 The primal Appendix problem find point G y smallest z value point y z figure In case minx f x g x equivalent minimize z y points G The equation z y represents straight line slope intercept z axis For given value minimize z y G need line z y parallel far possible remains contact G words G line touches Then intercept z axis gives The dual problem equivalent finding slope supporting hyperplane intercept z axis maximal Consider non convex region G Fig A0 illustrates duality gap condition The optimal primal point y z higher greatest intercept z axis achieved line supports G This example duality gap caused non convexity functions f g making set G non convex Bibliography M Anthony P L Bartlett Neural Neteowk Learning Theoretical Foundations Cambridge University Press K M Hall An r dimensional quadratic placement algorithm Manag Sci M J Kearns U V Vazirani An Introduction Computational Learning The ory MIT Press Y Linde A Buzo R M Gray An algorithm vector quantizer design IEEE Transactions Communications A Y Ng M I Jordan Y Weiss On spectral clustering Analysis algorithm In Proceedings conference Neural Information Processing Systems NIPS J Shi J Malik Normalized cuts image segmentation IEEE Transactions Pattern Analysis Machine Intelligence R Zass A Shashua A unifying approach hard probabilistic clustering In Proceedings International Conference Computer Vision Beijing China Oct Bayesian Decision Theory Maximum Likelihood Maximum Entropy Duality EM Algorithm ML Mixture Distributions Support Vector Machines Kernel Functions Spectral Analysis I PCA LDA CCA Spectral Analysis II Clustering The Formal PAC Learning Model The VC Dimension The Double Sampling Theorem Appendix Bibliography\n",
            "8 []\n",
            "8 Algorithms Reinforcement Learning Draft lecture published Synthesis Lectures Artificial Intelligence Machine Learning series Morgan Claypool Publishers Csaba Szepesva ri June Contents Overview Markov decision processes Preliminaries Markov Decision Processes Value functions Dynamic programming algorithms solving MDPs Value prediction problems Temporal difference learning finite state spaces Tabular TD Every visit Monte Carlo TD Unifying Monte Carlo TD Algorithms large state spaces TD function approximation Gradient temporal difference learning Least squares methods Last update March The choice function space Control A catalog learning problems Closed loop interactive learning Online learning bandits Active learning bandits Active learning Markov Decision Processes Online learning Markov Decision Processes Direct methods Q learning finite MDPs Q learning function approximation Actor critic methods Implementing critic Implementing actor For exploration Further reading Applications Software Acknowledgements A The theory discounted Markovian decision processes A Contractions Banach s fixed point theorem A Application MDPs Abstract Reinforcement learning learning paradigm concerned learning control system maximize numerical performance measure expresses long term objective What distinguishes reinforcement learning supervised learning partial feedback given learner learner s predictions Further predictions long term effects influencing future state controlled system Thus time plays special role The goal reinforcement learning develop efficient learning algorithms understand algorithms merits limitations Reinforcement learning great interest large number practical applications address ranging problems artificial intelligence operations research control engineering In book focus algorithms reinforcement learning build powerful theory dynamic programming We fairly comprehensive catalog learning problems Reward State Action SystemSystem ControllerController Figure The basic reinforcement learning scenario describe core ideas large number state art algorithms followed discussion theoretical properties limitations Keywords reinforcement learning Markov Decision Processes temporal difference learn ing stochastic approximation timescale stochastic approximation Monte Carlo meth ods simulation optimization function approximation stochastic gradient methods squares methods overfitting bias variance tradeoff online learning active learning plan ning simulation PAC learning Q learning actor critic methods policy gradient natural gradient Overview Reinforcement learning RL refers learning problem subfield machine learning As learning problem refers learning control system maxi mize numerical value represents long term objective A typical setting reinforcement learning operates shown Figure A controller receives controlled system s state reward associated state transition It calculates action sent system In response system makes transition new state cycle repeated The problem learn way controlling system maximize total reward The learning problems differ details data collected performance measured In book assume system wish control stochastic Further assume measurements available system s state detailed controller avoid reasoning collect information state Problems characteristics best described framework Markovian Decision Processes MDPs The standard approach solve MDPs use dynamic programming transforms problem finding good controller problem finding good value function However apart simplest cases MDP states actions dynamic programming infeasible The RL algorithms discuss thought way turning infeasible dynamic programming methods practical algorithms applied large scale problems There key ideas allow RL algorithms achieve goal The idea use samples compactly represent dynamics control problem This important reasons First allows deal learning scenarios dynamics unknown Second dynamics available exact reasoning uses intractable The second key idea RL algorithms use powerful function approximation methods compactly represent value functions The significance allows dealing large high dimensional state action spaces What ideas fit nicely Samples focused small subset spaces belong clever function approximation techniques exploit It understanding interplay dynamic programming samples function approximation heart designing analyzing applying RL algorithms The purpose book allow reader chance peek beautiful field However certainly set accomplish goal In Kaelbling et al written nice compact survey approaches algorithms available time Kaelbling et al This followed publication book Bertsekas Tsitsiklis detailed theoretical foundations A years later Sutton Barto fathers RL published book presented ideas RL clear accessible manner Sutton Barto A recent comprehensive overview tools techniques dynamic programming optimal control given volume book Bertsekas 2007a b devotes chapter RL methods At times field rapidly developing books date pretty quickly In fact growing body new results Bertsekas maintains online version Chapter Volume II book time writing survey counted pages Bertsekas Other recent books subject include book Gosavi devotes pages reinforcement learning algorithms Chapter concentrating average cost problems Cao focuses policy gradient methods Powell presents algorithms ideas operations research perspective emphasizes methods capable handling large 1In book RL called neuro dynamic programming approximate dynamic programming The term neuro dynamic programming stems fact cases RL algorithms artificial neural networks control spaces Chang et al focuses adaptive sampling e simulation based performance optimization center recent book Busoniu et al function approximation Thus means RL researchers lack good body literature However missing self contained relatively short summary help newcomers field develop good sense state art existing researchers broaden overview field article similar Kaelbling et al updated contents To fill gap purpose short book Having goal keeping text short hopefully trou bling compromises The compromise present results total expected discounted reward criterion This choice motivated criterion widely easiest deal mathematically The compro mise background MDPs dynamic programming kept ultra compact appendix added explains basic results Apart book aims cover bit aspects RL level reader able understand what has how has implement algorithms presented Naturally selective present Here decision focus basic algorithms ideas available theory Special attention paid describing choices user tradeoffs come We tried impartial possible personal bias usual surely remained The pseudocode algorithms included hoping easier practically inclined reader implement algorithms described The target audience advanced undergaduate graduate students researchers practitioners want good overview state art RL quickly Researchers working RL enjoy reading parts RL literature familiar broadening perspective RL The reader assumed familiar basics linear algebra calculus probability theory In particular assume reader familiar concepts random variables conditional expectations Markov chains It helpful necessary reader familiar statistical learning theory essential concepts explained needed In parts book knowledge regression techniques machine learning useful This book parts In Section provide necessary ground It notation introduced followed short overview theory Markov Decision Processes description basic dynamic programming algorithms Readers familiar MDPs dynamic programming skim familiarize notation Readers familiar MDPs spend time moving rest book builds heavily results ideas presented The remaining parts devoted basic RL problems cf Figure devoted In Section problem learning predict values associated states studied We start explaining basic ideas called tabular case MDP small store value state array allocated computer s main memory The algorithm explained TD viewed learning analogue value iteration dynamic programming After consider challenging situation states fits computer s memory Clearly case compress table representing values Abstractly relying appropriate function approximation method First describe TD situation This followed description new gradient based methods GTD2 TDC viewed improved versions TD avoid convergence difficulties TD faces We discuss squares methods particular LSTD LSPE compare incremental methods described earlier Finally describe choices available implementing function approximation tradeoffs choices come The second Section devoted algorithms developed control learning First describe methods goal optimizing online performance In particular describe optimism face uncertainty principle methods explore environment based principle State art algorithms given bandit problems MDPs The message clever exploration methods large difference work needed scale available methods large problems The rest section devoted methods aim developing methods large scale applications As learning large scale MDPs significantly difficult learning MDP small goal learning relaxed learning good policy limit First direct methods discussed aim estimating optimal action values directly These viewed learning analogue value iteration dynamic programming This followed description actor critic methods thought counterpart policy iteration algorithm dynamic programming Both methods based direct policy improvement policy gradient e use parametric policy classes presented The book concluded Section lists topics exploration Figure Types reinforcement learning problems approaches Markov decision processes The purpose section introduce notation subsequent parts essential facts need theory Markov Decision Processes MDPs rest book Readers familiar MDPs skim section familiarize notation Readers unfamiliar MDPs suggested spend time section understand details Proofs results simplifications included Appendix A The reader interested learning MDPs suggested consult excellent books subject books Bertsekas Shreve Puterman volume book Bertsekas 2007a b Preliminaries We use N denote set natural numbers N R denotes set reals By vector v transposed v mean column vector The inner product finite dimensional vectors u v Rd u v d uivi The resulting norm u u u The maximum norm vectors defined u maxi d ui function f X R defined f supx X f x A mapping T metric spaces M1 d1 M2 d2 called Lipschitz modulus L R b M1 d2 T T b Ld1 b If T Lipschitz modulus L called non expansion If L mapping called contraction The indicator function event S denoted I S e I S S holds I S If v v x v shall denote partial derivative v respect d dimensional row vector Rd The total derivative expression v respect denoted d d v treated row vector Further v ddv If P distribution probability measure X P means X random variable drawn P Markov Decision Processes For ease exposition restrict attention countable MDPs discounted total expected reward criterion However technical conditions results extend continuous state action MDPs This holds true results presented later parts book A countable MDP defined triplet M X A P0 X countable non set states A countable non set actions The transition probability kernel P0 assigns state action pair x X A probability measure X R shall denote P0 x The semantics P0 following For U X R P0 U x gives probability state associated reward belongs set U provided current state x action taken We fix discount factor role clear soon The transition probability kernel gives rise state transition probability kernel P x y X A X triplet gives probability moving state x state y provided action chosen state x P x y P0 y R x In addition P P0 gives rise immediate reward function r X A R gives expected immediate reward received action chosen state x If Y x R x P0 x r x E R x In follows shall assume rewards bounded quantity R x X A R x R surely It immediate random rewards bounded R r sup x X A r x R holds An MDP called finite X A finite Markov Decision Processes tool modeling sequential decision making problems decision maker interacts system sequential fashion Given MDP M interaction happens follows Let t N denote current time stage let Xt X 2The probability P0 U x defined U Borel measurable set Borel measurability technical notion purpose prevent pathologies The collection Borel measurable subsets X R include practically interesting subsets X R In particular include subsets form x b subsets obtained subsets taking complement union intersection countable collections sets recursive fashion Almost surely means probability refer fact statement concerned holds probability space exception set events measure zero At A denote random state system action chosen decision maker time t respectively Once action selected sent system makes transition Xt Rt P0 Xt At In particular Xt random P Xt y Xt x At P x y holds x y X A Further E Rt Xt At r Xt At The decision maker observes state Xt reward Rt chooses new action At A process repeated The goal decision maker come way choosing actions maximize expected total discounted reward The decision maker select actions stage based observed history A rule describing way actions selected called behavior A behavior decision maker initial random state X0 define random state action reward se quence Xt At Rt t Xt Rt connected Xt At At action prescribed behavior based history X0 A0 R1 Xt At Rt Xt The return underlying behavior defined total discounted sum rewards incurred R t tRt Thus rewards far future worth exponentially reward received stage An MDP return defined formula called discounted reward MDP When MDP called undiscounted The goal decision maker choose behavior maximizes expected return irrespectively process started Such maximizing behavior said optimal Example Inventory control lost sales Consider problem day day control inventory fixed maximum size face uncertain demand Every evening decision maker decide quantity ordered day In morning ordered quantity arrives inventory filled During day stochastic demand realized demands independent common fixed distribution Figure The goal inventory manager manage inventory maximize present monetary value expected total future income The payoff time step t determined follows The cost associated purchasing At items KI At cAt Thus fixed entry cost K ordering nonzero items item purchased fixed price c Here K c In addition cost holding inventory size x In simplest case cost proportional size 4Mathematically behavior infinite sequence probability kernels t t maps histories length t probability distribution action space A t t x0 a0 r0 xt rt xt inventory proportionality factor h Finally selling z units manager paid monetary p z p In order problem interesting p h incentive order new items This problem represented MDP follows Let state Xt day t size inventory evening day Thus X M M N maximum inventory size The action At gives number items ordered evening day t Thus choose A M need consider orders larger inventory size Given Xt At size inventory given Xt Xt At M Dt b shorthand notation minimum numbers b max positive Dt N demand t th day By assumption Dt t sequence independent identically distributed d integer valued random variables The revenue day t Rt K I At c Xt At M Xt hXt p Xt At M Xt Equations written compact form Xt Rt f Xt At Dt appropriately chosen function f Then P0 given P0 U x P f x D U d I f x d U pD d Here pD probability mass function random demands D pD This finishes definition MDP underlying inventory optimization problem Inventory control operations research problems rise MDP Other problems include optimizing transportation systems optimizing schedules production MDPs arise naturally engineering optimal control problems optimal control chemical electronic mechanical systems class includes problem controlling robots Quite information theory problems represented MDPs e g optimal coding optimizing channel allocation sensor networks Another important class problems comes finance These include optimal portfolio management option pricing Figure Illustration inventory management problem In case inventory control problem MDP conveniently specified transition function f cf In fact transition functions powerful transition kernels MDP gives rise transition function f transition function f gives rise MDP In problems actions meaningful states For example ordering items room inventory sense However meaningless actions forbidden actions remapped actions like In cases unnatural leads convoluted dynamics Then better introduce additional mapping assigns set admissible actions state In MDPs states impossible leave If x state Xt s x holds surely s provided Xt x matter actions selected time t By convention assume reward incurred terminal absorbing states An MDP states called episodic An episode generally random time period beginning time terminal state reached In episodic MDP consider undiscounted rewards e Example Gambling A gambler enters game stake fraction At current wealth Xt She wins stake probability p loses stake probability p Thus fortune gambler evolves according Xt St 1At Xt Here St t sequence independent random variables taking values P St p The goal gambler maximize probability wealth reaches priori given value w It assumed initial wealth w This problem represented episodic MDP state space X w action space A We define Xt St 1At Xt w Xt w w terminal state Xt Xt Xt w The immediate reward zero long Xt w state reaches w time Rt Xt w Xt w If set discount factor total reward trajectory zero depending wealth reaches w Thus expected total reward probability gambler s fortune reaches w Based examples presented far reader unfamiliar MDPs believe MDPs come handy finite dimensional state action spaces If true In fact practical applications state action spaces large multidimensional spaces For example robot control application dimensionality state space times number joints robot An industrial robot s state space easily dimensional state space humanoid robot easily dimensions In real world inventory control application items multiple types prices costs change based state market state MDP s state Hence state space practical application large high dimensional The holds action spaces Thus working large multidimensional state action spaces considered normal situation examples presented section dimensional small state spaces viewed exceptions Value functions The obvious way finding optimal behavior MDP list behaviors identify ones highest possible value initial state Since general behaviors plan viable A better approach based computing value functions In approach computes called optimal value function allows determine optimal behavior relative easiness The optimal value V x state x X gives highest achievable expected return process started state x The function V X R called optimal value 5Hence case state action spaces continuous Notice definition MDPs general encompass case function A behavior achieves optimal values states optimal Deterministic stationary policies represent special class behaviors shall soon play important role theory MDPs They specified mapping maps states actions e X A Following means time t action At selected At Xt More generally stochastic stationary policy stationary policy maps states distributions action space When referring policy shall use x denote probability action selected state x Note stationary policy followed MDP e At Xt t N state process Xt t time homogeneous Markov chain We use stat denote set stationary policies For brevity follows policy instead stationary policy hoping because confusion A stationary policy MDP induce called Markov reward processes MRP An MRP determined pair M X P0 P0 assigns probability measure X R state An MRPM gives rise stochastic process Xt Rt t Xt Rt P0 Xt Note Zt t Zt Xt Rt time homogeneous Markov process R0 arbitrary random variable Xt Rt t second order Markov process Given stationary policy MDP M X A P0 transition kernel MRP X P0 induced M defined P0 x A x P0 x An MRP called finite state space finite Let define value functions underlying stationary policies For let fix policy stat The value function V X R underlying defined V x E t tRt X0 x x X understanding process Rt t reward process Xt At Rt t obtained following policy ii X0 selected random P X0 x holds states x This second condition makes conditional expectation defined state If initial state distribution satisfies condition influence definition values 6Value functions defined underlying behavior analogously definition given The value function underlying MRP defined way denoted V V x E t tRt X0 x x X It useful define action value function Q X A R underlying policy stat MDP Assume action A0 selected randomly P A0 holds A subsequent stages decision process actions chosen following policy Let Xt At Rt t resulting stochastic process X0 definition V Then Q x E t tRt X0 x A0 x X A Similarly V x optimal action value Q x state action pair x defined maximum expected return constraints process starts state x action chosen The underlying function Q X A R called optimal action value function The optimal value action value functions connected following equations V x sup A Q x x X Q x r x y X P x y V y x X A In class MDPs considered optimal stationary policy exists V x sup stat V x x X In fact policy stat satisfies equality A x Q x V x simultaneously states x X optimal Notice order hold x concentrated set actions maximize Q x In general given action value function Q X A R action maximizes Q x state x called greedy respect Q state x A policy chooses greedy actions respect Q states called greedy w r t Q Thus greedy policy respect Q optimal e knowledge Q sufficient finding optimal policy Similarly knowing V r P suffices act optimally The question find V Q Let start simpler question find value function policy Fact Bellman Equations Deterministic Policies Fix MDP M X A P0 discount factor deterministic policy stat Let r immediate reward function M Then V satisfies V x r x x y X P x x y V y x X This system equations called Bellman equation V Define Bellman operator underlying T RX RX T V x r x x y X P x x y V y x X With help T Equation written compact form T V V Note linear system equations V T affine linear operator If T maximum norm contraction fixed point equation T V V unique solution When state space X finite D states RX identified D dimensional Euclidean space V RX thought D dimensional vector V RD With identification T V written r P V appropriately defined vector r RD matrix P RD D In case written form r P V V The facts hold true MRPs Bellman operator T RX RX defined TV x r x y X P x y V y x X The optimal value function known satisfy certain fixed point equation Fact Bellman Optimality Equations The optimal value function satisfies fixed point equation V x sup A r x y X P x y V y x X Define Bellman optimality operator operator T RX RX T V x sup A r x y X P x y V y x X Note nonlinear operator presence sup With help T Equation written compactly T V V If T maximum norm contraction fixed point equation T V V unique solution In order minimize clutter follows write expressions like T V x T V x understanding application operator T takes precedence applycation point evaluation operator x The action value functions underlying policy MRP optimal action value function satisfy fixed point equations similar previous ones Fact Bellman Operators Fixed point Equations Action value Functions With slight abuse notation define T RX A RX A T RX A RX A follows T Q x r x y X P x y Q y y x X A T Q x r x y X P x y sup A Q y x X A Note T affine linear T nonlinear The operators T T maximum norm contractions Further action value function Q satisfies T Q Q Q unique solution fixed point equation Similarly optimal action value function Q satisfies T Q Q Q unique solution fixed point equation Dynamic programming algorithms solving MDPs The facts provide basis value policy iteration algorithms Value iteration generates sequence value functions Vk T Vk k V0 arbitrary Thanks Banach s fixed point theorem Vk k converges V geometric rate Value iteration conjunction action value functions case takes form Qk T Qk k converges Q geometric rate The idea Vk Qk close V resp Q policy greedy respect Vk resps Qk close optimal In particular following bound known hold Fix action value function Q let greedy policy w r t Q Then value policy lower bounded follows e g Singh Yee Corollary V x V x Q Q x X Policy iteration works follows Fix arbitrary initial policy At iteration k compute action value function underlying k called policy evaluation step Next given Qk define k policy greedy respect Q k called policy improvement step After k iterations policy iteration gives policy worse policy greedy w r t value function computed k iterations value iteration procedures started initial value function However computational cost single step policy iteration higher policy evaluation step update value iteration Value prediction problems In section consider problem estimating value function V underlying Markov reward process MRP Value prediction problems arise number ways Estimating probability future event expected time event occurs action value function underlying policy MDP value prediction problems Specific applications estimating failure probability large power grid Frank et al estimating taxi times flights busy airports Balakrishna et al mention possibilities Since value state defined expectation random return process started given state obvious way estimating value compute average multiple independent realizations started given state This instance called Monte Carlo method Unfortunately variance returns high means quality estimates poor Also interacting system closed loop fashion e estimation happens interacting system impossible reset state system particular state In case Monte Carlo technique applied introducing additional bias Temporal difference TD learning Sutton doubt significant ideas reinforcement learning method address issues Temporal difference learning finite state spaces The unique feature TD learning uses bootstrapping predictions targets course learning In section introduce basic TD algorithm explain bootstrapping works Next compare TD learning vanilla Monte Carlo methods argue merits Finally present TD algorithm unifies approaches Here consider case small finite MRPs value estimates states stored main memory computer array table known tabular case reinforcement learning literature Extensions ideas presented large state spaces tabular representations feasible described subsequent sections Tabular TD Fix finite Markov Reward Process M We wish estimate value function V underlyingM given realization Xt Rt t ofM Let V t x denote estimate state x time t V In tth step TD performs following calculations t Rt V t Xt V t Xt V t x V t x t t I Xt x x X Here step size sequence t t consists small nonnegative numbers chosen user Algorithm shows pseudocode algorithm A closer inspection update equation reveals value changed associated Xt e state visited cf line pseudocode Further t value Xt moved target Rt V t Xt Since target depends estimated value function algorithm uses bootstrapping The term temporal difference algorithm comes t defined Algorithm The function implementing tabular TD algorithm This function called transition function TD0 X R Y V Input X state Y state R immediate reward associated transition V array storing current value estimates R V Y V X V X V X return V difference values states corresponding successive time steps In particular t called temporal difference error Just like algorithms reinforcement learning tabular TD stochastic approximation SA algorithm It easy converges converge function V expected temporal difference given V FV x def E Rt V Xt V Xt Xt x zero states x states sampled infinitely A simple calculation shows FV T V V T Bellman operator underlying MRP considered By Fact FV unique solution value function V Thus TD converges states sampled infinitely converge V To study algorithm s convergence properties simplicity assume Xt t N stationary ergodic Markov chain Further identify approximate value functions V t D dimensional vectors e g V t V t xi D D X X x1 xD Then assuming step size sequence satisfies Robbins Monro RM conditions t t t 2t sequence V t RD t N track trajectories ordinary differential equation ODE v t c F v t t c D v t RD e g Borkar Borrowing notation ODE written v r P I v Note linear ODE Since eigenvalues P I lie open left half complex plane ODE globally asymptotically stable From standard results 7Remember Markov chain Xt t N ergodic irreducible aperiodic positive recurrent Practically means law large number holds sufficiently regular functions chain SA follows V t converges surely V On step sizes Since algorithms discuss use step sizes worth spending time discussing choice A simple step size sequence satisfies conditions t c t c More generally step size sequence form t ct work long Of step size sequences gives smallest step sizes Asymptotically choice best point view transient behavior algorithm choosing closer work better choice step sizes bigger algorithm larger moves It possible better In fact simple method called iterate averaging Polyak Juditsky known achieve best possible asymptotic rate conver gence However despite appealing theoretical properties iterate averaging rarely practice In fact practice people use constant step sizes clearly violates RM conditions This choice justified based grounds First algorithms non stationary environment e policy evaluated change Second algorithms small sample regime When constant step size parameters converge distribution The variance limiting distri bution proportional step size chosen There great deal work going developing methods tune step sizes automatically Sutton Schraudolph George Powell references However jury methods best With small change algorithm observation sequence form Xt Rt Yt t Xt t arbitrary ergodic Markov chain X Yt Rt P0 Xt The change concerns definition temporal differences t Rt V Yt V Xt Then extra conditions V t converges surely value function un derlying MRP X P0 In particular distribution states Xt t play role This interesting multiple reasons For example samples generated simulator able control distribution states Xt t independently MRP This useful counterbalance unevenness stationary dis tribution underlying Markov kernel P Another use learn target policy MDP following policy called behavior policy Assume simplicity target policy deterministic Then Xt Rt Yt t obtained skipping state action reward state quadruples trajectory generated behavior policy action taken match action taken given state target policy keeping rest This technique allow learn multiple policies time generally multiple long term prediction problems When learning policy following called policy learning Because shall learn ing based triplets Xt Rt Yt t Yt Xt policy learning A technical use goal apply algorithm episodic problem In case triplets Xt Rt Yt chosen follows First Yt sampled transition kernel P X If Yt terminal state let Xt Yt Xt P0 P0 user chosen distribution X In words terminal state reached process restarted initial state distribution P0 The period time restart P0 reaching terminal state called episode episodic problems This way generating sample shall called continual sampling restarts P0 Being standard linear SA method rate convergence tabular TD usual order O t consult paper Tadic references precise results However constant factor rate largely influenced choice step size sequence properties kernel P0 value Every visit Monte Carlo As mentioned estimate value state computing sample means giving rise called visit Monte Carlo method Here define precisely mean compare resulting method TD To firm ideas consider episodic problem impossible finitely compute return given state trajectories infinitely long Let underlying MRP beM X P0 let Xt Rt Yt t generated continual sampling M restarts distribution P0 defined X Let Tk k sequence times episode starts k XTk sampled P0 For given time t let k t unique episode index t Tk Tk Let Rt Tk t s t s tRs denote return time t end episode Clearly V x E Rt Xt x state x P Xt x Hence sensible way updating estimates use V t x V t x t Rt V t x I Xt x x X Algorithm The function implements visit Monte Carlo algorithm es timate value functions episodic MDPs This routine called end episode state reward sequence collected episode Note algorithm shown linear time space complexity length episodes function EveryVisitMC X0 R1 X1 R2 XT RT V Input Xt state time t Rt reward associated t th transition T length episode V array storing current value function estimate sum t T downto sum Rt sum target Xt sum V Xt V Xt target Xt V Xt end return V Monte Carlo methods use multi step predictions value cf Equation called multi step methods The pseudo code update rule shown Algorithm This algorithm instance stochastic approximation As behavior governed ODE v t V v t Since unique globally asymptotically stable equilibrium ODE V V t converges V surely Since algorithms achieve goal wonder algorithm better TD Monte Carlo First let consider example TD converges faster Consider undiscounted episodic MRP shown Figure The initial states With high probability process starts state process starts state frequently Consider TD behave state By time state visited kth time average state visited k times Assume t t At state TD update reduces averaging Bernoulli rewards incurred leaving state At kth visit state Var V t k clearly E V t V Thus target update state estimate true value state accuracy increasing k Now consider Monte Carlo method The Monte Carlo method ignores estimate value state uses Bernoulli rewards directly In particular Var Rt Xt e variance target change time On example makes Monte Carlo method slower converge showing bootstrapping help To example bootstrapping helpful imagine problem modified reward associated transition state state deterministically P0 P0 R Ber Figure An episodic Markov reward process In example transitions determin istic The reward zero transitioning state state given Bernoulli random variable parameter State terminal state When process reaches terminal state reset start state The probability starting state probability starting state equal In case Monte Carlo method faster Rt true target value value state close true value TD wait estimate value state close true value This slows convergence TD In fact imagine longer chain states state follows state N time nonzero reward incurred transitioning state N state N In example rate convergence Monte Carlo method impacted value N TD slower N increasing informal argument Sutton formal exact rates Beleznay et al TD Unifying Monte Carlo TD The previous examples Monte Carlo TD merits Interestingly way unify approaches This achieved called TD family methods Sutton Here parameter allows interpolate Monte Carlo TD updates gives TD TD e TD equivalent Monte Carlo method In essence given targets TD update given mixture multi step return predictions Rt k t k s t s tRs k V t Xt k mixing coefficients exponential weights k k Thus TD multi step method The algorithm incremental introduction called eligibility traces In fact eligibility traces defined multiple ways TD exists corre spondingly multiple forms The update rule TD called accumulating traces follows t Rt V t Xt V t Xt zt x I x Xt zt x V t x V t x t t zt x z0 x x X Here zt x eligibility trace state x The rationale value zt x modulates influence TD error update value stored state x In variant algorithm eligibility traces updated according zt x max I x Xt zt x x X This called replacing traces update In updates trace decay parameter controls bootstrapping When algorithms identical TD lim k kRt k Rt Rt V t Xt When TD algorithm accumulating traces simulate previously described visit Monte Carlo algorithm episodic problems For exact equivalence needs assume value updates happen end trajectories point updates accumulated The statement follows discounted sum temporal differences trajectory start state terminal state telescopes gives difference return trajectory value estimate start state Replacing traces correspond version Monte Carlo algorithm state updated encountered time trajectory The corresponding algorithm called visit Monte Carlo method The formal correspondence visit Monte Carlo method TD replacing traces known hold undiscounted case Singh Sutton Algorithm The function implements tabular TD algorithm replacing traces This function called transition function TDLambda X R Y V z Input X state Y state R immediate reward associated transition V array storing current value function estimate z array storing eligibility traces R V Y V X x X z x z x X x z x end V x V x z x end return V z Algorithm gives pseudocode corresponding variant replacing traces In practice best value determined trial error In fact value changed algorithm impacting convergence This holds wide range possible eligibility trace updates precise conditions Bertsekas Tsitsiklis Section The replacing traces version algorithm believed perform better practice examples happens consult Sutton Barto Section It noted helpful learner partial knowledge state related situation function approximation approximate value functions large state space topic section In summary TD allows estimate value functions MRPs It generalizes Monte Carlo methods non episodic problems allows bootstrapping Further appropriately tuning converge significantly faster Monte Carlo methods TD Algorithms large state spaces When state space large infinite feasible separate value state memory In cases seek estimate values form V x x x X Rd vector parameters X Rd mapping states d dimensional vectors For state x components x vector x called features state x called feature extraction method The individual functions X R defining components called basis functions Examples function approximation methods Given access state features basis functions constructed great different ways If x R e X R use polynomial Fourier wavelet basis order For example case polynomial basis x x x2 xd orthogonal system polynomials suitable measure stationary distribution states available This choice help increase convergence speed incremental algorithms discuss soon In case multidimensional state spaces tensor product construction commonly way construct features given features states individual components The tensor product construction works follows Imagine X X1 X2 Xk Let Xi Rdi feature extractor defined ith state component The tensor product k feature extractor d d1d2 dk components conveniently indexed multi indices form i1 ik ij dj j k Then i1 ik x i1 x1 i2 x2 k ik xk When X Rk particularly popular choice use radial basis function RBF networks xi G xi x G xi x di Here x j R j di fixed user G suitable function A typical choice G G z exp z2 scale parameter The tensor product construct cases places Gaussians points regular grid ith basis function x exp x x x X denotes point regular d1 dk grid A related method use kernel smoothing V x d iG x x d j 1G x x j d G x x d j 1G x x j More generally use V x d isi x si d si x holds x X In case V averager Averagers important reinforcement learning mapping V non expansion max norm makes behaved approximate dynamic programming An alternative use binary features e x d Binary features advantageous computational point view x d V x x Thus value state x computed cost s additions x s sparse e s elements x non zero provided direct way computing index non zero components feature vector This case state aggregation define features In case coordinate functions individual features correspond indicators non overlapping regions state space X union covers X e regions form partition state space Clearly case x constant individual regions state aggregation essentially discretizes state space A state aggregator function approximator averager Another choice leads binary features tile coding originally called CMAC Albus In simplest version tile coding basis functions correspond indicator functions multiple shifted partitions tilings state space s tilings s sparse To tile coding effective function approximation method offsets tilings corresponding different dimensions different The curse dimensionality The issue tensor product constructions state aggre gation straightforward tile coding state space high dimensional quickly intractable For example tiling D cubical regions lengths gives rise d D dimensional feature parameter vectors If D enormous number d This problematic state representations hundreds dimensions common applications At stage wonder possible successfully deal applications state lives high dimensional space What comes rescue actual problem complexity lower predicted merely counting number dimensions state variable guarantee happens To holds note problem multiple representations come low dimensional state variables high Since cases state representation chosen user conservative fashion happen chosen representation state variables irrelevant It happen states actually encountered lie lie close low dimensional submanifold chosen high dimensional state space To illustrate imagine industrial robot arm joints degrees freedom The intrinsic dimensionality state twice number degrees freedom arm dynamics second order One approximate state representation high resolution camera images arm close succession account dynamics multiple angles account occlusions The dimensionality chosen state representation easily range millions intrinsic dimensionality In fact cameras higher dimensionality A simple minded approach aims minimizing dimensionality suggest use cameras possible But information hurt Therefore quest clever algorithms function approximation methods deal high dimensional low complexity problems Possibilities include strip like tilings combined hash functions interpolators use low discrepancy grids Lemieux Chapter random projections Das gupta Freund Nonlinear function approximation methods examples include neural networks sigmoidal transfer functions hidden layers RBF net works centers considered parameters nonparametric techniques hold great promise Nonparametric methods In nonparametric method user start fixed finite dimensional representation previous examples allows representation grow change needed For example k nearest neighbor method regression given data Dn x1 v1 xn vn xi Rk vi R value location x predicted V k D x n vi K k D x xi k K k D x x x closer x kth closest neighbor x D zero Note k n j 1K k D x xj Replacing k expression sum replacing K k D x data based kernel KD e g Gaussian centered x standard deviation proportional distance kth nearest neighbor arrive nonparametric kernel smoothing V k D x n vi KD x xi n j KD x xj compared parametric counterpart Other examples include meth ods work finding appropriate function large infinite dimensional function space fits empirical error The function space usually Reproducing Kernel Hilbert space convenient choice point view optimization In special cases spline smoothers Wahba Gaussian process regression Rasmussen Williams Another idea split input space recursively finer regions heuristic criterion predict simple method values leafs leading tree based methods The border parametric nonparametric methods blurry For example linear predictor number basis functions allowed change e new basis functions introduced needed nonparametric method Thus experiments different feature extraction methods point view overall tuning process uses nonparametric technique In fact viewpoint follows practice true parametric methods rarely The advantage nonparametric methods inherent flexibility However comes usually price increased computational complexity Therefore non parametric methods efficient implementations important e g use k D trees implementing nearest neighbor methods Fast Gaussian Transform imple menting Gaussian smoother Also nonparametric methods carefully tuned easily overfit underfit For example k nearest neighbor method k large method going introduce smoothing e underfit k small fit noise e overfit Overfitting discussed Section For information nonparametric regression reader advised consult books Ha rdle Gyo rfi et al Tsybakov Although discussion assume parametric function approximation method cases linear function approximation algorithms extended nonparametric techniques We mention extensions exist appropriate Up discussion implicitly assumed state accessible measurement This rarely case practical applications Luckily methods discuss actually need access states directly perform equally sufficiently descriptive feature based representation states available camera images robot arm example A common way arriving representation construct state estimators observers control terminology based history observations large literature machine learning control The discussion techniques lies outside scope present book TD function approximation Let return problem estimating value function V Markov reward process M X P0 assume state space large infinite Let D Xt Rt t realization M The goal estimate value function M given D incremental manner Choose smooth parametric function approximation method V Rd e Rd V X R V x exists x X The generalization Algorithm The function implementing TD algorithm linear function approx imation This function called transition function TDLambdaLinFApp X R Y z Input X state Y state R immediate reward associated transition Rd parameter vector linear function approximation z Rd vector eligibility traces R Y X z X z z return z tabular TD accumulating eligibility traces case value functions approximated members V Rd uses following updates Sutton t Rt Vt Xt Vt Xt zt Vt Xt zt t t t t zt z0 Here zt Rd Algorithm shows pseudocode algorithm To algorithm generalization tabular TD assume X x1 xD let V x x x I x xi Note V linear parameters e V holds V Hence identifying zt t zt xi resp V t xi update reduces previous In policy version TD definition t t Rt Vt Yt Vt Xt Unlike tabular case policy sampling convergence longer guaranteed fact parameters diverge e g Bertsekas Tsitsiklis Example p This true linear function approximation distributions Xt t match stationary distribution MRPM Another case algorithm diverge nonlinear function approximation method e g Bertsekas Tsitsiklis Example p For examples instability Baird Boyan Moore On positive sure convergence guaranteed linear function approximation method X Rd ii stochastic process Xt t ergodic Markov process stationary distribution stationary distribution MRP M iii step size sequence satisfies RM conditions Tsitsiklis Van Roy Bertsekas Tsitsiklis p Section In results cited assumed components e d linearly independent When holds limit parameter vector unique In case e features redundant parameters converge limit depend parameter vector s initial value However limiting value function unique Bertsekas Assuming TD converges let denote limiting value t Let F V Rd space functions represented chosen features Note F linear subspace vector space real valued functions domain X The limit known satisfy called projected fixed point equation V F T V operators T F defined follows For m N let T m m step lookahead Bellman operator T m V x E m t tRt m V Xm X0 x Clearly V value function estimated fixed point T m m Assume Then operator T defined exponentially weighted average T T T V x m m T m V x For let T V lim T V V Notice T T Operator F projection It projects functions states linear space F respect weighted norm f x X f x x F V argmin f F V f The essence proof convergence TD composite operator F T contraction respect norm This result heavily exploits stationary distribution underlying M defines T For distributions composite operator contraction case TD diverge As quality solution found following error bound holds fixed point V V F V V Here contraction modulus F T Tsitsiklis Van Roy 1999a Bertsekas 2007b For sharper bounds Yu Bertsekas Scherrer From error bound V best approximation V F respect norm come surprise TD minimizes mean squared error design We let bound allows larger errors It known artifact analysis In fact Example book Bertsekas Tsitsiklis p simple MRP n states dimensional feature extractor given V poor approximation V V reasonable approximation Thus order good accuracy working choose function space F best approximation V small error At stage wonder makes sense A recent paper Van Roy suggests considering performance loss bounds instead approximation errors control learning task cf Section general disadvantage compared state aggregation considered Thus mean squared error solution large solution control performance resulting policy good obtained calculating TD solution However major reason prefer TD TD empirical evidence suggests converges faster TD practical sample sizes produces poor estimates e g Sutton Barto Section TD solves model Sutton et al Parr et al observed independently solution obtained TD thought solution deterministic MRP linear dynamics In fact argue holds case TD This suggests deterministic MRP captures essential features original MRP V good approximation V To firm statement following Parr et al let study Bellman error V T V V V X R T Note V X R A simple contraction argument shows V V V Hence V small V close V The following error decomposition shown hold V m m r m m m m Here r m rm F rm m P m F Pm errors modeling m step rewards transitions respect features respectively rm X R defined rm x E Rm X0 x Pm denotes function maps states d dimensional row vectors defined Pm x Pm x P m 1d x Here Pmi X R function defined Pmi x E Xm X0 x Thus Bellman error small m step immediate rewards m step feature expectations captured features We gets closer important features capture structure value function gets closer important capture structure immediate rewards immediate feature expectations This suggests best value e minimizes V depend features successful capturing short term long term dynamics rewards Gradient temporal difference learning That TD diverge policy learning situations spoils immaculate record In Section introduce methods avoid issue However computational time storage complexity methods significantly larger TD In section present recent algorithms introduced Sutton et al 2009b overcome instability issue converge TD solutions policy case efficient TD For simplicity consider case Xt Rt Yt t stationary process Xt different stationary distribution P linear function approximation linearly independent features Assume solution exists Consider objective function J V F TV Notice solutions minimizers J minimizers J solutions Thus minimizing J solution Let denote minimizer J Since assumption features linearly independent minimizer 8Parr et al observed The extension new J unique e defined Introduce shorthand notations t Rt V Yt V Xt Rt t t t Xt t Yt A simple calculation allows rewrite J following form J E t t E t t E t t Taking gradient objective function J 2E t t t w w E t t E t t Let introduce sets weights t approximate wt approximate w In GTD2 gradient temporal difference learning version update t chosen follow negative stochastic gradient J based assuming wt w t update wt chosen fixed wt converge surely w t t t t t t wt wt wt t t t t wt t Here t t t t step size sequences Note update equation wt t basic Least Mean Square LMS rule widely update rule signal processing Widrow Stearns Sutton et al 2009a shown standard RM conditions step sizes mild technical conditions t converges minimizer J surely However unlike TD convergence guaranteed independently distribution Xt t At time update GTD2 costs twice cost TD Algorithm shows pseudocode GTD2 To arrive second algorithm called TDC temporal difference learning correc tions write gradient J E t t E t t w Algorithm The function implementing GTD2 algorithm This function called transition function GTD2 X R Y w Input X state Y state R immediate reward associated transition Rd parameter vector linear function approximation w Rd auxiliary weight f X f Y R f f f w f f w w f return w Leaving update wt unchanged arrive t t t t t t t t wt wt wt t t t t wt t The pseudocode update identical GTD2 line replaced f f In TDC update wt use larger step sizes update t t o t This makes TDC member family called timescale stochastic approximation algorithms Borkar If addition condition standard RM conditions satisfied step size sequences t holds surely Sutton et al 2009a More recently algorithms extended nonlinear function approximation Maei et al 2010a Also suffices t t Maei personal communication The algorithms extended use eligibility traces Maei Sutton Note algorithms derived gradient objective function true stochastic gradient methods sense expected weight update direction different direction negative gradient objective function In fact methods belong larger class pseudo gradient methods The methods differ approximate gradients remains seen better Least squares methods The methods discussed far similar LMS algorithm adaptive filtering taking small steps parameter space following noisy gradient like signal As similarly LMS algorithm sensitive choice step sizes distance initial parameter limit point eigenvalue structure matrix A determines dynamics updates e g TD A E t t t Over years ideas appeared literature address issues These essentially parallel available adaptive filtering literature A non exhaustive list includes use adaptive step sizes Sutton George Powell normalizing updates Bradtke reusing previous samples Lin Although techniques help weaknesses In adaptive filtering algorithm known address deficiencies LMS known LS squares algorithm In section review analogous methods reinforcement learning LSTD Least squares temporal difference learning In limit infinite number examples TD finds parameter vector satisfies E t t notation previous section Given finite sample Dn X0 R1 Y1 X1 R2 Y2 Xn Rn Yn approximate n n t t t Plugging t Rt t t equation linear In particular matrix A n n n t t t t non singular solution simply n A n b n b n n n t Rt 1t If inverting A n afforded e dimensionality features large method called times method better approximation equilibrium solution TD incremental order method negatively impacted eigenvalue spread matrix E A n The idea directly computing solution Bradtke Barto Algorithm The function implementing RLSTD algorithm This function called transition Initially C set diagonal matrix small positive diagonal elements C I function RLSTD X R Y C Input X state Y state R immediate reward associated transition C Rd d Rd parameter vector linear function approximation f X f Y g f f C g d row vector gf v Cf R f f v C C v g return C resulting algorithm squares temporal difference learning LSTD Using terminology stochastic programming LSTD seen use sample average ap proximation Shapiro In terminology statistics belongs called Z estimation family procedures e g Kosorok Section It simple obser vation LSTD solution exists LSTD minimizes empirical approximation projected squared Bellman error F TV V linear space F Antos et al Using Sherman Morrison formula derive incremental version LSTD anal ogously recursive squares RLS method derived adaptive filtering Widrow Stearns The resulting algorithm called recursive LSTD RLSTD works follows Bradtke Barto Choose Rd let C0 Rd d C0 small positive definite matrix e g C0 I small Then t Ct Ct Ct t t t Ct t t Ctt t t Ct t t Ctt t t t The computational complexity update O d2 Algorithm shows pseudocode algorithm Boyan extended LSTD incorporate parameter TD called resulting algorithm LSTD Note sense needs Xt Yt TD errors telescope The LSTD solution derived It defined parameter value makes cumulated updates zero n n t t zt zt t s t ss eligibility traces This linear previous comments apply The recursive form LSTD RLSTD studied Xu et al independently Nedic Bertsekas See Algorithm pseudocode closely related algorithm One issue LSTD stated Equation fail solution In policy case large sample sizes solution exist When solution exist commonly suggested trick add small positive diagonal matrix matrix inverted corresponds starting positive diagonal matrix RLSTD However trick guaranteed work A better approach based observation matrix invertible LSTD parameter vector minimizer projected Bellman error Since minimizer projected Bellman error defined instead solving zero aim minimizing projected Bellman error Under standard assumptions sample follows law large numbers simple continuity argument LSTD recursive variants converge surely solution projected fixed point equation solution exists This formally shown Bradtke Barto Xu et al Nedic Bertsekas Although results shown policy case easy hold policy case provided limiting solution exists As promised R LSTD avoids difficulties associated tuning incremental algorithms It relies step sizes sensitive eigenvalue structure ofA choice initial value Experimental results Bradtke Barto Boyan Xu et al confirmed parameters obtained R LSTD converge faster obtained TD However computational properties different TD We discuss implications reviewed LSPE algorithm LSPE Least squares policy evaluation An alternative LSTD LSTD squares policy evaluation LSPE short Bertsekas Ioffe The basic idea algorithm mimic multi step value iteration Again method assumes linear function approximation Algorithm The function implementing batch mode LSPE update This function called repeatedly convergence function LambdaLSPE D Input D Xt At Rt Yt t n list transitions Rd parameter vector A b A Rd d b Rd R t n downto f Xt v f Rt Yt v b b v f A A f f end A 1b return It works follows Define n s step prediction value Xs V s n s n q s q s q define loss Jn n n s s V s n Then LSPE updates parameters t t t argmin Jnt t t t t step size sequence nt t non decreasing sequence integers Bertsekas Ioffe considered case nt t logical choice algorithm online learning scenario When algorithm finite n observations set nt n nt min n t Note Jn quadratic solution minimization problem obtained closed form The resulting algorithm shown Algorithm A recursive incremental version LSPE available Similarly LSTD requires O d2 operations time step nt t To sense behavior LSPE consider update special case t t argmin nt nt s Xs Rs Vt Ys Thus case LSPE solves linear regression problem implementing called fitted value iteration algorithm policy evaluation linear function approximation For fixed non random value t true regression function underlying squares problem E Rs Vt Ys Xs x TVt x Thus function space F rich sample size nt large expect t close TVt x algorithm implements value iteration approximate manner The case given similar interpretation When t parameters moved minimizer Jnt t proportion size t The role smoothing updates way stabilize parameters small sample sizes e nt d range ii ensure policies changed gradually algorithm subroutine control algorithm cf Section The idea smoothing parameter updates LSTD Just like LSTD multi step version LSPE e requires Xt Yt The parameter plays role similar role TD methods Increasing expected reduce bias increase variance unlike TD LSPE bootstraps However effect bootstrapping diminishing nt Under standard assumptions sample nt t LSPE known converge surely solution projected fixed point equation decreasing Nedic Bertsekas constant step sizes Bertsekas et al In case convergence guaranteed t Note included range According Bertsekas et al LSPE competitive LSTD sense distance parameters updated LSTD LSPE soon smaller statistical inaccuracy resulting use finite sample Experimental results obtained Bertsekas et al earlier Bertsekas Ioffe train Tetris playing program indicate LSPE competitive algorithm Moreover LSPE defined inverses involved exist limit appropriate initialization LSTD I shall defined policy settings Comparing squares TD like methods The price increased stabil ity accuracy squares techniques increased computational complexity In particular sample size n complexity straightforward implementation LSTD O nd2 d3 complexity RLSTD O nd2 applies LSPE For comparison computational complexity lightweight incremental methods dis cussed previously O nd features sparse Thus lightweight algorithms d passes sample squares method makes single pass The trick saving reusing observations increase accuracy TD based al gorithms suggested Lin dubbed method experience replay When value d large lightweight methods perform squares methods given computation time When d large squares methods feasible For example Silver et al use million features building value function game Go When d range squares methods feasible It complicated compare approaches account fre quency observations arrive storage space available access time storage etc Hence look interesting case new observations avail able negligible cost In case need store reuse data quality solutions depend methods computation speed To compare approaches fix time T available computation In time T squares methods limited process sample size n T d2 lightweight methods process sample size n nd Let look precision resulting parameters Assume limit parameters Denote t parameter obtained LSTD processing t observations denote t parameter obtained TD method Then expects t C1t t C2t Thus n n C2 C1 d Hence C2 C1 d lightweight TD like method achieve better accuracy opposite case squares procedures perform better As usual difficult decide priori As rule thumb based expect d relatively small squares methods converging faster d large lightweight incremental methods better results given fixed computation budget Notice analysis specific reinforcement learning methods applies cases incremental lightweight procedure compared squares like procedure similar analysis supervised learning problem e g Bottou Bousquet Realizing need efficient robust methods Geramifard et al recently introduced incremental version LSTD called iLSTD like LSTD computes matrix A n vector b n time step dimension parameter vector updated For sparse features e s components feature vector nonzero iteration complexity method O sd experimentally demonstrated accurate LSTD given number samples assuming sparsity The storage space needed iLSTD processing n samples O min ns2 d d2 Thus features sparse ns2 d2 iLSTD competitive LSTD incremental TD methods The choice function space In order able discuss choice function space meaningful manner need define quality approximate value function measured When ultimate goal value prediction reasonable choice use mean squared error MSE respect appropriate distribution states The choice metric clear goal learn good controller value estimation subroutine complex algorithm ones reviewed Section Therefore lack good understanding case purpose section stick MSE quality measure However believe conclusions section hold measures Learning viewed process selecting function space functions F represented finitely computer s memory For simplicity assume functions available described d parameters F V Rd One measure characterizes choice F functions F approximate target function V leading definition approximation error underlying F inf V F V V To decrease approximation error encouraged choose larger function space e V linear add independent features F larger However argue learning definition uses incomplete information increasing size function space double edged sword For simplicity let consider linear function approximation assume LSTD specified obtain parameters To situation simpler assume discount factor zero Xt Rt t d sample Xt In case V x r x E Rt Xt x Thanks LSTD actually seen compute minimizer empirical loss 9We deal issues precision e computers represent real numbers function Ln n n t Xt Rt Assume dimensionality feature space d large matrix rows X0 Xn row rank particular assume d n This implies minimum Ln zero n denotes solution n Xt Rt holds t n If observed rewards noisy resulting function poor approximation value function V e estimation error n V large The phenomenon fitting noise called overfitting If smaller d chosen general smaller function space F chosen overfitting likely happen However case approximation error larger Hence tradeoff approximation estimation errors To quantify tradeoff let parameter vector minimizes loss L E Xt Rt That argmin L A simple argument shows V actually projection V F The following bound known hold random rewards bounded value R finding optimal weights prediction time predicted values projected R R Gyo rfi et al Theorem p E n V C1 d log n n C2 V Here C2 universal constant C1 constant scales linearly variance range random rewards The term right hand bounds estimation error second term approximation error Increasing d increases term generally expected decrease second The argument leads bounds form follows By law large numbers Ln converges L fixed value Hence minimizing Ln hopes obtain good approximation precisely However Ln converges L value mean function Ln uniformly close L Thus minimizer Ln small loss measured L cf Figure Guaranteeing functions uniformly close 10Note truncation C1 independent distribution Xt Although theorem stated expected error similar results shown hold high probability Loss Functions n L Ln L Ln Figure Convergence Ln L Shown curves empirical loss Ln true loss L function parameter If curves uniformly close e Ln L small expect loss n close loss set Ln Ln harder dimensionality larger tradeoff estimation approximation errors Bounds similar hold e g value functions estimated LSTD sequence Xt t dependent provided mixes initial steps direction consult work Antos et al In fact noise comes immediate rewards Rt states Yt The tradeoff approximation estimation errors shows control algorithms Munos Szepesva ri Antos et al derive finite sample performance bounds variants fitted value iteration fitted actor critic method approximate policy iteration method respectively Recognizing importance choice function space difficulty choosing right growing interest automating choice lately One class methods aims constructing parsimonious set features basis functions These include tuning parameter Gaussian RBF gradient cross entropy method context LSTD Menache et al deriving new basis functions nonparametric techniques Keller et al Parr et al combination numerical analysis nonparametric techniques Mahadevan These methods attempt control tradeoff approximation estimation errors To account deficiency researchers explore nonparametric techniques originating supervised learning Examples line research include use regression trees Ernst et al kernelizing value estimation algorithms e g Rasmussen Kuss Engel et al Ghavamzadeh Engel Xu et al Taylor Parr These approaches implicitly explicitly regularize estimates control loss Kolter Ng designed algorithm inspired LASSO uses regularization implement feature selection context LSTD Although approaches inspired principled methods supervised learning known statistical properties Recently Farahmand et al developed regularization based approach comes statistical guarantees The difficulty nonparametric techniques computationally ex pensive As result algorithms planning fast simulator generate data cost generating new data negligible better use appropriate fast incremental method simple linear function approximation method features use sophisticated computationally expensive non parametric method Computational efficiency important limited data available quality solutions primary concern problem complex tuning function approximation necessary hand tuning infeasible Control We turn problem learning near optimal policy We start discussing forms control learning problems Section followed discussion interactive learning Section In sections Sections learning counterparts classical methods dynamic programming discussed A catalog learning problems Figure shows basic types control learning problems The criterion space problems split learner actively influence observations In case talk interactive learning facing non Figure Types reinforcement learning problems interactive learning problem Interactive learning potentially easier learner additional option influence distribution sample However goal learning usually different cases making problems incomparable general In case non interactive learning natural goal find good policy given ob servations A common situation sample fixed For example sample result experimentation physical system happened learn ing started In machine learning terms corresponds batch learning Batch learning problems confused batch learning methods opposite incremental k recursive iterative methods Since observations uncontrolled learner working fixed sample deal policy learning situation In cases learner ask data e simulator generate new data Here goal learn good policy quickly possible Consider interactive learning One possibility learning happens interacting real system closed loop fashion A reasonable goal optimize online formance making learning problem instance online learning Online performance measured different ways A natural measure use sum rewards incurred learning An alternative cost measure number times learner s future expected return falls short optimal return e number times learner com mits mistake Another possible goal produce performing policy soon possible find good policy given finite number samples like non interactive learning As opposed non interactive situation learner option control samples maximize chance finding good policy This learning problem instance active learning When simulator available learning algorithms solve planning problems In planning previous performance metrics irrelevant algorithms running 11The terms active learning passive learning appeal meaning covers situations discussed However unfortunately term active learning reserved machine learning special case interactive learning As result decided calling non interactive learning passive learning tempted interactive learning active learning time memory requirements primary concern Closed loop interactive learning The special feature interactive learning need explore In section use bandits e MDPs single state illustrate need exploration online active learning Next discuss active learning MDPs This followed discussion algorithms available online learning MDPs Online learning bandits Consider MDP single state Let problem maximizing return learning Since state instance classical bandit problems Robbins A basic observation bandit learner chooses action best estimated payoff e makes greedy choice fail find best action positive probability turn leads large loss Thus good learner actions look suboptimal e explore The question balance frequency exploring exploiting e greedy actions A simple strategy fix choose randomly selected action probability greedy choice This called greedy strategy Another simple strategy called Boltzmann exploration strategy according given sample means Qt A actions time t action drawn multinomial distribution A exp Qt A exp Qt Here controls greediness action selection results greedy choice The difference Boltzmann exploration greedy greedy account relative values actions Boltzmann exploration These algorithms extend easily case unrestricted MDPs provided estimates action values available If parameter greedy function time resulting sequence appro priately tuned greedy competitive sophisticated algorithms However best choice problem dependent known automated way obtaining good results greedy Auer et al The holds Boltzmann exploration strategy A better approach implement called optimism face uncertainty OFU principle Lai Robbins according learner choose action best upper confidence bound UCB A successful recent algorithm UCB1 implements principle assigning following UCB action time t Auer et al Ut rt R log t nt Here nt number times action selected time t rt sample mean nt rewards observed action range R R It shown failure probability Ut t Notice action s UCB larger information available Further action s UCB value increases tried Algorithms pseudocode UCB1 form routines action selection updating internal statistics When variance rewards associated actions small makes sense estimate variances use place range R algo rithm A principled way proposed analyzed Audibert et al The resulting algorithm outperforms UCB1 shown essentially unim provable The algorithm describe Section implements OFU principle MDPs way similar UCB1 The setting considered called frequentist agnostic setting assump tion distribution rewards independent actions time steps belong interval However priori knowledge distributions An alternative historically significant variant problem reward distributions known parametric form parameters assumed drawn form known prior distribution The problem find policy maximizes total expected cumulated discounted reward expectation random rewards parameters distributions This problem represented MDP state time t posterior parameters reward distributions For example rewards assume Bernoulli distri butions parameters sampled Beta distribution state time t A dimensional vector Beta distribution parameters Thus state space MDP complicated simplest examples In groundbreaking paper Gittins showed surprisingly optimal policy MDP assumes simple index form special cases calculated exactly efficiently e g case Bernoulli reward distributions mentioned The conceptual difficulty called Bayesian approach policy optimal average collection randomly chosen environments guarantee policy perform individual environments The appeal Bayesian approach conceptually simple exploration Algorithm The function implementing action selection UCB1 By assumption initially n r reward received lie interval Further c c function UCB1Select r n t Input r n arrays size A t number time steps far Umax A U r R sqrt log t n U Umax Umax U end end return problem reduced computational problem Algorithm The function implementing update routine UCB1 The update updates action counters estimates average reward called interaction function UCB1Update A R r n Input A action selected R associated reward r n arrays size A t number time steps far n A n A r A r A n A R r A return r n Active learning bandits Consider active learning case MDP single state Let goal find action highest immediate reward given T interactions Since rewards received course interaction matter reason try action seen worse action sufficient certainty The remaining actions tried hope proving suboptimal A simple way achieve compute upper lower confidence bounds action Ut Qt R log A T 2t Lt Qt R log A T 2t eliminate action Ut maxa A Lt Here user chosen parameter specifies target confidence algorithm allowed fail return action highest expected reward Apart constant factors estimated variances confidence bounds algorithm unimprovable Even Dar et al Tsitsiklis Mannor Mnih et al Active learning Markov Decision Processes There exist theoretical works consider active learning MDPs Deterministic environments considered Thrun Berman It turns bounds given Thrun significantly improved follows Assume MDP M deterministic Then MDP s transition structure recovered n2m steps n X m A Ortner A procedure achieves follows The task explore actions states exactly Hence time t given known dynamics find closest state unexplored action In n steps state reached action chosen explored Since altogether nm state action pairs explore total number time steps needed n2m Given transition structure reward structure explored accuracy probability k log nm visits state action pairs If takes e n2m time steps visit state action pairs exploration policy ke steps learner accurate model environment Knowing model allows learner find policy value close optimal value state assuming simplicity Thus altogether find overall optimal policy n2m 4e log nm steps needed According author s knowledge exist works consider analogous problem finding uniformly optimal strategy stochastic MDP Even Dar et al consider active learning finite stochastic MDPs strong assumption learner reset state MDP arbitrary state This way avoid challenge navigating unknown MDP That major challenge seen exists MDPs random exploration takes exponential time MDPs size visit parts state space Consider example chain like MDP n states X n Let A L1 L2 R Actions L1 L2 decrement state action R increments The state changed boundaries action lead state 12Curiously argument new 13This bound improves bound Thrun The bound shown tight asymptotic sense outside X The policy selects actions uniformly random need 2n n steps average reach state n state Howard However policy systematically explores state space need O n actions reach state n state Assume immediate rewards zero state n reward Consider explore exploit learner explores MDP randomly estimates sufficient accurate e g visited state action pairs sufficiently times Clearly learner exponentially steps switching exploitation learner suffer huge regret The situation better agent uses simple exploration strategy based estimates values actions A problem closely related active learning reset studied Kearns Singh They proposed E3 algorithm explores unknown stochastic MDP stops knows good policy state visited They proved discounted MDPs E3 needs polynomial number interactions uses poly resources relevant parameters problem stops In follow work Brafman Tennenholtz introduced R max algorithm refines E3 algorithm proved similar results Another refinement E3 Domingo proposed use adaptive sampling increase efficiency MDP near deterministic transitions If problem undiscounted E3 R max need knowledge called mixing time MDP work properly When knowledge available algorithms know stop Brafman Tennenholtz Little known performance active learning algorithms practical problems Some experimental results heuristic algorithms found paper S ims ek Barto Online learning Markov Decision Processes Let return online learning MDPs One possible goal minimize regret e difference total reward achieved optimal policy received learner This problem considered section Another possible goal minimize number time steps algorithm s future expected return falls short optimal expected return prespecified This problem considered second section Regret minimization UCRL2 algorithm Consider finite small MDP M X A P0 Assume random immediate rewards bound lie simplicity assume deterministic stationary policies visit states eventually probability e MDP unichain Under condition policy gives rise recurrent Markov chain X unique stationary distribution Define long run average reward x X x r x x If want emphasize dependence long run average reward MDP M write M Let denote optimal long run average reward max stat Consider learning algorithm A e A history dependent behavior Define regret A RAT T RAT RAT T t Rt sum rewards received time T following A Minimizing regret clearly equivalent maximizing total reward Hence consider problem minimizing regret Notice RAT o T e rate growth regret sublinear long term average reward A e A consistent The UCRL2 algorithm described achieves logarithmic regret In order state bound define D called diameter MDP largest number steps average takes reach state state MDP Further let g gap performance optimal policy second best policy Then according Auer et al confidence parameter UCRL2 set 3T following holds expected regret E R UCRL2 3T T O D2 X A log T g One issue bound gap g small case bound vacuous small values T An alternative bound independent g takes form E R UCRL2 3T T O D X A T log T Auer et al Note bounds vacuous MDP infinite diameter happens case MDP states accessible states e MDP transient states The algorithm known enjoy regret bounds MDP transient states Bartlett Tewari However algorithm requires priori knowledge parameter MDP At present known knowledge necessary achieve non trivial bound The issue transient states costly distinguish transient states states hard reach The UCRL2 algorithm Algorithm implements optimism face uncertainty principle It constructs confidence intervals estimates transition probabil ities immediate reward function These define set plausible MDPs Ct When comes compute policy UCRL2 finds model M t Ct policy t gives approximately highest average reward class t M t max M Ct M t Note essential element UCRL2 update policy time step waits quality statistics available state action pair sufficiently improved This implemented line check visit count current state action pair A crucial step algorithm computation t This procedure OptSolve cf Algorithm undiscounted value iteration special MDP In MDP actions form pair p A p plausible state distribution given statistics collected far x The state distribution associated p exactly p Further immediate reward x associated p highest plausible reward given local statistics x PAC MDP algorithms As mentioned alternative minimizing regret minimize number times learner s future expected return falls short optimal return prespecified margin Kakade An online learning algorithm called PAC MDP measure bounded high probability polynomial function natural parameters MDP time step polynomially computational steps performed Algorithms known PAC MDP include R max Brafman Tennenholtz Kakade MBIE Strehl Littman Delayed Q learning Strehl et al optimistic initialization based algorithm Szita Lo rincz MorMax Szita Szepesva ri Of MorMax enjoys best bound number suboptimal steps T According bound 14The published proofs E3 Kearns Singh R max concern slightly different criterion discussion previous section Kakade proved improved version R max PAC MDP He proved lower bounds Algorithm The UCRL2 algorithm function UCRL2 Input confidence parameter x X x a1 Initialize policy n2 n3 r n n r Initialize arrays t true A X n X A max n2 X A Enough new information n2 n2 n n3 n3 n r r r Update model n n r OptSolve n2 n3 r t Update policy A X end R Y ExecuteInWorld A Execute action world n X A n X A n X A Y n X A Y r X A r X A R X Y t t end probability T O X A Vmax log O hides terms logarithmic MDP parameters Vmax upper bound optimal value function e Vmax r One notable feature bounds scales log linearly size state space A similar bound available Delayed Q learning dependence bound worse parameters bounds feature available algorithms The algorithms mentioned implement OFU principle way The main issue algorithms including UCRL variants inherently limited small finite spaces Larger state spaces explicitly considered Kakade et al Strehl Littman considered restricted classes MDPs provided meta algorithms address exploration problem There difficulties approaches First practice difficult verify particular problem interested belongs said classes MDPs Second proposed algorithms require black box MDP solvers Since solving large MDPs difficult problem algorithms hard implement Algorithm Procedure finding optimistic policy UCRL2 function OptSolve n2 n3 r t Input n2 n3 store counters r stores total rewards confidence parameter u a1 Initialize policy repeat M m idx sort u u idx u idx x X unew A r r x n2 x sqrt ln X A t max n2 x c sqrt ln A t max n2 x p n3 x n2 x p idx min p idx c j X repeat j j P sum p p idx j p idx j min P P p idx j v r inner product p u v unew x unew v end end M max M unew u x m min m unew u x u x unew end u u M m sqrt t return An alternative techniques use Bayesian approach address explo ration issue e g Dearden et al Strens Poupart et al Ross Pineau The pros contras approach case bandits difference computational challenges multiply To best knowledge experimental works concerns online learning continuous state MDPs Jong Stone Nouri Littman Jong Stone proposed method interpreted practical implemen tation ideas Kakade et al Nouri Littman experimented multi resolution regression trees fitted Q iteration The main message works explicit exploration control beneficial Despite potential huge performance gains result systematic explo ration current practitioners reinforcement learning largely neglect issue systematic exploration best use simple heuristics guide exploration Certainly cases systematic exploration needed e g Szepesva ri Nascimento Powell Further simple methods optimistic initialization reasonable performance practice Since systematic exploration hardly possible good collection learning algorithms aimed learning good policies efficient man ner follows focus reviewing algorithms belong collection Direct methods In section review algorithms aim approximate optimal action value function Q directly The reviewed algorithms thought sample based ap proximate versions value iteration generate sequence action value functions Qk k The idea Qk close Q policy greedy respect Qk close optimal shown bound The algorithm review Q learning Watkins We start describing algorithm small finite MDPs followed description extensions work large MDPs Q learning finite MDPs Fix finite MDP M X A P0 discount factor The Q learning algorithm Watkins keeps estimate Qt x Q x state action pair x X A Upon observing Xt At Rt Yt estimates updated follows t Q Rt max A Q Yt Q Xt At Qt x Qt x t t Qt I x Xt At x X A Here At A Yt Rt P0 Xt At When learning trajectory Xt Yt necessary convergence algorithm Q learning instance TD learning updates based TD error t Qt Algorithm shows pseudocode Q learning In stochastic equilibrium E t Q Xt x At x X A visited infinitely A trivial calculation shows E t Q Xt x At T Q x Q x x X A Algorithm The function implementing tabular Q learning algorithm This function called transition function QLearning X A R Y Q Input X state A action R immediate reward received Y state Q array storing current action value function estimate R maxa AQ Y Q X A Q X A Q X A return Q T Bellman optimality operator defined Hence minimal assumption state action pair visited infinitely stochastic equilibrium T Q Q Using Fact algorithm converges converge Q stated condition The sequence Qt t known converge Q appropriate local learning rates Tsitsiklis Jaakkola et al The rate convergence Q learning studied Szepesva ri asymptotic setting later Even Dar Mansour finite sample setting The key observation lead discovery Q learning unlike optimal state values optimal action values expressed expectations compare Equations This turn allows estimate action values incremental manner There exist multi step versions Q learning e g Sutton Barto Section However appealing straightforward multi step extensions TD Q learning inherently policy algorithm temporal differences un derlying Q learning telescope Xt Yt What policy follow learning A major attraction Q learning sim plicity allows use arbitrary sampling strategy generate training data provided limit state action pairs updated infinitely In closed loop situation commonly strategies sample actions following greedy action selection scheme Boltzmann scheme case probability selecting action time t chosen proportional eQt Xt With appropriate tuning achieve asymptotic consistency behavior policy cf Szepesva ri Section Singh et al However discussed Section closed loop learning systematic exploration necessary achieve reasonable online performance Post decision states In practical problems set Z set post decision states smaller X A identified transition probabilities de 15Watkins provide rigorous convergence analysis Watkins Dayan gave proof case policies eventually lead absorbing state compose according P x y PA f x y x y X A Here f X A Z known transition function PA Z X appropriate probability kernel Function f determines deterministic effect actions PA captures stochastic effect Many operations research problems enjoy structure For example inventory control problem Example f x x M Further examples given Powell Note Sutton Barto calls post decision states afterstates If problem admits post decision states learning immediate reward function known called post decision state optimal value function V A Z R defined V A z y X PA z y V y z Z economical efficient learning action value function Update rules action selection strategies derived based identity Q x r x V A f x follows immediately definitions To potential advantage post decision state value functions assume access transition probabilities In case tempted approximate state value function instead approximating action value function Then order compute greedy action necessary algorithms need compute argmaxa A r x y X P x y V y This called stochastic optimization problem modifier stochastic refers optimization objective defined expectation This problem computationally challenging number states large number actions large e g A large infinite subset Euclidean space P enjoy nice structure On hand uses post decision state value function VA computing greedy action reduces finding argmaxa A r x VA f x Thus expectation avoided e stochastic optimization problem needs solved Further judiciously chosen approximation architecture piecewise linear concave separable optimization problem tractable large infinite action spaces Thus post decision state value functions advantageous allow avoid layer complexity Of course applies action value functions discussed previously post decision state value functions require storage potentially require fewer samples learn action value functions For details ideas examples consult Powell Algorithm The function implementing Q learning algorithm linear function approximation This function called transition function QLearningLinFApp X A R Y Input X state Y state R immediate reward associated transition Rd parameter vector R maxa A Y X A X A return Q learning function approximation The obvious extension ofQ learning function approximation parametric forms Q Rd t t t t Qt Qt Xt At compare Algorithm shows pseudocode corresponding case linear function approximation method e Q X A Rd Although update rule widely practice little said convergence properties In fact TD special case algorithm action state like TD update rule fail converge policy sampling nonlinear function approximation cf Section The known convergence result Melo et al prove convergence restrictive conditions sample distribution More recently line recent gradient like TD algorithms Maei et al 2010b proposed greedy gradient Q learning greedy GQ algorithm lifts previous restrictive conditions This new algorithm guaranteed converge independently sampling distribution However objective function derivation algorithm non convex algorithm stuck local minima linear function approximation State aggregation Since update rule fail converge natural restrict value function approximation method employed modify update pro cedure necessary In spirit let consider case Q state action aggregator cf Section Then Xt At t stationary algorithm behave exactly like tabular Q learning appropriately defined induced MDP Hence converge approximation optimal action value function Q Bertsekas Tsitsiklis Section Soft state aggregation One undesirable property aggregation value function smooth boundaries underlying regions Singh et al proposed address softened version Q learning In algorithm approximate action value function form linear averager Q x d si x si x d d si x The update rule modified time component parameter vector t updated The component updated selected randomly drawing index It d multinomial distribution parameters s1 Xt At sd Xt At Interpolation based Q learning Szepesva ri Smart proposed modification algorithm interpolation based Q learning IBQ learning IBQ simul taneously updates components parameter vector reducing updates variance IBQ learning viewed generalization Q learning state action aggregation interpolators Tsitsiklis Van Roy Section discusses interpolators context fitted value iteration known models The idea treat component parameter vector value estimate representa tive state action pair xi ai X A d That Q Rd chosen Q xi ai holds d This makes Q interpolator explaining algorithm Next choose similarity functions si X A For example use si x exp c1d1 x xi c2d2 ai c1 c2 d1 d2 appropriate distance functions The update rule IBQ learning follows t Rt max A Qt Yt Qt xi ai t t t t si Xt At d Each component updated based predicts total future reward similar associated state action pair state action pair visited If similarity small impact error t change component small The algorithm uses local step size sequences t t e step size components Szepesva ri Smart prove algorithm converges surely long function class Q satisfies interpolation property mapping Q non expansion e Q Q holds Rd ii local step size sequences t t appropriately chosen iii regions state action space X A sufficiently visited Xt At t They provide error bounds quality action value function learned The heart analysis Q Algorithm The function implementing iteration fitted Q iteration algorithm The function called criterion convergence met The methods pre dict regress specific regression method chosen The method predict z return predicted value input z given regression parameters regress S given list input output pairs S implement regression algorithm solves regression problem given S returns new parameters predict function FittedQ D Input D Xi Ai Ri Yi n list transitions regressor parameters S Create list n T Ri maxa A predict Yi Target Xi Ai S append S Xi Ai T end regress S return non expansion algorithm implements incremental approximate version value iteration underlying operator contraction This non expansion applied contraction contraction applied non expansion contraction The idea non expansions appeared works Gordon Tsitsiklis Van Roy study fitted value iteration Fitted Q iteration Fitted Q iteration implements fitted value iteration action value functions Given previous iterate Qt idea form Monte Carlo approximation T Qt x selected state action pairs regress resulting points s favorite regression method Algorithm shows pseudocode method It known fitted Q iteration diverge special regressor Baird Boyan Moore Tsitsiklis Van Roy Ormoneit Sen suggest use kernel averaging Ernst et al suggest tree based regressors These guaranteed converge data fed algorithm iteration implement local averaging results Gordon Tsitsiklis Van Roy applicable Riedmiller reports good empirical results neural networks new observations obtained following policy greedy respect latest iterate incrementally added set samples updates That sample changed essential good initial policy available e initial sample states frequently visited good policies underrepresented theoretical argument important given Van Roy context state aggregation Antos et al Munos Szepesva ri prove finite sample performance bounds apply large class regression methods use empirical risk minimization fixed space F candidate action value functions Their bounds depend worst case Bellman error F e F sup Q F inf Q F Q T Q distribution state action pairs training sample That e F mea sures close F T F def T Q Q F The bounds derived form finite sample bounds hold supervised learning cf Equation approximation error measured e F Note earlier mentioned counterexam ples convergence fitted value iteration e F suggesting lack flexibility function approximation method causes divergence Actor critic methods Actor critic methods implement generalized policy iteration Remember policy iteration works alternating complete policy evaluation complete policy improve ment step When sample based methods function approximation exact evaluation policies require infinitely samples impossible restric tions function approximation technique Hence reinforcement learning algorithms simulating policy iteration change policy based incomplete knowledge value function Algorithms update policy completely evaluated said implement generalized policy iteration GPI In GPI closely interacting processes actor critic actor aims improving current policy critic evaluates current policy helping actor The interaction actor critic illustrated Figure closed loop learning situation Note general policy generate samples e behavior policy different evaluated improved actor critic system e target policy This useful critic learn actions preferred current target policy critic improve target policy This impossible achieve behavior policy target policy target policy deterministic This reason target policy usually stochastic policy However target policy stochastic quality estimates values low probability actions poor information received actions It appear choosing actions completely random information However clearly case random policy visit important parts state space discussed Therefore practice Reward State Action SystemSystem ActorActor CriticCritic Values Figure The Actor Critic Architecture behavior policy mixes certain small exploration target policy There ways implement actor critic architecture If action space small critic e g use approximate action value function actor follow greedy Boltzmann exploration strategy If action space large continuous actor use function approximation Note unlike perfect policy iteration GPI method generate policy sub stantially worse previous Thus quality sequence generated policies oscillate diverge policy evaluation step incomplete irrespective policy improvement exact approximate Bertsekas Tsitsiklis Exam ple p In practice GPI tends generate policies improve beginning However later stages policies oscillate A common practice store sequence policies obtained learning measure performance stored policies running tests select empirically best performing Just like case fitted value iteration performance actor critic methods controlled increasing flexibility function approximation methods Finite sample performance bounds given Antos et al actor critic use function approximation In section Section describe value estimation methods critic Section describe methods implement policy improvement actor In particular describe greedy methods policy improvement followed somewhat different idea actor uses gradient ascent performance function defined parametric family policies Implementing critic The job critic estimate value current target policy actor This value prediction problem Therefore critic use methods described Section Since actor needs action values algorithms typically modified estimate action values directly When TD appropriately extended algorithm known SARSA obtained This algorithm describe When LSTD extended LSTD Q described LSPE extended sake brevity extension discussed SARSA In case finite small state action spaces similarly Q learning SARSA keeps track action value underlying possible state action pairs Rummery Niranjan t Q Rt Q Yt A t Q Xt At Qt x Qt x t t Qt I x Xt At x X A Here Yt Rt P0 Xt At A t Yt Compared Q learning dif ference definition TD error The algorithm got use current State current Action Reward State Action When fixed SARSA TD applied state action pairs Hence convergence follows convergence results underlying TD The multi step extension SARSA follows lines similar extension TD giving rise SARSA algorithm Rummery Niranjan Rummery The tabular algorithms extended case function approximation way tabular TD extended Algorithm shows pseudocode SARSA linear function approximation Being TD algorithm resulting algorithm subject limitations TD cf Section e diverge policy situations It possible extend GTD2 TDC work action values use resulting algorithms free limitations For details consult Maei Sutton LSTD Q When LSTD generalized action value functions LSTD Q algorithm solves t Xt At X A Rd t Rt Vt Q Xt At Algorithm The function implementing SARSA algorithm linear function approximation This function called transition function SARSALambdaLinFApp X A R Y A z Input X state A action chosen R immediate reward received transitioning Y action A chosen Rd parameter vector linear function approximation z Rd vector eligibility traces R Y A X A z X A z z return z assuming policy evaluated stochastic policy Vt given Vt A Yt Q Yt A Yt Yt For deterministic policies simplifies Vt Q Yt Yt If action space large stochastic policies considered evaluating sums A x x integrals case continuous action spaces infea sible One possibility sample actions policy A t Yt use Vt Q Yt A t When sample consists trajectories set A t At gives rise SARSA like version LSTD Q An alternative expected produce better estimates introduce state features X Rd restrict A x x holds state x X define Q x x x Then V x A x Q x x Hence setting Vt V Yt introduce bias expected reduce variance Vt depend randomness A t Peters et al Peters Schaal We discuss choice section The pseudocode LSTD Q shown Algorithm Note like case LSTD inverse line exist Following standard steps possible derive recursive version LSTD Q Finally note TD errors defined section SARSA algorithm Implementing actor Policy improvement implemented ways One idea moving current policy greedy policy underlying approximate action value function obtained 16Peters et al Peters Schaal consider special case parameters approximate state value function V action value function shared Algorithm The function implementing LSTD Q algorithm linear function approximation evaluate policy Note deterministic policy sum line replaced g Yt Yt function LSTDQLambda D Input D Xt At Rt Yt t n list transitions stochastic policy evaluated A b z A Rd d b z Rd t n f Xt At z z f g sum Yt Yt A A z f g b b Rt z end A 1b return critic Another idea perform gradient ascent directly performance surface un derlying chosen parametric policy class In sections describe specific methods implement ideas Greedy improvements The closest policy iteration let critic evaluate current policy based lot data switch policy greedy respect obtained action value function Notice action space finite action choices greedy policy computed fly needed basis e greedy policy need explicitly computed stored making possible use algorithm large infinite state spaces If policy evaluated LSTD Q strategy gives rise LSPI squares policy iteration algorithm Lagoudakis Parr The variant uses LSTD Q evaluate policies batch data shown Algorithm Finite sample performance bounds LSPI generalizations obtained Antos et al Antos et al extend results continuous action spaces given current action value function Q policy chosen maximize Q x X x A Q x da x restricted policy class They argue necessity restricting policies prevent overfitting case The methods mentioned switch policies enforcing continuity This Algorithm The function implementing LSPI algorithm linear function ap proximation In practice convergence criterion replaced criterion GreedyPolicy return function takes arguments pair form x return action maximizes x returns function LSPI D Input D Xt At Rt Yt t n list transitions accuracy parameter repeat LSTDQLambda D GreedyPolicy return dangerous action value function estimate policy inaccurate new policy radically different previous hard algorithm recover failure In cases incremental changes work better One way ensure incremental changes update parameters parametric policy class Rd performing stochastic gradient ascent Q e g Bertsekas Tsitsiklis p Kakade Langford considers incremental updates policies given tabular form An indirect way performing approximately greedy updates choose target policy greedy policy Boltzmann policy corresponding current action value function Perkins Precup analyze choice linear function approximation behavior target policies They prove following result Let mapping action value functions policies defines policy updates Assume exact TD solution obtained iteration ii globally Lipschitz Lipschitz constant smaller c M image space contains soft policies fixed Then sequence policies generated algorithm converges surely The Lipschitzness property means Q1 Q2 L Q1 Q2 holds action value functions norms unweighted norms The constant c M depends MDPM A policy called soft x holds x X A More recently Van Roy obtained non trivial performance bounds state aggregation similar setting The methods discussed far update policy infrequently An alternative inter leave updates policy value function Singh et al prove asymptotic consistency GLIE greedy limit infinite exploration policies followed tabular SARSA critic Policy gradient In section review policy gradient methods sensitivity based approach Cao These methods perform stochastic gradient ascent performance surface induced smoothly parameterized policy class Rd stochastic stationary policies When action space finite popular choice use called Gibbs policies x exp x A exp x x X A Here X A Rd appropriate feature extraction function If action space subset dA dimension Euclidean space popular choice use Gaussian policies given parametric mean g x covariance x functions density specifying action selection distribution defined x dAdet x exp g x x g x Care taken ensure positive definite For simplicity taken I Given formally problem find value corresponding best performing policy argmax Here performance measured expected return policy respect initial distribution states The initial distribution stationary distribution underlying policy chosen case maximizing equivalent maximizing long run average reward Sutton et al 1999a The policy gradient theorem Assume Markov chain resulting following policy ergodic regardless choice The question estimate gradient Let X A Rd score function underlying x log x x X A For example case Gibbs policies score function takes form x x A x x Define G Q X A h X X A 17An overall best policy measured value function exist restricted class Here X A sample stationary state action distribution underlying policy Q action value function h arbitrary bounded function According policy gradient theorem e g Bhatnagar et al references G unbiased estimate gradient E G Let Xt At sample stationary distribution underlying t Then t t update rule G t Q t Xt At h Xt Xt At t t t G t implements stochastic gradient ascent long E Q t Xt At t Xt At E Qt X A t Xt At The role h reduce variance gradient estimate G t speed rate convergence algorithm Although good choice gain constant factor terms speeding convergence practice gain substantial One choice h V t e value function underlying policy t Although explicitly minimize variance G t G t expected reduce variance compared h generally recommended Of course value function current policy normally available estimated This constructing estimator Q t shall soon As update rule instance stochastic gradient ascent sequence t converge surely local optimum provided step size sequence t t satisfies RM conditions problem sufficiently regular general convergence stationary point proven The difficulty implementing twofold One needs construct appropriate estimator Q t possibly h ii The random variables Xt At come stationary distribution t In episodic problems difficulties addressed updating parameters end episodes giving rise Williams REINFORCE algorithm Williams Note REINFORCE direct policy search algorithm use value functions It member family likelihood ratio methods Glynn In non episodic problems timescale algorithm constructs esti mator Q t faster timescale appropriate value function estimation method updates policy parameters slower timescale We describe interesting proposal implement Sutton et al 1999a Konda Tsitsiklis Compatible function approximation Assume linear parameters function approximation estimate Q t choose feature extraction function score function underlying policy class Q x x x X A This choice function approximation method called compatible policy pa rameterization Note basis functions depend t changes change What suitable value fixed value t Substituting Q Q t E t Xt At t Xt At E Qt Xt At t Xt At Define F E X A X A g E Q X A X A let solution linear system equations F g When equation holds Q t satisfies Notice parameter minimizes mean squared error E Q X A Q X A The derivations suggest following closed loop learning algorithm time t policy t followed ii t updated faster timescale appropriate version SARSA iii policy parameters updated slower timescale t t t Qt Xt At h Xt t Xt At Algorithm shows corresponding pseudocode Konda Tsitsiklis proved regularity conditions lim inft t holds surely average cost version SARSA update t They shown SARSA m lim inft t lim 1m Natural actor critic Another possible update rule t t t t Algorithm An actor critic algorithm uses compatible function approximation SARSA function SARSAActorCritic X Input X current state z A a1 Pick action True R Y ExecuteInWorld A A Draw Y z SARSALambdaLinFApp X A R Y A z Use log X A v sum Y X X A v X Y A A end defines natural actor critic NAC algorithm pseudocode resulting algorithm differs Algorithm line update replaced Assuming F positive definite g F F This shows algorithm implements stochastic pseudo gradient algorithm converges conditions Interestingly NAC update result faster convergence rate previous rule The reason shown called natural gradient Amari This noted Kakade Following natural gradient means algorithm performs gradient ascent directly metric space underlying objects interest case space stochastic policies appropriate metric opposed performing gradient ascent Euclidean metric space parameters note definition gradient dependent metric In particular natural gradient implies actual parameterization irrelevant sense trajectories underlying ODE invariant arbitrary smooth equivalent reparameterizations policy class Rd In case Gibbs policy class non singular linear transformation features simple example reparameterization Because invariance property natural gradient said covariant It believed following natural gradient generally improves behavior gradient ascent methods This nicely demonstrated Kakade simple state MDP normal gradient small large parameter space natural gradient behaves reasonable manner Other positive examples given Bagnell Schneider Peters et al Peters Schaal As estimation Peters Schaal earlier Peters et al suggest use LSTD Q In particular suggested state features compatible state action features described Section note gives unbiased estimate gradient Their algorithm keeps value t fixed parameter t calculated LSTD Q stabilizes When happens t updated internal statistics collected LSTD Q discounted discount factor They observe original actor critic Barto et al Sutton finite MDP function approximation implements NAC update More recently Bhatnagar et al proposed timescale algorithms proved convergence policy parameters neighborhood local maxima objective function critic uses TD like updates For exploration Inevitably space constraints review miss large portion reinforce ment learning literature Further reading One topic particular interest discussed efficient sampling based planning Kearns et al Szepesva ri Kocsis Szepesva ri Chang et al The main lesson line planning worst case scale exponentially dimen sionality state space Chow Tsitsiklis online planning e planning current state break curse dimensionality amortizing planning effort multiple time steps Rust Szepesva ri Other topics interest include linear programming based approaches de Farias Van Roy dual dynamic programming Wang et al techniques based sample average approximation Shapiro PEGASUS Ng Jordan online learning MDPs arbitrary reward processes Even Dar et al Yu et al Neu et al learning restrictions competitive framework Hutter Other important topics include learning acting partially observed MDPs recent developments e g Littman et al Toussaint et al Ross et al learning acting games optimization criteria Littman Heger Szepesva ri Littman Borkar Meyn development hierarchical multi time scale methods Dietterich Sutton et al 1999b Applications The numerous successful applications reinforcement learning include particular der learning games e g Backgammon Tesauro Go Silver et al applications networking e g packet routing Boyan Littman channel alloca tion Singh Bertsekas applications operations research problems e g tar geted marketing Abe et al maintenance problems Gosavi job shop schedul ing Zhang Dietterich elevator control Crites Barto pricing Rus mevichientong et al vehicle routing Proper Tadepalli inventory control Chang et al fleet management Sima o et al learning robotics e g con trolling quadrupedales Kohl Stone humanoid robots Peters et al helicopters Abbeel et al applications finance e g option pricing Tsitsiklis Van Roy 1999b Yu Bertsekas Li et al For applications lists URLs http www cs ualberta szepesva RESEARCH RLApplications html http umichrl pbworks com Successes Reinforcement Learning Software There numerous software packages support development testing RL algorithms Perhaps notable RL Glue RL Library pack ages The RL Glue package available http glue rl community org intended helping standardize RL experiments It free language neutral software package implements standardized RL interface Tanner White The RL Library http library rl community org builds RL Glue It is purpose provide trusted implementations RL testbeds algorithms The notable RL software packages CLSquare PIQLE RL Toolbox JRLF21 LibPG These offer implementation large number algorithms testbeds intuitive visual izations programming tools etc Many packages support RL Glue Acknowledgements I truly indebted family love support patience Thank Mom Bea ta Da vid Re ka Eszter Csongor Special thanks Re ka helped draw Figure 18http www ni uos de index php id 19http piqle sourceforge net 20http www igi tugraz ril toolbox 21http mykel kochenderfer com page_id 22http code google com p libpgrl http www cs ualberta szepesva RESEARCH RLApplications html http umichrl pbworks com Successes Reinforcement Learning http glue rl community org http library rl community org http www ni uos de index php id http piqle sourceforge net http www igi tugraz ril toolbox http mykel kochenderfer com page_id http code google com p libpgrl A number individuals read versions manuscript parts helped reduce number mistakes sending corrections They include Dimitri Bertsekas Ga bor Bala zs Bernardo Avila Pires Warren Powell Rich Sutton Nikos Vlassis Hengshuai Yao Shimon Whiteson Thank You Of course remaining mistakes If I left list means intentional If case remind e mail better send comments suggestions Independently contacted readers encouraged e mail find errors typos think topic included left I plan periodically update text I try accommodate requests Finally I wish thank Remi Munos Rich Sutton closest collaborators years I learned continue learn lot I wish thank students members RLAI group researchers RL continue strive push boundaries reinforcement learning This book possible A The theory discounted Markovian decision pro cesses The purpose section short proof basic results theory Markovian decision processes All results worked discounted expected total cost criterion First short overview contraction mappings Banach s fixed point theorem Next powerful result applied proof number basic results value functions optimal policies A Contractions Banach s fixed point theorem We start basic definitions need rest section Definition Norm Let V vector space reals Then f V R norm V provided following hold If f v v V v For R v V f v f v For v u V f v u f v f u A vector space norm called normed vector space According definition norm function assigns nonnegative number vector This number called length norm vector The norm vector v denoted v Example Here examples norms vector space V Rd p norms For p v p d vi p p norm v max d vi The weighted variants norms defined follows v p d vi p wi p p max1 d vi wi p wi The matrix weighted norm defined follows v 2P v TPv Here P fixed positive definite matrix Similarly define norms spaces functions For example V vector space functions domain X bounded f sup x X f x A function called bounded exactly f We interested convergence sequences normed vector spaces Definition Convergence norm Let V V normed vector space Let vn V sequence vectors n N The sequence vn n said converge vector v norm limn vn v This denoted vn v Note d dimensional vector space vn v requiring d vn vi vn denotes ith component vn However hold infinite dimensional vector spaces Take example X space bounded functions X Let fn x x n Define f f x x f Then fn x f x x e fn converges f x pointwise However fn f If sequence real numbers n test sequence converges knowledge limiting value verifying Cauchy sequence e limn supm n Sequences vanishing oscillations possibly descriptive Cauchy sequences It notable property real numbers Cauchy sequence reals assumes limit The extension concept Cauchy sequences normed vector spaces straightforward Definition Cauchy sequence Let vn n sequence vectors normed vector space V V Then vn called Cauchy sequence limn supm n vn vm Normed vector spaces Cauchy sequences convergent special find examples normed vector spaces Cauchy sequences vector space limit Definition Completeness A normed vector space V called complete Cauchy sequence V convergent norm vector space To pay tribute Banach great Polish mathematician half 20th century following definition Definition Banach space A complete normed vector space called Banach space One powerful result theory Banach spaces concerns contraction mappings con traction operators These special Lipschitzian mappings Definition Let V V normed vector space A mapping T V V called L Lipschitz u v V Tu Tv L u v A mapping T called non expansion Lipschitzian L It called contraction Lipschitzian L In case L called contraction factor T T called L contraction Note T Lipschitz continuous sense vn v Tvn Tv This Tvn Tv L vn v n Definition Fixed point Let T V V mapping The vector v V called fixed point T Tv v Theorem Banach s fixed point theorem Let V Banach space T V V contraction mapping Then T unique fixed point Further v0 V vn Tvn vn v v unique fixed point T convergence geometric vn v n v0 v Proof Pick v0 V define vn statement theorem We demon strate vn converges vector Then vector fixed point T Finally T single fixed point Assume T contraction To vn converges suffices vn Cauchy sequence V Banach e complete normed vector space We vn k vn Tvn k Tvn vn k vn Tvn k Tvn vn k vn n vk v0 Now vk v0 vk vk vk vk v1 v0 logic vi vi v1 v0 Hence vk v0 k k v1 v0 v1 v0 Thus vn k vn n v1 v0 lim n sup k vn k vn showing vn n Cauchy sequence Let v limit Now let definition sequence vn n vn Tvn Taking limes sides hand vn v On hand Tvn Tv T contraction continuous Thus left hand converges v right hand converges Tv left right hand sides equal Therefore v Tv showing v fixed point T Let consider problem uniqueness fixed point T Let assume v v fixed points T Then v v Tv Tv v v v v Since norm takes nonnegative values v v Thus v v v v finishing proof statement For second vn v Tvn Tv vn v Tvn Tv vn v n v0 v A Application MDPs For purpose section define V V x sup stat V x x X Thus V x upper bound value achieve choosing stationary policy Note supremum taken larger class policies possibly larger function However case MDPs considered section optimal value functions actually Although hard prove omit proof Let B X space bounded functions domain X B X V X R V In follows view B X normed vector space norm It easy B X complete If Vn n Cauchy sequence x X Vn x n Cauchy sequence reals Denoting V x limit Vn x Vn V Vaguely speaking holds Vn n Cauchy sequence norm rate convergence Vn x V x independent x Pick stationary policy Remember Bellman operator underlying T B X B X defined T V x r x x y X P x x y V y x X Note T defined If U B X T U B X holds true It easy V defined fixed point T V x E R1 X0 x y X P x x y E t tRt X1 y T V x It easy T contraction T U T V sup x X y X P x x y U y V y sup x X y X P x x y U y V y sup x X y X P x x y U V U V line follows y X P x x y It follows order find V construct sequence V0 T V0 T 2V0 Banach s fixed point theorem converge V geometric rate Now recall definition Bellman optimality operator T B X B X T V x sup A r x y X P x y V y x X Again T defined We T contraction respect supremum norm To note sup A f sup A g sup A f g seen elementary case analysis Using inequality pro ceeding analysis T T U T V sup x X A y X P x y U y V y sup x X A y X P x y U V U V proving statement Here equality follows y X P x y The main result section following theorem Theorem Let V fixed point T assume policy greedy w r t V T V T V Then V V optimal policy Proof Pick stationary policy Then T T sense function V B X T V T V holds U V means U x V x holds x X Thus V T V T V e V T V Since T U T V follows U V T V T 2V Chaining inequalities V T 2V Continuing way n V T nV Since T contraction right hand converges V unique fixed point T stage know V V Thus V V Since arbitrary V V Pick policy T V T V Since V fixed point T T V V Since T unique fixed point V V V showing V V optimal policy In statement theorem careful assuming greedy policy w r t V exists Note holds finite action spaces hold infinite action spaces extra continuity assumptions The following theorem serves basis policy iteration algorithm Theorem Policy improvement theorem Choose stationary policy let greedy w r t V T V T V Then V V e improvement In particular T V x V x state x strictly improves x V x V x On hand T V V optimal policy Proof We T V T V T 0V V Applying T sides T 2V T V V Continuing way n T nV V Taking limit sides V V For second notice T nV x T V x V x Hence taking limit V x T V x V x The proven follows Since T V V V fixed point T Since T contraction single fixed point V Thus V V But know V V V Hence optimal policy The policy iteration procedure generates sequence policy greedy w r t V Let assume choosing greedy policy improvement possible previous policy stop iteration We following immediate corollary Corollary If MDP finite policy iteration procedure terminates finite number steps returns optimal policy Further stationary policy MDP optimal value function fixed point T Proof From previous theorem know sequence policies strictly improv ing Since finite MDP finite number policies procedure ter minate When procedure terminates final policy T V T V V Thus previous theorem optimal policy The second follows immediately Theorem Corollary Let V unique fixed point T Then policy greedy w r t V optimal policy Further exists optimal stationary policy V V policy greedy w r t V Proof The follows immediately Theorem For second assume optimal stationary policy Hence V V Thus V T V T V By second Corollary fact T V V Thus V V V V e equal T V T V The second corollary essence shows policies optimal ones greedy w r t V References A Prieditis S J R editor Proceedings 12th International Conference Machine Learning ICML San Francisco CA USA Morgan Kaufmann Abbeel P Coates A Quigley M Ng A Y An application reinforcement learning aerobatic helicopter flight In Scho lkopf et al pages Abe N Verma N K Apte C Schroko R Cross channel optimized marketing reinforcement learning In Kim W Kohavi R Gehrke J DuMouchel W editors Proceedings Tenth ACM SIGKDD International Conference Knowledge Discovery Data Mining pages New York NY USA ACM Albus J S A theory cerebellar function Mathematical Biosciences Albus J S Brains Behavior Robotics BYTE Books Subsidiary McGraw Hill Peterborough New Hampshire Amari S Natural gradient works efficiently learning Neural Computation Antos A Munos R Szepesva ri C Fitted Q iteration continuous action space MDPs In Platt et al pages Antos A Szepesva ri C Munos R Learning near optimal policies Bellman residual minimization based fitted policy iteration single sample path Machine Learning Published Online First Nov Audibert J Y Munos R Szepesva ri C Exploration exploitation trade variance estimates multi armed bandits Theoretical Computer Science Auer P Cesa Bianchi N Fischer P Finite time analysis multiarmed bandit problem Machine Learning Auer P Jaksch T Ortner R Near optimal regret bounds reinforcement learning Journal Machine Learning Research Bagnell J A Schneider J G Covariant policy search In Gottlob G Walsh T editors Proceedings Eighteenth International Joint Conference Artificial Intelligence IJCAI pages San Francisco CA USA Morgan Kaufmann Baird L C Residual algorithms Reinforcement learning function approxima tion In A Prieditis pages Balakrishna P Ganesan R Sherry L Levy B Estimating taxi times reinforcement learning algorithm In 27th IEEE AIAA Digital Avionics Systems Conference pages D D Bartlett P L Tewari A REGAL A regularization based algorithm rein forcement learning weakly communicating MDPs In Proceedings 25th Annual Conference Uncertainty Artificial Intelligence Barto A G Sutton R S Anderson C W Neuronlike adaptive elements solve difficult learning control problems IEEE Transactions Systems Man Cybernetics Beleznay F Gro bler T Szepesva ri C Comparing value function estima tion algorithms undiscounted problems Technical Report TR Mindmaker Ltd Budapest Konkoly Th M u Hungary Berman P On line searching navigation In Fiat A Woeginger G editors Online Algorithms The State Art chapter Springer Berlin Heidelberg Bertsekas D P 2007a Dynamic Programming Optimal Control volume Athena Scientific Belmont MA edition Bertsekas D P 2007b Dynamic Programming Optimal Control volume Athena Scientific Belmont MA edition Bertsekas D P Approximate dynamic programming online chapter In Dynamic Programming Optimal Control volume chapter Athena Scientific Belmont MA edition Bertsekas D P Borkar V S Nedic A Improved temporal difference methods linear function approximation In Si J Barto A G Powell W B Wunsch II D editors Learning Approximate Dynamic Programming chapter pages IEEE Press Bertsekas D P Ioffe S Temporal differences based policy iteration appli cations neuro dynamic programming LIDS P MIT Bertsekas D P Shreve S Stochastic Optimal Control The Discrete Time Case Academic Press New York Bertsekas D P Tsitsiklis J N Neuro Dynamic Programming Athena Scientific Belmont MA Bhatnagar S Sutton R S Ghavamzadeh M Lee M Natural actor critic algorithms Automatica press Borkar V S Stochastic approximation time scales Systems Control Letters Borkar V S Asynchronous stochastic approximations SIAM J Control Opti mization Borkar V S Stochastic Approximation A Dynamical Systems Viewpoint Cam bridge University Press Borkar V S Meyn S P Risk sensitive optimal control Markov decision processes monotone cost Mathematics Operations Research Bottou L Bousquet O The tradeoffs large scale learning In Platt et al pages Boyan J A Technical update Least squares temporal difference learning Machine Learning Boyan J A Littman M L Packet routing dynamically changing networks A reinforcement learning approach In Cowan J D Tesauro G Alspector J editors NIPS Advances Neural Information Processing Systems Proceedings Conference pages Morgan Kauffman San Francisco CA USA Boyan J A Moore A W Generalization reinforcement learning Safely approximating value function In Tesauro et al pages Bradtke S J Incremental Dynamic Programming On line Adaptive Optimal Control PhD thesis Department Computer Information Science University Massachusetts Amherst Massachusetts Bradtke S J Barto A G Linear squares algorithms temporal differ ence learning Machine Learning Brafman R I Tennenholtz M R MAX general polynomial time algorithm near optimal reinforcement learning Journal Machine Learning Research Busoniu L Babuska R Schutter B Ernst D Reinforcement Learning Dynamic Programming Using Function Approximators Automation Control Engineering Series CRC Press Cao X R Stochastic Learning Optimization A Sensitivity Based Approach Springer New York Chang H S Fu M C Hu J Marcus S I An asymptotically efficient simulation based algorithm finite horizon stochastic dynamic programming IEEE Transactions Automatic Control Chang H S Fu M C Hu J Marcus S I Simulation based Algorithms Markov Decision Processes Springer Verlag Chow C S Tsitsiklis J N The complexity dynamic programming Journal Complexity Cohen W W Hirsh H editors Proceedings 11th International Conference Machine Learning ICML San Francisco CA USA Morgan Kaufmann Cohen W W McCallum A Roweis S T editors Proceedings 25th In ternational Conference Machine Learning ICML volume ACM International Conference Proceeding Series New York NY USA ACM Cohen W W Moore A editors Proceedings 23rd International Confer ence Machine Learning ICML volume ACM International Conference Proceeding Series New York NY USA ACM Crites R H Barto A G Improving elevator performance reinforcement learning In Touretzky D Mozer M C Hasselmo M E editors NIPS Advances Neural Information Processing Systems Proceedings Conference pages Cambridge MA USA MIT Press S ims ek O Barto A An intrinsic reward mechanism efficient exploration In Cohen Moore pages Danyluk A P Bottou L Littman M L editors Proceedings 26th Annual International Conference Machine Learning ICML volume ACM International Conference Proceeding Series New York NY USA ACM Dasgupta S Freund Y Random projection trees low dimensional manifolds In Ladner R E Dwork C editors 40th Annual ACM Symposium Theory Computing pages ACM de Farias D P Van Roy B The linear programming approach approximate dynamic programming Operations Research de Farias D P Van Roy B On constraint sampling linear programming approach approximate dynamic programming Mathematics Operations Research de Farias D P Van Roy B A cost shaping linear program average cost approximate dynamic programming performance guarantees Mathematics Oper ations Research De Raedt L Wrobel S editors Proceedings 22nd International Confer ence Machine Learning ICML volume ACM International Conference Proceeding Series New York NY USA ACM Dearden R Friedman N Andre D Model based Bayesian exploration In Laskey K Prade H editors Proceedings Fifteenth Conference Uncertainty Artificial Intelligence UAI pages Morgan Kaufmann Dearden R Friedman N Russell S Bayesian Q learning In Proceedings 15th National Conference Artificial Intelligence AAAI pages AAAI Press Dietterich T The MAXQ method hierarchical reinforcement learning In Shavlik pages Dietterich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems Cambridge MA USA MIT Press Domingo C Faster near optimal reinforcement learning Adding adaptiveness E3 algorithm In Watanabe O Yokomori T editors Proc 10th International Conference Algorithmic Learning Theory volume Lecture Notes Computer Science pages Springer Engel Y Mannor S Meir R Reinforcement learning Gaussian processes In De Raedt Wrobel pages Ernst D Geurts P Wehenkel L Tree based batch mode reinforcement learning Journal Machine Learning Research Even Dar E Kakade S M Mansour Y Experts Markov decision pro cess In Saul L K Weiss Y Bottou L editors Advances Neural Information Processing Systems pages Cambridge MA USA MIT Press Even Dar E Mannor S Mansour Y PAC bounds multi armed bandit Markov decision processes In Kivinen J Sloan R H editors Proceedings 15th Annual Conference Computational Learning Theory Computational Learning Theory COLT volume Lecture Notes Computer Science pages Springer Even Dar E Mansour Y Learning rates Q learning Journal Machine Learning Research Farahmand A Ghavamzadeh M Szepesva ri C Mannor S Regularized fitted Q iteration Application planning In Girgin S Loth M Munos R Preux P Ryabko D editors Revised Selected Papers 8th European Workshop Recent Advances Reinforcement Learning EWRL volume Lecture Notes Computer Science pages Springer Farahmand A Ghavamzadeh M Szepesva ri C Mannor S Regularized policy iteration In Koller et al pages Frank J Mannor S Precup D Reinforcement learning presence rare events In Cohen et al pages Fu rnkranz J Scheffer T Spiliopoulou M editors Proceedings 17th European Conference Machine Learning ECML Springer George A P Powell W B Adaptive stepsizes recursive estimation applications approximate dynamic programming Machine Learning Geramifard A Bowling M H Zinkevich M Sutton R S iLSTD Eligibility traces convergence analysis In Scho lkopf et al pages Ghahramani Z editor Proceedings 24th International Conference Machine Learning ICML volume ACM International Conference Proceeding Series New York NY USA ACM Ghavamzadeh M Engel Y Bayesian actor critic algorithms In Ghahramani pages Gittins J C Multi armed Bandit Allocation Indices Wiley Interscience series systems optimization Wiley Chichester NY Glynn P W Likelihood ratio gradient estimation stochastic systems Commu nications ACM Gordon G J Stable function approximation dynamic programming In A Priedi it is pages Gosavi A Simulation based optimization parametric optimization techniques reinforcement learning Springer Netherlands Gosavi A Reinforcement learning long run average cost European Journal Operational Research Gyo rfi L Kohler M Krzyz ak A Walk H A distribution free theory nonparametric regression Springer Verlag New York Gyo rfi L Kohler M Krzyz ak A Walk H A Distribution Free Theory Nonparametric Regression Springer Verlag Ha rdle W Applied nonparametric regression Cambridge University Press Cam bridge Heger M Consideration risk reinforcement learning In Cohen Hirsh pages Howard R A Dynamic Programming Markov Processes The MIT Press Cambridge MA Hutter M Universal Artificial Intelligence Sequential Deci sions based Algorithmic Probability Springer Berlin pages http www idsia ch marcus ai uaibook htm Jaakkola T Jordan M Singh S On convergence stochastic iterative dynamic programming algorithms Neural Computation Jong N K Stone P Model based exploration continuous state spaces In Miguel I Ruml W editors 7th International Symposium Abstraction Refor mulation Approximation SARA volume Lecture Notes Computer Science pages Whistler Canada Springer Kaelbling L Littman M Moore A Reinforcement learning A survey Journal Artificial Intelligence Research Kakade S A natural policy gradient In Dietterich et al pages Kakade S On sample complexity reinforcement learning PhD thesis Gatsby Computational Neuroscience Unit University College London Kakade S Kearns M J Langford J Exploration metric state spaces In Fawcett T Mishra N editors Proceedings 20th International Conference Machine Learning ICML pages AAAI Press Kakade S Langford J Approximately optimal approximate reinforcement learning In Sammut C Hoffmann A G editors Proceedings 19th Interna tional Conference Machine Learning ICML pages San Francisco CA USA Morgan Kaufmann Kearns M Singh S Near optimal reinforcement learning polynomial time Machine Learning Kearns M Singh S P Near optimal performance reinforcement learning polynomial time In Shavlik pages Kearns M J Mansour Y Ng A Y Approximate planning large POMDPs reusable trajectories In Solla et al pages Keller P W Mannor S Precup D Automatic basis function construction approximate dynamic programming reinforcement learning In Cohen Moore pages Kocsis L Szepesva ri C Bandit based Monte Carlo planning In Fu rnkranz et al pages Kohl N Stone P Policy gradient reinforcement learning fast quadrupedal locomotion In Proceedings IEEE International Conference Robotics Automation pages IEEE Koller D Schuurmans D Bengio Y Bottou L editors Advances Neural Information Processing Systems Cambridge MA USA MIT Press Kolter J Z Ng A Y Regularization feature selection squares temporal difference learning In Danyluk et al pages Konda V R Tsitsiklis J N Actor critic algorithms In Solla et al pages Konda V R Tsitsiklis J N On actor critic algorithms SIAM J Control Optimization Kosorok M R Introduction Empirical Processes Semiparametric Inference Springer Lagoudakis M Parr R Least squares policy iteration Journal Machine Learning Research Lai T L Robbins H Asymptotically efficient adaptive allocation rules Ad vances Applied Mathematics Lemieux C Monte Carlo Quasi Monte Carlo Sampling Springer Li Y Szepesva ri C Schuurmans D Learning exercise policies american options In Proc Twelfth International Conference Artificial Intelligence Statistics JMLR W CP volume pages Lin L J Self improving reactive agents based reinforcement learning planning teaching Machine Learning Littman M L Markov games framework multi agent reinforcement learning In Cohen Hirsh pages Littman M L Sutton R S Singh S P Predictive representations state In Dietterich et al pages Maei H Szepesva ri C Bhatnagar S Silver D Precup D Sutton R 2010a Convergent temporal difference learning arbitrary smooth function approximation In NIPS pages Maei H Szepesva ri C Bhatnagar S Sutton R 2010b Toward policy learning control function approximation In Wrobel et al Maei H R Sutton R S GQ A general gradient algorithm temporal difference prediction learning eligibility traces In Baum E Hutter M Kitzel mann E editors Proceedings Third Conference Artificial General Intelligence pages Atlantis Press Mahadevan S Learning representation control Markov decision processes New frontiers Foundations Trends Machine Learning McAllester D A Myllyma ki P editors Proceedings 24th Conference Uncertainty Artificial Intelligence UAI AUAI Press Melo F S Meyn S P Ribeiro M I An analysis reinforcement learning function approximation In Cohen et al pages Menache I Mannor S Shimkin N Basis function adaptation temporal difference reinforcement learning Annals Operations Research Mnih V Szepesva ri C Audibert J Y Empirical Bernstein stopping In Cohen et al pages Munos R Szepesva ri C Finite time bounds fitted value iteration Journal Machine Learning Research Nascimento J Powell W An optimal approximate dynamic programming algorithm lagged asset acquisition problem Mathematics Operations Research Nedic A Bertsekas D P Least squares policy evaluation algorithms linear function approximation Discrete Event Dynamic Systems Neu G Gyo rgy A Szepesva ri C The online loop free stochastic shortest path problem In COLT Ng A Y Jordan M PEGASUS A policy search method large MDPs POMDPs In Boutilier C Goldszmidt M editors Proceedings 16th Confer ence Uncertainty Artificial Intelligence UAI pages San Francisco CA Morgan Kaufmann Nouri A Littman M Multi resolution exploration continuous spaces In Koller et al pages Ormoneit D Sen S Kernel based reinforcement learning Machine Learning Ortner R Online regret bounds Markov decision processes deterministic transitions In Freund Y Gyo rfi L Tura n G Zeugmann T editors Proc 19th International Conference Algorithmic Learning Theory ALT volume Lecture Notes Computer Science pages Springer Parr R Li L Taylor G Painter Wakefield C Littman M L An analysis linear models linear value function approximation feature selection reinforcement learning In Cohen et al pages Parr R Painter Wakefield C Li L Littman M L Analyzing feature gener ation value function approximation In Ghahramani pages Perkins T Precup D A convergent form approximate policy iteration In S Becker S T Obermayer K editors Advances Neural Information Processing Systems pages Cambridge MA USA MIT Press Peters J Schaal S Natural actor critic Neurocomputing Peters J Vijayakumar S Schaal S Reinforcement learning humanoid robotics In Humanoids2003 Third IEEE RAS International Conference Humanoid Robots pages Platt J C Koller D Singer Y Roweis S T editors Advances Neural Information Processing Systems Cambridge MA USA MIT Press Polyak B Juditsky A Acceleration stochastic approximation averaging SIAM Journal Control Optimization Poupart P Vlassis N Hoey J Regan K An analytic solution discrete Bayesian reinforcement learning In Cohen Moore pages Powell W B Approximate Dynamic Programming Solving curses dimen sionality John Wiley Sons New York Proper S Tadepalli P Scaling model based average reward reinforcement learning product delivery In Fu rnkranz et al pages Puterman M Markov Decision Processes Discrete Stochastic Dynamic Program ming John Wiley Sons Inc New York NY Rasmussen C Williams C Gaussian Processes Machine Learning Adaptive Computation Machine Learning The MIT Press Rasmussen C E Kuss M Gaussian processes reinforcement learning In Thrun S Saul L K Scho lkopf B editors Advances Neural Information Pro cessing Systems pages Cambridge MA USA MIT Press Riedmiller M Neural fitted Q iteration experiences data efficient neural reinforcement learning method In Gama J Camacho R Brazdil P Jorge A Torgo L editors Proceedings 16th European Conference Machine Learning ECML volume Lecture Notes Computer Science pages Springer Robbins H Some aspects sequential design experiments Bulletin American Mathematics Society Ross S Pineau J Model based Bayesian reinforcement learning large struc tured domains In McAllester Myllyma ki pages Ross S Pineau J Paquet S Chaib draa B Online planning algorithms POMDPs Journal Artificial Intelligence Research Rummery G A Problem solving reinforcement learning PhD thesis Cambridge University Rummery G A Niranjan M On line Q learning connectionist systems Technical Report CUED F INFENG TR Cambridge University Engineering Depart ment Rusmevichientong P Salisbury J A Truss L T Van Roy B Glynn P W Opportunities challenges online preference data vehicle pricing A case study General Motors Journal Revenue Pricing Management Rust J Using randomization break curse dimensionality Econometrica Scherrer B Should compute temporal difference fix point minimize Bellman residual The unified oblique projection view In Wrobel et al Scho lkopf B Platt J C Hoffman T editors Advances Neural Information Processing Systems Cambridge MA USA MIT Press Schraudolph N Local gain adaptation stochastic gradient descent In Ninth International Conference Artificial Neural Networks ICANN volume pages Shapiro A Monte Carlo sampling methods In Stochastic Programming Handbooks OR MS volume North Holland Publishing Company Amsterdam Shavlik J W editor Proceedings 15th International Conference Machine Learning ICML San Francisco CA USA Morgan Kauffmann Silver D Sutton R S Mu ller M Reinforcement learning local shape game Go In Veloso M M editor Proceedings 20th International Joint Conference Artificial Intelligence IJCAI pages Sima o H P Day J George A P Gifford T Nienow J Powell W B An approximate dynamic programming algorithm large scale fleet management A case application Transportation Science Singh S P Bertsekas D P Reinforcement learning dynamic channel al location cellular telephone systems In Mozer M C Jordan M I Petsche T editors NIPS Advances Neural Information Processing Systems Proceedings Conference pages Cambridge MA USA MIT Press Singh S P Jaakkola T Jordan M I Reinforcement learning soft state aggregation In Tesauro et al pages Singh S P Jaakkola T Littman M L Szepesva ri C Convergence results single step policy reinforcement learning algorithms Machine Learning Singh S P Sutton R S Reinforcement learning replacing eligibility traces Machine Learning Singh S P Yee R C An upper bound loss approximate optimal value functions Machine Learning Solla S A Leen T K Mu ller K R editors Advances Neural Information Processing Systems Cambridge MA USA MIT Press Strehl A L Li L Wiewiora E Langford J Littman M L PAC model free reinforcement learning In Cohen Moore pages Strehl A L Littman M L A theoretical analysis model based interval estimation In De Raedt Wrobel pages Strehl A L Littman M L Online linear regression application model based reinforcement learning In Platt et al pages Strens M A Bayesian framework reinforcement learning In Langley P edi tor Proceedings 17th International Conference Machine Learning ICML pages Morgan Kaufmann Sutton R S Temporal Credit Assignment Reinforcement Learning PhD thesis University Massachusetts Amherst MA Sutton R S Learning predict method temporal differences Machine Learning Sutton R S Gain adaptation beats squares In Proceedings 7th Yale Workshop Adaptive Learning Systems pages Sutton R S Barto A G Reinforcement Learning An Introduction Bradford Book MIT Press Sutton R S Maei H R Precup D Bhatnagar S Silver D Szepesva ri C Wiewiora E 2009a Fast gradient descent methods temporal difference learning linear function approximation In Danyluk et al pages Sutton R S McAllester D A Singh S P Mansour Y 1999a Policy gradient methods reinforcement learning function approximation In Solla et al pages Sutton R S Precup D Singh S P 1999b Between MDPs semi MDPs A framework temporal abstraction reinforcement learning Artificial Intelligence Sutton R S Szepesva ri C Geramifard A Bowling M H Dyna style planning linear function approximation prioritized sweeping In McAllester Myllyma ki pages Sutton R S Szepesva ri C Maei H R 2009b A convergent O n temporal difference algorithm policy learning linear function approximation In Koller et al pages Szepesva ri C The asymptotic convergence rate Q learning In Jordan M I Kearns M J Solla S A editors Advances Neural Information Processing Sys tems pages Cambridge MA USA MIT Press Szepesva ri C Learning exploitation conflict minimax optimality In Someren M Widmer G editors Machine Learning ECML 9th European Conf Machine Learning Proceedings volume Lecture Notes Artificial Intelligence pages Springer Berlin Szepesva ri C Static Dynamic Aspects Optimal Sequential Decision Making PhD thesis Bolyai Institute Mathematics University Szeged Szeged Aradi vrt tere HUNGARY Szepesva ri C Efficient approximate planning continuous space Markovian decision problems AI Communications Szepesva ri C Littman M L A unified analysis value function based reinforcement learning algorithms Neural Computation Szepesva ri C Smart W D Interpolation based Q learning In Brodley C E editor Proceedings 21st International Conference Machine Learning ICML pages ACM Szita I Lo rincz A The faces optimism unifying approach In Cohen et al pages Szita I Szepesva ri C Model based reinforcement learning nearly tight exploration complexity bounds In Wrobel et al Tadic V B On sure rate convergence linear stochastic approximation algorithms IEEE Transactions Information Theory Tanner B White A RL Glue Language independent software reinforcement learning experiments Journal Machine Learning Research Taylor G Parr R Kernelized value function approximation reinforcement learning In Danyluk et al pages Tesauro G TD Gammon self teaching backgammon program achieves master level play Neural Computation Tesauro G Touretzky D Leen T editors NIPS Advances Neural Information Processing Systems Proceedings Conference Cambridge MA USA MIT Press Thrun S B Efficient exploration reinforcement learning Technical Report CMU CS Carnegie Mellon University Pittsburgh PA Toussaint M Charlin L Poupart P Hierarchical POMDP controller opti mization likelihood maximization In McAllester Myllyma ki pages Tsitsiklis J N Asynchronous stochastic approximation Q learning Machine Learning Tsitsiklis J N Mannor S The sample complexity exploration multi armed bandit problem Journal Machine Learning Research Tsitsiklis J N Van Roy B Feature based methods large scale dynamic programming Machine Learning Tsitsiklis J N Van Roy B An analysis temporal difference learning function approximation IEEE Transactions Automatic Control Tsitsiklis J N Van Roy B 1999a Average cost temporal difference learning Auto matica Tsitsiklis J N Van Roy B 1999b Optimal stopping Markov processes Hilbert space theory approximation algorithms application pricing financial derivatives IEEE Transactions Automatic Control Tsitsiklis J N Van Roy B Regression methods pricing complex American style options IEEE Transactions Neural Networks Tsybakov A Introduction nonparametric estimation Springer Verlag Van Roy B Performance loss bounds approximate value iteration state aggregation Mathematics Operations Research Wahba G Reproducing kernel Hilbert spaces brief reviews In Proceedings 13th IFAC Symposium System Identification pages Wang T Lizotte D J Bowling M H Schuurmans D Stable dual dynamic programming In Platt et al Watkins C J C H Learning Delayed Rewards PhD thesis King s College Cambridge UK Watkins C J C H Dayan P Q learning Machine Learning Widrow B Stearns S Adaptive Signal Processing Prentice Hall Englewood Cliffs NJ Williams R J A class gradient estimating algorithms reinforcement learning neural networks In Proceedings IEEE First International Conference Neural Networks San Diego CA Wrobel S Fu rnkranz J Joachims T editors Proceedings 27th An nual International Conference Machine Learning ICML ACM International Conference Proceeding Series New York NY USA ACM Xu X He H Hu D Efficient reinforcement learning recursive squares methods Journal Artificial Intelligence Research Xu X Hu D Lu X Kernel based squares policy iteration reinforce ment learning IEEE Transactions Neural Networks Yu H Bertsekas D Q learning algorithms optimal stopping based squares In Proceedings European Control Conference Yu J Bertsekas D P New error bounds approximations projected lin ear equations Technical Report C Department Computer Science University Helsinki revised July Yu J Y Mannor S Shimkin N Markov decision processes arbitrary reward processes Mathematics Operations Research appear Zhang W Dietterich T G A reinforcement learning approach job shop scheduling In Perrault C R Mellish C S editors Proceedings Fourteenth International Joint Conference Artificial Intelligence IJCAI pages San Francisco CA USA Morgan Kaufmann Overview Markov decision processes Preliminaries Markov Decision Processes Value functions Dynamic programming algorithms solving MDPs Value prediction problems Temporal difference learning finite state spaces Tabular TD Every visit Monte Carlo TD Unifying Monte Carlo TD Algorithms large state spaces TD function approximation Gradient temporal difference learning Least squares methods The choice function space Control A catalog learning problems Closed loop interactive learning Online learning bandits Active learning bandits Active learning Markov Decision Processes Online learning Markov Decision Processes Direct methods Q learning finite MDPs Q learning function approximation Actor critic methods Implementing critic Implementing actor For exploration Further reading Applications Software Acknowledgements The theory discounted Markovian decision processes Contractions Banach s fixed point theorem Application MDPs\n",
            "9 []\n",
            "9 Gaussian Processes Machine Learning C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Processes Machine Learning C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Adaptive Computation Machine Learning Thomas Dietterich Editor Christopher Bishop David Heckerman Michael Jordan Michael Kearns Associate Editors Bioinformatics The Machine Learning Approach Pierre Baldi Sren Brunak Reinforcement Learning An Introduction Richard S Sutton Andrew G Barto Graphical Models Machine Learning Digital Communication Brendan J Frey Learning Graphical Models Michael I Jordan Causation Prediction Search second edition Peter Spirtes Clark Glymour Richard Scheines Principles Data Mining David Hand Heikki Mannila Padhraic Smyth Bioinformatics The Machine Learning Approach second edition Pierre Baldi Sren Brunak Learning Kernel Classifiers Theory Algorithms Ralf Herbrich Learning Kernels Support Vector Machines Regularization Optimization Beyond Bernhard Scho lkopf Alexander J Smola Introduction Machine Learning Ethem Alpaydin Gaussian Processes Machine Learning Carl Edward Rasmussen Christopher K I Williams C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Processes Machine Learning Carl Edward Rasmussen Christopher K I Williams The MIT Press Cambridge Massachusetts London England C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml c Massachusetts Institute Technology All rights reserved No book reproduced form electronic mechanical means including photocopying recording information storage retrieval permission writing publisher MIT Press books purchased special quantity discounts business sales promotional use For information email special sales mitpress mit edu write Special Sales Department The MIT Press Hayward Street Cambridge MA Typeset authors LATEX2 This book printed bound United States America Library Congress Cataloging Publication Data Rasmussen Carl Edward Gaussian processes machine learning Carl Edward Rasmussen Christopher K I Williams p cm Adaptive computation machine learning Includes bibliographical references indexes ISBN X Gaussian processes Data processing Machine learning Mathematical models I Williams Christopher K I II Title III Series QA274 R37 dc22 C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The actual science logic conversant present things certain impossible entirely doubtful fortunately reason Therefore true logic world calculus Probabilities takes account magnitude probability ought reasonable man s mind James Clerk Maxwell C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Contents Series Foreword xi Preface xiii Symbols Notation xvii Introduction A Pictorial Introduction Bayesian Modelling Roadmap Regression Weight space View The Standard Linear Model Projections Inputs Feature Space Function space View Varying Hyperparameters Decision Theory Regression An Example Application Smoothing Weight Functions Equivalent Kernels Incorporating Explicit Basis Functions Marginal Likelihood History Related Work Exercises Classification Classification Problems Decision Theory Classification Linear Models Classification Gaussian Process Classification The Laplace Approximation Binary GP Classifier Posterior Predictions Implementation Marginal Likelihood Multi class Laplace Approximation Implementation Expectation Propagation Predictions Marginal Likelihood Implementation Experiments A Toy Problem One dimensional Example Binary Handwritten Digit Classification Example class Handwritten Digit Classification Example Discussion Sections marked asterisk contain advanced material omitted reading C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml viii Contents Appendix Moment Derivations Exercises Covariance Functions Preliminaries Mean Square Continuity Differentiability Examples Covariance Functions Stationary Covariance Functions Dot Product Covariance Functions Other Non stationary Covariance Functions Making New Kernels Old Eigenfunction Analysis Kernels An Analytic Example Numerical Approximation Eigenfunctions Kernels Non vectorial Inputs String Kernels Fisher Kernels Exercises Model Selection Adaptation Hyperparameters The Model Selection Problem Bayesian Model Selection Cross validation Model Selection GP Regression Marginal Likelihood Cross validation Examples Discussion Model Selection GP Classification Derivatives Marginal Likelihood Laplace s Approximation Derivatives Marginal Likelihood EP Cross validation Example Exercises Relationships GPs Other Models Reproducing Kernel Hilbert Spaces Regularization Regularization Defined Differential Operators Obtaining Regularized Solution The Relationship Regularization View Gaussian Process Prediction Spline Models A d Gaussian Process Spline Construction Support Vector Machines Support Vector Classification Support Vector Regression Least squares Classification Probabilistic Least squares Classification C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Contents ix Relevance Vector Machines Exercises Theoretical Perspectives The Equivalent Kernel Some Specific Examples Equivalent Kernels Asymptotic Analysis Consistency Equivalence Orthogonality Average case Learning Curves PAC Bayesian Analysis The PAC Framework PAC Bayesian Analysis PAC Bayesian Analysis GP Classification Comparison Other Supervised Learning Methods Appendix Learning Curve Ornstein Uhlenbeck Process Exercises Approximation Methods Large Datasets Reduced rank Approximations Gram Matrix Greedy Approximation Approximations GPR Fixed Hyperparameters Subset Regressors The Nystro m Method Subset Datapoints Projected Process Approximation Bayesian Committee Machine Iterative Solution Linear Systems Comparison Approximate GPR Methods Approximations GPC Fixed Hyperparameters Approximating Marginal Likelihood Derivatives Appendix Equivalence SR GPR Using Nystro m Approximate Kernel Exercises Further Issues Conclusions Multiple Outputs Noise Models Dependencies Non Gaussian Likelihoods Derivative Observations Prediction Uncertain Inputs Mixtures Gaussian Processes Global Optimization Evaluation Integrals Student s t Process Invariances Latent Variable Models Conclusions Future Directions C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml x Contents Appendix A Mathematical Background A Joint Marginal Conditional Probability A Gaussian Identities A Matrix Identities A Matrix Derivatives A Matrix Norms A Cholesky Decomposition A Entropy Kullback Leibler Divergence A Limits A Measure Integration A Lp Spaces A Fourier Transforms A Convexity Appendix B Gaussian Markov Processes B Fourier Analysis B Sampling Periodization B Continuous time Gaussian Markov Processes B Continuous time GMPs R B The Solution Corresponding SDE Circle B Discrete time Gaussian Markov Processes B Discrete time GMPs Z B The Solution Corresponding Difference Equation PN B The Relationship Between Discrete time Sampled Continuous time GMPs B Markov Processes Higher Dimensions Appendix C Datasets Code Bibliography Author Index Subject Index C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Series Foreword The goal building systems adapt environments learn experience attracted researchers fields including com puter science engineering mathematics physics neuroscience cognitive science Out research come wide variety learning techniques potential transform scientific industrial fields Recently research communities converged common set issues sur rounding supervised unsupervised reinforcement learning problems The MIT Press series Adaptive Computation Machine Learning seeks unify diverse strands machine learning research foster high quality research innovative applications One active directions machine learning de velopment practical Bayesian methods challenging learning problems Gaussian Processes Machine Learning presents important Bayesian machine learning approaches based particularly effective method placing prior distribution space functions Carl Edward Ras mussen Chris Williams pioneers area book describes mathematical foundations practical application Gaussian processes regression classification tasks They Gaussian processes interpreted Bayesian version known support vector machine methods Students researchers study book able apply Gaussian process methods creative ways solve wide range problems science engineering Thomas Dietterich C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Preface Over decade explosion work kernel ma kernel machines chines area machine learning Probably best known example work support vector machines period activity concerning application Gaussian process models ma chine learning tasks The goal book provide systematic uni fied treatment area Gaussian processes provide principled practical probabilistic approach learning kernel machines This gives advantages respect interpretation model predictions provides founded framework learning model selection Theoretical practical developments decade Gaussian processes competitor real supervised learning applications Roughly speaking stochastic process generalization probability Gaussian process distribution describes finite dimensional random variable func tions By focussing processes Gaussian turns computations required inference learning relatively easy Thus supervised learning problems machine learning thought learning function examples cast directly Gaussian process framework Our interest Gaussian process GP models context machine Gaussian processes machine learninglearning aroused graduate students Geoff Hinton s Neural Networks lab University Toronto This time field neural networks mature con nections statistical physics probabilistic models statistics known kernel based learning algorithms popular In retrospect clear time ripe application Gaussian processes machine learning problems Many researchers realizing neural networks easy neural networks apply practice decisions needed architecture activation functions learning rate etc lack principled framework answer questions The probabilistic framework pursued approximations MacKay 1992b Markov chain Monte Carlo MCMC methods Neal Neal graduate stu dent lab thesis sought demonstrate Bayesian formalism necessarily problems overfitting models large pursue limit large models While work focused sophisticated Markov chain methods inference large finite networks point networks Gaussian processes limit infinite size sim large neural networks Gaussian processespler ways inference case It interesting mention slightly wider historical perspective The main reason neural networks popular allowed use adaptive basis functions opposed known linear models adaptive basis functions The adaptive basis functions hidden units learn hidden features C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml xiv Preface useful modelling problem hand However adaptivity came cost lot practical problems Later advancement kernel era realized limitation fixed basis functions bigmany fixed basis functions restriction e typically infinitely careful control problems overfitting priors regularization The resulting models easier handle adaptive basis function models similar expressive power Thus claim far machine learning concerned adaptive basis functions merely decade long digression came This view reasonable think models solving practical learning problems MacKay ch example raises concerns asking throw baby bath water kernel view hidden representations tellinguseful representations useful features solving particular problem As argue book answer learn sophisticated covariance functions hidden properties problem found An important area future developments GP models use expressive covariance functions Supervised learning problems studied centurysupervised learning statistics statistics large body established theory developed More recently advance affordable fast computation machine learning community addressed increasingly large complex problems Much basic theory algorithms shared thestatistics machine learning statistics machine learning community The primary differences types problems attacked goal learning At risk oversimplification statistics prime focus indata models understanding data relationships terms models giving approximate summaries linear relations independencies In contrast goals machine learning primarily predictions accurately possible andalgorithms predictions understand behaviour learning algorithms These differing objectives led different developments fields example neural network algorithms extensively black box function approximators machine learning statisticians satisfactory difficulties interpreting models Gaussian process models sense bring work com bridging gap munities As Gaussian processes mathematically equivalent known models including Bayesian linear models spline models large neural networks suitable conditions closely related support vector machines Under Gaussian process viewpoint models easier handle interpret conventional coun terparts e g neural networks In statistics community Gaussian processes discussed times probably excessive claim use widespread certain specific appli cations spatial models meteorology geology analysis computer experiments A rich theory exists Gaussian process models C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Preface xv time series analysis literature pointers literature given Appendix B The book primarily intended graduate students researchers intended audience machine learning departments Computer Science Statistics Applied Mathematics As prerequisites require good basic grounding calculus linear algebra probability theory obtained graduates nu merate disciplines electrical engineering physics computer science For preparation calculus linear algebra good university level text book mathematics physics engineering Arfken fine For probability theory familiarity multivariate distributions especially Gaussian conditional probability required Some ground mathematical material provided Appendix A The main focus book present clearly concisely overview focus main ideas Gaussian processes machine learning context We covered wide range connections existing models literature cover approximate inference faster practical algorithms We pre sented detailed algorithms methods aid practitioner Software implementations available website book Appendix C We included small set exercises chapter hope help gaining deeper understanding material In order limit size volume omit topics scope example Markov chain Monte Carlo methods inference One difficult things decide writing book sections write Within sections chosen describe algorithm particular depth mention related work passing Although causes omission material feel best approach monograph hope reader gain general understanding able push growing literature GP models The book natural split parts chapters book organization including chapter covering core material remaining sections covering connections methods fast approximations specialized properties Some sections marked asterisk These sections omitted reading pre requisites later un starred material We wish express considerable gratitude people acknowledgements interacted writing book In particular Moray Allan David Barber Peter Bartlett Miguel Carreira Perpin n Marcus Gal lagher Manfred Opper Anton Schwaighofer Matthias Seeger Hanna Wallach Joe Whittaker Andrew Zisserman read parts book provided valuable feedback Dilan Go ru r Malte Kuss Iain Murray Joaquin Quin onero Candela Leif Rasmussen Sam Roweis especially heroic provided comments manuscript We thank Chris Bishop Miguel Carreira Perpin n Nando de Freitas Zoubin Ghahramani Peter Gru nwald Mike Jor dan John Kent Radford Neal Joaquin Quin onero Candela Ryan Rifkin Ste fan Schaal Anton Schwaighofer Matthias Seeger Peter Sollich Ingo Steinwart C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml xvi Preface Amos Storkey Volker Tresp Sethu Vijayakumar Grace Wahba Joe Whittaker Tong Zhang valuable discussions specific issues We thank Bob Prior staff MIT Press support writing book We thank Gatsby Computational Neuroscience Unit UCL Neil Lawrence Department Computer Science University Sheffield hosting visits kindly providing space work Depart ment Computer Science University Toronto computer support Thanks John Fiona hospitality numerous occasions Some diagrams book inspired similar diagrams appearing published work follows Figure Scho lkopf Smola Fig ure MacKay 1992b CER gratefully acknowledges financial support German Research Foundation DFG CKIW thanks School Infor matics University Edinburgh granting sabbatical leave period October March Finally reserve deepest appreciation wives Agnes Bar bara children Ezra Kate Miro Ruth patience standing book written Despite best efforts inevitable errors througherrata printed version book Errata available book s website http www GaussianProcess org gpml We found joint writing book excellent experience Although hard times confident end result better written Now years introduction machine learning com looking ahead munity Gaussian processes receiving growing attention Although GPs known long time statistics geostatistics fields use traced far end 19th century application real problems early phases This contrasts somewhat application non probabilistic analogue GP support vec tor machine taken quickly practitioners Perhaps probabilistic mind set needed understand GPs generally appreciated Perhaps need computational short cuts implement inference large datasets Or lack self contained introduction exciting field volume hope contribute momentum gained Gaussian processes machine learning Carl Edward Rasmussen Chris Williams Tu bingen Edinburgh summer Second printing We thank Baback Moghaddam Mikhail Parakhin Leif Ras mussen Benjamin Sobotta Kevin S Van Horn Aki Vehtari reporting errors printing corrected C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Symbols Notation Matrices capitalized vectors bold type We generally distinguish proba bilities probability densities A subscript asterisk X indicates reference test set quantity A superscript asterisk denotes complex conjugate Symbol Meaning left matrix divide A b vector x solves Ax b equality acts definition c equality additive constant K determinant K matrix y Euclidean length vector y e y f g H RKHS inner product f H RKHS norm y transpose vector y proportional e g p x y f x y means p x y equal f x y times factor independent x distributed according example x N f partial derivatives w r t f Hessian matrix second derivatives 0n vector s length n 1n vector s length n C number classes classification problem cholesky A Cholesky decomposition L lower triangular matrix LL A cov f Gaussian process posterior covariance D dimension input space X D data set D xi yi n diag w vector argument diagonal matrix containing elements vector w diag W matrix argument vector containing diagonal elements matrix W pq Kronecker delta pq iff p q E Eq x z x expectation expectation z x x q x f x f Gaussian process vector latent function values f f x1 f xn f Gaussian process posterior prediction random variable f Gaussian process posterior mean GP Gaussian process f GP m x k x x function f distributed Gaussian process mean function m x covariance function k x x h x h x fixed basis function set basis functions weight function H H X set basis functions evaluated training points I In identity matrix size n J z Bessel function kind k x x covariance kernel function evaluated x x K K X X n n covariance Gram matrix K n n matrix K X X covariance training test cases k x k vector short K X x single test case Kf K covariance matrix noise free f values C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml xviii Symbols Notation Symbol Meaning Ky covariance matrix noisy y values independent homoscedastic noise Ky Kf 2nI K z modified Bessel function L b loss function loss predicting b true note argument order log z natural logarithm base e log2 z logarithm base d characteristic length scale input dimension d z logistic function z exp z m x mean function Gaussian process measure section A N N x variable x Gaussian Normal distribution mean vector covariance matrix N x short unit Gaussian x N I n n number training test cases N dimension feature space NH number hidden units neural network N natural numbers positive integers O big Oh functions f g N write f n O g n ratio f n g n remains bounded n O matrix zeros differential operator y x p y x conditional random variable y given x probability density PN regular n polygon xi X feature map input xi input set X z cumulative unit Gaussian z z exp t dt x sigmoid latent value x f x stochastic f x stochastic x MAP prediction evaluated f x x mean prediction expected value x Note general x x R real numbers RL f RL c risk expected loss f classifier c averaged w r t inputs outputs R L l x expected loss predicting l averaged w r t model s pred distr x Rc decision region class c S s power spectrum z sigmoid function e g logistic z cumulative Gaussian z etc 2f variance noise free signal 2n noise variance vector hyperparameters parameters covariance function tr A trace square matrix A Tl circle circumference l V Vq x z x variance variance z x x q x X input space index set stochastic process X D n matrix training inputs xi ni design matrix X matrix test inputs xi ith training input xdi dth coordinate ith training input xi Z integers C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Introduction In book concerned supervised learning problem learning input output mappings empirical data training dataset Depending characteristics output problem known regression continuous outputs classification outputs discrete A known example classification images handwritten digits digit classification The training set consists small digitized images classification normally provided human The goal learn mapping image classification label new unseen images Supervised learning attractive way attempt tackle problem easy specify accurately characteristics handwritten digit An example regression problem found robotics wish robotic control learn inverse dynamics robot arm Here task map state arm given positions velocities accelerations joints corresponding torques joints Such model compute torques needed arm given trajectory Another example chemical plant wish predict yield function process parameters temperature pressure catalyst etc In general denote input x output target y The dataset input usually represented vector x general input variables handwritten digit recognition example dimensional input obtained raster scan image robot arm example input measurements joint arm The target y continuous regression case discrete classification case We dataset D n observations D xi yi n Given training data wish predictions new inputs x training inductive seen training set Thus clear problem hand inductive need finite training data D C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Introduction function f makes predictions possible input values To assumptions characteristics underlying function function consistent training data equally valid A wide variety methods proposed deal supervised learning problem describe common approaches Thetwo approaches restrict class functions consider example considering linear functions input The second approach speaking loosely prior probability possible function higher probabilities given functions consider likely example smoother functions The approach obvious problem decide richness class functions considered model based certain class functions e g linear functions target function modelled class predictions poor One tempted increase flexibility class functions runs danger overfitting obtain good fit training data perform badly making test predictions The second approach appears problem surely uncountably infinite set possible functions going compute set finite time This GaussianGaussian process process comes rescue A Gaussian process generalization Gaussian probability distribution Whereas probability distribution describes random variables scalars vectors multivariate distributions stochastic process governs properties functions Leaving mathematical sophistication aside loosely think function long vector entry vector specifying function value f x particular input x It turns idea little na ve surprisingly close need Indeed question deal computationally infinite dimensional objects pleasant resolution imaginable ask properties function finite number points inference Gaussian process answer ignore infinitely points taken account And answers consistent answers finite queries youconsistency One main attractions Gaussian process framework precisely unites sophisticated consistent view computationaltractability tractability It come surprise ideas time known Indeed models commonly employed machine learning statis tics fact special cases restricted kinds Gaussian processes In volume aim systematic unified treatment area showing connections related models 1These approaches regarded imposing restriction bias preference bias respectively e g Mitchell C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml A Pictorial Introduction Bayesian Modelling input x f x input x f x prior b posterior Figure Panel shows samples drawn prior distribution Panel b shows situation datapoints observed The mean prediction shown solid line samples posterior shown dashed lines In plots shaded region denotes twice standard deviation input value x A Pictorial Introduction Bayesian Mod elling In section graphical illustrations second Bayesian method works simple regression classification examples We consider simple d regression problem mapping input regression x output f x In Figure number sample functions drawn random prior distribution functions specified par random functions ticular Gaussian process favours smooth functions This prior taken represent prior beliefs kinds functions expect observe seeing data In absence knowledge contrary assumed average value sample functions x zero mean function Although specific random functions drawn Figure mean zero mean f x values fixed x zero dependent x kept drawing functions At value x characterize variability sample functions computing pointwise variance variance point The shaded region denotes twice pointwise standard deviation case Gaussian process specifies prior variance depend x Suppose given dataset D x1 y1 x2 y2 consist functions agree observationsing observations wish consider functions pass data points exactly It possible higher pref erence functions merely pass close datapoints This situation illustrated Figure b The dashed lines sample functions consistent D solid line depicts mean value func tions Notice uncertainty reduced close observations The combination prior data leads posterior distribution posterior functions functions C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Introduction If datapoints added mean function adjust pass points posterior uncertainty reduce close observations Notice Gaussian process parametric model worry possible thenon parametric model fit data case e g tried linear model strongly non linear data Even lot observations added flexibility left functions One way imagine reduction flexibility distribution functions data arrives draw random functions prior reject ones agree observations While perfectly valid way inference inference impractical purposes exact analytical computations required quantify properties detailed chapter The specification prior important fixes properties ofprior specification functions considered inference Above briefly touched mean pointwise variance functions However characteristics specified manipulated Note functions Figure smooth stationary informally stationarity means functions look similar x locations These properties induced co variance function Gaussian process covariance functions arecovariance function possible Suppose particular application think functions Figure vary rapidly e characteristic length scale short Slower variation achieved simply adjusting parameters covariance function The problem learning Gaussian processes exactly problem finding suitable properties covariance function Note gives model data characteristics smoothness modelling interpreting characteristic length scale etc interpret We turn classification case consider binary classification class classification problem An example classifying objects detected astronomical sky surveys stars galaxies Our data label stars galaxies task predict x probability example input vector x star inputs features describe object Obviously x lie interval A Gaussian process prior functions restrict output lie interval seen Figure The approach shall adopt squash prior function f pointwise response function whichsquashing function restricts output lie A common choice function logistic function z exp z illustrated Figure b Thus prior f induces prior probabilistic classifications This set illustrated Figure d input space In panel sample drawn prior functions f squashed logistic function panel b A dataset shown panel c white black circles denote classes respectively As regression case effect data downweight posterior functions incompatible data A contour plot posterior mean x shown panel d In example chosen short characteristic length scale process vary fairly rapidly C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Roadmap logistic function b c d Figure Panel shows sample prior distribution f d input space Panel b plot logistic function z Panel c shows location data points open circles denote class label closed circles denote class label Panel d shows contour plot mean predictive probability function x decision boundaries classes shown thicker lines case notice training points correctly classified including outliers NE SW corners By choosing different length scale change behaviour illustrated section Roadmap The book natural split parts chapters includ ing chapter covering core material remaining chapters covering connections methods fast approximations specialized prop erties Some sections marked asterisk These sections omitted reading pre requisites later un starred material C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Introduction Chapter contains definition Gaussian processes particular theregression use regression It discusses computations needed predic tions regression Under assumption Gaussian observation noise computations needed predictions tractable dominated inversion n n matrix In short experimental section Gaussian process model applied robotics task Chapter considers classification problem binary multi classification class cases The use non linear response function means exact compu tation predictions longer possible analytically We discuss number approximation schemes include detailed algorithms implementation discuss experimental comparisons As discussed key factor controls properties Gaussiancovariance functions process covariance function Much work machine learning far limited set covariance functions possibly limiting power resulting models In chapter discuss number valid covariance functions properties provide guidelines combine covariance functions new ones tailored specific needs Many covariance functions adjustable parameters char learning acteristic length scale variance illustrated Figure Chapter de scribes parameters inferred learned data based Bayesian methods marginal likelihood methods cross validation Explicit algorithms provided schemes simple practical examples demonstrated Gaussian process predictors example class methods known asconnections kernel machines distinguished probabilistic viewpoint taken In chapter discuss kernel machines support vector machines SVMs splines squares classifiers relevance vector machines RVMs relationships Gaussian process prediction In chapter discuss number theoretical issues relating totheory Gaussian process methods including asymptotic analysis average case learning curves PAC Bayesian framework One issue Gaussian process prediction methods basic com fast approximations plexity O n3 inversion n n matrix For large datasets prohibitive time space number approximation methods developed described chapter The main focus book core supervised learning problems regression classification In chapter discuss standard settings GPs complete main book conclusions Appendix A gives mathematical background Appendix B deals specifically Gaussian Markov processes Appendix C gives details access data programs figures run experiments described book C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Regression Supervised learning divided regression classification problems Whereas outputs classification discrete class labels regression concerned prediction continuous quantities For example fi nancial application attempt predict price commodity function interest rates currency exchange rates availability demand In chapter describe Gaussian process methods regression problems classification problems discussed chapter There ways interpret Gaussian process GP regression models One think Gaussian process defining distribution functions inference taking place directly space functions function space equivalent views view Although view appealing initially difficult grasp start exposition section equivalent weight space view familiar accessible continue section function space view Gaussian processes characteristics changed setting certain parameters section discuss properties change parameters varied The predictions GP model form predictive distribution section discuss combine loss function predictive distributions decision theory point predictions optimal way A practical comparative example involving learning inverse dynamics robot arm presented section We theoretical analysis Gaussian process regression section discuss incorporate explicit basis functions models section As material chapter considered fairly standard postpone references historical overview section Weight space View The simple linear regression model output linear combination inputs studied extensively It is main virtues simplic C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression ity implementation interpretability It is main drawback allows limited flexibility relationship input output reasonably approximated linear function model poor predictions In section discuss Bayesian treatment linear model We simple enhancement class models projecting inputs high dimensional feature space applying linear model We feature spaces apply kernel trick carry computations implicitly high dimensional space step leads computational savings dimensionality feature space large compared number data points We training set D n observations D xi yi n training set x denotes input vector covariates dimension D y denotes scalar output target dependent variable column vector inputs n cases aggregated D n design matrix X targetsdesign matrix collected vector y write D X y In regression setting targets real values We interested making inferences relationship inputs targets e conditional distribution targets given inputs interested modelling input distribution The Standard Linear Model We review Bayesian analysis standard linear regression model Gaussian noise f x x w y f x x input vector w vector weights parameters linear model f function value y observed target value Often biasbias offset weight offset included implemented augmenting input vector x additional element value explicitly include notation We assumed observed values y differ function values f x additive noise assume noise follows independent identically distributed Gaussian distribution zero mean variance 2n N 2n This noise assumption model directly gives rise likeli likelihood hood probability density observations given parameters 1In statistics texts design matrix usually taken transpose definition choice deliberate advantage data point standard column vector C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Weight space View factored cases training set independence assumption p y X w n p yi xi w n 2n exp yi x w 22n 22n n exp 22n y X w N X w 2nI z denotes Euclidean length vector z In Bayesian formalism need specify prior parameters expressing beliefs prior parameters look observations We zero mean Gaussian prior covariance matrix p weights w N p The ro le properties prior discussed section continue derivation prior specified Inference Bayesian linear model based posterior distribution posterior weights computed Bayes rule eq A posterior likelihood prior marginal likelihood p w y X p y X w p w p y X normalizing constant known marginal likelihood page marginal likelihood independent weights given p y X p y X w p w dw The posterior eq combines likelihood prior captures know parameters Writing terms likelihood prior depend weights completing square obtain p w X y exp 22n y X w y X w exp w 1p w exp w w 2n XX 1p w w w 2n n XX 1p 1Xy recognize form posterior distribution Gaussian mean w covariance matrix A p w X y N w 2n A 1Xy A A 2n XX 1p Notice model Gaussian posterior mean posterior distribution p w y X mode called maximum posteriori MAP estimate MAP estimate 2Often Bayes rule stated p b p b p p b use form additionally condition inputs X neglect extra conditioning prior independent inputs C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression intercept w sl o p e w input x o u tp u t y b intercept w sl o p e w intercept w sl o p e w c d Figure Example Bayesian linear model f x w1 w2x intercept w1 slope parameter w2 Panel shows contours prior distribution p w N I eq Panel b shows training points marked crosses Panel c shows contours likelihood p y X w eq assuming noise level n note slope determined intercept Panel d shows posterior p w X y eq comparing maximum posterior likelihood intercept shrunk zero determined slope unchanged All contour plots standard deviation equi probability contours Superimposed data panel b predictive mean plus minus standard deviations noise free predictive distribution p f x X y eq w In non Bayesian setting negative log prior thought penalty term MAP point known penalized maximum likelihood estimate weights because confusion approaches Note Bayesian setting MAP estimate plays special ro le The penalized maximum likelihood procedure 3In case symmetries model posterior happens mean predictive distribution prediction mean posterior However case general C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Weight space View known case ridge regression Hoerl Kennard ridge regression effect quadratic penalty term w 1p w log prior To predictions test case average possible parameter predictive distribution values weighted posterior probability This contrast non Bayesian schemes single parameter typically chosen crite rion Thus predictive distribution f f x x given averaging output possible linear models w r t Gaussian posterior p f x X y p f x w p w X y dw N 2n x A 1Xy x A 1x The predictive distribution Gaussian mean given poste rior mean weights eq multiplied test input expect symmetry considerations The predictive variance quadratic form test input posterior covariance matrix showing predictive uncertainties grow magnitude test input expect linear model An example Bayesian linear regression given Figure Here chosen d input space weight space dimensional easily visualized Contours Gaussian prior shown panel The data depicted crosses panel b This gives rise likelihood shown panel c posterior distribution panel d The predictive distribution error bars marked panel b Projections Inputs Feature Space In previous section reviewed Bayesian linear model suffers limited expressiveness A simple idea overcome problem project inputs high dimensional space set basis feature space functions apply linear model space instead directly inputs For example scalar input x projected space powers x x x x2 x3 implement polynomial polynomial regression regression As long projections fixed functions e independent parameters w model linear parameters linear parameters analytically tractable This idea classification dataset linearly separable original data space linearly separable high dimensional feature space section Application idea begs question choose basis functions As shall demonstrate chapter Gaussian process formalism allows answer question For assume basis functions given Specifically introduce function x maps D dimensional input vector x N dimensional feature space Further let matrix 4Models adaptive basis functions e g multilayer perceptrons like useful extension harder treat limit infinite number hidden units section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression X aggregation columns x cases training set Now model f x x w vector parameters length N The analysis model analogous standard linear model X substituted X Thus predictive distribution becomesexplicit feature space formulation f x X y N 2n x A 1y x A x X A 2n 1p To predictions equation need invert A matrix size N N convenient N dimension feature space large However rewrite equation following wayalternative formulation f x X y N p K nI 1y p p K nI p shorthand x defined K p To mean note definitions A K 2n K nI n p 2nI Ap Now multiplying A left K 2nI right gives 2n A p K 2nI showing equivalence mean expressions eq eq For variance use matrix inversion lemma eq A setting Z p W 2nI V U In eq need invert matrices size n n convenient n N computational load Geometrically note n datapoints span n dimensions feature space Notice eq feature space enters form p p p entries matrices invariably form x p x x x training test sets Let define k x x x p x For reasons clear later k covariance function kernel Notice x p x ankernel inner product respect p As p positive definite define p 2p p example SVD singular value decomposition p UDU D diagonal form p UD1 2U Then defining x 2p x obtain simple dot product representation k x x x x If algorithm defined solely terms inner products input space lifted feature space replacing occurrences inner products k x x called kernel trick This technique iskernel trick particularly valuable situations convenient compute kernel feature vectors As coming sections leads considering kernel object primary interest corresponding feature space having secondary practical importance C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Function space View Function space View An alternative equivalent way reaching identical results previous section possible considering inference directly function space We use Gaussian process GP describe distribution functions Formally Definition A Gaussian process collection random variables Gaussian process finite number joint Gaussian distribution A Gaussian process completely specified mean function co covariance mean functionvariance function We define mean function m x covariance function k x x real process f x m x E f x k x x E f x m x f x m x write Gaussian process f x GP m x k x x Usually notational simplicity mean function zero need section In case random variables represent value function f x location x Often Gaussian processes defined time e index set random variables time This normally case index set input domainour use GPs index set X set possible inputs general e g RD For notational convenience use arbitrary enumeration cases training set identify random variables fi f xi random variable corresponding case xi yi expected A Gaussian process defined collection random variables Thus definition automatically implies consistency requirement times known marginalization property This property simply means marginalization propertythat GP e g specifies y1 y2 N specify y1 N relevant submatrix eq A In words examination larger set variables change distribution smaller set Notice consistency requirement au tomatically fulfilled covariance function specifies entries covariance matrix The definition exclude Gaussian processes finite index finite index set sets simply Gaussian distributions partic ularly interesting purposes 5Note instead specified e g function entries inverse covariance matrix marginalization property longer fulfilled think consistent collection random variables qualify Gaussian process C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression A simple example Gaussian process obtained BayesianBayesian linear model Gaussian process linear regression model f x x w prior w N p We mean covariance E f x x E w E f x f x x E ww x x p x Thus f x f x jointly Gaussian zero mean covariance given x p x Indeed function values f x1 f xn corresponding number input points n jointly Gaussian N n Gaussian singular joint covariance matrix rank N In chapter running example covariance function squared exponential6 SE covariance function covariance functions discussed chapter The covariance function specifies covariance pairs random variables cov f xp f xq k xp xq exp xp xq Note covariance outputs written function inputs For particular covariance function covariance unity variables corresponding inputs close decreases distance input space increases It shown section squared exponential covariance function corresponds Bayesian linear regression model infinite number basis functions Indeed positive definite covariance functionbasis functions k exists possibly infinite expansion terms basis functions Mercer s theorem section We obtain SE covariance function linear combination infinite number Gaussian shaped basis functions eq eq The specification covariance function implies distribution func tions To draw samples distribution functions evalu ated number points detail choose number input points X write corresponding covariance matrix eq elementwise Then generate random Gaussian vector covariance matrix f N K X X plot generated values function inputs Figure shows samples The generation multivariate Gaussian samples de scribed section A In example Figure input values equidistant need case Notice informally functions look smooth smoothness In fact squared exponential covariance function infinitely differentiable leading process infinitely mean square differentiable section We functions characteristic length scale characteristic length scale 6Sometimes covariance function called Radial Basis Function RBF Gaussian prefer squared exponential 7Technically input points play ro le test inputs carry subscript asterisk clearer later training test points involved C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Function space View input x o u tp u t f x input x o u tp u t f x prior b posterior Figure Panel shows functions drawn random GP prior dots indicate values y actually generated functions correctly drawn lines joining large number evaluated points Panel b shows random functions drawn posterior e prior conditioned noise free observations indicated In plots shaded area represents pointwise mean plus minus times standard deviation input value corresponding confidence region prior posterior respectively informally thought roughly distance input space function value change significantly section For eq characteristic length scale unit By replacing xp xq xp xq eq positive constant change characteristic length scale process Also overall variance magnitude random function controlled positive pre factor exp eq We discuss factors affect predictions section set scale parameters chapter Prediction Noise free Observations We usually primarily interested drawing random functions prior want incorporate knowledge training data provides function Initially consider simple special case observations noise free know xi fi n The joint joint prior distribution training outputs f test outputs f according prior f f N K X X K X X K X X K X X If n training points n test points K X X denotes n n matrix covariances evaluated pairs training test points similarly entries K X X K X X K X X To posterior distribution functions need restrict joint prior distribution contain functions agree observed data points Graphically Figure think generating functions prior rejecting ones disagree observations al graphical rejection C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression strategy computationally efficient Fortunately probabilistic terms operation extremely simple corresponding con ditioning joint Gaussian prior distribution observations section A details givenoise free predictive distribution f X X f N K X X K X X 1f K X X K X X K X X 1K X X Function values f corresponding test inputs X sampled joint posterior distribution evaluating mean covariance matrix eq generating samples according method described section A Figure b shows results computations given data points marked symbols Notice trivial extend compu tations multidimensional inputs simply needs change evaluation covariance function accordance eq resulting functions harder display graphically Prediction Noisy Observations It typical realistic modelling situations access function values noisy versions thereof y f x Assuming additive independent identically distributed Gaussian noise variance 2n prior noisy observations cov yp yq k xp xq npq cov y K X X nI pq Kronecker delta iff p q zero It follows independence9 assumption noise diagonal matrix10 added comparison noise free case eq Introducing noise term eq write joint distribution observed target values function values test locations prior y f N K X X 2nI K X X K X X K X X Deriving conditional distribution corresponding eq arrive atpredictive distribution key predictive equations Gaussian process regression f X y X N f cov f f E f X y X K X X K X X 2nI 1y cov f K X X K X X K X X 2nI K X X 8There situations reasonable assume observations noise free example computer simulations e g Sacks et al 9More complicated noise models non trivial covariance structure handled section 10Notice Kronecker delta index cases value input signal covariance function input value index set random variables describing function noise identity point C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Function space View Observations Gaussian field Inputs y1 yc x1 x2 x xc y f1 f fc Figure Graphical model chain graph GP regression Squares rep resent observed variables circles represent unknowns The thick horizontal bar represents set fully connected nodes Note observation yi conditionally independent nodes given corresponding latent variable fi Because marginalization property GPs addition inputs x latent variables f unobserved targets y change distribution variables Notice exact correspondence weight space view eq identifying K C D C p D C D stand ei ther X X For set basis functions compute corresponding correspondence weight space viewcovariance function k xp xq xp p xq conversely posi tive definite covariance function k exists possibly infinite expansion terms basis functions section The expressions involving K X X K X X K X X etc look compact notation unwieldy introduce compact form notation setting K K X X K K X X In case test point x write k x k denote vector covariances test point n training points Using compact notation single test point x equations reduce f k K nI 1y V f k x x k K nI 1k Let examine predictive distribution given equations predictive distribution Note mean prediction eq linear combination obser vations y referred linear predictor Another way linear predictor look equation linear combination n kernel functions centered training point writing f x n ik xi x K 2nI 1y The fact mean prediction f x written eq despite fact GP represented terms possibly infinite number basis functions manifestation representer theorem section point We understand representer theorem result intuitively GP defines joint Gaussian dis tribution y variables point index set X C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression input x o u tp u t f x input x p o st co va ri n ce co v f x f x x x x posterior b posterior covariance Figure Panel identical Figure b showing random functions drawn posterior Panel b shows posterior co variance f x f x data different values x Note covariance close points high falling zero training points variance noise free process negative etc This happens smooth function happens mean data point tends exceed mean causing reversal sign covariance data points Note contrast prior covariance simply Gaussian shape negative making predictions x care n dimensional distribution defined n training points test point As Gaussian distribu tion marginalized taking relevant block joint covariance matrix section A clear conditioning n dimensional distribution observations gives desired result A graphical model representation GP given Figure Note variance eq depend observed targets inputs property Gaussian distribution The variance difference terms term K X X simply prior covariance subtracted positive term repre senting information observations gives function We simply compute predictive distribution test targets y addingnoisy predictions 2nI variance expression cov f The predictive distribution GP model gives pointwisejoint predictions errorbars simplified eq Although stated explicitly eq holds unchanged X denotes multiple test inputs case co variance test targets computed diagonal elements pointwise variances In fact eq mean function eq covariance function Gaussian posterior process recall definitionposterior process Gaussian process page The posterior covariance illustrated Figure b It useful particularly chapter introduce marginal likeli hood evidence p y X point The marginal likelihood integralmarginal likelihood C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Varying Hyperparameters input X inputs y targets k covariance function 2n noise level x test input L cholesky K 2nI L L y f k predictive mean eq v L k V f k x x v v predictive variance eq log p y X y logLii n log eq return f mean V f variance log p y X log marginal likelihood Algorithm Predictions log marginal likelihood Gaussian process regres sion The implementation addresses matrix inversion required eq Cholesky factorization section A For multiple test cases lines repeated The log determinant required eq computed Cholesky factor large n possible represent determinant The computational complexity n3 Cholesky decomposition line n2 solving triangular systems line test case line likelihood times prior p y X p y f X p f X df The term marginal likelihood refers marginalization function values f Under Gaussian process model prior Gaussian f X N K log p f X f K 1f log K n log likelihood factorized Gaussian y f N f 2nI use equations A A perform integration yielding log marginal likelihood log p y X y K 2nI 1y log K 2nI n log This result obtained directly observing y N K 2nI A practical implementation Gaussian process regression GPR shown Algorithm The algorithm uses Cholesky decomposition instead di rectly inverting matrix faster numerically stable section A The algorithm returns predictive mean variance noise free test data compute predictive distribution noisy test data y simply add noise variance 2n predictive variance f Varying Hyperparameters Typically covariance functions use free parameters For example squared exponential covariance function dimension following form ky xp xq f exp xp xq 2npq C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression input x o u tp u t y input x o u tp u t y input x o u tp u t y b c Figure Data generated GP hyperparameters f n shown symbols Using Gaussian process prediction hyperparameters obtain confidence region underlying function f shown grey Panels b c confidence region time hyperparameter values respectively The covariance denoted ky noisy targets y underlying function f Observe length scale signal variance 2f noise variance 2n varied In general free parametershyperparameters hyperparameters In chapter consider methods determining hyperpa rameters training data However section aim simply explore effects varying hyperparameters GP prediction Consider data shown signs Figure This generated GP SE kernel f n The figure shows standard deviation error bars predictions obtained values hyperparameters eq Notice error bars larger input values distant training points Indeed x axis 11We refer parameters covariance function hyperparameters emphasize parameters non parametric model accordance weight space view section parameters weights underlying parametric model integrated C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Decision Theory Regression extended error bars reflect prior standard deviation process f away data If set length scale shorter kept pa rameters generating process expect plots like Figure x axis rescaled factor equivalently x axis kept Figure sample function look wiggly If predictions process data generated short length scale process obtain result Figure b The remaining parameters set optimizing marginal likelihood explained chapter In case noise parameter reduced n greater flexibility signal means noise level reduced This observed datapoints near x plots In Figure essentially explained similar function value differing noise However Figure b noise level low points explained sharp variation value underlying function f Notice short length scale means error bars Figure b grow rapidly away datapoints In contrast set length scale longer example shown long length scale Figure c Again remaining parameters set optimizing marginal likelihood In case noise level increased n data explained slowly varying function lot noise Of course position quickly varying signal low noise slowly varying signal high noise extremes rise white noise process model signal rise constant signal added white noise Under models datapoints produced look like white noise However studying Figure white noise convincing model data sequence y s alternate sufficiently quickly correlations variability underlying function Of course relatively easy dimension model comparison methods marginal likelihood discussed chapter generalize higher dimensions allow score models In case marginal likelihood gives clear preference f n alternatives Decision Theory Regression In previous sections shown compute predictive distributions outputs y corresponding novel test input x The predictive dis tribution Gaussian mean variance given eq eq In practical applications forced decision act e need point like prediction optimal sense optimal predictions To end need loss function L ytrue yguess specifies loss loss function C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression penalty incurred guessing value yguess true value ytrue For example loss function equal absolute deviation guess truth Notice computed predictive distribution reference loss function In non Bayesian paradigms model typically trainednon Bayesian paradigm minimizing empirical risk loss In contrast Bayesian settingBayesian paradigm clear separation likelihood function training addition prior loss function The likelihood function describes noisy measurements assumed deviate underlying noise free function The loss function hand captures consequences making specific choice given actual true state The likelihood loss function need common Our goal point prediction yguess incurs smallest loss achieve don t know ytrue Instead minimize expected loss risk averaging w r t model s opinion theexpected loss risk truth R L yguess x L y yguess p y x D dy Thus best guess sense minimizes expected loss yoptimal x argmin yguess R L yguess x In general value yguess minimizes risk loss function yguess absolute error loss y median p y x D squared loss yguess y issquared error loss mean distribution When predictive distribution Gaussian mean median coincide symmetric loss function symmetric predictive distribution yguess mean predictive distribution However practical problems loss functions asymmetric e g safety critical applications point predictions computed directly eq eq A comprehensive treatment decision theory found Berger An Example Application In section use Gaussian process regression learn inverse dynamics seven degrees freedom SARCOS anthropomorphic robot arm The taskrobot arm map dimensional input space joint positions joint velocities joint accelerations corresponding joint torques This task pre viously study regression algorithms Vijayakumar Schaal Vijayakumar et al Vijayakumar et al Following 12Beware fallacious arguments like Gaussian likelihood implies squared error loss function 13We thank Sethu Vijayakumar providing data C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml An Example Application previous work present results seven mappings input variables seven torques One ask necessary learn mapping exist learning physics based rigid body dynamics models allow obtain torques position velocity acceleration variables However real robot arm actuated hydraulically lightweight compliant assumptions rigid body dynamics model violated It worth noting rigid body dynamics model nonlinear involving trigonometric functions squares input variables An inverse dynamics model following manner planning module decides trajectory takes robot start goal states specifies desired positions velocities accelerations time The inverse dynamics model compute torques needed achieve trajectory errors corrected feedback controller The dataset consists input output pairs training set remaining test set The inputs linearly rescaled zero mean unit variance training set The outputs centered zero mean training set Given prediction method evaluate quality predictions ways Perhaps simplest squared error loss compute squared residual y f x mean prediction target test point This summarized mean squared error MSE MSE averaging test set However quantity sensitive overall scale target values makes sense normalize variance targets test cases obtain standardized mean squared error SMSE This causes trivial method guessing mean training targets SMSE SMSE approximately Additionally produce predictive distribution test input evaluate negative log probability target model As GPR produces Gaussian predictive density obtains log p y D x log y f x predictive variance GPR computed V f n V f given eq include noise variance 2n predicting noisy target y This loss standardized subtracting loss obtained trivial model predicts Gaussian mean variance training data We denote standardized log loss SLL The mean SLL denoted MSLL Thus MSLL MSLL approximately zero simple methods negative better methods A number models tested data A linear regression LR model provides simple baseline SMSE By estimating noise level It makes sense use negative log probability obtain loss utility C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression Method SMSE MSLL LR RBD LWPR GPR Table Test results inverse dynamics problem number different methods The denotes missing entry caused methods producing predictive distributions MSLL evaluated residuals training set obtain predictive variance MSLL value LR The rigid body dynamics RBD model number free parameters estimated Vijayakumar et al squares fitting procedure We results locally weighted projection regression LWPR method Vijayakumar et al line method cycles dataset multiple times For GP models computationally expensive use training cases O n3 scaling basic algorithm In chapter present different approximate GP methods large datasets The result given Table obtained subset regressors SR approximation subset size This result taken Table gives results approximation methods applied inverse dynamics problem The squared exponential covariance function separate length scale parameter input dimensions plus signal noise variance parameters 2f n These parameters set optimizing marginal likelihood eq subset data chapter The results methods presented Table Notice problem non linear linear regression model poorly comparison non linear methods The non linear method LWPR improves linear regression outperformed GPR Smoothing Weight Functions Equiva lent Kernels Gaussian process regression aims reconstruct underlying signal f removing contaminating noise To computes weighted average noisy observations y f x k x K 2nI 1y f x linear combination y values Gaussian process regression linear smootherlinear smoother Hastie Tibshirani sec details In section study smoothing terms matrix analysis predictions training points terms equivalent kernel 15It surprising RBD worse linear regression However Stefan Schaal pers comm states RBD parameters optimized large dataset training data subset RBD model optimized w r t training set expect outperform linear regression C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Smoothing Weight Functions Equivalent Kernels The predicted mean values f training points given f K K 2nI 1y Let K eigendecomposition K n 1iuiu ith eigendecomposition eigenvalue ui corresponding eigenvector As K real sym metric positive semidefinite eigenvalues real non negative eigenvectors mutually orthogonal Let y n 1iui coefficients u y Then f n ii 2n ui Notice 2n component y ui effectively eliminated For covariance functions practice eigen values larger slowly varying eigenvectors e g fewer zero crossings means high frequency components y smoothed The effective number parameters degrees freedom smoother degrees freedom defined tr K K 2nI n n Hastie Tibshirani sec Notice counts number eigenvectors eliminated We define vector functions h x K 2nI 1k x Thus f x h x y making clear mean prediction point x linear combination target values y For fixed test point x h x gives vector weights applied targets y h x called weight function Silverman As Gaussian process regression linear smoother weight function weight function depend y Note difference linear model prediction linear combination inputs linear smoother prediction linear combination training set targets Understanding form weight function complicated matrix inversion ofK 2nI fact thatK depends specific locations n datapoints Idealizing situation consider observations smeared x space density observations In case analytic tools brought bear problem shown section By analogy kernel smoothing Silverman called idealized weight function equivalent kernel Girosi et al sec equivalent kernel A kernel smoother centres kernel function16 x computes kernel smoother xi x data point xi yi length scale The Gaussian commonly kernel function The prediction f x computed f x n 1wiyi wi n j j This known Nadaraya Watson estimator e g Scott sec The weight function equivalent kernel Gaussian process illus trated Figure dimensional input variable x We squared exponential covariance function set length scale There n training points spaced randomly 16Note kernel function need valid covariance function C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression b c d Figure Panels c weight function h x dots corresponding n training points equivalent kernel solid original squared exponential kernel dashed Panel d shows equivalent kernels different data densities See text details The small cross test point scale plots x axis Figures b weight function equivalent kernel x x respectively 2n Figure c x uses 2n In case dots correspond weight function h x solid line equivalent kernel construction explained The dashed line shows squared exponential kernel centered test point scaled height maximum value equivalent kernel Figure d shows variation equivalent kernel function n number datapoints unit interval Many interesting observations plots Observe equivalent kernel general shape different original SE kernel In Figure equivalent kernel clearly oscillatory negative sidelobes higher spatial frequency original kernel Figure b shows similar behaviour edge effects equivalent kernel truncated relative Figure In Figure c higher noise levels negative sidelobes reduced width equivalent kernel similar original kernel Also note overall height equivalent kernel c reduced compared C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Incorporating Explicit Basis Functions b averages wider area The oscillatory equivalent kernel lower noise levels understood terms eigenanalysis higher noise levels large slowly varying components y remain smaller noise levels oscillatory components retained In Figure d plotted equivalent kernel n n datapoints notice width equivalent kernel decreases n increases We discuss behaviour section The plots equivalent kernels Figure dense grid ngrid points computing smoother matrix K K 2gridI Each row matrix equivalent kernel appropriate location However order scaling right set 2grid 2nngrid n ngrid n means effective variance ngrid points larger correspondingly points effect cancels This understood imagining situation ngrid n independent Gaussian observations variance 2grid single x position equivalent Gaussian observation variance 2n In effect n observations smoothed uniformly interval The form equivalent kernel obtained analytically continuum limit look smooth noisy function The relevant theory example equivalent kernels given section Incorporating Explicit Basis Functions It common means necessary consider GPs zero mean func tion Note necessarily drastic limitation mean posterior process confined zero Yet reasons wish explicitly model mean function including interpretability model convenience expressing prior information number alytical limits need subsequent chapters The use explicit basis functions way specify non zero mean functions section use achieve interesting effects Using fixed deterministic mean function m x trivial Simply apply fixed mean function usual zero mean GP difference observations fixed mean function With f x GP m x k x x predictive mean f m X K X X K y y m X Ky K 2nI predictive variance remains unchanged eq However practice difficult specify fixed mean function In cases convenient specify fixed basis functions C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression coefficients inferred data Considerstochastic mean function g x f x h x f x GP k x x f x zero mean GP h x set fixed basis functions additional parameters This formulation expresses data close global linear model residuals modelled GP This idea explored explicitly early Blight Ott GP model residuals polynomial regression e h x x x2 polynomial regression When fitting model optimize parameters jointly hyperparameters covariance function Alternatively prior Gaussian N b B integrate parameters Following O Hagan obtain GP g x GP h x b k x x h x Bh x added contribution covariance function caused un certainty parameters mean Predictions plugging mean covariance functions g x eq eq After rearranging obtain g X H K K y y H f X R cov g cov f R B HK 1y H 1R H matrix collects h x vectors training H test cases B HK 1y H HK 1y y B 1b R H HK 1y K Notice nice interpretation mean expression eq line mean global linear model parameters compromise data term prior predictive mean simply mean linear output plus GP model predicts residuals The covariance sum usual covariance term new non negative contribution Exploring limit expressions prior param eter vague B O O matrix zeros obtain predictive distribution independent b g X f X R cov g cov f R HK 1y H 1R limiting HK 1y H 1HK 1y y Notice predictions limit B O implemented na vely plugging modified covariance function eq standard prediction equations entries covariance function tend infinity making unsuitable numerical implementation Instead eq Even non limiting case interest eq numerically preferable direct implementation based eq global linear add large eigenvalues covariance matrix affecting condition number C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml History Related Work Marginal Likelihood In short section briefly discuss marginal likelihood model Gaussian prior N b B explicit parameters eq useful later particularly section We express marginal likelihood eq log p y X b B H b y Ky H BH H b y log Ky H BH n2 log included explicit mean We interested exploring limit B O e prior vague In limit mean prior irrelevant case eq loss generality limiting case assume mean zero b giving log p y X b B y K 1y y y Cy log Ky log B log A n log A B HK 1y H C K 1y H A 1HK 1y matrix inversion lemma eq A eq A We explore behaviour log marginal likelihood limit vague priors In limit variances Gaussian directions spanned columns H infinite clear require special treatment The log marginal likelihood consists terms quadratic form y log determinant term term involving log Performing eigendecomposition covariance matrix contributions quadratic form term infinite variance directions zero However log determinant term tend minus infinity The standard solution Wahba Ansley Kohn case project y directions orthogonal span H compute marginal likelihood subspace Let rank H m Then shown Ansley Kohn means discard terms log B m log eq log p y X y K 1y y y Cy log Ky log A n m log A HK 1y H C K 1y H A 1HK 1y History Related Work Prediction Gaussian processes certainly recent topic espe cially time series analysis basic theory goes far time series work Wiener Kolmogorov s Indeed Lauritzen discusses relevant work Danish astronomer T N Thiele dating C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regression Gaussian process prediction known geostatistics field geostatistics e g Matheron Journel Huijbregts known krig ing meteorology Thompson Daley litera kriging ture naturally focussed dimensional input spaces Whittle sec suggests use methods spatial pre diction Ripley Cressie provide useful overviews Gaussian process prediction spatial statistics Gradually realized Gaussian process prediction general regression context For example O Hagan presents general theory given equations applies number dimensional regression problems Sacks et al describe GPR context computer experiments observations y noise free andcomputer experiments discuss number interesting directions optimization parameters covariance function chapter experimental design e choice x points provide information f The authors describe number computer simulations modelled including example response variable clock asynchronization circuit inputs transistor widths Santner et al recent book use GPs design analysis computer experiments Williams Rasmussen described Gaussian process regression inmachine learning machine learning context described optimization parameters covariance function Rasmussen They inspired use Gaussian process connection infinite neural networks described section Neal The kernelization linear ridge regression described known kernel ridge regression e g Saunders et al Relationships Gaussian process prediction regularization ory splines support vector machines SVMs relevance vector machines RVMs discussed chapter Exercises Replicate generation random functions Figure Use regular random grid scalar inputs covariance function eq Hints generate random samples multi variate Gaussian distributions given section A Invent training data points random draws resulting GP posterior eq In eq saw predictive variance x feature space regression model var f x x A x Show cov f x f x x A x Check compatible expression given eq 17Matheron named method South African mining engineer D G Krige C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Exercises The Wiener process defined x f See sec tion B details It mean zero non stationary covariance function k x x min x x If condition Wiener process passing f obtain process known Brow nian bridge tied Wiener process Show process covariance k x x min x x xx x x mean Write computer program draw samples process finite grid x points Let varn f x predictive variance Gaussian process regres sion model x given dataset size n The corresponding predictive variance dataset n training points de noted varn f x Show varn f x varn f x e predictive variance x increase training data ob tained One way approach problem use partitioned matrix equations given section A decompose varn f x k x x k K nI 1k An alternative information theoretic argument given Williams Vivarelli Note conclusion true Gaussian process priors Gaussian noise models hold generally Barber Saad C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Classification In chapter considered regression problems targets real valued Another important class problems classification1 problems wish assign input pattern x C classes C1 CC Practical examples classification problems handwritten digit recognition wish classify digitized image handwritten digit classes classification objects detected astronomical sky surveys stars galaxies Information distribution galaxies universe important theories early universe These examples nicely illustrate classification problems binary class binary multi class C multi class C We focus attention probabilistic classification test predictions probabilistic classificationtake form class probabilities contrasts methods provide guess class label distinction analogous difference predictive distributions point predictions regression setting Since generalization test cases inherently involves level uncertainty natural attempt predictions way reflects uncertainties In practical application seek class guess obtained solution decision problem involving predictive probabilities specification consequences making specific predictions loss function Both classification regression viewed function approximation problems Unfortunately solution classification problems Gaussian processes demanding regression problems considered chapter This assumed previous chapter likelihood function Gaussian Gaussian process prior combined Gaussian likelihood gives rise posterior Gaussian process functions remains analytically tractable For classification models targets discrete class labels Gaussian likelihood inappropriate non Gaussian likelihood 1In statistics literature classification called discrimination 2One choose ignore discreteness target values use regression treatment targets happen binary classification This known C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification chapter treat methods approximate inference classification exact inference feasible Section provides general discussion classification problems de scribes generative discriminative approaches problems In section saw Gaussian process regression GPR obtained generalizing linear regression In section describe analogue linear regression classification case logistic regression In section logistic regression generalized yield Gaussian process classification GPC ideas generalization linear regression GPR For GPR combination GP prior Gaussian likelihood gives rise posterior Gaussian process In classification case likelihood non Gaussian posterior process approximated GP The Laplace approximation GPC described section binary classification section multi class classification expecta tion propagation algorithm binary classification described section Both methods use Gaussian approximation posterior Experimental results GPC given section discussion results provided section Classification Problems The natural starting point discussing approaches classification joint probability p y x y denotes class label Using Bayes theorem joint probability decomposed p y p x y p x p y x This gives rise different approaches classification problems The generative approach models class conditional distribu generative approach tions p x y y C1 CC prior probabilities class computes posterior probability class p y x p y p x y C c p Cc p x Cc The alternative approach discriminative approach focussesdiscriminative approach modelling p y x directly Dawid calls generative discrimina tive approaches sampling diagnostic paradigms respectively To turn generative discriminative approaches practical methods need create models p x y p y x respectively These parametric form non parametric models based nearest neighbours For generative case simple com generative model example squares classification section 3Note important distinction Gaussian non Gaussian likelihoods regression non Gaussian likelihood requires similar treatment classification defines important conceptual application area chosen treat separate chapter non Gaussian likelihoods general section 4For generative approach inference p y generally straightforward esti mation binomial probability binary case multinomial probability multi class case C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification Problems mon choice model class conditional densities Gaussians p x Cc N c c A Bayesian treatment obtained placing appro priate priors mean covariance Gaussians However note Gaussian model makes strong assumption form class conditional density inappropriate model perform poorly For binary discriminative case simple idea turn output discriminative model exampleregression model class probability response function inverse link function squashes argument lie domain range guaranteeing valid probabilistic interpretation One example linear logistic regression model p C1 x x w z exp z combines linear model logistic response function Another response function common choice response function cumulative density function standard normal distribution z z N x dx This approach known probit regression Just gave Bayesian approach linear regression probit regression chapter parallel approach logistic regression discussed section As regression case model important step Gaussian process classifier Given generative discriminative approaches generative discriminative prefer This biggest question classification believe right answer ways writing joint p y x correct However possible identify strengths weaknesses approaches The discriminative approach appealing directly modelling want p y x Also density estimation class conditional distributions hard problem particularly x high dimensional interested classification generative approach mean trying solve harder problem need However deal missing input values outliers unlabelled data missing values points principled fashion helpful access p x obtained marginalizing class label y joint p x y p y p x y generative approach A factor choice generative discriminative approach conducive incorporation prior information available See Ripley sec discussion issues The Gaussian process classifiers developed chapter discriminative Decision Theory Classification The classifiers described provide predictive probabilities p y x test input x However actually needs decision need consider decision theory Decision theory regres sion problem considered section discuss decision theory classification problems A comprehensive treatment decision theory found Berger C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification Let L c c loss incurred making decision c true class Cc Usually L c c c The expected loss5 risk taking decision c loss risk given x RL c x c L c c p Cc x optimal decision c minimizes RL c x One common choice loss function zero loss penalty unit paid incorrect classification 0zero loss correct In case optimal decision rule choose class Cc maximizes6 p Cc x minimizes expected error x However zero loss appropriate A classic example theasymmetric loss difference loss failing spot disease carrying medical test compared cost false positive test L c c L c c The optimal classifier zero loss known Bayes classi Bayes classifier fier By construction feature space divided decision regions R1 RC pattern falling decision region Rc assigned classdecision regions Cc There decision region corresponding single class The boundaries decision regions known decision surfaces decision boundaries One expect misclassification errors occur regions max imum class probability maxj p Cj x relatively low This region strong overlap classes lack training examples region Thus sensible strategy add reject option thatreject option maxj p Cj x threshold ahead classify pattern reject leave classification task sophisticated system For multi class classification alternatively quire gap probable second probable class exceed reject As varied obtains error reject curve plotting percentage patterns classified incorrectly percentage rejected Typically error rate fall rejection rate increases Hansen et al provide analysis error reject trade We focused probabilistic approach classification involves stage approach computing posterior distribution functions combining loss function produce decision However worth noting authors argue goal eventually decision aim approximate classification function minimizes risk expected loss defined asrisk minimization RL c L y c x p y x dydx p y x joint distribution inputs targets c x clas sification function assigns input pattern x C classes e g Vapnik ch As p y x unknown approach seeks minimize objective function includes empirical risk n L yi c xi regularization term While reasonable 5In Economics usually talks maximizing expected utility minimizing expected loss loss negative utility This suggests statisticians pessimists economists optimists 6If class equal posterior probability ties broken arbitrarily C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Linear Models Classification method note probabilistic approach allows inference stage different loss functions help incorporate prior knowledge function noise model advantage giving probabilistic predictions helpful e g reject option Linear Models Classification In section briefly review linear models binary classification form foundation Gaussian process classification models sec tion We follow SVM literature use labels y y distinguish classes multi class case section use labels The likelihood p y x w x w given weight vector w z sigmoid7 function When logistic z z eq model usually called simply logistic linear logistic regression regression emphasize parallels linear regression prefer term linear logistic regression When cumulative Gaussian z z model linear probit regression linear probit regression As probability classes sum p y x w p y x w Thus data point xi yi likelihood given x w yi x w yi For symmetric likelihood functions logistic probit z z written concisely concise notation p yi xi w yifi fi f xi x w Defining logit transformation logit x logit log p y x p y x logistic regression model written logit x x w The logit x function called log odds log odds ratio ratio Generalized linear modelling McCullagh Nelder deals issue extending linear models non Gaussian data scenarios logit transformation canonical link function binary data choice simplifies algebra algorithms Given dataset D xi yi n assume labels generated independently conditional f x Using Gaussian prior w N p regression eq obtain un normalized log posterior log p w X y c w 1p w n log yifi In linear regression case Gaussian noise posterior Gaussian mean covariance given eq For classification posterior 7A sigmoid function monotonically increasing function mapping R It derives shaped like letter S C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification simple analytic form However easy sigmoid functions logistic cumulative Gaussian log likelihood concave function w fixed D As quadratic penalty onconcavity w concave log posterior concave function means relatively easy find unique maximum The concavity beunique maximum derived fact Hessian log p w X y negative definite section A details The standard algorithm finding maxi mum Newton s method context usually called iteratively reweighted squares IRLS algorithm described e g McCullagh andIRLS algorithm Nelder However note Minka provides evidence optimization methods e g conjugate gradient ascent faster IRLS Notice maximum likelihood treatment corresponding unpe properties maximum likelihood nalized version eq result undesirable outcomes If dataset linearly separable e exists hyperplane separates positive negative examples maximizing unpenalized likelihood because w tend infinity However predictions p y x w predictions hard e zero If problem I shall conditioned e g duplicate linearly dependent input dimensions unique solution As example consider linear logistic regression case x space dimensional bias weight w dimensional The prior weight space Gaussian simplicity set p I Contours prior p w illustrated Figure If data set D shown Figure b induces posterior distribution weight space shown Figure c Notice posterior non Gaussian unimodal expected The dataset linearly separable weight vector direction clearly reasonable choice posterior distribution shows To predictions based training set D testpredictions point x p y x D p y w x p w D dw integrating prediction p y w x x w posterior distri bution weights This leads contours predictive distribution shown Figure d Notice contours bent reflecting integration different plausible w s In multi class case use multiple logistic softmax functionsoftmax multiple logistic p y Cc x W exp x wc c exp x wc wc weight vector class c weight vectors col lected matrix W The corresponding log likelihood form n C c c yi x wc log c exp x wc As binary case log likelihood concave function W It interesting note generative approach class conditional distributions p x y Gaussian covariance matrix C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Process Classification w w x x b w w x x c d Figure Linear logistic regression Panel shows contours prior distri bution p w N I Panel b shows dataset circles indicating class crosses denoting class Panel c shows contours posterior distribution p w D Panel d shows contours predictive distribution p y x p y x form given eq eq multi class cases respectively constant function included x Gaussian Process Classification For binary classification basic idea Gaussian process prediction simple place GP prior latent function f x latent function squash logistic function obtain prior x p y x f x Note deterministic function f f stochastic This construction illustrated Figure dimensional input space It natural generalization linear logistic C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification input x la te n t fu n ct io n f x input x cl ss p ro b b ili ty x b Figure Panel shows sample latent function f x drawn Gaussian process function x Panel b shows result squashing sample func tion logistic logit function z exp z obtain class probability x f x regression model parallels development linear regression GP regression explored section Specifically replace linear f x function linear logistic model eq Gaussian process correspondingly Gaussian prior weights GP prior The latent function f plays ro le nuisance function notnuisance function observe values f observe inputs X class labels y particularly interested values f particular test cases x The purpose f solely allow convenient formulation model computational goal pursued coming sections remove integrate f We tacitly assumed latent Gaussian process noise free andnoise free latent process combined smooth likelihood functions logistic probit However equivalently think adding independent noise latent process combination step function likelihood In particular assuming Gaussian noise step function likelihood exactly equivalent noise free8 latent process probit likelihood exercise Inference naturally divided steps computing distribution latent variable corresponding test case p f X y x p f X x f p f X y df p f X y p y f p f X p y X posterior latent vari ables subsequently distribution latent f produce probabilistic prediction p y X y x f p f X y x df 8This equivalence explains numerical problems arise considering noise free process care taken implementation comment end section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Laplace Approximation Binary GP Classifier In regression case Gaussian likelihood computation predictions straightforward relevant integrals Gaussian computed analytically In classification non Gaussian likelihood eq makes integral analytically intractable Similarly eq intractable analytically certain sigmoid functions binary case dimensional integral simple numerical techniques generally adequate Thus need use analytic approximations integrals solutions based Monte Carlo sampling In coming sections describe ana lytic approximations approximate non Gaussian joint posterior Gaussian straightforward Laplace approximation method Williams Barber second sophisticated expectation propagation EP method Minka The cavity TAP ap proximation Opper Winther closely related EP method A number approximations suggested e g Gibbs MacKay Jaakkola Haussler Seeger Neal describes use Markov chain Monte Carlo MCMC approximations All methods typically scale O n3 large datasets work approximations reduce computation time discussed chapter The Laplace approximation binary case described section multi class case section The EP method binary clas sification described section Relationships Gaussian process classifiers techniques spline classifiers support vector ma chines squares classification discussed sections respectively The Laplace Approximation Binary GP Classifier Laplace s method utilizes Gaussian approximation q f X y poste rior p f X y integral Doing second order Taylor expansion log p f X y maximum posterior obtain Gaussian approximation q f X y N f f A exp f f A f f f argmaxf p f X y A log p f X y f f Hessian negative log posterior point The structure rest section follows In section describe find f A Section explains predictions having obtained q f y section gives implementation details Laplace GP classifier The Laplace approximation marginal likelihood described section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification latent times target z y f lo g li ke lih o o d lo g p y f log likelihood 1st derivative 2nd derivative latent times target z y f lo g li ke lih o o d lo g p y f log likelihood 1st derivative 2nd derivative logistic b probit Figure The log likelihood derivatives single case function zi yifi logistic b cumulative Gaussian likelihood The likelihood functions fairly similar main qualitative difference large negative arguments log logistic behaves linearly log cumulative Gaussian quadratic penalty Both likelihoods log concave Posterior By Bayes rule posterior latent variables given p f X y p y f p f X p y X p y X independent f need consider un normalized posterior maximizing w r t f Taking logarithmun normalized posterior introducing expression eq GP prior gives f log p y f log p f X log p y f f K 1f log K n log Differentiating eq w r t f obtain f log p y f K 1f f log p y f K W K W log p y f diagonal likelihood factorizes cases distribution yi depends fi fj Note likelihood p y f log concave diagonal elements W non negative Hessian eq negative definite f concave unique maximum section A details There possible functional forms likelihood gives target class probability function latent variable f Two commonly likelihood functions logistic cumulative Gaussian seelog likelihoods derivatives Figure The expressions log likelihood likelihood functions second derivatives w r t latent variable given C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Laplace Approximation Binary GP Classifier following table log p yi fi fi log p yi fi f2i log p yi fi log exp yifi ti log yifi yiN fi yifi N fi yifi yifiN fi yifi defined p yi fi t y At maximum f f K log p y f self consistent equation f log p y f non linear function f eq solved directly To find maximum use Newton s method iteration Newton s method fnew f f K W log p y f K 1f K W W f log p y f To gain intuition update let consider happens datapoints explained f log p yi fi fi Wii close zero points As approximation break f subvectors f1 corresponds points explained f2 Then easy exercise fnew1 K11 I11 W11K11 W11f1 log p y1 f1 fnew2 K21K f new K21 denotes n2 n1 block K containing covariance groups points etc This means fnew1 computed ignoring intuition influence explained pointsentirely explained points fnew2 predicted f new usual GP prediction methods e treating points like test points Of course predictions fnew2 fail match targets correctly cease explained updated iteration Having found maximum posterior f specify Laplace approximation posterior Gaussian mean f covariance matrix given negative inverse Hessian eq q f X y N f K W One problem Laplace approximation essentially un controlled Hessian evaluated f poor approximation true shape posterior The peak broader nar rower Hessian indicates skew peak Laplace approximation assumes elliptical contours C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification Predictions The posterior mean f Laplace approximation expressed combining GP predictive mean eq eq intolatent mean Eq f X y x k x K 1f k x log p y f Compare exact mean given Opper Winther Ep f X y x E f f X x p f X y df k x K 1f p f X y df k x K 1E f X y fact GP E f f X x k x K 1f let E f X y denote posterior mean f given X y Notice similarity middle expression eq eq exact intractable average E f X y replaced modal value f Eq f X y A simple observation eq positive training examples rise positive coefficient kernel function log p yi fi 0sign kernel coefficients case negative examples rise negative coefficient analogous solution support vector machine eq Also note training points log p yi fi e explained f contribute strongly predictions novel test points similar behaviour non support vectors support vector machine section We compute Vq f X y variance f X y Gaussian approximation This comprises terms e Vq f X y x Ep f X x f f E f X x f Eq f X y E f X x f E f X y x The term variance f condition particular value f given k x x k x K 1k x cf eq The second term eq fact E f X x f k x K 1f depends f additional term k x K cov f X y K 1k x Under Gaussian approximation cov f X y K W thuslatent variance Vq f X y x k x x k K 1k k K K W 1K 1k k x x k K W 1k line obtained matrix inversion lemma eq A Given mean variance f predictions computingaveraged predictive probability Eq X y x f q f X y x df C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Laplace Approximation Binary GP Classifier q f X y x Gaussian mean variance given equations respectively Notice non linear form sigmoid predictive probability eq different sigmoid expectation f Eq f y We MAP prediction distinguish averaged predictions eq MAP prediction In fact shown Bishop sec predicted test labels identical binary decisionsgiven choosing class highest probability obtained averaged MAP predictions identical binary9 classification To note decision boundary MAP value Eq f X y x corre sponds Eq f X y x Eq f X y x The decision bound ary averaged prediction Eq X y x corresponds Eq f X y x This follows fact f antisym metric q f X y x symmetric Thus concerned probable classification necessary compute predictions eq However soon need confidence prediction e g concerned reject option need Eq X y x If z cumulative Gaussian function eq computed analytically shown section On hand logistic function need resort sampling methods analytical approximations compute dimensional integral One attractive method note logistic function z c d f cumulative density function corresponding p d f probability density function p z sech2 z known logistic sech squared distribution Johnson et al ch Then approximating p z mixture Gaussians approximate z linear combination error functions This approximation Williams Barber app A Wood Kohn Another approximation suggested MacKay 1992d f y f f y Vq f X y x The effect latent predictive variance approximation suggests soften prediction obtained MAP prediction f e Implementation We implementations finding Laplace approximation Algorithm making predictions Algorithm Care taken avoid numer ically unstable computations minimizing computational effort achieved simultaneously It turns desired terms expressed terms symmetric positive definite matrix B I W 2KW computation costs O n2 W diagonal The B matrix eigenvalues bounded bounded nmaxij Kij covariance functions B guaranteed conditioned 9For multi class predictions discussed section situation complicated C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification input K covariance matrix y targets p y f likelihood function f initialization repeat Newton iteration W log p y f eval W e g eq L cholesky I W 2KW B I W 2KW b W f log p y f b W 2L L W 2Kb eq eq f Ka convergence objective f log p y f log q y X f log p y f logLii eq return f f post mode log q y X approx log marg likelihood Algorithm Mode finding binary Laplace GPC Commonly convergence criteria depend difference successive values objective function f eq magnitude gradient vector f eq magnitude difference successive values f In practical implementation needs secure divergence checking iteration leads increase objective trying smaller step size The computational complexity dominated Cholesky decomposition line takes n3 operations times number Newton iterations operations quadratic n numerically safe compute Cholesky decomposition LL B useful computing terms involving B B The mode finding procedure uses Newton iteration given eq involving matrix K W Using matrix inversion lemma eq A K W K KW 2B 1W 2K B given eq The advantage K eigenvalues arbitrarily close zero numerically unstable invert safely work B In addition Algorithm keeps vector K 1f addition f allows evaluation objective f eq depends f explicit reference K avoid possible numerical problems Similarly computation predictive variance Vq f y eq need evaluate quadratic form involving matrix K W Re writing K W W 2W K W 1W 2W W 2B 1W achieves numerical stability opposed inverting W arbitrarily small eigenvalues Thus predictive variance eq computed Vq f y k x x k x W LL 1W k x k x x v v v L W k x Seeger p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Laplace Approximation Binary GP Classifier input f mode X inputs y targets k covariance function p y f likelihood function x test input W log p y f L cholesky I W 2KW B I W 2KW f k x log p y f eq v L W k x V f k x x v v eq eq z N z f V f dz eq return predictive class probability class Algorithm Predictions binary Laplace GPC The posterior mode f computed Algorithm input For multiple test inputs lines applied test input Computational complexity n3 operations line plus n2 operations test case line The dimensional integral line analytically cumulative Gaussian likelihood computed approximation numerical quadrature In practice compute Cholesky decomposition LL B Newton steps Algorithm compute predictive variance backsubstitution L discussed In addition L compute In W 2KW B needed computation marginal likelihood eq log B logLii To save computation use incomplete Cholesky factorization incomplete Cholesky factorizationNewton steps suggested Fine Scheinberg Sometimes suggested useful replace K K I small constant improve numerical conditioning10 K However taking care implementation details necessary Marginal Likelihood It useful particularly chapter compute Laplace ap proximation marginal likelihood p y X For regression case Gaussian noise marginal likelihood calculated analytically eq We p y X p y f p f X df exp f df Using Taylor expansion f locally f obtain f f f f A f f approximation q y X marginal likelihood p y X q y X exp f exp f f A f f df 10Neal refers adding jitter context Markov chain Monte Carlo MCMC based inference work latent variables f explicitly represented Markov chain makes addition jitter difficult avoid Within analytical approximations distribution f considered jitter unnecessary C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification This Gaussian integral evaluated analytically obtain approximation log marginal likelihood log q y X f K 1f log p y f log B B K K W In W 2KW vector hyper parameters covariance function previously suppressed notation brevity Multi class Laplace Approximation Our presentation follows Williams Barber We introduce vector latent function values n training points C classes f f11 f n f f n f C f C n Thus f length Cn In following generally refer quantities pertaining particular class superscript c particular case subscript usual e g vector C latents particular case fi However exception vectors matrices formed covariance function class c subscript c The prior f form f N K As assumed C latent processes uncorrelated covariance matrix K block diagonal matrices K1 KC Each individual matrix Kc expresses correlations latent function values class c Note covariance functions pertaining different classes different Let y vector length f n entry class label example C entries Let ci denote output softmax training point e softmax p yci fi c exp fci c exp f c Then vector length f entries ci The multi classun normalized posterior analogue eq log un normalized posterior f f K 1f y f n log C c exp fci log K Cn log As binary case seek MAP value f p f X y By differentiating eq w r t f obtain K 1f y Thus maximum f K y Differentiating f ci f c log j exp f ji c cc c c C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Multi class Laplace Approximation obtain11 K W W diag Cn nmatrix obtained stacking vertically diagonal matrices diag c c subvector pertaining class c As binary case notice positive definite f concave maximum unique exercise As binary case use Newton s method search mode giving fnew K W W f y This update coded na vely O C3n3 matrices size Cn inverted However described section utilize structure W bring computational load O Cn3 The Laplace approximation gives Gaussian approximation q f X y posterior p f X y To predictions test point x need com predictive distribution f pute posterior distribution q f X y x f x f f1 fC In general q f X y x p f X x f q f X y df As p f X x f q f X y Gaussian q f X y x Gaussian need compute mean covariance The predictive mean class c given Eq fc x X y x kc x K 1c f c kc x yc c kc x vector covariances test point training points cth covariance function f c subvector f pertaining class c The equality comes eq maximum f Note close correspondence eq This vector form Eq f y Q y defining Cn C matrix Q k1 x k2 x kC x Using similar argument eq obtain covq f X y x Q K K W 1K 1Q diag k x x Q K W 1Q diagonal C C matrix cc kc x x k c x K 1c kc x k x x vector covariances c th element kc x x 11There sign error equation Williams Barber implementation C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification input K covariance matrix y targets f initialization repeat Newton iteration compute f eq defn eq c C L cholesky In D c KcD c Ec D c L L D c E block diag D ICn D 2KD 1D zc logLii compute log determinant end M cholesky cEc b D f y b W f y eq c EKb b c ERM M R c eq eq f Ka convergence objective f y f log c exp f c log q y X f y f log c exp f c c zc eq return f f post mode log q y X approx log marg likelihood Algorithm Mode finding multi class Laplace GPC D diag R matrix stacked identity matrices subscript c block diagonal matrix indicates n n submatrix pertaining class c The computational complexity dominated Cholesky decomposition lines forward backward substitutions line total complexity O C n3 times num ber Newton iterations operations O Cn2 exploiting diagonal block diagonal structures The memory requirement O Cn2 For comments convergence criteria line avoiding divergence refer caption Algorithm page We need consider predictive distribution q y ob tained softmaxing Gaussian q f y In binary case saw predicted classification obtained thresholding mean value Gaussian In multi class case need variability mean account affect overall classification exercise One simple way Algorithm estimate mean prediction Eq y draw samples Gaussian q f y softmax average The Laplace approximation marginal likelihood obtained inmarginal likelihood way binary case yielding log p y X log q y X f K 1f y f n log C c exp f ci log ICn W 2KW As inversion K W determinant term computed effi ciently exploiting structure W section In section described Laplace approximation multi class classification However work EP type methods multi class case Seeger Jordan C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Multi class Laplace Approximation input K covariance matrix f posterior mode x test input compute f eq defn eq c C L cholesky In D c KcD c Ec D c L L D c E block diag D ICn D 2KD 1D end M cholesky cEc c C c y c c kc latent test mean eq b Eckc c Ec R M M R b c C cc c kc end latent test covariance eq cc cc kc x x b kc end initialize Monte Carlo loop estimate S predictive class probabilities S samples f N sample latent values joint Gaussian posterior exp fc c exp f c accumulate probability eq end S normalize MC estimate prediction vector return Eq f f x x X y predicted class probability vector Algorithm Predictions multi class Laplace GPC D diag R matrix stacked identity matrices subscript c block diagonal matrix indicates n n submatrix pertaining class c The computational complexity dominated Cholesky decomposition lines total complexity O C n3 memory requirement O Cn2 For multiple test cases repeat line test case practice multiple test cases reorder computations lines avoid referring Ec matrices repeatedly Implementation The implementation follows closely implementation binary case de tailed section slight complications K block diagonal matrix size Cn Cn W matrix longer diagonal eq Care taken exploit structure matrices reduce computational burden The Newton iteration eq requires inversion K W write K W K K K W 1K matrix inversion lemma eq A In following inversion matrix K W main concern First apply C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification matrix inversion lemma eq A W matrix W D D R I R DR 1R D RO 1R D diag R D Cn n matrix stacked In unit matrices use fact normalizes classes R DR cDc In O zero matrix Introducing K W applying matrix inversion lemma eq A K W K D RO 1R E ER O R ER 1R E E ER cEc 1R E E K D D I D 2KD 1D block diagonal matrix R ER cEc The Newton iterations computed inserting eq eq detailed Algorithm The predictions use equivalent route compute Gaussian posterior final step deriving predictive class probabilities Monte Carlo shown Algorithm Expectation Propagation The expectation propagation EP algorithm Minka general approxi mation tool wide range applications In section present application specific case GP model binary classification We note Opper Winther presented similar method binary GPC based fixed point equations Thouless Anderson Palmer TAP type mean field approximation statistical physics The fixed points methods precise details algorithms different The EP algorithm naturally lends sparse approximations discussed detail touched section The object central importance posterior distribution latent variables p f X y In following notation suppress explicit depen dence hyperparameters section treatment The posterior given Bayes rule product normalization term prior likelihood p f X y Z p f X n p yi fi prior p f X Gaussian utilized fact likeli hood factorizes training cases The normalization term marginal likelihood Z p y X p f X n p yi fi df 12Readers disturbed sloppy treatment inverse singular matrices invited insert matrix In eq verify eq coincides limit C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Expectation Propagation So far exactly regression case discussed chapter However case classification likelihood p yi fi Gaussian property heavily arriving analytical solutions regression framework In section use probit likelihood page binary classification p yi fi fiyi makes posterior eq analytically intractable To overcome hurdle EP framework approximate likelihood local like lihood approximation13 form un normalized Gaussian function latent variable fi p yi fi ti fi Z 2i Z iN fi defines site parameters Z 2i Remember notation site parameters N normalized Gaussian distribution Notice approxi mating likelihood e probability distribution normalizes targets yi un normalized Gaussian distribution latent variables fi This reasonable interested likelihood behaves function latent fi In regression setting utilized Gaussian shape likelihood point Gaussian distribution outputs yi implied Gaussian shape function latent vari able fi In order compute posterior course primarily interested likelihood behaves function fi The property likelihood normalize yi value fi simultaneously achievable desideratum Gaussian dependence fi EP ap proximation abandon exact normalization tractability The product independent local likelihoods ti n ti fi Z 2i N Z vector diagonal ii 2i We approximate posterior p f X y q f X y q f X y ZEP p f X n ti fi Z 2i N K eq A compute product definition know distribution normalize correctly f Notice use tilde parameters Z local likelihood approximations 13Note likelihood approximation local posterior approximation produced EP algorithm global latent variables coupled prior 14However computing marginal likelihood normalization crucial section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification plain parameters approximate posterior The malizing term eq ZEP q y X EP algorithm s approximation normalizing term Z eq eq How choose parameters local approximating distributions ti One obvious ideas minimize Kullback Leibler KL divergence section A posterior approximation KL divergence KL p f X y q f X y Direct minimization KL divergence joint distribution f turns intractable One alternatively choose minimize reversed KL divergence KL q f X y p f X y respect distribution q f X y carry variational inference GPC e g Seeger Instead key idea EP algorithm update individual ti ap proximations sequentially Conceptually iterating following steps start current approximate posterior leave current ti giving rise marginal cavity distribution Secondly combine cavity distribution exact likelihood p yi fi desired non Gaussian marginal Thirdly choose Gaussian approximation non Gaussian marginal final step compute ti makes posterior desired marginal step These steps iterated convergence In detail optimize ti approximations sequentially approximation far variables In particular approximate posterior fi contains kinds terms prior p f X local approximate likelihoods tj cases j exact likelihood case p yi fi yifi Our goal combine sources information choose parameters ti marginal posterior accurate possible We combine prior local likelihood approximations cavity distribution q fi p f X j tj fj Z j j 2j dfj subsequently combine exact likelihood case Concep tually think combination prior n approximate likelihoods eq ways explicitly multiplying terms equivalently removing approximate likelihood approx imate posterior eq Here follow approach The marginal fi q f X y obtained eq A eq q fi X y N fi 2i 2i ii This marginal eq contains approximate term C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Expectation Propagation ti need divide ti cavity dis cavity distribution tribution q fi N fi Note cavity distribution parameters carry subscript indicating include cases number The easiest way verify eq multiply cavity distribution local likelihood approximation ti eq eq A recover marginal eq Notice despite appearance eq cavity mean variance course dependent 2i exercise To proceed need find new un normalized Gaussian marginal best approximates product cavity distribution exact likelihood q fi Z iN 2i q fi p yi fi It known q x Gaussian distribution q x min imizes KL p x q x second moments match p x eq A As q fi un normalized choose additionally impose condition zero th moments normalizing constants match choosing parameters q fi match right hand eq This process illustrated Figure The derivation moments somewhat lengthy moved details section The desired posterior marginal moments Z zi yi iN zi zi 2i iN zi zi zi N zi zi zi yi The final step compute parameters approximation ti achieves match desired moments In particular product cavity distribution local approximation desired moments leading Z Z exp easily verified multiplying cavity distribution local ap proximation eq A obtain eq Note desired marginal posterior variance 2i given eq guaranteed smaller cavity variance 2i satisfied This completes update local likelihood approximation ti We update approximate posterior eq 15In cases likelihood log concave 2i general likelihood guarantee C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification likelihood cavity posterior approximation b Figure Approximating single likelihood term Gaussian Panel dash dotted exact likelihood fi corresponding target yi function latent fi dotted Gaussian cavity distribution N fi solid posterior dashed posterior approximation Panel b shows enlargement panel single site changed computationally efficient rank update section The EP algorithm iteratively updating local approximation turn It clear passes data required update local approximation potentially influences approximate marginal posteriors Predictions The procedure making predictions EP framework closely resembles algorithm Laplace approximation section EP gives Gaus sian approximation posterior distribution eq The approximate predictive mean latent variable f Eq f X y x k K k K K k K The approximate latent predictive variance analogous derivation eq eq playing ro le W Vq f X y x k x x k K 1k The approximate predictive distribution binary target q y X y x Eq X y x f q f X y x df q f X y x approximate latent predictive Gaussian mean variance given eq eq This integral readily evaluated eq giving predictive probability q y X y x k K k x x k K 1k C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Expectation Propagation Marginal Likelihood The EP approximation marginal likelihood found malization eq ZEP q y X p f X n ti fi Z 2i df Using eq A eq A analogous way treatment regression setting equations arrive log ZEP log K K n log yi n log n denotes hyperparameters covariance function This expres sion nice intuitive interpretation terms marginal likelihood regression model component inde pendent Gaussian noise variance ii diagonal cf eq The remaining terms come normalization constants Z The penalizes cavity leave distributions agreeing classification labels eq In words marginal likelihood combines desiderata means local likelihood approximations predicted GP corre sponding latent function ignoring particular training example able predict corresponding classification label Implementation The implementation EP algorithm follows derivation previous section closely care taken achieve numerical stability similar ways considerations Laplace s method section In addition wish able specifically handle case site variances 2i tend infinity corresponds ignoring corresponding likelihood terms form basis sparse approximations touched section In limit remains defined obvious e g looking eq It turns slightly convenient use natural parameters site cavity natural parameters parameters S diag S 2i The symmetric matrix central importance B I S 2KS plays ro le equivalent eq Expressions involving inverse B computed Cholesky factorization numerically stable C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification input K covariance matrix y targets K initialization eq repeat n compute approximate cavity para meters eq compute marginal moments 2i eq 2i update site parameters eq ii si s update eq eq si column end L cholesky In S 2KS compute approximate V L S 2K posterior parameters K V V eq eq convergence compute logZEP eq existing L return natural site param logZEP approx log marg likelihood Algorithm Expectation Propagation binary classification The targets y line In lines parameters approximate posterior computed exist large number rank updates line eventually because loss numerical precision The computational complexity dominated rank updates line takes O n2 variable e O n3 entire sweep variables Similarly computing lines O n3 eigenvalues B bounded The parameters Gaussian approximate posterior eq computed K S K K K S 1K K KS 2B 1S 2K After updating parameters site need update approximate posterior eq taking new site parameters account For inverse covariance matrix approximate posterior eq K S 1new K S old new old eie ei unit vector direction S diag Using matrix inversion lemma eq A eq obtain new new old newi old newi old old ii sis time O n2 si th column old The posterior mean calculated eq In EP algorithm site updated turn passes sites required Pseudocode EP GPC algorithm given Algorithm C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Expectation Propagation input natural site param X inputs y targets k covariance function x test input L cholesky In S 2KS B In S 2KS z S 2L L S 2K f k x z eq eq v L S k x V f k x x v v eq eq f V f eq return predictive class probability class Algorithm Predictions expectation propagation The natural site parameters posterior computed algorithm input For multiple test inputs lines applied test input Computational complexity n3 n2 operations line plus n2 operations test case line Cholesky decomposition line avoided storing Algorithm Note close similarity Algorithm page There formal guarantee convergence authors reported EP Gaussian process models works relatively For predictive distribution mean eq evaluated Eq f X y x k K S 1S k I K S 1K k I S 2B 1S 2K predictive variance eq similarly Vq f X y x k x x k K S 1k k x x k S 2B 1S k Pseudocode making predictions EP given Algorithm Finally need evaluate approximate log marginal likelihood eq There terms need careful consideration principally fact values arbitrarily small safely inverted We start fourth terms eq log T S log K log S I S T log S 1B log logLii T diagonal matrix cavity precisions Tii L Cholesky factorization B In eq factored matrix S determinants terms cancel Continuing 16It conjectured proven L Csato personal communication EP guaranteed converge likelihood log concave C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification fifth term eq quadratic second term T S K S T S K S S K S T S K KS 2B 1S 2K T S eq apply matrix inversion lemma eq A parenthesis inverted The remainder fifth term eq evaluated identity T S iT S T S vector cavity means The term eq requires special treatment evaluated written Experiments In section present results applying algorithms GP clas sification discussed previous sections data sets The purpose firstly illustrate behaviour methods secondly gain insights good performance compared commonly machine learning methods classification Section illustrates action GP classifier toy binary pre diction problem d input space shows effect varying length scale SE covariance function In section illustrate compare behaviour approximate GP methods simple dimensional binary task In section present results binary GP classifier handwritten digit classification task study effect vary ing kernel parameters In section carry similar study multi class GP classifier classify digits classes In section discuss methods experimental theoretical viewpoints A Toy Problem Figure illustrates operation Gaussian process classifier binary problem squared exponential kernel variable length scale logistic response function The Laplace approximation plots The data points lie square shown panel Notice particular lone white point black points NE corner lone black point white points SW corner In panel b length scale relatively short value In case latent function free vary relatively quickly classifications C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments b c d Figure Panel shows location data points dimensional space The classes labelled open circles closed circles Panels b d contour plots predictive probability Eq x y signal variance 2f length scales respectively The de cision boundaries classes shown thicker black lines The maximum value attained minimum provided thresholding predictive probability Eq x y agrees training labels data points In contrast panel d length scale set Now latent function vary smoothly lone points misclassified Panel c obtained As expected decision boundaries complex shorter length scales Methods setting hyperparameters based data discussed chapter C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification input x p d ic tiv e p ro b b ili ty Class Class Laplace EP p y x input x la te n t fu n ct io n f x Laplace EP b Figure One dimensional toy classification dataset Panel shows dataset points class plotted class predictive probability Laplace s method EP approximation Also shown probability p y x data generating process Panel b shows corresponding distribution latent function f x showing curves mean standard deviations corresponding confidence regions One dimensional Example Although Laplace s method EP approximation similar sults present simple dimensional problem highlights differences methods The data shown Figure consists data points groups generated mixture Gaussians centered points points points middle component label components label components standard deviation left components separated right components overlap Both approximation methods shown value hyperpa rameters f chosen maximize approximate marginal likelihood Laplace s method Notice Figure consid erable difference value predictive probability negative inputs The Laplace approximation overly cautious given clear separa tion data This effect explained consequence intuition influence explained data points effectively reduced discussion eq Because points left hand cluster relatively explained model don t contribute strongly posterior predictive probability gets close Notice Figure b confidence region latent function Laplace s method actually includes functions negative x appropriate For positive examples centered x right hand Figure b effect visible points transition classes x explained points near boundary competing points class attempting pull latent function opposite di rections Consequently datapoints region contribute strongly C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments Another sign effect uncertainty latent function closely related effective local density data small region x small uncertainty reveals high effective density caused data points region contributing weight It emphasized example artificially constructed specifically highlight effect Finally Figure shows clearly effects uncertainty latent function Eq y In region x x latent mean panel b increases slightly predictive probability decreases region panel This caused increase uncertainty latent function widely varying functions squashed non linearity possible classes high probability average prediction extreme Binary Handwritten Digit Classification Example Handwritten digit character recognition popular real world tasks testing benchmarking classifiers obvious application e g postal services In section consider discrimination images digit images digit example binary classification specific choice guided experience probably difficult binary subtasks class classification digits described following section We use US Postal Service USPS database handwritten digits USPS dataset consists segmented greyscale images normalized intensity pixels lies The data originally split training set cases testset remaining cases configuration Unfortunately data partitions collected slightly different ways data sets stem distribution Since basic underlying assumption machine learning algorithms distribution training test data identical original data partitions suitable test bed learning algorithms interpretation results hampered change distribution Secondly original test set small making difficult differentiate performance different algorithms To overcome problems decided pool USPS repartitioned partitions randomly split data identically sized partitions cases A effect trivial compare results obtained original partitions All experiments reported use repartitioned data The binary 3s vs 5s data training cases divided 3s vs 5s test set cases split We present results Laplace s method EP identical ex squared exponential covariance functionperimental setups The squared exponential covariance function k x x 17It known e g original test partition difficult cases training set C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification log lengthscale log l lo g m g n itu d e lo g f Log marginal likelihood log lengthscale log l lo g m g n itu d e lo g f Information test targets bits b latent means f fr e q u e n cy Training set latent means latent means f fr e q u e n cy Test set latent means log lengthscale log l lo g m g n itu d e lo g f Number test misclassifications c d Figure Binary Laplace approximation 3s vs 5s discrimination USPS data Panel shows contour plot log marginal likelihood function log log f The marginal likelihood optimum log log f optimum value log p y X Panel b shows contour plot information excess simple base line model text test cases bits function variables The statistical uncertainty finite number test cases bits confidence interval Panel c shows histogram latent means training test sets respectively values hyperparameters optimal marginal likelihood panel Panel d shows number test errors predicting sign latent mean 2f exp x x free parameters f process standard deviation controls vertical scaling length scale controls input length scale Let log log f hyperparameters denote vector hyperparameters We present results Laplace s method Figure discuss length We briefly compare results EP method Figure C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments log lengthscale log l lo g m g n itu d e lo g f Log marginal likelihood log lengthscale log l lo g m g n itu d e lo g f Information test targets bits b latent means f fr e q u e n cy Training set latent means latent means f fr e q u e n cy Test set latent means log lengthscale log l lo g m g n itu d e lo g f Number test misclassifications c d Figure The EP algorithm 3s vs 5s digit discrimination task USPS data Panel shows contour plot log marginal likelihood function hyperparameters log log f The marginal likelihood optimum log maximum value log f log marginal likelihood essentially flat function log f region good point log f log marginal likelihood value Panel b shows contour plot information excess baseline model test cases bits function variables Zero bits corresponds information bit perfect binary generalization The test cases allows information determined bits Panel c shows histogram latent means training test sets respectively values hyperparameters optimal marginal likelihood panel Panel d shows number test errors predicting sign latent mean In Figure contour plot approximate log marginal Laplace results likelihood LML log q y X function log log f obtained runs grid evenly spaced values log evenly spaced values log f Notice maximum marginal likelihood C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification near log log f As explained chapter expect hyperparameters yield high marginal likelihood rise good predictions Notice increase unit log scale means probability times larger marginal likelihood Figure fairly peaked There ways measure quality predictions test points The test log predictive probability log2 p y x D test log predictive probability In Figure b plot average test set test log predictive probability range hyperparameters We express information bits targets log base Further set value subtracting information simple base line method achieve As base line model use bestbase line method possible model use inputs case model produce predictive distribution reflecting frequency classes training set e log2 log2 bits essentially bit If classes perfectly balanced training test partitions exactly balanced arrive exactly bit Thus scaled information score Figure b zero method random guessing bit method perfect classification complete confidence The information score measures howinterpretation information score information model able extract inputs identity output Note mutual information model output test targets Kullback Leibler KL divergence Figure shows good qualitative agreement marginal likelihood test information compare panels b The second commonly method measuring quality predictions compute number test errors whenerror rate predictions This computing Eq y eq test point thresholding hard predictions counting number errors Figure d shows number errors produced entry grid values hyperparameters The general trend table number errors lowest left hand corner increases moves right downwards The number errors rises dramatically far righthand corner However note general number errors small cases test set The qualitative differences evaluation criteria depicted Figure panels b d sight alarming And panels b similar trends worry select hyperparameters interested minimizing test misclassification rate Indeed understanding aspects plots involved following discussion suggests explain major trends First bear mind effect increasing kernel function broader expect observe effects like Figure C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments large widths rise lack flexibility Keeping constant effect increasing f increase magnitude values obtained f By lead harder predictions e predictive probabilities closer bear mind variances associated increase increased uncertainty latent variables tends soften predictive probabilities e closer The marked difference Figure b d behaviour left corner classification error rate remains small test information marginal likelihood poor In left hand plots length scale short This causes points deemed far away points In regime prediction dominated class label nearest neighbours task hand happens low misclassification rate In parameter region test latent variables f close zero corresponding probabilities close Consequently predictive probabilities carry information targets In left corner predictive probabilities test cases lie interval Notice large information implies high degree correct classification vice versa At optimal marginal likelihood values hyperparameters misclassifications slightly higher minimum number attained errors In exercise readers encouraged investigate behaviour f predictive probabilities etc functions log log f In Figure results experiment EP EP results method The findings qualitatively similar significant dif ferences In panel approximate log marginal likelihood different shape Laplace s method maximum log marginal likeli hood units natural log scale larger e marginal probability exp times higher Also note marginal likelihood ridge log extends large values log f For large latent amplitudes panel c probit likelihood function approximated step function transitions low high values domain Once regime course irrelevant exactly large magnitude ridge Notice imply prediction hard variance latent function grows Figure shows good qualitative agreement approximate log marginal likelihood test information compare panels b The best value test information significantly higher EP Laplace s method The classification error rates panel d fairly similar behaviour Laplace s method In Figure c latent means training test cases These clear separation training set larger magnitudes Laplace s method The absolute values entries f large excess suggest hard predictions probabilities close zero C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification MAP ve ra g e d MAP fr e q u e n cy averaged fr e q u e n cy b Figure MAP vs averaged predictions EP algorithm s vs s digit discrimination USPS data The optimal values hyperparameters Figure log log f The MAP predictions Eq f y hard close zero On hand averaged predictions Eq y eq lot extreme In panel cases misclassified indicated crosses correctly classified cases shown points Note misclassified points confident predictions e outside Notice points fall triangles horizontal line confirming averaging change probable class makes probabilities extreme e closer Panel b shows histograms averaged MAP predictions truncated values sigmoid saturates smaller arguments However taking uncertainties latent variables account computing predictions averaging eq predictive probabilities softened In Figure verify averaged predictive probabilities extreme MAP predictions In order evaluate performance approximate methods GP classification compared linear probit model support vector ma chine squares classifier nearest neighbour approach commonly machine learning community In Figure showerror reject curve error reject curves misclassification rate test information mea sure The error reject curve shows performance develops function fraction test cases rejected To compute modify methods naturally produce probabilistic predictions described Based predictive probabilities reject test cases maximum predictive probability smaller threshold Varying threshold produces error reject curve The GP classifiers applied Figure hyperparameters optimized approximate marginal likelihood methods For GP classifiers free parameters f The linear pro bit model linear logistic models probably common chose thelinear probit model probit likelihood based methods probit C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments rejection rate m cl ss ifi tio n r te EP Laplace SVM P1NN LSC lin probit rejection rate te st fo rm tio n b EP Laplace SVM P1NN LSC lin probit b Figure Panel shows error reject curve panel b information test cases function rejection rate The probabilistic nearest neighbour P1NN method worse performance methods Gaussian processes EP behaves similarly SVM s clas sification rate SVM low rejection rates little better Laplace s method worse EP SVM The GP squares classifier LSC described section performs best implemented GP model Laplace s method equivalent al computationally efficient iteratively reweighted squares IRLS The covariance function k x x 2x x single hyperparam eter set maximizing log marginal likelihood This gives log p y X marginal likelihood linear covariance function units natural log scale lower max imum log marginal likelihood Laplace approximation squared exponential covariance function The support vector machine SVM classifier section de support vector machine tails SVM SE kernel GP classifiers For SVM ro le identical trade parameter C SVM formulation eq plays similar ro le 2f We carried fold cross validation grid parameter space identify best combination parameters w r t error rate turned C Our experiments conducted SVMTorch software Collobert Bengio In order compute probabilistic predictions squashed test activities cumulative Gaussian methods proposed Platt parameterized linear transformation test activities fed cumulative Gaussian The parameters linear trans formation chosen maximize log predictive probability evaluated hold sets fold cross validation The probabilistic nearest neighbour P1NN method simple nat probabilistic nearest neighbourural extension classical nearest neighbour method provides probabilistic predictions It computes leave LOO nearest neighbour prediction training set records fraction cases LOO predictions correct On test cases method pre 18Platt logistic use cumulative Gaussian C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification dicts nearest neighbour class probability class probability Rejections based thresholding distance nearest neighbour The squares classifier LSC described section In order produce probabilistic predictions method Platt de scribed SVM predictive means predictive variances ignored19 instead fold cross validation leave cross validation LOO CV kernel parameters set LOO CV Figure shows best methods EP approximation GPC SVM squares classifier LSC Presenting error rates test information helps highlight differences apparent single plot For example Laplace s method EP similar error rates different test information Notice error reject curve reveals interesting differences e g notice P1NN method error rate comparable methods zero rejections things don t improve rejections allowed Refer section discussion results class Handwritten Digit Classification Example We apply multi class Laplace approximation developed section class handwritten digit classification problem repartitioned USPS dataset having n training cases n cases testing page We squared exponential covariance function hyper parameters single signal amplitude f common latent functions single length scale parameter common latent functions common input dimensions The behaviour method investigated grid values hyperparameters Figure Note correspondence log marginal likelihood test information close Laplace s method binary classification Figure page The maximum value log marginal likelihood attained hyperparameters corresponding point error rate test information bits As binary classification problem test information standardized subtracting negative entropy information targets bits The classification error rate Figure c shows clear minimum attained shorter length scale marginal likelihood test information maxima This effect seen experiments binary classification To gain insight level performance compared sults obtained probabilistic nearest neighbour method P1NN multiple logistic regression model SVM The P1NN uses 19Of course tried variant latent predictive distribution averaged C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Experiments log lengthscale log l lo g m g n itu d e lo g f Log marginal likelihood log lengthscale log l lo g m g n itu d e lo g f Information test targets bits log lengthscale log l lo g m g n itu d e lo g f Test set misclassification percentage b c Figure way digit classification Laplace approximation Panel shows approximate log marginal likelihood reaching maximum value log p y X log log f In panel b information test cases shown The maximum possible information test targets corresponding perfect classification bits entropy targets At point maximum marginal likelihood test information bits In panel c test set misclassification rate shown percent At point maximum marginal likelihood test error rate internal leave assessment training set estimate probabil ity correct For test set predicts nearest neighbour probability classes equal probability We obtained test information bits test set classification error rate We compare multiple linear logistic regression One way imple ment method view Gaussian process linear covariance C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification function equivalent computationally efficient Laplace approximation weights linear model In case weights inputs bias latent function values GP The linear covariance function k x x 2x x single hyperparameter latent functions Optimizing log marginal likelihood w r t gives log p y X Using value hyperparameter test information bits test set error rate Finally support vector machine SVM classifier trained SE kernel Gaussian process classifiers See section details SVM As binary SVM case free parameters length scale kernel trade parameter C eq plays similar ro le 2f We carried fold cross validation grid parameter space identify best combination parameters w r t error rate turned C Our experiments conducted SVMTorch software Collobert Bengio implements multi class SVM classification versus rest method de scribed section The test set error rate SVM attempt evaluate test information multi class SVM Discussion In previous section presented sets experiments comparing approximate methods inference GPC models comparing commonly supervised learning methods In section discuss results attempt relate properties models For binary examples Figures saw ap proximations showed different qualitative behaviour approximated log marginal likelihood exact marginal likelihood course iden tical The EP approximation gave higher maximum value log marginal likelihood units log scale test information somewhat better Laplace s method test set error rates comparable However experiment favour EP approximation interesting know close approximations exact analytically intractable solutions In Figure resultsMonte Carlo results running sophisticated Markov chain Monte Carlo method called Annealed Importance Sampling Neal carried Kuss Rasmussen The USPS dataset experiments identical Fig ures results directly comparable It seen MCMC results indicate EP method achieves high level accu racy e difference EP Laplace s method caused exclusively approximation errors Laplace s method The main reason inaccuracy Laplace s method high dimensional posterior skew symmetric approximation centered mode characterizing posterior volume The posterior C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Discussion log lengthscale log l lo g m g n itu d e lo g f Log marginal likelihood log lengthscale log l lo g m g n itu d e lo g f Information test targets bits b Figure The log marginal likelihood panel test information panel b USPS s vs s binary classification task computed Markov chain Monte Carlo MCMC Comparing Laplace approximation Figure Figure shows EP approximation surprisingly accurate The slight wiggliness contour lines caused finite sample effects MCMC runs combination correlated Gaussian prior centered origin likelihood terms softly cut half spaces agree training set labels Therefore posterior looks like correlated Gaussian restricted orthant agrees labels It is mode located close origin orthant decrease rapidly direction origin conflicts likelihood terms decrease slowly opposite direction prior Seen light surprising Laplace approximation somewhat inaccurate This explanation corroborated Kuss Rasmussen It noted methods compared binary digits clas sification task linear probit model squared distance digitized digit images measured directly image space suitablility covariance functionsole input algorithm This distance measure suited digit discrimination task example similar images slight translations huge squared distance course identical labels One strengths GP formalism use prior distributions latent case functions inference based If prior functions depends particular pect data squared distance image space suited discrimination prior appropriate It interesting design covariance functions parameterized hyperparame ters appropriate digit discrimination task e g reflecting known invariances images tangent distance ideas Simard et al Scho lkopf Smola ch section The results shown follow common approach generic C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification covariance function minimum hyperparameters doesn t allow incorporate prior information problem For example GP framework inference multiple hyperparameters complex covariance functions provide clearly interpretable infor mation data carbon dioxide modelling problem discussed page Appendix Moment Derivations Consider integral cumulative Gaussian respect Gaussian Z x m v N x dx x x N y dy initially special case v Writing substituting z y x m w x interchanging order integrals Zv 2v x exp y m 2v2 x dy dx 2v m exp z w 2v2 w2 dw dz matrix notation Zv 2v m exp w z v2 v2 v2 v2 w z dw dz m N w z v2 dw dz e incomplete integral joint Gaussian The inner integral corre sponds marginalizing w eq A yielding Zv v2 m exp z2 v2 dz m v2 assumed v If v negative substitute symmetry z z eq Zv m v2 m v2 Collecting cases eq eq arrive Z x m v N x dx z z m v v2 general v We wish compute moments q x Z x m v N x C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Exercises Z given eq Perhaps easiest way differ entiate w r t sides eq Z x x m v N x dx z x x m v N x dx Z N z v v2 z N z z We recognize term integral line eq Z times moment q seeking Multiplying Z rearranging obtain moment Eq x 2N z z v v2 Similarly second moment obtained differentiating eq twice 2Z x2 2x x m v N x dx zN z v2 Eq x2 2Eq x 4zN z z v2 second terms integral line eq multiples second moments The second central moment reintroducing eq eq simplifying given second moment Eq x Eq x Eq x2 Eq x 4N z v2 z z N z z Exercises For binary GPC equivalence noise free latent process combined probit likelihood latent process Gaussian noise combined step function likelihood Hint introduce explicitly additional noisy latent variables f differ fi Gaussian noise Write step function likelihood single case function f integrate noisy variable arrive probit likelihood function noise free process Consider multinomial random variable y having C states yc variable state c State c occurs probability c Show cov y E y y diag Ob serve cov y covariance matrix necessarily positive semidefinite Using fact matrix W diag eq positive semidefinite By showing vector ones eigenvector cov y eigenvalue zero verify ma trix positive semidefinite positive definite See section definitions positive semidefinite positive definite matrices C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Classification R R R z z Figure The decision regions class softmax function z2 z3 space Consider class softmax function p Cc exp fc exp f1 exp f2 exp f3 c f1 f2 f3 corresponding activations To easily visualize decision boundaries let z2 f2 f1 z3 f3 f1 Thus p C1 exp z2 exp z3 similarly classes The decision boundary relating p C1 curve exp z2 exp z3 The decision regions classes illustrated Figure Let f f1 f2 f3 Gaussian distribution centered origin let f softmax f We consider effect distribution f p f df For Gaussian given covariance structure integral easily approxi mated drawing samples p f Show classification fall categories depending covariance matrix Thus considering displacements mean Gaussian origin regions shown overall classification depends mean Gaussian covariance Show conclusion valid recalled z derived f z T f T cov z T cov f T Consider update equation fnew given eq training points explained f ti Wii C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Exercises points Break f subvectors f1 corresponds points explained f2 Re write K W eq K I WK let K partitioned K11 K12 K21 K22 similarly matrices Using partitioned matrix inverse equations section A fnew1 K11 I11 W11K11 W11f1 log p y1 f1 fnew2 K21K f new See section consequences result Show expressions eq cavity mean vari ance depend approximate likelihood terms corresponding case despite appearance eq Consider USPS 3s vs 5s prediction problem discussed section Use implementation Laplace binary GPC provided investi gate f predictive probabilities etc vary functions log log f C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Covariance Functions We seen covariance function crucial ingredient Gaussian process predictor encodes assumptions function wish learn From slightly different viewpoint clear supervised learning notion similarity data points crucial basic similarity assumption points inputs x close likely similar target values y training points near test point informative prediction point Under Gaussian process view covariance function defines nearness similarity An arbitrary function input pairs x x general valid valid covariance functionscovariance function The purpose chapter examples commonly covariance functions examine properties Section defines number basic terms relating covariance functions Section gives examples stationary dot product non stationary covariance functions gives ways new ones old Section introduces important topic eigenfunction analysis covariance functions states Mercer s theorem allows express covariance function certain conditions terms eigenfunctions eigenvalues The covariance functions given section valid input domain X subset RD In section describe ways define covariance functions input domain structured objects strings trees Preliminaries A stationary covariance function function x x Thus invariant stationarity translations input space For example squared exponential co 1To valid covariance function positive semidefinite eq 2In stochastic process theory process constant mean covariance function invariant translations called weakly stationary A process strictly sta tionary finite dimensional distributions invariant translations Papoulis sec C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions variance function given equation stationary If covariance function function x x called isotropic isotropy variant rigid motions For example squared exponential covariance function given equation isotropic As k function r x x known radial basis functions RBFs If covariance function depends x x x x itdot product covariance dot product covariance function A simple example covariance function k x x x x obtained linear regression putting N priors coefficients xd d D prior N bias constant function eq Another important example inhomogeneous polynomial kernel k x x x x p p positive integer Dot product covariance functions invariant rotation coordinates origin translations A general function k arguments mapping pair inputskernel x X x X R kernel This term arises theory integral operators operator Tk defined Tkf x X k x x f x d x denotes measure section A explanation point A real kernel said symmetric k x x k x x clearly covariance functions symmetric definition Given set input points xi n compute Gram matrix K entries Kij k xi xj If k covariance function weGram matrix matrix K covariance matrix covariance matrix A real n n matrix K satisfies Q v v Kv vectors v Rn called positive semidefinite PSD If Q v v 0positive semidefinite matrix positive definite Q v called quadratic form A symmetric matrix PSD eigenvalues non negative A Gram matrix corresponding general kernel function need PSD Gram matrix corresponding covariance function PSD A kernel said positive semidefinite k x x f x f x d x d x f L2 X Equivalently kernel function gives rise PSD Gram matrices choice n N D positive semidefinite To let f weighted sum delta functions xi Since functions limits functions L2 X eq implies Gram matrix corresponding D PSD For dimensional Gaussian process way understand charac teristic length scale process exists terms number ofupcrossing rate upcrossings level u Adler Theorem states expected 3Informally speaking readers usually able substitute dx p x dx d x C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions number upcrossings E Nu level u unit interval zero mean stationary surely continuous Gaussian process given E Nu k k exp u2 2k If k exist process mean square differentiable process zero x0 surely infinite number zeros arbitrarily small interval x0 x0 Blake Lindsey p Mean Square Continuity Differentiability We describe mean square continuity differentiability stochastic pro cesses following Adler sec Let x1 x2 sequence points x fixed point RD xk x k Then process f x continuous mean square x E f xk f x mean square continuity k If holds x A A subset RD f x said continuous mean square MS A A random field continuous mean square x covariance function k x x continuous point x x x For stationary covariance functions reduces checking continuity k Note MS continuity necessarily imply sample function continuity discussion sample function continuity differentiability Adler ch The mean square derivative f x ith direction defined f x xi l m h f x hei f x h limit exists l m denotes limit mean square ei mean square differentiabilityis unit vector ith direction The covariance function f x xi given 2k x x xi x These definitions extended higher order derivatives For stationary processes 2kth order partial derivative 2kk x 2xi1 2xik exists finite x kth order partial derivative kf x xi1 xik exists x R D mean square limit Notice properties kernel k determine smoothness properties MS differentiability stationary process Examples Covariance Functions In section consider covariance functions input domain X subset vector space RD More general input spaces considered section We start section stationary covariance functions consider dot product covariance functions section varieties non stationary covariance functions section We overview commonly covariance functions Table section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions describe general methods constructing new kernels old There exist good overviews covariance functions e g Abrahamsen Stationary Covariance Functions In section section convenient allow kernels map x X x X C R If zero mean process f complex valued covariance function defined k x x E f x f x denotes complex conjugation A stationary covariance function function x x Sometimes case write k function single argument e k The covariance function stationary process represented Fourier transform positive finite measure Theorem Bochner s theorem A complex valued function k RD theBochner s theorem covariance function weakly stationary mean square continuous complex valued random process RD represented k RD e2is d s positive finite measure The statement Bochner s theorem quoted Stein p proof found Gihman Skorohod p If density S s spectral density power spectrum S known spectral density power spectrum corresponding k The construction given eq puts non negative power fre quency s analogous requirement prior covariance matrix p weights equation non negative definite In case spectral density S s exists covariance function spectral density Fourier duals shown eq known Wiener Khintchine theorem e g Chatfield k S s e2is ds S s k e 2is d Notice variance process k S s ds power spectrum integrable define valid Gaussian process To gain intuition definition power spectrum given eq important realize complex exponentials e2is x eigenfunctions stationary kernel respect Lebesgue measure section details Thus S s loosely speaking power allocated average eigenfunction e2is x frequency s S s eventually decay sufficiently fast s integrable 4See Appendix A details Fourier transforms C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions rate decay power spectrum gives important information smoothness associated stochastic process For example deter mean square differentiability process section details If covariance function isotropic function r r shown S s function s s Adler Theorem In case integrals eq simplified changing spherical polar coordinates integrating angular variables e g Bracewell ch obtain k r rD S s JD 2rs s D ds S s sD k r JD 2rs r D dr JD Bessel function orderD Note dependence dimensionality D equation means isotropic functional form spectral density rise different isotropic covariance func tions different dimensions Similarly start particular isotropic covariance function k r form spectral density general depend D e g Mate rn class spectral density given eq fact k r valid D A necessary condition spectral density exist rD k r dr Stein sec details We examples commonly isotropic covariance func tions The covariance functions given normalized form k multiply k positive constant 2f desired process vari ance Squared Exponential Covariance Function The squared exponential SE covariance function introduced squared exponential chapter eq form kSE r exp r2 parameter defining characteristic length scale Using eq characteristic length scalesee mean number level zero upcrossings SE process d confirms ro le length scale This covari ance function infinitely differentiable means GP covariance function mean square derivatives orders smooth The spectral density SE covariance function S s D exp 2s2 Stein argues strong smoothness assumptions unrealistic modelling physical processes rec ommends Mate rn class However squared exponential probably widely kernel kernel machines field C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions The SE kernel infinitely divisible k r t valid kernel allinfinitely divisible t effect raising k power t simply rescale We digress briefly squared exponential covariance function obtained expanding input x feature space defined Gaussian shaped basis functions centered densely x space Forinfinite network construction SE covariance function simplicity exposition consider scalar inputs basis functions c x exp x c c denotes centre basis function From sections recall Gaussian prior weights w N 2pI gives rise GP covariance function k xp xq p N c c xp c xq Now allowing infinite number basis functions centered interval scaling variance prior weights number basis functions obtain limit lim N 2p N N c c xp c xq p cmax cmin c xp c xq dc Plugging Gaussian shaped basis functions eq letting tegration limits infinity obtain k xp xq p exp xp c exp xq c dc 2p exp xp xq recognize squared exponential covariance function times longer length scale The derivation adapted MacKay It straightforward generalize construction multivariate x See eq similar construction centres basis functions sampled Gaussian distribution constructions equivalent variance Gaussian tends infinity The Mate rn Class Covariance Functions The Mate rn class covariance functions given byMate rn class kMatern r 2r K 2r positive parameters K modified Bessel function Abramowitz Stegun sec This covariance function spectral density S s 2DD D 42s2 D C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions input distance r co va ri n ce k r input x o u tp u t f x b Figure Panel covariance functions b random functions drawn Gaussian processes Mate rn covariance functions eq different values The sample functions right obtained discretization x axis equally spaced points D dimensions Note scaling chosen obtain SE covariance function e r eq A Stein named Mate rn class work Mate rn For Mate rn class process f x k times MS differentiable k The Mate rn covariance functions especially simple half integer p p non negative integer In case covariance function product exponential polynomial order p general expression derived Abramowitz Stegun eq giving k p r exp 2r p 2p p p p 8r p It possible interesting cases machine learning k r 3r exp 3r k r 5r 5r2 exp 5r process rough absence explicit prior knowledge existence higher order derivatives probably hard finite noisy training examples distinguish values distinguish finite values smooth squared exponential case For example value Cornford et al Ornstein Uhlenbeck Process Exponential Covariance Function The special case obtained setting Mate rn class gives exponential exponential covariance function k r exp r The corresponding process C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions input distance co va ri n ce input x o u tp u t f x b Figure Panel covariance functions b random functions drawn Gaussian processes exponential covariance function eq different values The sample functions differentiable SE case The sample functions right obtained discretization x axis equally spaced points MS continuous MS differentiable In D covariance function Ornstein Uhlenbeck OU process The OU process UhlenbeckOrnstein Uhlenbeck process Ornstein introduced mathematical model velocity particle undergoing Brownian motion More generally D setting p integer p gives rise particular form continuous time AR p Gaussian process details section B The form Mate rn covariance function samples drawn illustrated Figure The exponential Covariance Function The exponential family covariance functions includes ex exponential ponential squared exponential given k r exp r Although function similar number parameters Mate rn class Stein notes sense flexible This corre sponding process MS differentiable finitely MS differentiable The covariance function random samples process shown Figure A proof positive definiteness covariance function found Schoenberg Rational Quadratic Covariance Function The rational quadratic RQ covariance functionrational quadratic kRQ r r2 C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions input distance co va ri n ce input x o u tp u t f x b Figure Panel covariance functions b random functions drawn Gaussian processes rational quadratic covariance functions eq differ ent values The sample functions right obtained discretization x axis equally spaced points seen scale mixture infinite sum squared scale mixture exponential SE covariance functions different characteristic length scales sums covariance functions valid covariance section Parameterizing terms inverse squared length scales putting gamma distribution p exp add contributions following integral kRQ r p kSE r d exp exp r2 d r2 set The rational quadratic discussed Mate rn p slightly different parameterization notation limit RQ covariance eq A SE covariance function characteristic length scale eq Figure illustrates behaviour different values note process infinitely MS differentiable contrast Mate rn covariance function Figure The previous example special case kernels written superpositions SE kernels distribution p length scales k r exp r2 p d This fact general representation isotropic kernel defines valid covariance function dimension D Stein sec Piecewise Polynomial Covariance Functions Compact Support A family piecewise polynomial functions compact support provide piecewise polynomial covariance functions compact support interesting class covariance functions Compact support means Note common ways parameterize Gamma distribution choice convenient shape mean C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions input distance r co va ri n ce k r D q D q D q input x o u tp u t f x b Figure Panel covariance functions b random functions drawn Gaussian processes piecewise polynomial covariance functions compact sup port eq specified parameters covariance points exactly zero distance exceeds certain threshold This means covariance matrix sparse construction leading possibility computational advantages The challenge designing functions guarantee positive definite positive definiteness ness Multiple algorithms deriving covariance functions discussed Wendland ch These functions usually positive definite input dimensions validity restricted maximumrestricted dimension dimension D Below examples covariance functions kppD q r positive definite RD kppD r r j j b D c q kppD r r j j r kppD r r j j2 4j r2 3j r kppD r r j j3 9j2 23j r3 6j2 36j r2 15j r The properties covariance functions illustrated Fig ure These covariance functions 2q times continuously differentiable corresponding processes q times mean square differentiable section It interesting ask extent use compactly supported covariance functions described place covariance functions mentioned section obtaining inferences similar One advantage compact support gives rise spar sity Gram matrix exploited example iterative solutions GPR problem section 6If product inverse covariance matrix vector needed e g prediction computed conjugate gradient algorithm products covariance matrix vectors basic computational unit obviously carried faster matrix sparse C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions Further Properties Stationary Covariance Functions The covariance functions given decay monotonically r positive However necessary condition covariance function For example Yaglom shows k r c r J r valid covari ance function D function form damped oscillation Anisotropic versions isotropic covariance functions created anisotropy setting r2 x x x x M x x positive semidefinite M If M diagonal implements use different length scales different dimensions discussion automatic relevance determination section General M s considered Mate rn p Poggio Girosi Vivarelli Williams work low rank M implement linear dimensionality reduction step input space lower dimensional feature space More generally assume form M D k matrix columns define k directions high relevance diagonal matrix positive entries capturing usual axis aligned relevances Figure page ThusM factor analysis factor analysis distance form For appropriate choices k represent good trade flexibility required number parameters Stationary kernels defined periodic domain readily constructed stationary kernels R Given stationary kernel k x kernel kT x m Z k x ml periodic period l shown periodization section B Scho lkopf Smola eq Dot Product Covariance Functions As mentioned kernel k x x x x obtained linear regression If homogeneous linear kernel inhomogeneous Of course generalized k x x x px general covariance matrix p components x described eq It case k x x x px p valid covariance function positive integer p general result positive integer power given covariance function valid covariance function described section However interesting explicit feature space construction polynomial covariance function We consider homogeneous polynomial case inhomogeneous case simply obtained considering x extended 7Indeed bias term included general expression C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions concatenating constant We write k x x x x p D d xdx d p D d1 xd1x d1 D dp xdpx dp D d1 D dp xd1 xdp x d1 x dp x x Notice sum apparently contains Dp terms fact order indices monomial xd1 xdp unimportant e g p x1x2 x2x1 monomial We remove redundancy defining vector m entry md specifies number times index d appears monomial constraint D 1mi p Thus m x feature corresponding vector m proportional monomial xm11 x mD D The degeneracy m x p m1 mD usual define giving feature map m x p m1 mD xm11 x mD D For example p D x x21 x 2x1x2 Dot product kernels normalized form given eq For regression problems polynomial kernel strange choice prior variance grows rapidly x x However kernels proved effective high dimensional classification problems e g x vectorized binary image input data binary greyscale normalized dimension Scho lkopf Smola sec Other Non stationary Covariance Functions Above seen examples non stationary dot product kernels However interesting kernels form In section describe covariance function belonging particular type neural network construction Neal Consider network takes input x hidden layer NH units linearly combines outputs hidden units bias b obtain f x The mapping written f x b NH j vjh x uj vjs hidden output weights h x u hidden unit transfer function shall assume bounded depends input hidden weights u For example choose h x u tanh x u This architecture important shown Hornik networks hidden layer universal approximators number C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions hidden units tends infinity wide class transfer functions exclud ing polynomials Let b v s independent zero mean distributions variance 2b v respectively let weights uj hidden unit independently identically distributed Denoting weights w obtain following Neal Ew f x Ew f x f x 2b j 2vEu h x uj h x uj 2b NH vEu h x u h x u eq follows hidden units identically dis tributed The final term equation 2Eu h x u h x u letting 2v scale NH The sum eq NH identically independently distributed random variables As transfer function bounded moments distribution bounded central limit theorem applied showing stochastic process converge Gaussian process limit NH By evaluating Eu h x u h x u obtain covariance function neural network For example choose error function h z erf z neural network covariance function2 z e t dt transfer function let h x u erf u0 D j 1ujxj choose u N obtain Williams kNN x x sin 2x x 2x x 2x x x x1 xd augmented input vector This true neural network covariance function The sigmoid kernel k x x tanh bx x proposed fact kernel positive defi nite valid covariance function e g Scho lkopf Smola p Figure shows plot neural network covariance function samples prior We set diag Samples GP covariance function viewed superpositions functions erf u0 ux controls variance u0 offset functions origin controls u scaling x axis In Figure b observe sample functions larger vary quickly Notice samples display non stationarity covariance function large values x x tend constant value consistent construction superposition sigmoid functions Another interesting construction set h x u exp x u 22g modulated squared exponentialwhere g sets scale Gaussian basis function With u N 2uI C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions input x p u t x input x o u tp u t f x covariance b sample functions Figure Panel plot covariance function kNN x x Panel b samples drawn neural network covariance function shown legend The samples obtained discretization x axis equally spaced points obtain kG x x 22u d exp x u 22g x u 22g u u 22u du e u d exp x x 22m exp x x 22s exp x x 22m 2e g u s g g u m u g This general non stationary covariance function 2u scaling appropriately recover squared exponential kG x x exp x x 42g For finite value 2u kG x x comprises squared exponen tial covariance function modulated Gaussian decay envelope function exp x x 22m exp x x 22m cf vertical rescaling construction de scribed section One way introduce non stationarity introduce arbitrary non linear mapping warping u x input x use stationary covariancewarping function u space Note x u need dimensionality This approach Sampson Guttorp model patterns solar radiation southwestern British Columbia Gaussian processes Another interesting example warping construction given MacKay dimensional input variable x mapped dimensional u x cos x sin x rise periodic random function x If weperiodic random function use squared exponential kernel u space k x x exp sin2 x x cos x cos x sin x sin x sin2 x x C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions le ng th sc al e l x input x input x ou tp ut f x b Figure Panel shows chosen length scale function x Panel b shows samples GP prior Gibbs covariance function eq This figure based Fig Gibbs We described anisotropic covariance function varying length scale scaling different dimensions differently However free length scales d functions x general produce valid covariance function Gibbs derived covariance function k x x D d d x d x 2d x d x exp D d xd x d 2d x d x x arbitrary positive function x Note k x x x This covariance function obtained considering grid N Gaussian basis functions centres cj corresponding length scale input dimension d varies positive function d cj Taking limit N sum turns integral algebra eq obtained An example variable length scale function samples prior corresponding eq shown Figure Notice length scale gets shorter sample functions vary rapidly expect The large length scale regions short length scale region strongly correlated If tries converse experiment creating length scale function x longer length scale region shorter ones behaviour expected initially transitioning long length scale region covariance drops sharply prefactor eq stabilizing slower variation See Gibbs sec details Exercises invite investigate Paciorek Schervish generalized Gibbs construction obtain non stationary versions arbitrary isotropic covariance functions Let kS C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions covariance function expression S ND constant linear D d dxdx d polynomial x x p squared exponential exp r Mate rn r K r exponential exp r exponential exp r rational quadratic r neural network sin 2x x 2x x 2x x Table Summary commonly covariance functions The covariances written function x x function r x x Two columns marked S ND indicate covariance functions stationary nondegenerate respectively Degenerate covariance functions finite rank section discussion issue stationary isotropic covariance function valid Euclidean space RD D Let x D D matrix valued function positive definite x let xi The set Gibbs x functions define diagonal x Then define quadratic form Qij xi xj j xi xj Paciorek Schervish kNS xi xj D j j 2kS Qij valid non stationary covariance function In chapter described linear regression model feature space f x x w O Hagan suggested making w function x allow different values w appropriate different regions Thus Gaussian process prior w form cov w x w x W0kw x x positive definite matrix W0 giving rise prior f x covariance kf x x x W0 x kw x x Finally note Wiener process covariance function k x x min x x fundamental non stationary process See section B textsWiener process Grimmett Stirzaker ch details Making New Kernels Old In previous sections developed covariance functions summarized Table In section combine modify existing covariance functions new ones C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Examples Covariance Functions The sum kernels kernel Proof consider random process sum f x f1 x f2 x f1 x f2 x independent Then k x x k1 x x k2 x x This construction e g add kernels different characteristic length scales The product kernels kernel Proof consider random process f x f1 x f2 x f1 x f2 x independent Then k x x product k1 x x k2 x x A simple extension argument means kp x x valid covariance function p N Let x given deterministic function consider g x x f x f x random process Then cov g x g x x k x x x vertical rescaling Such construction normalize kernels choosing x k x x assuming k x x x k x x k x x k x x k x x This ensures k x x x We obtain new process convolution blurring Consider arbitrary fixed kernel h x z map g x h x z f z dz Then convolution clearly cov g x g x h x z k z z h x z dz dz If k x1 x k x2 x covariance functions different spaces X1 X2 direct sum k x x k1 x1 x k2 x2 x tensor direct sum tensor productproduct k x x k1 x1 x k2 x2 x covariance functions defined product space X1 X2 virtue sum product constructions The direct sum construction generalized Consider func tion f x x D dimensional An additive model Hastie Tib shirani form f x c D 1fi xi e linear combina additive model tion functions variable If individual fi s taken dependent stochastic processes covariance function f form direct sum If admit interactions variables f x c D 1fi xi ij j fij xi xj fi s fij s independent stochastic processes covariance function form k x x D 1ki xi x D j kij xi xj x x j Indeed pro cess extended provide functional ANOVA9 decomposition ranging simple additive model interaction D input vari functional ANOVA ables The sum truncated stage Wahba ch Stitson et al suggest tensor products kernels inter actions example kij xi xj x x j form ki xi x kj xj x j Note D large large number pairwise higher order terms problematic Plate investigated combination additive GP models plus general covariance function permits interactions 8If f1 f2 Gaussian processes product f general Gaussian process exists GP covariance function 9ANOVA stands analysis variance statistical technique analyzes interac tions attributes C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions Eigenfunction Analysis Kernels We define eigenvalues eigenfunctions discuss Mercer s theorem allows express kernel certain conditions terms quantities Section gives analytical solution eigenproblem SE kernel Gaussian measure Section discusses compute approximate eigenfunctions numerically cases exact solution known It turns Gaussian process regression viewed Bayesian linear regression possibly infinite number basis functions discussed chapter One possible basis set eigenfunctions covariance function A function obeys integral equation k x x x d x x called eigenfunction kernel k eigenvalue respect measure10eigenvalue eigenfunction The measures particular interest Lebesgue measure compact subset C RD ii density p x d x written p x dx In general infinite number eigenfunctions label x x We assume ordering chosen The eigenfunctions orthogonal respect chosen normalized x j x d x ij ij Kronecker delta Mercer s theorem e g Ko nig allows express kernel kMercer s theorem terms eigenvalues eigenfunctions Theorem Mercer s theorem Let X finite measure space k L X kernel Tk L2 X L2 X positive definite eq Let L2 X normalized eigenfunctions Tk associated eigenvalues Then eigenvalues absolutely summable k x x ii x x holds series converges absolutely uniformly This decomposition infinite dimensional analogue diagonaliza tion Hermitian matrix Note sum terminate value N N e eigenvalues N zero sum infinite We following definition Press et al p 10For explanation measure Appendix A C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Eigenfunction Analysis Kernels Definition A degenerate kernel finite number non zero eigen values A degenerate kernel said finite rank If kernel degenerate degenerate nondegenerate kernel said nondegenerate As example N dimensional linear regression model feature space eq gives rise degenerate kernel N non zero eigenvalues Of course measure puts weight finite number points n x space eigendecomposition simply n n matrix kernel nondegenerate The statement Mercer s theorem referred finite measure If replace Lebesgue measure consider stationary covariance function directly Bochner s theorem eq obtain k x x RD e2is x x d s RD e2is x e2is x d s The complex exponentials e2is x eigenfunctions stationary kernel w r t Lebesgue measure Note similarity eq summation replaced integral The rate decay eigenvalues gives important information smoothness kernel For example Ritter et al showed d uniform processes r times mean square differentiable 2r asymptotically This makes sense rougher processes power high frequencies eigenvalue spectrum decays slowly The phenomenon read power spectrum Mate rn class given eq Hawkins gives exact eigenvalue spectrum OU process Widom gives asymptotic analysis eigenvalues stationary kernels taking account effect density d x p x dx Bach Jordan Table use results effect varying p x SE kernel An exact eigenanalysis SE kernel Gaussian density given section An Analytic Example For case p x Gaussian squared exponential kernel k x x exp x x analytic results eigenvalues eigenfunctions given Zhu et al sec Putting p x N x find eigenvalues k eigenfunctions k convenience let k given k 2a A Bk k x exp c x2 Hk 2cx C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions Figure The eigenfunctions squared exponential kernel w r t Gaussian density The value k equal number zero crossings function The dashed line proportional density p x whereHk x k exp x2 d k dxk exp x2 kth order Hermite polynomial Gradshteyn Ryzhik sec b c a2 2ab A b c B b A Hints proof result given exercise A plot eigenfunctions b shown Figure The result eigenvalues eigenfunctions readily generalized multivariate case kernel Gaussian density products univariate expressions eigenfunctions eigenvalues simply products For case b equal D dimensions degeneracy eigenvalue 2a A D 2Bk k D D O kD As k j j D D k D D k D D th eigenvalue value given 2a A D 2Bk determine rate decay spectrum Numerical Approximation Eigenfunctions The standard numerical method approximating eigenfunctions eigen values eq use numerical routine approximate integral e g Baker ch For example letting d x p x dx eq use approximation ii x k x x p x x dx n n l k xl x xl C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Kernels Non vectorial Inputs xl s sampled p x Plugging x xl l n eq obtain matrix eigenproblem Kui mat ui K n n Gram matrix entries Kij k xi xj mati ith matrix eigenvalue ui corresponding eigenvector normalized u ui We xj n ui j n factor arises differing normalizations eigenvector eigenfunction Thus n mati obvious estimator n For fixed n expect larger eigenvalues better estimated smaller ones The theory numerical solution eigenvalue problems shows fixed n mati converge limit n Baker Theorem It possible study convergence example easy properties principal components analysis PCA feature space l l n En 1n l mat l 1i En 1n n l mat N l 1i En denotes expectation respect samples size n drawn p x For details Shawe Taylor Williams The Nystro m method approximating ith eigenfunction Baker Nystro m method Press et al section given x n mati k x ui k x k x1 x k xn x obtained eq dividing sides Equation extends approximation xj n ui j sample points x1 xn x There interesting relationship kernel PCA method Scho lkopf et al eigenfunction expansion discussed The kernel PCA eigenfunction expansion potentially infinite number non zero eigenvalues In contrast kernel PCA algorithm operates n n matrix K yields n eigenvalues eigenvectors Eq clarifies relationship However note eq identical scaling factors Scho lkopf et al eq describes projection new point x ith eigenvector kernel PCA feature space Kernels Non vectorial Inputs So far chapter assumed input x vector measuring values number attributes features However learning problems inputs vectors structured objects strings trees general graphs For example biological problem want classify proteins represented strings amino acid symbols 11Proteins initially different amino acids later modified bringing total number C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions Or input parse trees derived linguistic analysis Or wish represent chemical compounds labelled graphs vertices denoting atoms edges denoting bonds To follow discriminative approach need extract features input objects build predictor features For classification problem alternative generative approach construct class conditional models objects Below describe approaches feature extraction problem efficient computation kernels section cover string kernels section describe Fisher kernels There exist proposals constructing kernels strings example Watkins describes use pair hidden Markov models HMMs generate output symbols strings conditional hidden state purpose String Kernels We start defining notation strings Let A finite alphabet characters The concatenation strings x y written xy x denotes length string x The string s substring x write x usv possibly u s v Let s x denote number times substring s appears string x Then define kernel strings x x k x x s A wss x s x ws non negative weight substring s For example set ws s shorter substrings weight longer ones A number interesting special cases contained definition Setting ws s gives bag characters kernel This takesbag characters feature vector string x number times character A appears x In text analysis wish consider frequencies word occur bag words rence If require s bordered whitespace bag words representation obtained Although simple model text ignores word order surprisingly effective document classification retrieval tasks e g Hand et al sec The weights set differently different words e g term frequency inverse document frequency TF IDF weighting scheme de veloped information retrieval area Salton Buckley If consider substrings length k obtain k spectrum kernel Leslie et al k spectrum kernel C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Kernels Non vectorial Inputs Importantly efficient methods suffix trees compute string kernel k x x time linear x x restrictions weights ws Leslie et al Vishwanathan Smola Work string kernels started Watkins Haussler There developments methods described example Lodhi et al substrings consider subsequences x necessarily contiguous Leslie et al describe mismatch string kernels allow substrings s s x x respectively match m mismatches We expect developments area tailoring engineering string kernels properties sense particular domain The idea string kernels consider matches substrings easily extended trees e g looking matches subtrees Collins Duffy Leslie et al applied string kernels classification protein domains SCOP12 superfamilies The results obtained significantly better methods based PSI BLAST13 searches generative hidden Markov model classifier Similar results obtained Jaakkola et al Fisher kernel described section Saunders et al described use string kernels problem classifying natural language newswire stories Reuters database classes Fisher Kernels As explained problem input x structured object arbitrary size e g string wish extract features The Fisher kernel introduced Jaakkola et al taking generative model p x vector parameters computing feature vector x log p x x called score vector score vector Take example Markov model strings Let xk kth symbol string x Then Markov model gives p x p x1 x p xi xi A A Here j gives probability x1 jth symbol alphabet A A A A stochastic matrix ajk giving probability p xi k xi j Given model straightforward compute score vector given x It possible consider generative models p x For example try kth order Markov model xi predicted preceding k symbols See Leslie et al Saunders et al interesting discussion similarities features k spectrum kernel score vector derived order k Markov model exercise 12Structural classification proteins database http scop mrc lmb cam ac uk scop 13Position Specific Iterative Basic Local Alignment Search Tool http www ncbi nlm nih gov Education BLASTinfo psi1 html 14http www daviddlewis com resources testcollections reuters21578 C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions Another interesting choice use hidden Markov model HMM generative model discussed Jaakkola et al See exercise linear kernel derived isotropic Gaussian model x RD We define kernel k x x based score vectors x x One simple choice set k x x x M x M strictly positive definite matrix Alternatively use squared exponential kernel k x x exp x x The structure p x varies studied extensively informa tion geometry e g Amari It shown manifold log p x Riemannian metric tensor inverse Fisher information matrix F whereFisher information matrix F Ex x x Setting M F eq gives Fisher kernel If F difficult computeFisher kernel resort setting M I The advantage Fisher information matrix makes arc length manifold invariant reparameterizations The Fisher kernel uses class independent model p x Tsuda et al developed tangent posterior odds TOP kernel based onTOP kernel log p y x log p y x makes use class conditional distributions C C classes Exercises The OU process covariance function k x x exp x x unique stationary order Markovian Gaussian process Ap pendix B details Consider training inputs x1 x2 xn xn R corresponding function values f f x1 f xn Let xl denote nearest training input left test point x similarly let xu denote nearest training input right x Then Markovian property means p f x f p f x f xl f xu Demonstrate choosing x points line computing predictive distribution p f x f eq observing non zero contributions arise xl xu Note occurs noise free case allows training points cor rupted noise equations points contribute general Computer exercise write code draw samples neural network covariance function eq d d Consider cases var u0 non zero Explain form plots obtained var u0 C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Exercises Consider random process f x erf u0 D 1ujxj u N Show non linear transform process inho mogeneous linear covariance function covariance function erf neural network However note process Gaussian process Draw samples given process compare results exercise Derive Gibbs non stationary covariance function eq Computer exercise write code draw samples Gibbs non stationary covariance function eq d d Investigate forms length scale function x Show SE process infinitely MS differentiable OU process MS differentiable Prove eigenfunctions symmetric kernel orthogonal w r t measure Let k x x p1 x k x x p1 x assume p x x Show eigenproblem k x x x dx x eigenvalues k x x p x x dx ii x eigenfunc tions related x p1 x x Also matrix version problem Hint introduce diagonal matrix P ro le p x The significance connection easier find eigenvalues symmetric matrices general matrices Apply construction previous exercise eigenproblem SE kernel Gaussian density given section p x 2a exp 2ax2 Thus consider modified kernel given k x x exp ax2 exp b x x exp x Using equation Grad shteyn Ryzhik exp x y Hn x dx n 2Hn y verify k x exp cx2 Hk 2cx confirm equations Computer exercise The analytic form eigenvalues eigenfunc tions SE kernel Gaussian density given section Compare exact results obtained Nystro m approxi mation values n choice samples Let x N 2I Consider Fisher kernel derived model respect variation e regard constant Show log p x x F 2I Thus Fisher kernel model linear kernel k x x x x C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Covariance Functions Consider k order Markov model strings finite alphabet Let model parameters t s1 sk denoting probability p xi t xi s1 xk sk Of course probabilities obey constraint t t s1 sk Enforcing constraint achieved automatically setting t s1 sk t s1 sk t t s1 sk t s1 sk parameters independent suggested Jaakkola et al The current parameter values denoted Let current values 0t s1 sk set t t s1 sk e 0t s1 sk t s1 sk Show log p x nt s1 sk log t s1 sk nt s1 sk number instances substring sk s1t x Thus following Leslie et al log p x t s1 sk nt s1 sk t s1 sk ns1 sk ns1 sk number instances substring sk s1 x As ns1 sk t s1 sk expected number occurrences string sk s1t given count ns1 sk Fisher score captures degree string represented relative model For k spectrum kernel relevant feature sk s1 t x nt s1 sk C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Model Selection Adaptation Hyperparameters In chapters seen regression classification Gaussian process given fixed covariance function However practical applications easy specify aspects covari ance function confidence While properties stationarity covariance function easy determine context typically vague information properties value free hyper parameters e g length scales In chapter examples covariance functions presented large numbers parameters In addition exact form possible free parameters likelihood function known advance Thus order turn Gaussian processes powerful practical tools essential develop meth ods address model selection problem We interpret model selection model selection problem broadly include aspects model including dis crete choice functional form covariance function values hyperparameters In section outline model selection problem In following sec tions different methodologies presented section Bayesian principles covered section cross validation discussed particular leave estimator In remaining sections different methodolo gies applied specifically learning GP models regression section classification section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters The Model Selection Problem In order model practical tool application needs decisions details specification Some properties easy specify typically vague information available aspects We use term model selection cover discrete choices setting continuous hyper parameters covariance functions In fact model selection help refine predictions model valuable interpretation user properties data e g thatenable interpretation non stationary covariance function preferred stationary A multitude possible families covariance functions exists including squared exponential polynomial neural network etc section overview Each families typically number free hyperparametershyperparameters values need determined Choosing covariance function particular application comprises setting hyperparameters family comparing different families Both problems treated methods need distinguish use term model selection cover meanings We refer selection covariance function parameters training oftraining Gaussian process In following paragraphs example choices parameterizations distance measures stationary covariance functions Covariance functions squared exponential parameterized terms hyperparameters For example k xp xq f exp xp xq M xp xq 2npq M 2f n vector containing hyperparameters M denotes parameters symmetric matrix M Possible choices matrix M include M1 2I M2 diag M3 diag vector positive values D k matrix k D The properties functions covariance functions depend values hyperparameters For covariance functions easy interpret meaning hyperparameters great importance trying understand data For squared exponential covariance function eq distance measure M2 eq D hyperparameters play ro le characteristic length scales loosely speaking far needcharacteristic length scale particular axis input space function values come uncorrelated Such covariance function implements automatic relevanceautomatic relevance determination determination ARD Neal inverse length scale deter mines relevant input length scale large value 1This contrasts use word SVM literature training usually refers finding support vectors fixed kernel 2Sometimes noise level parameter 2n considered hyperparameter plays analogous role treated way simply consider hyperpa rameter C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Model Selection Problem input x1input x2 o u tp u t y input x1input x2 o u tp u t y input x1input x2 o u tp u t y b c Figure Functions dimensional input drawn random noise free squared exponential covariance function Gaussian processes corresponding different distance measures eq respectively The parameters b c In panel inputs equally important b function varies rapidly function x2 x1 In c column gives direction rapid variation covariance independent input effectively removing inference ARD successfully removing irrelevant input authors e g Williams Rasmussen We pa rameterization M3 eq factor analysis distance analogy factor analysis distance unsupervised factor analysis model seeks explain data low rank plus diagonal decomposition For high dimensional datasets k columns matrix identify directions input space specially high relevance lengths inverse characteristic length scale directions In Figure functions drawn random squared exponential covariance function Gaussian processes different choices M In panel isotropic behaviour In panel b characteristic length scale different input axes function varies rapidly function x1 rapidly function x2 In panel c direction rapid variation perpendicular direction As figure illustrates C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters plenty scope variation inside single family covariance functions Our task based set training data inferences form parameters covariance function equivalently relationships data It clear example model selection essentially open ended Even squared exponential covariance function huge variety possible distance measures However because despair seen possibility learn It requires sys tematic practical approach model selection In nutshell need able compare methods differing values particular param eters shape covariance function compare Gaussian process model kind model Although endless variations suggestions model selection literature general principles cover compute probability model given data estimate generalization error bound generalization error We use term generalization error mean average error unseen test examples distribution training cases Note training error usually poor proxy generalization error model fit noise training set fit leading low training error poor generalization performance In section describe Bayesian view model selection involves computation probability model given data based marginal likelihood In section cover cross validation estimates generalization performance These paradigms applied Gaussian process models remainder chapter The probably approximately correct PAC framework example bound gen eralization error covered section Bayesian Model Selection In section short outline description main ideas Bayesian model selection The discussion general focusses issues relevant specific treatment Gaussian process models regression section classification section It common use hierarchical specification models At lowest levelhierarchical models parameters w For example parameters parameters linear model weights neural network model At second level hyperparameters control distribution parameters level For example weight decay term neural network ridge term ridge regression hyperparameters At level discrete set possible model structures Hi consideration We mechanistic description computations needed Bayesian inference continue discussion providing intuition going Inference takes place level time applying C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bayesian Model Selection rules probability theory e g MacKay 1992b framework MacKay 1992a context neural networks At level level inference posterior parameters given Bayes rule p w y X Hi p y X w Hi p w Hi p y X Hi p y X w Hi likelihood p w Hi parameter prior The prior encodes probability distribution knowledge pa rameters prior seeing data If vague prior information parameters prior distribution chosen broad reflect The posterior combines information prior data likelihood The normalizing constant denominator eq p y X Hi independent parameters called marginal likelihood evidence given p y X Hi p y X w Hi p w Hi dw At level analogously express posterior hyperparam eters marginal likelihood level plays ro le level inference likelihood p y X Hi p y X Hi p Hi p y X Hi p Hi hyper prior prior hyperparameters The normalizing constant given p y X Hi p y X Hi p Hi d At level compute posterior model level inference p Hi y X p y X Hi p Hi p y X p y X p y X Hi p Hi We note implementation Bayesian inference calls evaluation integrals Depending details models integrals analytically tractable general resort analytical approximations Markov chain Monte Carlo MCMC methods In practice especially evaluation integral eq difficult approximation shy away hyperparameter posterior eq instead maximize marginal likelihood eq w r t hyperparameters This approximation known type II maximum likelihood ML II Of course ML II careful optimization step opens possibility overfitting especially hyperparameters The integral eq approximated local expansion maximum Laplace approximation This approximation good posterior fairly peaked case C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters y m rg l l ik e lih o o d p y X H possible data sets simple intermediate complex Figure The marginal likelihood p y X Hi probability data given model The number data points n inputs X fixed shown The horizontal axis idealized representation possible vectors targets y The marginal likelihood models different complexities shown Note marginal likelihood probability distribution normalize unity For particular dataset indicated y dotted line marginal likelihood prefers model intermediate complexity simple complex alternatives hyperparameters parameters MacKay illuminating discussion The prior models Hi eq taken flat priori favour model In case probability model proportional expression eq It primarily marginal likelihood eq involving integral parameter space distinguishes Bayesian scheme inference schemes based optimization It property marginal likelihood automatically incorporates trade model fit model complexity This reason marginal likelihood valuable solving model selection problem In Figure schematic behaviour marginal likelihood different model complexities Let number data points n inputs X fixed horizontal axis idealized representation possible vectors targets y vertical axis plots marginal likelihood p y X Hi A simple model account limited range possible sets target values marginal likelihood probability distribution y normalize unity data sets model account large value marginal likelihood Conversely complex model capable accounting wider range data sets consequently marginal likelihood doesn t attain large values simple model For example simple model linear model complex model large neural network The figure illustrates marginal likelihood doesn t simply favour models fit training data best This effect called Occam s razor William Occam Occam s razor principle plurality assumed necessity encourage simplicity explanations See Rasmussen Ghahramani investigation Occam s razor statistical models C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Cross validation Notice trade data fit model complexity automatic automatic trade need set parameter externally fix trade Do confuse automatic Occam s razor principle use priors Bayesian method Even priors flat complexity marginal likelihood tend favour complex model able explain data Thus model complexity suited data selected marginal likelihood In preceding paragraphs thought specification model model structure parameters priors etc If unclear set parameters prior treat hyperparameters model selection determine set At time emphasized priors correspond proba bilistic assumptions data If priors grossly odds distribution data inference place assumptions encoded prior step function example section To avoid situation careful employ priors narrow ruling reasonable explanations data Cross validation In section consider use methods cross validation CV cross validation model selection The basic idea split training set disjoint sets actually training validation set monitor performance The performance validation set proxy generalization error model selection carried measure In practice drawback hold method fraction data set training validation set small performance estimate obtained large variance To minimize problems CV k fold cross validation setting k fold cross validation data split k disjoint equally sized subsets validation single subset training union remaining k subsets entire procedure repeated k times time different subset validation Thus large fraction data training cases appear validation cases The price k models trained instead Typical values k range An extreme case k fold cross validation obtained k n number training cases known leave cross validation LOO CV Of leave cross validation LOO CV computational cost LOO CV training nmodels prohibitive certain cases Gaussian process regression computational shortcuts 3This known Cromwell s dictum Lindley Oliver Cromwell August 5th wrote synod Church Scotland I beseech bowels Christ consider possible mistaken C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters Cross validation loss function Although squared error loss far common regression reason allow loss functions For probabilistic models Gaussian processesother loss functions natural consider cross validation negative log probabil ity loss Craven Wahba describe variant cross validation squared error known generalized cross validation gives different weight ings different datapoints achieve certain invariance properites See Wahba sec details Model Selection GP Regression We apply Bayesian inference section cross validation section Gaussian process regression Gaussian noise We conclude section detailed examples use model selection principles tailor covariance functions Marginal Likelihood Bayesian principles provide persuasive consistent framework inference Unfortunately interesting models machine learning required computations integrals parameter space analytically intractable good approximations easily derived Gaussian process regression mod els Gaussian noise rare exception integrals parameters analytically tractable time models flexible In section apply general Bayesian inference principles section specific Gaussian process model simplified form hy perparameters optimized We derive expressions marginal likelihood interpret Since Gaussian process model non parametric model immediately obvious parameters model Generally onemodel parameters regard noise free latent function values training inputs f parameters The training cases parameters Using weight space view developed section equivalently think parameters weights linear model uses basis functions chosen eigenfunctions covariance function Of course seen view inconvenient nondegen erate covariance functions infinite number weights We proceed applying eq eq 1st level inference find chapter The predictive dis tribution eq given weight space view eq eq equivalently function space view eq The marginal likelihood evidence eq computed eq C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression lo g p ro b b ili ty characteristic lengthscale minus complexity penalty data fit marginal likelihood Characteristic lengthscale lo g m rg l l ik e lih o o d conf int b Figure Panel shows decomposition log marginal likelihood constituents data fit complexity penalty function characteristic length scale The training data drawn Gaussian process SE covariance function parameters f n Figure fitting length scale parameter parameters set accordance generating process Panel b shows log marginal likelihood function characteristic length scale different sizes training sets Also shown confidence intervals posterior length scales state result log p y X y K 1y y log Ky n log Ky Kf 2nI covariance matrix noisy targets y Kf covariance matrix noise free latent f explicitly write marginal likelihood conditioned hyperparameters parameters covariance function From perspective clear eq log marginal likelihood obtained marginaliza marginal likelihood tion latent function Otherwise thinks entirely terms function space view term marginal appear bit mysterious similarly hyper parameters covariance function The terms marginal likelihood eq readily inter interpretation pretable ro les term involving observed targets data fit y K 1y y log Ky complexity penalty depending co variance function inputs n log normalization constant In Figure illustrate breakdown log marginal likelihood The data fit decreases monotonically length scale model comes flexible The negative complexity penalty increases length scale model gets complex growing length scale The marginal likelihood peaks value close For length scales somewhat longer marginal likelihood decreases rapidly note 4Another reason like stick term marginal likelihood likelihood non parametric model e model requires access training data making predictions contrasts situation parametric model absorbs information training data posterior parameter distribution This difference makes likelihoods behave differently function C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters characteristic lengthscale n o e s ta n d rd d e vi tio n Figure Contour plot showing log marginal likelihood function characteristic length scale noise level data Figure Figure The signal variance hyperparameter set 2f The optimum close parameters generating data Note ridges small noise length scale long length scale noise 2n The contour lines spaced units apart log probability density log scale poor ability model explain data compare Figure c For smaller length scales marginal likelihood decreases slowly corresponding models accommodate data waste predictive mass regions far away underlying function compare Figure b In Figure b dependence log marginal likelihood charac teristic length scale shown different numbers training cases Generally data peaked marginal likelihood For small numbers training data points slope log marginal likelihood shallow little data observed short intermediate values length scale consistent data With data complexity term gets severe discourages short length scales To set hyperparameters maximizing marginal likelihood seekmarginal likelihood gradient partial derivatives marginal likelihood w r t hyperparameters Using eq eq A A obtain j log p y X y K K j K 1y tr K K j tr K K j K 1y The complexity computing marginal likelihood eq dominated need invert K matrix log determinant K easily com puted product inverse Standard methods matrix inversion positive definite symmetric matrices require time O n3 inversion n n matrix Once K known computation derivatives eq requires time O n2 hyperparameter Thus computational 5Note matrix matrix products eq computed directly term vector matrix multiplications trace term compute diagonal terms product C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression head computing derivatives small gradient based optimizer advantageous Estimation optimzation marginal likelihood long history spatial statistics e g Mardia Marshall As n increases hope data increasingly informative However necessary contrast Stein sec calls fixed domain asymp totics gets increasingly dense observations region increasing domain asymptotics size observation region grows n Increasing domain asymptotics natural choice time series context fixed domain asymptotics natural spatial ma chine learning settings For discussion Stein sec Figure shows example log marginal likelihood function characteristic length scale noise standard deviation hyperpa rameters squared exponential covariance function eq The signal variance 2f set The marginal likelihood clear maximum hyperparameter values Gaussian process data generated Note long length scales noise level 2n marginal likelihood independent length scale caused model explaining noise longer needing signal covariance Similarly small noise length scale marginal likelihood independent noise level caused ability model exactly interpolate data short length scale We note model hyperparameter region explains data points exactly model disfavoured marginal likelihood Figure There guarantee marginal likelihood suffer mul multiple local maxima tiple local optima Practical experience simple covariance functions indicate local maxima devastating problem certainly exist In fact local maximum corresponds particular interpre tation data In Figure example local optima shown corresponding noise free predictions model local optima One optimum corresponds relatively complicated model low noise corresponds simpler model noise With data points possible model confidently reject possibilities The numerical value marginal likelihood complex model higher simple model According Bayesian formalism ought weight predictions alternative explanations according posterior probabil ities In practice data sets larger sizes finds local optimum orders magnitude probable local optima averaging alternative explanations necessary However care taken doesn t end bad local optimum Above described adapt parameters covariance function given dataset However happen given datasets assumed share hyperparameters known multi task learning e g Caruana In case multi task learning C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters characteristic lengthscale n o e s ta n d rd d e vi tio n input x o u tp u t y input x o u tp u t y b c Figure Panel shows marginal likelihood function hyperparame ters length scale 2n noise standard deviation f signal standard deviation data set observations seen panels b c There local optima indicated global optimum low noise short length scale local optimum high noise long length scale In b c inferred underlying functions confidence intervals shown solutions In fact data points generated Gaussian process 2f n eq simply sum log marginal likelihoods individual problems optimize sum w r t hyperparameters Minka Picard Cross validation The predictive log probability leaving training case isnegative log validation density loss log p yi X y log 2i yi 22i log notation y means targets number 2i computed according eq respectively training sets taken X y Accordingly LOO log predictive probability LLOO X y n log p yi X y C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression Geisser Eddy discussion related approaches LLOO eq called log pseudo likelihood Notice pseudo likelihood n LOO CV rotations inference Gaussian process model fixed hyperparameters essentially consists computing inverse co variance matrix allow predictive mean variance eq evaluated e parameter fitting parametric model The key insight repeatedly applying pre diction eq expressions identical need inverses covariance matrices single column row removed turn This computed efficiently inverse complete covariance matrix inversion partitioning eq A A A similar insight spline models e g Wahba sec The ap proach hyperparameter selection Gaussian process models Sundararajan Keerthi The expressions LOO CV predictive mean variance yi K 1y K ii 2i K ii careful inspection reveals mean fact independent yi The computational expense computing quantities O n3 computing inverse K plus O n2 entire LOO CV procedure K known Thus computational overhead LOO CV quantities negligible Plugging expressions eq produces performance estimator optimize w r t hy perparameters model selection In particular compute partial derivatives LLOO w r t hyperparameters eq A use con jugate gradient optimization To end need partial derivatives LOO CV predictive mean variances eq w r t hyperpa rameters j Zj K ii ZjK ii K 2ii 2i j ZjK ii K 2ii K 1y Zj K K j The partial derivatives eq obtained chain rule eq LLOO j n log p yi X y j log p yi X y 2i 2i j n Zj 2i K ii ZjK ii K ii The computational complexity O n3 computing inverse K O n3 hyperparameter6 derivative eq Thus computa tional burden derivatives greater LOO CV method method based marginal likelihood eq 6Computation matrix matrix product K K j hyperparameter un avoidable C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters In eq log validation density cross validation measure fit equivalently negative log validation density loss function One envisage loss functions commonly squared error However loss function functionLOO CV squared error loss predicted mean ignores validation set variances Further mean prediction eq independent scale covariances e multiply covariance signal noise arbitrary positive constant changing mean predictions degree freedom left undetermined7 LOO CV procedure based squared error loss loss function depends mean predictions But course predictive distribution depend scale covariance function Also computation derivatives based squared error loss similar computational complexity negative log validation density loss In conclusion unattractive use LOO CV based squared error loss hyperparameter selection Comparing pseudo likelihood LOO CV methodology marginal likelihood previous section interesting ask circumstances method preferable Their computational demands roughly identical This issue studied empir ically However interesting note marginal likelihood tells probability observations given assumptions model This contrasts frequentist LOO CV value gives estimate log predictive probability assumptions model fulfilled Thus Wahba sec argued CV procedures robust model mis specification Examples Discussion In following examples model selection regression models We describe d modelling task illustrates special covariance functions designed achieve useful effects evaluated marginal likelihood Secondly short reference model selection carried robot arm problem discussed chapter chapter Finally discuss example deliberately choose covariance function suited problem called mis specified model scenario Mauna Loa Atmospheric Carbon Dioxide We use modelling problem concerning concentration CO2 atmosphere illustrate marginal likelihood set multiple hyperparameters hierarchical Gaussian process models A complex covari ance function derived combining different kinds simple covariance functions resulting model provides excellent fit data 7In special case know signal noise variance indeterminancy C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression year C O c o n ce n tr tio n p p m Figure The observations monthly averages atmospheric concentra tion CO2 end predictive confidence region Gaussian process regression model years future Rising trend seasonal variations clearly visible Note confidence interval gets wider predictions extrapolated insights properties interpretation adapted hyperparame ters Although data dimensional easy visualize total hyperparameters practice rules use cross validation setting parameters gradient based LOO CV procedure previous section The data Keeling Whorf consists monthly average atmospheric CO2 concentrations parts million volume ppmv derived situ air samples collected Mauna Loa Observatory Hawaii missing values The data shown Figure Our goal model CO2 concentration function time x Several features immediately apparent long term rising trend pronounced seasonal variation smaller irregularities In following suggest contributions combined covariance function takes care individual properties This meant primarily illustrate power flexibility Gaussian process framework possible choices appropriate data set To model long term smooth rising trend use squared exponential smooth trend SE covariance term hyperparameters controlling amplitude characteristic length scale k1 x x exp x x Note use smooth trend actually enforcing trend priori increasing probably simple hopefully desirable We use periodic covariance function eq period year seasonal component model seasonal variation However clear seasonal trend 8The data available http cdiac esd ornl gov ftp trends co2 maunaloa co2 C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters C O c o n ce n tr tio n p p m year C O c o n ce n tr tio n p p m J F M A M J J A S O N D C O c o n ce n tr tio n p p m month b Figure Panel long term trend dashed left hand scale predicted squared exponential contribution superimposed medium term trend line right hand scale predicted rational quadratic contribution vertical dash dotted line indicates upper limit training data Panel b shows seasonal variation year different years The concentration peaks mid May low beginning October The seasonal variation smooth exactly sinusoidal shape The peak peak amplitude increases ppm ppm shape change The characteristic decay length periodic component inferred years seasonal trend changes slowly suggested gradual progression years shown exactly periodic modify eq taking product squared exponential component product construction section allow decay away exact periodicity k2 x x exp x x sin2 x x gives magnitude decay time periodic component smoothness periodic component period fixed year The seasonal component data caused primarily different rates CO2 uptake plants depending season probably reasonable assume pattern change slowly time partially elevation CO2 level effect turns relevant effectively removed fitting stage allowing large To model small medium term irregularities rational quadratic termmedium term irregularities eq k3 x x x x magnitude typical length scale shape pa rameter determining diffuseness length scales discussion page One squared exponential form component turns rational quadratic works better gives higher marginal likelihood probably accommodate length scales C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression J F M A M J J A S O N D Month Y e r Figure The time course seasonal effect plotted months vs year plot wrap continuity edges The labels contours ppmv CO2 The training period extends dashed line Note slow development height May peak started recede low October currently deepening The seasonal effects particular years plotted Figure b Finally specify noise model sum squared exponential con noise terms tribution independent component k4 xp xq exp xp xq 211pq magnitude correlated noise component length scale magnitude independent noise component Noise series caused measurement inaccuracies local short term weather phenomena probably reasonable assume modest correlation time Notice correlated noise component term eq identical expression long term component eq When optimizing hyperparameters components large long length scale long term trend remains small short length scale noise The fact chosen components signal noise question interpretation Presumably interested short term effect noise hand interested effect signal The final covariance function k x x k1 x x k2 x x k3 x x k4 x x hyperparameters We subtract empirical mean data ppm fit hyperparameters optimizing parameter estimation marginal likelihood conjugate gradient optimizer To avoid bad local minima e g caused swapping ro les rational quadratic squared exponential terms random restarts tried picking run best marginal likelihood log p y X C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters We examine interpret hyperparameters optimize marginal likelihood The long term trend magnitude ppm length scale years The mean predictions inside range training data extending years future depicted Figure In plot right hand axis medium term effects modelled rational quadratic component magnitude ppm typical length years shape The small shape value allows covariance different length scales evident Figure Notice edge training data mean contribution smoothly decays zero course contribution uncertainty Figure The hyperparameter values decaying periodic contribution mag nitude ppm decay time years smoothness periodic component The long decay time shows data close periodic component short term In Figure b mean periodic contribution years corresponding beginning middle end training data This component exactly sinusoidal changes shape slowly time notably amplitude increasing Figure For noise components amplitude correlated compo nent ppm length scale months independent noise magnitude ppm Thus correlation length noise component inferred short total magnitude noise ppm indicating data explained model Note Figure model makes relatively confident predictions confidence region ppm wide year prediction horizon In conclusion seen example non trivial structure inferred composite covariance functions ability leave hyperparameters determined data useful practice Of course treatment data probably require modelling effects demographic economic indicators Finally want use real time series approach regression time CO2 level accommodate causality etc Nevertheless ability Gaussian process avoid simple parametric assumptions build lot structure makes seen attractive model application domains Robot Arm Inverse Dynamics We discussed use GPR SARCOS robot arm inverse dynamics problem section This example studied section variety approximation methods compared size training set examples precludes use simple GPR O n2 storage O n3 time complexity C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Regression o u tp u t y input x o u tp u t y input x b Figure Mis specification example Fit datapoints drawn step func tion Gaussian noise standard deviation n The Gaussian process models squared exponential covariance function Panel shows mean confidence interval noisy signal grey hyperparameters chosen maximize marginal likelihood Panel b shows resulting model hyperparameters chosen leave cross validation LOO CV Note marginal likelihood chooses high noise level long length scale LOO CV chooses smaller noise level shorter length scale It immediately obvious fit worse One techniques considered section subset datapoints SD method simply discard data use m n training examples Given subset training data size m selected random adjusted hyperparameters optimizing marginal likelihood LLOO As ARD involved adjusting D hyperparameters This process repeated times different random subsets data selected m m The results predictive accuracy obtained optimization methods similar standardized mean squared error SMSE mean standardized log loss MSLL criteria marginal likelihood optimization quicker Step function example illustrating mis specification In section discuss mis specified model scenario attempt learn hyperparameters covariance function suited data The mis specification arises data comes function zero low probability GP prior One ask interesting discuss scenario surely simply avoid choosing model practice While true theory practical reasons convenience standard forms covariance function vague prior information inevitably ends situation resembles level mis specification As example use data noisy step function fit GP model squared exponential covariance function Figure There mis specification unlikely samples drawn GP C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters o u tp u t y input x o u tp u t y input x b Figure Same data Figure Panel shows result covariance function sum squared exponential terms Although stationary covariance function gives rise higher marginal likelihood squared exponential covariance function Figure probably better fit In panel b neural network covariance function eq providing larger marginal likelihood good fit stationary SE covariance function look like step function For short length scales samples vary quickly tend vary rapidly near step Conversely stationary SE covariance function long length scale model flat parts step function rapid transition Note Gibbs covariance function eq way achieve desired effect It interesting note dif ferences model optimized marginal likelihood Figure optimized LOO CV panel b figure See exercise criteria weight influence prior For comparison predictive distribution covari ance functions Figure In panel sum squared exponential terms covariance Notice covariance function stationary flexible single squared exponential magnitude length scale parameters The predictive distribution looks little bit better value log marginal likelihood improves Figure Figure We tried neural network covariance function eq ideally suited case allows saturation different values positive negative direc tions x As shown Figure b predictions near perfect log marginal likelihood larger Model Selection GP Classification In section compute derivatives approximate marginal likeli hood Laplace EP methods binary classification needed training We detailed algorithms briefly discuss possible use cross validation methods training binary GP C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Classification classifiers Derivatives Marginal Likelihood Laplace s Approximation Recall section approximate log marginal likelihood given eq log q y X f K 1f log p y f log B B I W 2KW f maximum posterior eq found Newton s method Algorithm W diagonal matrix W log p y f We optimize approximate marginal likeli hood q y X w r t hyperparameters To end seek partial derivatives q y X j The covariance matrix K function hy perparameters f W implicitly functions changes optimum posterior f changes Thus log q y X j log q y X j explicit n log q y X f f j chain rule Using eq A eq A explicit term given log q y X j explicit f K K j K 1f tr W K K j When evaluating remaining term eq utilize fact f maximum posterior f f f f un normalized log posterior f defined eq implicit derivatives terms eq vanish leaving log q y X f log B f tr K W W f K W ii f3i log p y f In order evaluate derivative f j differentiate self consistent eq f K log p y f obtain f j K j log p y f K log p y f f f j I KW K j log p y f chain rule j f j f identity log p y f f W The desired derivatives obtained plugging eq eq C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters input X inputs y targets hypers p y f likelihood function compute K compute covariance matrix X f mode K y p y f locate posterior mode Algorithm W log p y f L cholesky I W 2KW solve LL B I W 2KW logZ f log p y f log diag L eq R W 2L L W R W I W 2KW 1W C L W 2K s2 diag diag K diag C C log p y f eq j dim C K j compute derivative matrix X s1 12a Ca tr RC eq b C log p y f s3 b KRb eq j logZ s1 s s3 eq end return logZ log marginal likelihood logZ partial derivatives Algorithm Compute approximate log marginal likelihood derivatives w r t hyperparameters binary Laplace GPC use optimization routine conjugate gradient optimization In line Algorithm page called locate posterior mode In line diagonal elements matrix product computed In line notation j means partial derivative w r t j th hyperparameter An actual implementation return value f initial guess subsequent alternative zero initialization line Algorithm Details Implementation The implementation log marginal likelihood partial derivatives w r t hyperparameters shown Algorithm It advantageous write equations previous section terms conditioned sym metric positive definite matrices solutions obtained Cholesky factorization combining numerical stability computational speed In detail matrix central importance turns R W K W I W 2KW 1W right hand suitable numerical evaluation line Algorithm reusing Cholesky factor L Newton scheme Remember W diagonal eq require real matrix matrix products Rewriting eq straightforward eq apply matrix inversion lemma eq A I KW obtain I KR implementation The computational complexity dominated Cholesky factorization line takes n3 operations iteration Newton scheme In addition computation R line O n3 computations O n2 hyperparameter C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection GP Classification input X inputs y targets hyperparameters compute K compute covariance matrix X logZEP EP K y run EP Algorithm L cholesky I S 2KS solve LL B I S 2KS b S 2L L S 2K b eq R bb S 2L L S R bb S 2B 1S j dim C K j compute derivative matrix X j logZEP tr RC eq end return logZEP log marginal likelihood logZEP partial derivatives Algorithm Compute log marginal likelihood derivatives w r t hyperparameters EP binary GP classification use optimization routine conjugate gradient optimization S diagonal precision matrix entries S ii In line Algorithm page called compute parameters EP approximation In line diagonal matrix product computed notation j means partial derivative w r t j th hyperparameter The computational complexity dominated Cholesky factorization line solution line O n3 Derivatives Marginal Likelihood EP Optimization EP approximation marginal likelihood w r t hyperparameters covariance function requires evaluation partial derivatives eq Luckily turns implicit terms derivatives caused solution EP function hyperparam eters exactly zero We present proof Seeger Consequently account explicit dependencies logZEP j j K log K K S K j K S tr K S K j In Algorithm derivatives eq implemented logZEP j tr bb S 2B 1S K j b I S 2B 1S 2K Cross validation Whereas LOO CV estimates easily computed regression use rank updates obvious generalize classification Opper Winther sec use cavity distributions mean field approach LOO CV estimates similarly use cavity distributions closely related EP algorithm discussed C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Model Selection Adaptation Hyperparameters section Although technically cavity distribution site depend label yi algorithm uses cases converging fixed point effect probably small Opper Winther sec report high precision LOO CV estimates As alternative k fold CV explicitly moderate value k Other Methods Setting Hyperparameters Above considered setting hyperparameters optimizing marginal likelihood cross validation criteria However criteria proposed literature For example Cristianini et al define alignment Gram matrix K corresponding vector ofalignment targets y A K y y Ky n K F K F denotes Frobenius norm matrixK defined eq A Lanckriet et al K convex combination Gram ma trices Ki K iKi optimization alignment score w r t s achieved solving semidefinite programming problem Example For example model selection refer section Although experi ments exhaustively evaluating marginal likelihood grid hyperparameter values techniques described chapter locate solutions efficiently Exercises The optimization marginal likelihood w r t hyperparameters generally possible closed form Consider situation hyperparameter gives overall scale covariance ky x x 0k y x x ky covariance function noisy targets e including noise contributions k y x x depend hyperparam eters Show marginal likelihood optimized w r t closed form Consider difference log marginal likelihood given log p yi yj j LOO CV log probability given log p yi yj j From viewpoint marginal likelihood LOO CV conditions data Show expected LOO CV loss greater expected marginal likelihood C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Relationships GPs Other Models In chapter discuss number concepts models related Gaussian process prediction In section cover reproducing kernel Hilbert spaces RKHSs define Hilbert space sufficiently smooth functions corresponding given positive semidefinite kernel k As discussed chapter functions consistent given dataset D We seen GP approach puts prior functions order deal issue A related viewpoint provided regularization theory described section seeks trade data fit RKHS norm function This closely related MAP estimator GP prediction omits uncertainty predictions marginal likelihood In section discuss splines special case regularization obtained RKHS defined terms differential operators given order There number families kernel machines related Gaussian process prediction In section describe support vector ma chines section discuss squares classification LSC section cover relevance vector machines RVMs Reproducing Kernel Hilbert Spaces Here present brief introduction reproducing kernel Hilbert spaces The theory developed Aronszajn recent treatise Saitoh Information found Wahba Scho lkopf Smola Wegman The collection papers edited Weinert provides overview uses RKHSs statistical signal processing C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models We start formal definition RKHS describe specific bases RKHS firstly Mercer s theorem eigenfunctions k secondly reproducing kernel map Definition Reproducing kernel Hilbert space Let H Hilbert space real functions f defined index set X Then H called reproducing kernel Hilbert space endowed inner product H norm f H f f H exists function k X X R following properties x k x x function x belongs H k reproducing property f k x H f x reproducing property See e g Scho lkopf Smola Wegman Note k x k x H k x k x H k x x The RKHS uniquely determines k vice versa stated following theorem Theorem Moore Aronszajn theorem Aronszajn Let X dex set Then positive definite function k X X exists unique RKHS vice versa The Hilbert space L2 dot product f g L2 f x g x dx contains non smooth functions In L2 RKHS delta function representer evaluation e f x f x x x dx Kernels analogues delta functions smoother RKHS Note delta function L2 contrast RKHS kernel k representer evaluation RKHS The description abstract For purposes key intuition RKHS formalism squared norm f 2H thought generalization functions n dimensional quadratic form f K 1f seen earlier chapters Consider real positive semidefinite kernel k x x eigenfunction expansion k x x N 1ii x x relative measure Recall Mercer s theorem eigenfunctions orthonormal w r t e x j x d x ij We consider Hilbert space comprised linear combinations eigenfunctions e f x N 1fii x N 1f We assert inner product f g H Hilbert space betweeninner product f g H functions f x g x N 1gii x defined f g H N figi Thus Hilbert space equipped norm f H f 2H f f H N 1f Note f H finite sequence coefficients fi decay quickly effectively imposes smoothness condition space C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Reproducing Kernel Hilbert Spaces We need Hilbert space RKHS corresponding kernel k e reproducing property This easily achieved f k x H N fiii x f x Similarly k x k x H N ii x ii x k x x Notice k x RKHS norm N ii x k x x We demonstrated Hilbert space comprised linear combinations eigenfunctions restriction N 1f fulfils conditions given Definition As unique RKHS associated k Hilbert space RKHS The advantage abstract formulation RKHS eigenbasis change use different measures Mercer s theorem However RKHS norm fact solely property kernel invariant change measure This seen fact proof RKHS properties dependent measure Kailath sec II B A finite dimensional example measure invariance explored exercise Notice analogy RKHS norm f 2H f f H N 1f quadratic form f K 1f express K f terms eigen vectors K obtain exactly form sum n terms f length n If sample coefficients fi eigenexpansion f x N 1fii x N E f 2H N E f2i N Thus N infinite sample functions H probability expected value RKHS norm infinite Wahba p Kailath sec II B details However note sample functions Gaussian process H posterior mean observing data lie RKHS smoothing properties averaging Another view RKHS obtained reproducing kernel map construction We consider space functions f defined f x n ik x xi n N xi X R C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models Now let g x n j jk x x j Then define inner product f g H n n j jk xi x j Clearly condition Definition fulfilled reproducing kernel map construction We demonstrate reproducing property k x f H n ik x xi f x Regularization The problem inferring underlying function f x finite possibly noisy dataset additional assumptions clearly I shall posed For example noise free case function passes given data points acceptable Under Bayesian approach assumptions charac terized prior functions given data obtain posterior functions The problem bringing prior assumptions bear addressed regularization viewpoint assumptions encoded terms smoothness f We consider functional J f f 2H Q y f y vector targets predicting f f x1 f xn corresponding vector function values scaling parameter trades terms The term called regularizer representsregularizer smoothness assumptions f encoded suitable RKHS second term data fit term assessing quality prediction f xi observed datum yi e g negative log likelihood Ridge regression described section seen particular case kernel ridge regression regularization Indeed recalling f 2H N 1f fi coefficient eigenfunction x penalizing weighted squared coefficients This taking place feature space simply input space standard formulation ridge regression eq corresponds kernel ridge regression The representer theorem shows minimizer f H J f therepresenter theorem form f x n ik x xi The representer theorem stated Kimeldorf Wahba case squared error O Sullivan et al showed representer theorem extended likelihood 1If RKHS contains null space unpenalized functions given form correct modulo term lies null space This explained section 2Schoenberg proved representer theorem special case cubic splines squared error This result extended general RKHSs Kimeldorf Wahba C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regularization functions arising generalized linear models The representer theorem generalized e g Scho lkopf Smola sec If data fit term convex section A unique minimizer f J f For Gaussian process prediction likelihoods involve values f n training points Q y f negative log likelihood terms involving f analogue representer theorem obvious This predictive distribution f x f test point x given data y p f y p f f p f y df As derived eq E f y k x K 1E f y formulae conditional distribution multivariate Gaussian Thus E f y n 1ik x xi K 1E f y The regularization approach long tradition inverse problems dat ing far Tikhonov Tikhonov Arsenin For application approach machine learning literature e g Poggio Girosi In section consider RKHSs defined terms differential operators In section demonstrate solve regularization problem specific case squared error section compare contrast regularization approach Gaussian process viewpoint Regularization Defined Differential Operators For x RD define Omf j1 jD m mf x x j1 x jD D dx For example m D O2f 2f x21 2f x1 x2 2f x22 dx1 dx2 Now set Pf M m O mf non negative coefficients Notice Pf translation rotation invariant In section assume a0 case ak non zero coefficient null space functions null space unpenalized For example k constant linear functions null space This case dealt section Pf penalizes f terms variability function values derivatives order M How correspond RKHS formulation section The key recognize complex exponentials exp 2is x eigenfunctions differential operator X RD In case Pf M m 2s s m f s 2ds C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models f s Fourier transform f x Comparing eq eq kernel power spectrum S s M m 2s s m Fourier inversion obtain stationary kernel k x e2is x M m 2s s m ds A slightly different approach obtaining kernel use calculus variations minimize J f respect f The Euler Lagrange equation leads f x n iG x xi M m mam 2mG x x G x x known Green s function Notice Green s func Green s function kernel tion depends boundary conditions For case X RD Fourier transforming eq recognize G fact kernel k The differential operator M m mam 2m integral operator k fact inverses shown eq See Poggio Girosi details Arfken provides introduction calculus variations Green s functions RKHSs regularizers defined differential operators Sobolev spaces e g Adams details Sobolev spaces We specific examples kernels derived differential oper ators Example Set a0 a1 m D Using Fourier pair e x 42s2 obtain k x x e x x Note covariance function Ornstein Uhlenbeck process section Example By setting 2m m 2m power series ey k y k k obtain k x x exp 2is x x exp 42s s ds D exp x x x x shown Yuille Grzywacz This squared exponential co variance function seen earlier C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Regularization Obtaining Regularized Solution The representer theorem tells general form solution eq We consider specific functional J f f 2H 22n n yi f xi uses squared error data fit term corresponding negative log likelihood Gaussian noise model variance 2n Substituting f x n 1ik x xi k xi k xj H k xi xj obtain J K 22n y K K 2n K2 2n y K 22n y y Minimizing J differentiating w r t vector coefficients obtain K 2nI 1y prediction test point x f x k x K 2nI 1y This look familiar exactly form predictive mean obtained eq In section compare contrast regularization GP views problem The solution f x n 1ik x xi minimizes eq called regularization network regularization network Poggio Girosi The Relationship Regularization View Gaus sian Process Prediction The regularization method returns f argminf J f For Gaussian process predictor obtain posterior distribution functions Can connection views In fact shall section f viewed maximum posteriori MAP function posterior Following Szeliski Poggio Girosi consider exp J f exp Pf exp Q y f The term RHS Gaussian process prior f second proportional likelihood As f minimizer J f MAP function To intuition Gaussian process prior imagine f x represented grid x space f infinite dimensional vector f Thus obtain Pf M m Dmf Dmf f m amD mDm f Dm appropriate finite difference approximation differential operator Om Observe prior term quadratic form f To detail concerning MAP relationship consider cases Q y f quadratic corresponding Gaussian likelihood C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models ii Q y f quadratic convex iii Q y f convex In case seen chapter posterior mean function obtained exactly posterior Gaussian As mean Gaussian mode MAP solution The correspondence GP posterior mean solution regularization problem f Kimeldorf Wahba In case ii seen chapter classification problems logistic probit softmax response functions Q y f convex Here MAP solution found finding f MAP solution n dimensional problem defined training points extending x values posterior mean conditioned f In case iii local minimum J f regularization approach One check minima find deepest However case argument MAP weak especially multiple optima similar depth suggests need fully Bayesian treatment While regularization solution gives Gaussian process solu tion following limitations It characterize uncertainty predictions handle multimodality posterior The analysis focussed approximating level Bayesian infer ence concerning predictions f It usually extended level e g computation marginal likelihood The marginal likelihood useful setting parameters covariance func tion model comparison chapter In addition find specification smoothness penalties deriva tives intuitive The regularization viewpoint thought directly specifying inverse covariance covariance As marginalization achieved Gaussian distribution directly covari ance inverse covariance natural specify covariance function Also non stationary covariance functions obtained regularization viewpoint e g replacing Lebesgue measure eq non uniform measure x calculation cor responding covariance function difficult Spline Models In section discussed regularizers a0 eq We consider case a0 particular consider regularizer form Omf defined eq In case polynomials degree C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Spline Models m null space regularization operator penalized In case X RD use Fourier techniques ob tain Green s function G corresponding Euler Lagrange equation m 2mG x x The result shown Duchon Meinguet G x x cm D x x 2m D log x x 2m D D cm D x x 2m D cm D constant Wahba p gives explicit form Note constraint 2m D imposed avoid having Green s function singular origin Explicit calculation Green s function domains X possible example Wahba sec splines sphere Because null space minimizer regularization functional form f x n iG x xi k j jhj x h1 x hk x polynomials span null space The exact values coefficients specific problem obtained analogous manner derivation section fact solution equivalent given eq To gain insight form Green s function consider equation m 2mG x x Fourier space leading G s 42s s m G s plays ro le like power spectrum eq notice G s ds infinite imply corresponding process infinite variance The problem course null space unpenalized example arbitrary constant function added f changing regularizer Because null space seen obtain simple connection spline solution corresponding Gaussian process problem However introducing notion intrinsic random function IRF define generalized covariance Cressie sec IRF Stein section details The basic idea consider linear combina tions f x form g x k aif x g x second order stationary hj hj k j k A careful de scription equivalence spline IRF prediction given Kent Mardia The power law form G s 42s s m means character istic length scale random functions drawn improper prior Thus obtain self similar property characteristic fractals details Szeliski Mandelbrot Some authors argue lack characteristic length scale appealing This case believe appropriate length scale set length scales C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models given problem unknown advance argue hierarchical Bayesian formulation problem described chapter appropriate Splines originally introduced dimensional interpolation smoothing problems generalized multivariate setting Schoen berg considered problem finding function minimizesspline interpolation b f m x dx f m denotes m th derivative f subject interpolation con straints f xi fi xi b n f appropriate Sobolev space He showed solution natural polynomial spline natural polynomial spline piecewise polynomial order 2m interval xi xi n order m outermost intervals The pieces joined solution 2m continuous derivatives Schoen berg proved solution univariate smoothing problem seesmoothing spline eq natural polynomial spline A common choice m leading cubic spline One possible way writing solution f x j jx j n x xi x x x It turns coefficients computed time O n algorithm Reinsch Green Silverman sec details Splines regression problems However general ized linear modelling McCullagh Nelder extended classification problems non Gaussian likelihoods GP classification section Early references direction include Silverman O Sullivan et al There vast literature relation splines statistics numerical analysis literatures entry points citations Wahba Green Silverman A d Gaussian Process Spline Construction In section clarify relationship splines Gaus sian processes giving GP construction solution univariate cubic spline smoothing problem cost functional n f xi yi f x dx observed data xi yi n x1 xn smoothing parameter controlling trade term C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Spline Models data fit second term regularizer complexity penalty Recall solution piecewise polynomial eq Following Wahba consider random function g x j jx j f x N I f x Gaussian process covariance fksp x x ksp x x x u x u du x x v2 v3 v min x x To complete analogue regularizer eq need remove penalty polynomial terms null space making prior vague e taking limit Notice covariance form contributions explicit basis functions h x x regular covari ance function ksp x x problem studied section Indeed computed limit prior vague result given eq Plugging mean equation eq predictive mean f x k x K 1y y H h x Ky covariance matrix corresponding 2fksp xi xj nij eval uated training points H matrix collects h xi vectors training points HK 1y H 1HK 1y y given eq It difficult predictive mean function piecewise cu bic polynomial elements k x piecewise3 cubic polynomials Showing mean function order polynomial outer intervals x1 xn left exercise So far ksp produced mysteriously hat provide explanation Shepp defined l fold integrated Wiener process Wl x x u l l Z u du l Z u denotes Gaussian white noise process covariance u u Note W0 standard Wiener process It easy ksp x x covariance integrated Wiener process writing W1 x W1 x eq taking expectation covariance white noise process Note Wl solution stochastic differential equation SDE X l Z Appendix B details SDEs Thus 3The pieces joined datapoints points min x x covari ance function non differentiable C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models input x o u tp u t y input x o u tp u t y spline covariance b squared exponential cov Figure Panel shows application spline covariance simple dataset The line shows predictive mean piecewise cubic polyno mial grey area indicates confidence area The thin dashed dash dotted lines samples posterior Note posterior samples smooth mean For comparison GP squared exponential covariance function shown panel b The hyperparameters cases optimized marginal likelihood cubic spline set l obtain SDE X Z corresponding regularizer f x 2dx We explicit basis function construction covariance function ksp Consider family random functions given fN x N N x N vector parameters N I Note sum form evenly spaced ramps magnitudes given entries vector Thus E fN x fN x N N x N x N Taking limit N obtain eq derivation found Vapnik sec Notice covariance function ksp given eq corresponds Gaussian process MS continuous MS differentiable Thus samples prior rough noted section posterior mean eq smoother The constructions generalized regularizer f m x dx replacing x u x u m m eq similarly eq setting h x x xm Thus use Gaussian process formulation alternative usual spline fitting procedure Note trade parameter eq C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Support Vector Machines xi xj w x w0 w x w0 w margin xi xj b Figure Panel shows linearly separable binary classification problem separating hyperplane Panel b shows maximum margin hyperplane given ratio 2n f The hyperparameters f n set techniques section optimizing marginal likelihood given eq Kohn Ansley details O n algorithm based Kalman filtering computation spline marginal likelihood In addition predictive mean GP treatment yields explicit estimate noise level predictive error bars Figure shows simple example Notice mean function piecewise cubic polynomial samples posterior smooth In contrast squared exponential covariance functions shown panel b mean functions drawn posterior infinitely differentiable Support Vector Machines Since mid s explosion interest kernel machines particular support vector machine SVM The aim section provide brief introduction SVMs particular compare Gaussian process predictors We consider SVMs classification gression problems sections respectively More comprehensive treatments found Vapnik Cristianini Shawe Taylor Scho lkopf Smola Support Vector Classification For support vector classifiers key notion need introduce maximum margin hyperplane linear classifier Then kernel trick lifted feature space We consider sep arable case non separable case We conclude section comparison GP classifiers SVMs C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models The Separable Case Figure illustrates case data linearly separable For linear classifier weight vector w offset w0 let decision boundary defined w x w0 let w w w0 Clearly version space weight vectors rise classification training points The SVM algorithm chooses particular weight vector gives rise maximum margin separation Let training set pairs form xi yi n yi For given weight vector compute quantity yi w x w0 known functional margin Notice training pointfunctional margin correctly classified If equation f x w x w0 defines discriminant function output sgn f x hyperplane cw x cw0 defines discriminant function c Thus freedom choose scaling w mini case w known canonical form hyperplane The geometrical margin defined w For training point xigeometrical margin correctly classified simply distance xi hyperplane To let c w w w w unit vector direction w w corresponding offset Then w x computes length projection x direction orthogonal hyperplane w x w computes distance hyperplane For training points misclassified geometrical margin negative distance hyperplane The geometrical margin dataset D defined D mini Thus canonical separating hyperplane margin w We wish find maximum margin hyperplane e maximizes D By considering canonical hyperplanes led following op timization problem determine maximum margin hyperplane optimization problem minimize w w w0 subject yi w xi w0 n It clear considering geometry maximum margin solution data point class yi w xi w0 Figure b Let hyperplanes pass points denoted H H respectively This constrained optimization problem set Lagrange multi pliers solved numerical methods quadratic programming4 QP problems The form solution w iyixi 4A quadratic programming problem optimization problem objective func tion quadratic constraints linear unknowns C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Support Vector Machines s non negative Lagrange multipliers Notice solution linear combination xi s The key feature equation zero xi lie hyperplanes H H points called support vectors The fact training points contribute final support vectors solution referred sparsity solution The support vectors lie closest decision boundary Note training points removed moved crossing H H maximum margin hyperplane found The quadratic programming problem finding s convex e local minima Notice similarity convexity optimization problem Gaussian process classifiers described section To predictions new input x compute sgn w x w0 sgn n iyi xi x w0 In QP problem eq training points xi test point x enter computations terms inner products Thus kernel trick replace occurrences inner product kernel kernel trick obtain equivalent result feature space The Non Separable Case For linear classifiers original x space datasets linearly separable One way generalize SVM problem case allow violations constraint yi w xi w0 impose penalty occurs This leads soft margin support vector machine soft margin problem minimization w C n yifi respect w w0 fi f xi w xi w0 z z z Here C parameter specifies relative importance terms This convex optimization problem solved QP methods yields solution form given eq In case support vectors data points lie separating hyperplanes incur penalties This occur ways data point falls H H correct decision surface ii data point falls wrong decision surface In feature space dimension N N n separating hyperplane However hyperplane rise good generalization performance especially labels incorrect soft margin SVM formulation practice C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models log exp z log z max z z g z b Figure A comparison hinge error g g b The insensitive error function SVR For hard soft margin SVM QP problems wide variety algorithms developed solution Scho lkopf Smola ch details Basic interior point methods involve inversions n n matrices scale asO n3 Gaussian process prediction However algorithms sequential minimal optimization SMO algorithm Platt better scaling practice Above described SVMs class binary classification prob lem There ways generalizing SVMs multi class problem Scho lkopf Smola sec details Comparing Support Vector Gaussian Process Classifiers For soft margin classifier obtain solution form w ixi iyi w j ij xi xj Kernelizing obtain w K f K 1f as5 K f Thus soft margin objective function written f K 1f C n yifi For binary GP classifier obtain MAP value f p f y minimize quantity f K 1f n log p yi fi cf eq The final terms eq constant kernel fixed For log concave likelihoods derived logistic pro bit response functions strong similarity optimiza tion problems convex Let g z log e z g 5Here offset w0 absorbed kernel explicit extra param eter C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Support Vector Machines log z ghinge z z z yifi We refer ghinge hinge error function shape As shown Figure data hinge error function fit terms monotonically decreasing functions z All functions tend infinity z decay zero z The key difference hinge function takes value z decay slowly It flat hinge function gives rise sparsity SVM solution Thus close correspondence MAP solution GP classifier SVM solution Can correspondence closer considering hinge function negative log likelihood The answer Seeger Sollich If Cghinge z defined negative log likelihood exp Cghinge f exp Cghinge f constant independent f case To consider quantity f C C exp C f exp C f C chosen f C independent value f C By comparison logistic probit likelihoods analogous expression equal Sollich suggests choosing C exp 2C ensures f C equality f He gives ingenious interpretation involving don t know class soak unassigned probability mass yield SVM solution MAP solution certain Bayesian problem find construction contrived Exercise invites plot f C function f values C One attraction GP classifier produces output clear probabilistic interpretation prediction p y x One try interpret function value f x output SVM probabilistically Platt suggested probabilistic predictions generated SVM computing af x b constants b fitted unbiased version training set e g cross validation One disadvantage ad hoc procedure unlike GP classifiers account predictive variance f x cf eq Seeger sec shows better error reject curves obtained experiment MNIST digit classification problem effect uncertainty taken account Support Vector Regression The SVM originally introduced classification problem extended deal regression case The key concept insensitive error function This defined g z z z This function plotted Figure b As eq interpret exp g z likelihood model regression residuals c f squared C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models error function corresponding Gaussian model However note unusual choice model distribution residuals basically motivated desire obtain sparse solution support vector classifier If error model Laplacian distribution corresponds absolute values regression Edgeworth cited Rousseeuw heavier tailed distribution Gaussian provides protection outliers Girosi showed Laplacian distribution viewed continuous mixture zero mean Gaussians certain distribution variances Pontil et al extended result allowing means uniformly shift order obtain probabilistic model corresponding insensitive error function See section work robustification GP regression problem For linear regression case insensitive error function Gaussian prior w MAP value w obtained minimizing w C n g yi fi w r t w The solution6 f x n ixi x coefficients obtained QP problem The problem kernelized solution f x n ik xi x As support vector classification coefficients zero The data points lie inside tube edge outside non zero Least squares Classification In chapter argued use logistic probit likelihoods pro vides natural route develop GP classifier attractive outputs interpreted probabilistically However simpler approach treats classification regression problem Our starting point binary classification linear predictor f x w x This trained linear regression target y patterns label target y patterns label Targets y y slightly flexibility targets As shown Duda Hart section choosing y y appropriately allows obtain solution Fisher s linear discriminant decision criterion f x Also targets y y squares error function gives minimum squared error approximation Bayes discriminant function p C x p C x n Following Rifkin Klautau methods squares classification LSC Note probabilistic interpretation squared error criterion 6Here assumed constant included input vector x C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Least squares Classification odd choice implies Gaussian noise model values target y y observed It natural extend squares classifier kernel trick This suggested number authors including Poggio Girosi Suykens Vanderwalle Experimental results reported Rifkin Klautau indicate performance comparable SVMs obtained kernel LSC regularized squares classifier RLSC Consider single random variable y takes value proba bility p value probability p Then value f minimizes squared error function E p f p f f 2p linear rescaling p interval Equivalently targets obtain f p Hence observe LSC estimate p correctly large data limit If consider single random variable wish estimate p C x linear rescaling long approximating function f x sufficiently flexible expect limit n converge p C x For technical detail issue section consistency Hence LSC sensible procedure classification note guarantee f x constrained lie interval y y If wish guarantee proba bilistic interpretation squash predictions sigmoid suggested SVMs Platt described page When generalizing binary multi class situation freedom set problem Scho lkopf Smola sec identify methods versus rest C binary classifiers trained classify class rest pairs C C binary classifiers trained error correcting output coding class assigned binary codeword binary classifiers trained bit separately multi class objective functions aim train C classifiers simultaneously creating number binary classification problems One needs specify outputs classifiers trained combined produce overall answer For versus rest7 method simple criterion choose classifier produces positive output Rifkin Klautau performed ex tensive experiments came conclusion versus rest scheme SVMs RLSC accurate method overall merit conceptually simple straightforward implement Probabilistic Least squares Classification The LSC algorithm discussed attractive computational point view guarantee valid probabilistic interpretation need use separate post processing stage squash predictions sigmoid However easy enforce probabilistic interpretation 7This method called versus C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models training stage One possible solution combine ideas training leave cross validation covered section use parameterized sigmoid function Platt We method probabilistic squares classifier PLSC In section saw compute Gaussian leave LOO predictive probabilities training hyperparameters based sum log LOO probabilities Using idea express LOO probability squashing linear function Gaussian predictive probability cumulative Gaussian p yi X y yi fi N fi 2i dfi yi 22i integral given eq leave predictive mean variance 2i given eq The objective function sum log LOO probabilities eq set hyperparameters additional parameters linear transformation eq Introducing likelihood eq objective eq taking derivatives obtain LLOO j n log p yi X y j log p yi X y 2i 2i j n N ri yiri yi 22i j 22i 2i j ri 22i partial derivatives Gaussian LOO parameters j 2i j given eq Finally linear transformation parameters LLOO n N ri yiri yi 22i 2i 22i LLOO n N ri yiri yi 22i These partial derivatives train parameters GP There options predictions natural compute predictive mean variance squash sigmoid parallelling eq Applying model USPS 3s vs 5s binary classification task discussed section test set error rate compares favourably results reported methods Figure However test set information bits poor 8The test information dominated single test case predicted confidently belong wrong class Visual inspection digit reveals looks testset label wrong case This observation highlights danger explicitly allowing data mislabelling model kind data C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relevance Vector Machines Relevance Vector Machines Although usually presented relevance vector machine RVM introduced Tipping actually special case Gaussian process The covariance function form k x x N j j j x j x j hyperparameters N basis functions j x usually necessarily taken Gaussian shaped basis functions centered n training data points j x exp x xj length scale hyperparameter controlling width basis function Notice simply construction covariance function corresponding N dimensional set basis functions given section p diag N The covariance function eq interesting properties firstly clear feature space corresponding covariance function finite dimensional e covariance function degenerate secondly covariance function odd property depends training data This dependency means prior functions depends data property odds strict Bayesian interpretation Although usual treatment model possible dependency prior data lead surprising effects discussed Training RVM analogous GP models optimize marginal likelihood w r t hyperparameters This optimization leads sig nificant number j hyperparameters tending infinity effectively removing pruning corresponding basis function covariance function eq The basic idea basis functions sig nificantly contributing explaining data removed resulting sparse model The basis functions survive called relevance vectors relevance vectors Empirically observed number relevance vectors smaller number support vectors problem Tipping The original RVM algorithm Tipping able exploit sparsity effectively model fitting initialized set finite values meaning basis functions contributed model However careful analysis RVM marginal likelihood Faul Tipping showed carry optimization w r t single analytically This led accelerated training algorithm described Tipping Faul starts model e set infinity adds basis functions sequentially As number relevance vectors usually number training cases faster train predictions RVM non sparse C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Relationships GPs Other Models GP Also note basis functions include additional hyperparameters e g use automatic relevance determination ARD form basis function different length scales different dimensions eq These additional hyperparameters set optimizing marginal likelihood The use degenerate covariance function depends data undesirable effects Imagine test point x lies far away relevance vectors At x basis functions values close zero basis function appreciable signal predictive distribution Gaussian mean close zero variance close zero inferred noise level This behaviour undesirable lead dangerously false conclusions If x far relevance vectors model shouldn t able draw strong conclusions output extrapolating predictive uncertainty small opposite behaviour expect reasonable model Here argued localized basis functions RVM undesirable properties argued Rasmussen Quin onero Candela actually degeneracy covariance function core problem Although work Rasmussen Quin onero Candela goes way fixing problem inherent conflict degeneracy covariance function good computational reasons bad modelling reasons Exercises We motivate fact RKHS norm depend den sity p x finite dimensional analogue Consider n dimensional vector f let n n matrix comprised non colinear columns n Then f expressed linear combination ba sis vectors f n cii c coefficients ci Let s eigenvectors covariance matrix K w r t diagonal matrix P non negative entries KP diagonal matrix containing eigenvalues Note P In Show n c c 1c f K 1f observe f K 1f expressed c 1c valid P corresponding Hint find useful set P K P 2KP etc Plot eq function f different values C Show value C C makes f C equal values f Try setting C exp 2C suggested Sollich observe effect Show predictive mean spline covariance GP eq linear function x x located left right training points Hint consider eigenvectors corresponding largest eigenvalues training set covariance matrix eq vague limit C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Theoretical Perspectives This chapter covers number theoretical issues relating Gaussian processes In section saw GPR carries linear smoothing datapoints weight function The form weight function understood terms equivalent kernel discussed section As gets data hope GP predictions converge true underlying predictive distribution This question consistency reviewed section discuss concepts equivalence orthogonality GPs When generating process data assumed GP particu larly easy obtain results learning curves describe accuracy predictor increases function n described section An alternative approach analysis generalization error provided PAC Bayesian analysis discussed section Here seek relate high probability error observed training set generalization error GP predictor Gaussian processes methods devel oped supervised learning problems In section compare contrast GP predictors supervised learning methods The Equivalent Kernel In section consider regression problems We seen section posterior mean GP regression obtained function minimizes functional J f f 2H 22n n yi f xi f H RKHS norm corresponding kernel k Our goal understand behaviour solution n C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives Let x y probability measure data pairs xi yi generated Observe E n yi f xi n y f x d x y Let x E y x regression function corresponding probability measure The variance x denoted x y x 2d y x Then writing y f y f obtain y f x d x y x f x d x x d x cross term vanishes definition x As second term right hand eq independent f idealization regression problem consists minimizing functional J f n 22n x f x d x f 2H The form minimizing solution easily understood terms eigenfunctions x kernel k w r t x x j x d x ij section Assuming kernel nondegenerate s form complete orthonormal basis write f x fii x Similarly x ii x x x d x Thus J f n 22n fi f2i This readily minimized differentiation w r t fi obtain fi 2n n Notice term 2n n n limit expect f x converge x There caveats assumed x sufficiently behaved represented generalized Fourier series ii x assumed kernel nondegenerate If kernel degenerate e g polynomial kernel f converge best weighted L2 approximation span s In section rates convergence f clearly general depend smoothness kernel k measure x y From Bayesian perspective happening prior f overwhelmed data n Looking eq 2n ni fi effectively zero This means find coefficients eigenfunctions small eigenvalues sufficient amounts data Ferrari Trecate et al demonstrated C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml The Equivalent Kernel showing regression performance certain nondegenerate GP approximated taking m eigenfunctions m chosen m 2n n Of course data obtained m increased Using fact x x d x defining 2eff n n obtain f x ii 2eff x ii x x 2eff x d x The term square brackets eq equivalent kernel smooth ing problem denote hn x x Notice similarity vector valued equivalent kernel weight function h x defined section The difference pre diction obtained linear combination finite number observations yi weights given hi x noisy function y x instead f x hn x x y x d x Notice limit n 2eff equivalent kernel tends delta function The form equivalent kernel given eq useful practice requires knowledge eigenvalues functions combina tion k However case stationary kernels use Fourier methods compute equivalent kernel Consider functional J f 22n y x f x dx f 2H dimensions number observations unit x space length area volume etc appropriate Using derivation similar eq obtain h s Sf s Sf s 2n S 1f s n Sf s power spectrum kernel k The term 2n corresponds power spectrum white noise process delta function covari ance function white noise corresponds constant Fourier domain This analysis known Wiener filtering e g Papoulis sec Wiener filtering Equation eq discrete eigenspectrum replaced continuous As observed Figure equivalent kernel essentially gives weighting observations locally x Thus identifying np x obtain approximation equivalent kernel stationary kernels width kernel smaller length scale variations p x This form analysis Silverman splines dimension Some Specific Examples Equivalent Kernels We consider OU process d This k r exp r setting relative previous notation r x x power spectrum C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives S s 42s2 Let vn 2n Using eq obtain h s vn 42s2 vn This form Fourier transform OU covariance function1 inverted obtain h r vn e r In particular notice n increases vn decreases inverse length scale h r increases asymptotically n1 large n This shows width equivalent kernel OU covariance function scale n asymptotically Similarly width scale p x asymptotically A similar analysis carried AR Gaussian process d section B power spectrum 42s2 e Mate rn class In case Fourier relationships given Papoulis p width equivalent kernel scales n asymptotically Analysis equivalent kernel carried spline models Silverman gives explicit form equivalent kernel case dimensional cubic spline corresponding regularizer Pf f 2dx Thomas Agnan gives general expression equivalent kernel spline regularizer Pf f m 2dx dimension analyzes end effects domain interest bounded open interval For regularizer Pf 2f 2dx dimensions equivalent kernel given terms Kelvin function kei Poggio et al Stein Silverman shown splines order m d corre sponding roughness penalty f m dx width equivalent kernel scale n 2m asymptotically In fact shown true splines D dimensions exercise Another interesting case consider squared exponential kernel S s D exp s Thus h SE s b exp s b 2n D We unaware exact result case following approximation Sollich Williams simple effective For large e large n b small Thus small s s h SE large s approximately The change takes place point sc b exp 2s2c e s c log b As exp 2s2 grows quickly s transition h SE expected rapid approximated step function By standard result Fourier transform step function obtain hSE x 2scsinc 2scx 1The fact h s form Sf s particular OU covariance function generally case C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Asymptotic Analysis D sinc z sin z z A similar calculation D eq gives hSE r sc r D JD 2scr Notice sc scales log n width equivalent kernel decay slowly n increases Notice plots Figure sinc type shape sidelobes large predicted sinc curve transition smoother step function Fourier space ringing Asymptotic Analysis In section consider asymptotic properties Gaussian processes consistency equivalence orthogonality Consistency In section analyzed asymptotics GP regression seen minimizer functional eq converges regression function n We broaden focus considering loss functions squared loss case work directly eq smoothed version eq The set follows Let L pointwise loss function Consider procedure takes training data D loss function returns function fD x For measurable function f risk expected loss defined RL f L y f x d x y Let f L denote function minimizes risk For squared loss f L x E y x For loss classification problems choose f L x class c x p Cc x p Cj x j c breaking ties arbitrarily Definition We procedure returns fD consistent consistency given measure x y loss function L RL fD RL f L n convergence assessed suitable manner e g probability If fD x consistent Borel probability measures x y said uni versally consistent A simple example consistent procedure kernel regression method As described section obtains prediction test point x comput ing f x n wiyi wi n j 1j Nadaraya Watson estima tor Let h width kernel D dimension input C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives space It shown suitable regularity conditions h nhD n procedure consistent e g Gyo rfi et al Theorem regression case squared loss Devroye et al Theorem classification case loss An intuitive understanding result obtained noting h means datapoints close x contribute prediction eliminating bias condition nhD means large number datapoints contribute prediction eliminating noise variance It useful consider hope GPR GPC universally consistent As discussed section key property non degenerate kernel infinite number eigenfunctions forming orthonormal set Thus generalized Fourier analysis linear combination eigenfunctions cii x able represent suf ficiently behaved target function f L However estimate infinite number coefficients ci noisy observations This makes clear playing game involving infinities needs played care results Diaconis Freedman Freedman Gru nwald Langford certain circumstances Bayesian inference infinite dimensional objects inconsistent However positive recent results consistency GPR GPC Choudhuri et al binary classification case certain assumptions GPC consistent The assumptions include smooth ness mean covariance function GP smoothness E y x assumption domain bounded subset RD Their result holds class response functions c d f s unimodal symmetric density includes probit logistic functions For GPR Choi Schervish dimensional input space finite length certain assumptions consistency holds Here assumptions include smoothness mean covariance function GP smoothness E y x An additional assumption noise normal Laplacian distribution unknown variance inferred There consistency results relating functional Jn f n f 2H n n L yi f xi n n Note agree previous formulations set n n decay rates n considered In splines literature Cox showed regression problems ing regularizer f 2m m k O kf definitions eq consistency obtained certain technical conditions Cox O Sulli van considered wide range problems including regression problems squared loss classification logistic loss solution obtained minimizing regularized risk spline smoothness term They showed f L H H RKHS corresponding spline C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Asymptotic Analysis regularizer n n appropriate rate gets convergence fD f L More recently Zhang Theorem shown classifica tion problem number different loss functions including logistic loss hinge loss quadratic loss general RKHSs nondegenerate kernel n nn x y sufficiently regular classification error fD converge Bayes optimal error probability n Similar results obtained Steinwart rates decay n depending smoothness kernel Bartlett et al characterized loss functions lead universal consistency Above focussed regression classification problems However similar analyses given problems density estimation deconvolution Wahba chs references Also discussed consistency fixed decay rate n However possible analyze asymptotics methods n set data dependent way e g cross validation Wahba sec references details Consistency evidently desirable property supervised learning proce dures However asymptotic property given prediction procedure perform particular problem given dataset For instance note required general prop erties kernel function e g non degeneracy consistency results However choice kernel huge difference procedure performs practice Some analyses related issue given section Equivalence Orthogonality The presentation section based mainly Stein ch For probability measures defined measurable space F said absolutely continuous w r t A F A implies A If absolutely continuous w r t absolutely continuous w r t measures said equivalent written said orthogonal written exists A F A A Note case Ac Ac Ac complement A The dichotomy theorem Gaussian processes Hajek independently Feldman states Gaussian processes equivalent orthogonal Equivalence orthogonality Gaussian measures corre sponding probability densities p0 p1 characterized terms 2Cross validation discussed section 3See section A background measurable spaces C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives symmetrized Kullback Leibler divergence KLsym given KLsym p0 p1 p0 f p1 f log p0 f p1 f df The measures equivalent KLsym orthogonal For finite dimensional Gaussian distributions N K0 N K1 Kullback sec KLsym tr K0 K1 K K tr K K This expression simplified considerably simultaneously diagonalizing K0 K1 Two finite dimensional Gaussian distributions equivalent null spaces covariance matrices coincide orthogonal Things interesting consider infinite dimensional distribu tions e Gaussian processes Consider closed subset R RD Choose finite number n x points R let f f1 fn denote values corresponding inputs We consider KLsym divergence limit n KLsym diverge rates decay eigenvalues processes For example consider zero mean periodic processes period eigenvalue ij indicates power sin cos terms frequency 2j process Then eq KLsym j 0j j 0j j Stein p Some corresponding results equiva lence orthogonality non periodic Gaussian processes given Stein pp Stein p gives example equivalent Gaussian processes R covariance functions exp r exp 2r It easy check large s power spectrum We turn consequences equivalence model selection problem Suppose know GP0 GP1 correct model Then GP0 GP1 possible determine model correct probability However Bayesian setting means prior probabilities hypotheses observing data D posterior probabilities p GPi D heavily skewed model The important observation consider predictions GP0 GP1 Consider case GP0 correct model GP1 GP0 Then Stein sec shows predictions GP1 asymptotically optimal sense expected relative prediction error GP1 GP0 tends n technical conditions Stein s Corol lary p shows conclusion remains true additive noise un noisy GPs equivalent One caveat equivalence predictions GP1 asymptotically optimal GP0 correct model GP0 GP1 differing predictions finite n C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Average case Learning Curves Average case Learning Curves In section discussed asymptotic properties Gaussian process predictors related methods In section speed convergence certain specific assumptions Our goal obtain learning curve describing generalization error function training set size n This average case analysis averaging choice target functions drawn GP x locations training points In detail consider target function f drawn Gaussian process n locations chosen observations giving rise train ing set D X y The yis possibly noisy observations underlying function f Given loss function L measures difference prediction f f obtain estimator fD f Below use squared loss posterior mean f D x estimator Then generalization error given f D given generalization error E g D f L f x f D x p x dx As expected loss technically risk term generalization error commonly E g D f depends choice f X Note y depends choice f noise present The level averaging consider functions f drawn GP prior obtain Eg X E g D f p f df It turn regression problems Gaussian process priors predictors average readily calculated The second level averaging assumes x locations training set drawn d p x Eg n Eg X p x1 p xn dx1 dxn A plot Eg n n known learning curve learning curve Rather averaging X alternative minimize Eg X w r t X This gives rise optimal experimental design problem We problem subject large investigation An early paper subject Ylvisaker These questions addressed statistical literature theoretical numerical analysis area book Ritter provides useful overview We proceed develop average case analysis specific case GP predictors GP priors regression case squared loss Let f drawn zero mean GP covariance function k0 noise level Similarly predictor assumes zero mean process covariance C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives function k1 noise level At particular test location x averaging f E f x k1 x K yy E f2 x 2k1 x K yE f x y k1 x K yE yy K yk1 x k0 x x 2k1 x K yk0 x k1 x K yK0 yK yk1 x Ki y Ki f 2i e covariance matrix including assumed noise If k1 k0 predictor correctly specified expression reduces k0 x x k0 x K yk0 x predictive variance GP Averaging error p x obtain Eg X E f x k1 x K yy p x dx k0 x x p x dx tr K y k0 x k1 x p x dx tr K yK0 yK y k1 x k1 x p x dx For choices p x covariance functions integrals alytically tractable reducing computation Eg X n n matrix computation To obtain Eg n need perform final level averaging X In general difficult Eg X computed exactly times possible e g noise free OU process real line section The form Eg X simplified considerably express covari ance functions terms eigenfunction expansions In case k0 k1 use definition k x x ii x x k x x x p x dx ii x Let diagonal matrix eigenvalues N n design matrix defined section Then eq obtain Eg X tr tr 2nI tr 2n second line follows use matrix inversion lemma eq A directly use eq shown Sollich Opper Vivarelli Using fact EX nI na ve approximation replace inside trace expectation fact Opper Vivarelli showed gives lower bound Eg n tr n 2n I N 2n ni Examining asymptotics eq eigenvalue 2n n add 2n n bound generalization error As saw C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml PAC Bayesian Analysis section eigenfunctions come play n increases rate decay Eg n slower n Sollich derives number accurate approximations learning curve eq For noiseless case k1 k0 simple lower bound Eg n n Micchelli Wahba This bound obtained demonstrating optimal n pieces information projections random function f n eigenfunctions As observations simply consist function evaluations general provide information lower bound Plaskota generalized result bound learning curve observations noisy Some asymptotic results learning curves known For example Ritter sec V covariance functions obeying Sacks Ylvisaker conditions4 order r d considered He shows optimal sampling input space generalization error goes O n 2r 2r noisy problem Similar rates found Sollich random designs For noise free case Ritter p gives rate O n 2r One examine learning curve asymptotically small n typically curve roughly linear decrease n Williams Vivarelli explained behaviour observing introduction datapoint x1 reduces variance locally x1 assuming stationary covariance function The addition datapoint x2 create hole With small number datapoints likely holes far apart contributions add explaining initial linear trend Sollich investigated mismatched case k0 k1 This rise rich variety behaviours learning curves includ ing plateaux Stein chs carried analysis mismatched case Although focused GP regression squared loss note Malzahn Opper developed general techniques analyze learning curves situations GP classification PAC Bayesian Analysis In section gave average case analysis generalization taking average respect GP prior functions In section present different kind analysis probably approximately correct PAC PAC framework Valiant Seeger presented PAC Bayesian analysis generalization Gaussian process classifiers number stages present introduction PAC framework section describe PAC Bayesian approach section 4Roughly speaking stochastic process possesses r MS derivatives r said satisfy Sacks Ylvisaker conditions order r d gives rise spectrum 2r asymptotically The OU process obeys Sacks Ylvisaker conditions order C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives finally application GP classification section Our presentation based mainly Seeger The PAC Framework Consider fixed measure x y Given loss function L exists function x minimizes expected risk By running learning algorithm data set D size n drawn d x y obtain estimate fD attains expected risk RL fD We able evaluate RL fD know However access empirical distribution training set x y n x xi y yi compute empirical risk R L fD 1n L yi fD xi Because training set compute fD expect R L fD underestimate RL fD aim PAC analysis provide bound RL fD based R L fD A PAC bound following format pD RL fD R L fD gap fD D pD denotes probability distribution datasets drawn d x y called confidence parameter The bound states averaged draws dataset D x y RL fD exceed sum R L fD gap term probability The accounts probably PAC approximately derives fact gap term positive n It important note PAC analyses distribution free e eq hold measure There kinds PAC bounds depending gap fD D actually depends particular sample D simple statistics like n Bounds depend D called data dependent called data independent The PAC Bayesian bounds given data dependent It important understand interpretation PAC bound clarify consider simpler case statistical inference We given dataset D x1 xn drawn d distribution x mean m An estimate m given sample mean x xi n Under certain assumptions obtain bounds sampling distribution p x m relates choice dataset D However wish perform probabilistic inference m need combine p x m prior distribution p m use Bayes theorem obtain posterior The situation similar somewhat complex PAC bounds concern sampling distribution expected empirical risks fD w r t D 5It possible consider PAC analyses empirical quantities cross validation error section bias 6In introductory treatments frequentist statistics logical hiatus going sampling distribution inference parameter interest explained C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml PAC Bayesian Analysis We wish conditional statement like pD RL fD r gap fD D R L fD r r small value statement inferred directly PAC bound This gap heavily anti correlated R L fD gap large empirical risk small PAC bounds carry model selection given learn ing machine depends discrete continuous parameter vector seek minimize generalization bound function However procedure justified generalization bounds loose Let slack denote difference value bound generalization error The danger choosing minimize bound slack depends value minimizes bound different value minimizes generalization error See Seeger sec discussion PAC Bayesian Analysis We consider Bayesian set prior distribution p w pa rameters w posterior distribution q w Strictly speaking analysis require q w posterior distribution distribu tion practice consider q approximate posterior distri bution We limit discussion binary classification labels general cases considered Seeger sec The predictive distribution f test point x given q w q f x q f w x q w dw predictive classifier outputs sgn q f x predictive classifier The Gibbs classifier studied learning theory given test point Gibbs classifier x draws sample w q w predicts label sgn f x w The main reason introducing Gibbs classifier PAC Bayesian theorems given apply Gibbs classifiers For given parameter vector w giving rise classifier c x w ex pected risk empirical risk given RL w L y c x w d x y R L w n n L yi c xi w As Gibbs classifier draws samples q w consider averaged risks RL q RL w q w dw R L q R L w q w dw Theorem McAllester s PAC Bayesian theorem For probability mea McAllester s PAC Bayesian theoremsures p q w bounded loss function L L y c x classifier c input x pD RL q R L q KL q p log log n 2n q C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives The proof found McAllester The Kullback Leibler KL diver gence KL q p defined section A An example loss function obeys conditions theorem loss For special case loss Seeger gives following tighter bound Theorem Seeger s PAC Bayesian theorem For distribution X Seeger s PAC Bayesian theorem probability measures p q w following bound holds d samples drawn data distribution pD KLBer R L q RL q n KL q p log n q Here KLBer KL divergence Bernoulli distributions de fined eq A Thus theorem bounds high probability KL divergence R L q RL q The PAC Bayesian theorems refer Gibbs classifier If interested predictive classifier sgn q f x Seeger shows q f x symmetric mean expected risk predictive classifier twice expected risk Gibbs classifier However result based simple bounding argument practice expect predictive classifier usually better performance Gibbs classifier Recent work Meir Zhang provides PAC bounds directly Bayesian algorithms like predictive classifier predictions basis data dependent posterior distribution PAC Bayesian Analysis GP Classification To apply bound Gaussian process case need compute KL divergence KL q p posterior distribution q w prior distribution p w Although considered w r t weight vector w eigenfunction expansion fact turns convenient consider latent function value f x possible point input space X parameter We divide possibly infinite vector parts values corresponding training points x1 xn denoted f remaining points x space test points f The key observation methods described dealing GP classification problems produce posterior approximation q f y defined training points This approximation Laplace s method EP MCMC methods sample exact posterior This posterior f extended test points setting q f f y q f y p f f Of course prior distribution similar decomposition p f f C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Comparison Other Supervised Learning Methods p f p f f Thus KL divergence given KL q p q f y p f f log q f y p f f p f p f f dfdf q f y log q f y p f df shown e g Seeger Notice reduced scary infinite dimensional integration manageable n dimensional integra tion case q f y Gaussian Laplace EP approxima tions KL divergence computed eq A For Laplace approximation p f N K q f y N f A gives KL q p log K log A tr A K A f K 1f Seeger evaluated quality bound produced PAC Bayesian method Laplace GPC task discriminating handwritten 2s 3s MNIST handwritten digits database He reserved test set examples training sets size The classifications replicated times draws training sets pool examples We quote example results n training error test error PAC Bayesian bound generalization error evaluated The figures denote confidence interval The classification results Gibbs classifier predictive classifier test error rate Thus generalization error PAC bound Many PAC bounds struggle predict error rates impressive highly non trivial result Further details experiments found Seeger Comparison Other Supervised Learn ing Methods The focus book Gaussian process methods supervised learning However techniques available supervised learning linear regression logistic regression decision trees neural networks support vector machines kernel smoothers k nearest neighbour classifiers etc need consider relative strengths weaknesses approaches Supervised learning inductive process given finite training set wish infer function f makes predictions possible input values The additional assumptions learning algorithm known inductive bias e g Mitchell p Sometimes assumptions inductive bias explicit algorithms e g decision tree induction implicit 7See http yann lecun com exdb mnist C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives However variety supervised learning algorithms based idea similar input patterns usually rise similar outputs output distributions precise notion similarity differentiates algorithms For example algorithms feature selection decide input dimensions irrelevant predictive task Some algorithms construct new features provided measure similarity derived space As seen regression techniques seen linear smoothers section techniques vary definition weight function One important distinction different learning algorithms relate question universal consistency section For example linear regression model inconsistent function minimizes risk represented linear function inputs In general model finite dimensional parameter vector universally consistent Examples models linear regression logistic regression finite dimensional feature vector neural networks fixed number hidden units In contrast parametric models non parametric models k nearest neighbour classifiers kernel smoothers Gaussian processes SVMs nondegenerate kernels compress training data finite dimensional parameter vector An intermediate po sition taken semi parametric models neural networks number hidden units k allowed increase n increases In case uni versal consistency results obtained Devroye et al ch certain technical conditions growth rates k Although universal consistency good thing necessarily mean consider procedures property example specific problem knew linear regression model consistent problem natural use In s large surge interest artificial neural networksneural networks ANNs feedforward networks consisting input layer followed layers non linear transformations weighted combinations activity previous layers output layer One reason surge interest use backpropagation algorithm training ANNs Initial excitement centered fact training non linear networks possible later focus came generalization performance ANNs deal questions layers hidden units use units layer type non linearities etc For particular ANN search good set weights given training set complicated fact local optima optimization problem because significant difficulties practice In contrast Gaus sian process regression classification posterior latent variables convex One approach problems raised ANNs BayesianBayesian neural networks framework developed MacKay 1992a Neal This gives rise C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Comparison Other Supervised Learning Methods posterior distributions weights given architecture use marginal likelihood section model comparison selection In contrast Gaussian process regression marginal likelihood given ANN model analytically tractable approximation techniques Laplace approximation MacKay 1992a Markov chain Monte Carlo methods Neal Neal s observation certain ANNs hidden layer converge Gaussian process prior functions section led consider GPs alternatives ANNs MacKay sec raises interesting question moving neural networks Gaussian processes thrown baby bathwater This question arises statements neural networks meant intelligent models discovered features patterns data Gaussian processes simply smoothing devices Our answer question GPs computationally attractive method dealing smoothing problem given kernel issues feature discovery etc addressed methods select kernel function chapter details Note distance function r2 x x x x M x x M having low rank form M eq features described columns However non convexity neural network optimization problem returns optimizing marginal likelihood terms parameters M local optima As seen chapters linear regression logistic regres linear logistic regressionsion Gaussian priors parameters natural starting point development Gaussian process regression Gaussian process classifi cation However need enhance flexibility models use non degenerate kernels opens possibility universal consistency Kernel smoothers classifiers described sections kernel smoothers classifiers7 At high level similarities GP prediction methods kernel placed training example prediction obtained weighted sum kernel functions details prediction underlying logic differ Note GP prediction view gives e g error bars predictions use marginal likelihood set parameters kernel section On hand computational problem needs solved carry GP prediction demanding simple kernel based methods Kernel smoothers classifiers non parametric methods consis tency obtained conditions width h kernel tends zero nhD The equivalent kernel analysis GP regression section shows close connections kernel regression method GPR note equivalent kernel automatically reduces width n grows contrast decay h imposed kernel regression Also kernel smoothing classification algorithms width kernel increased areas low observation density ex ample occur algorithms consider k nearest neighbours test point Again notice equivalent kernel analysis width C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives equivalent kernel larger regions low density exact dependence density depend kernel The similarities differences GP prediction regularizationregularization networks splines SVMs RVMs networks splines SVMs RVMs discussed chapter Appendix Learning Curve Ornstein Uhlenbeck Process We consider calculation learning curve OU covariance function k r exp r interval assuming training x s drawn uniform distribution U Our treatment based Williams Vivarelli We calculate Eg X fixed design integrate possible designs obtain Eg n In absence noise OU process Markovian discussed Ap pendix B exercise We consider interval points x1 x2 xn xn placed interval Also let x0 xn Due Markovian nature process prediction test point x depends function values training points immediately left right x Thus th interval counting bounding points xi xi Let interval length Using eq Eg X 2f x dx n xi xi 2f x dx 2f x predictive variance input x Using Markovian property interval n 2f x k k x K 1k x K Gram matrix K k k k k k x corresponding vector length Thus K k k k k k2 k2 2f x k k k2 xi x k2 x xi 2k k x xi k xi x Thus xi xi 2f x dx ik I1 I2 8CW thanks Manfred Opper pointing upper bound developed Williams Vivarelli exact noise free OU process C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Exercises I1 k k2 z dz I2 k k z k z dz For k r exp r equations reduce I1 e I2 e e Thus xi xi 2f x dx 2ie 2i e 2i This calculation correct intervals x1 xn relevant respectively For 0th interval 2f x k k2 x1 x k x1 2f x 0k k x1 k2 x1 x dx e similar result holds xn 2f x Putting obtain Eg X n e e 2n n 2ie 2i e 2i Choosing regular grid n 2n n n straightforward exercise Eg scales O n agreement general Sacks Ylvisaker result Ritter p recalled OU process obeys Sacks Ylvisaker conditions order A similar calculation given Plaskota sec Wiener process note Markovian non stationary We worked generalization error fixed design X However compute Eg n need average Eg X draws X uniform distribution The theory order statistics David eq tells p n n n Taking expectation Eg X turns problem evaluating dimensional integrals e 2p d e e 1p d Exercise asks compute integrals numerically Exercises Consider spline regularizer Sf s c s 2m As noted section strictly power spectrum spline I am proper prior power spectrum eq C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Theoretical Perspectives purposes analysis The equivalent kernel corresponding spline given h x exp 2is x s 2m ds c2n By changing variables integration t 2m s width h x scales n 2m Equation gives form equivalent kernel spline regular izer Show h finite 2m D Hint transform inte gration polar coordinates This observation P Whittle discussion Silverman shows need condition 2m D spline smoothing Computer exercise Space n points evenly interval Take n sample points falls Calculate weight function section corresponding Gaussian process regression particular covariance function noise level plot point x Now compute equivalent kernel cor responding covariance function e g examples section plot axes compare results Hint Recall equivalent kernel defined terms integration eq scaling factor n Hint If wish use large n use ngrid method described section Consider Eg X given eq choose regular grid design X n 2n n n Show Eg X scales O n asymptotically Hint expanding exp 2i sure extend expansion sufficient order Compute numerically expectation Eg X eq random designs OU process example discussed section Make use fact David eq p n n n Investigate scaling behaviour Eg n w r t n C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Approximation Methods Large Datasets As seen preceding chapters significant problem Gaus sian process prediction typically scales O n3 For large problems e g n storing Gram matrix solving associated linear systems prohibitive modern workstations boundary pushed high performance computers An extensive range proposals suggested deal prob lem Below divide parts section consider reduced rank approximations Gram matrix section general strategy greedy approximations described section discuss methods approximating GP regression problem fixed hyperparameters sec tion describe methods approximating GP classification problem fixed hyperparameters section describe methods approximate marginal likelihood derivatives Many methods use subset size m n training examples Reduced rank Approximations Gram Matrix In GP regression problem need invert matrix K 2nI solve linear system K 2nI v y v If matrix K rank q represented form K QQ Q n q matrix matrix inversion speeded matrix inversion lemma eq A QQ 2nIn 2n In 2n Q 2nIq Q Q 1Q Notice inversion n n matrix transformed inversion q q matrix 1For numerical reasons best way solve linear system illustrate savings obtained reduced rank representations C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets In case kernel derived explicit feature expansion N features Gram matrix rank min n N exploitation structure beneficial n N Even kernel non degenerate happen fast decaying eigenspectrum e g section reduced rank approximation accurate If K rank n consider reduced rank approximations K The optimal reduced rank approximation K w r t Frobenius norm eq A UqqU q q diagonal matrix leading q eigenvalues K Uq matrix corresponding orthonormal eigenvectors Golub Van Loan Theorem Unfortunately limited interest practice computing eigendecomposition O n3 operation However suggest cheaply obtain approximate eigendecomposition rise useful reduced rank approximation K We consider selecting subset I n datapoints set I size m n The remaining n m datapoints form set R As mnemonic I included datapoints R remaining points We included set active set Without loss generality assume datapoints ordered set I comes Thus K partitioned K Kmm Km n m K n m m K n m n m The m n block referred Kmn transpose Knm In section saw approximate eigenfunctions kernel Nystro m method We apply idea approximating eigenvalues vectors K We compute eigenvectors eigenvalues Kmm denote m m u m m These extended n points eq n n m m m u n m n m Knmu m m scaling u n chosen u n In general choice approximate eigenvalues vectors include approximation K choosing p K p n u n u n Below set p m obtain K KnmK mmKmn equations Nystro m approximation K Nystro m approximation Computation K takes time O m2n eigendecomposition Kmm O m3 computation u n O mn Fowlkes et al applied Nystro m method approximate eigenvectors computer vision problem matrices question larger size C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Reduced rank Approximations Gram Matrix The Nystro m approximation applied approximate elements K However approximation ith eigenfunction x m m km x u m km x k x x1 k x xm restatement eq current notation m m easy general obtain approximation kernel k x x N ii x x k x x m m m x x m m m m m km x u m u m km x km x K 1mmkm x Clearly eq obtained evaluating eq pairs datapoints training set By multiplying eq Kmn KmmKm n m easy Kmm K mm Km n m K m n m K n m m K n m m K n m n m K n m mK 1mmKm n m The difference K n m n m K n m n m fact Schur complement Kmm Golub Van Loan p It easy K n m n m K n m n m positive semi definite vector f partitioned f f m f n m f Gaussian distribution zero mean covariance K fn m fm Schur complement covariance matrix eq A The Nystro m approximation derived fashion Williams Seeger application kernel machines An alternative view gives rise approximation Smola Scho lkopf Scho lkopf Smola sec Here starting point wish approximate kernel centered point xi linear combination kernels active set k xi x j I cijk xj x k xi x coefficients cij determined optimize approximation A reasonable criterion minimize E C n k xi x k xi x 2H trK tr CKmn tr CKmmC coefficients arranged n m matrix C Minimizing E C w r t C gives Copt KnmK 1mm obtain approximation K KnmK mmKmn agreement eq Also shown E Copt tr K K Smola Scho lkopf suggest greedy algorithm choose points include active set minimize error criterion As takes C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets O mn operations evaluate change E including new dat apoint exercise infeasible consider members set R inclusion iteration instead Smola Scho lkopf suggest find ing best point include randomly chosen subset set R iteration Recent work Drineas Mahoney analyzes similar algorithm Nystro m approximation use biased sampling placement choosing column K probability k2ii pseudoinverse inner m m matrix For algorithm able provide prob abilistic bounds quality approximation Earlier work Frieze et al developed approximation singular value decomposi tion SVD rectangular matrix weighted random subsampling rows columns probabilistic error bounds However differ ent Nystro m approximation Drineas Mahoney sec details Fine Scheinberg suggest alternative low rank approximation K incomplete Cholesky factorization Golub Van Loan sec The idea computing Cholesky de composition K pivots certain threshold skipped If number pivots greater threshold k incomplete Cholesky factorization takes time O nk2 Greedy Approximation Many methods described use active set training points size m selected training set size n m We assume impossible search optimal subset size m combinatorics The points active set selected randomly general expect better performance points selected greedily w r t criterion In statistics literature greedy approaches known forward selection strategies A general recipe greedy approximation given Algorithm The algorithm starts active set I set R containing indices training examples On iteration index selected R added I This achieved evaluating criterion selecting data point optimizes criterion For algorithms expensive evaluate points R working set J R chosen instead usually random R Greedy selection methods subset regressors SR subset datapoints SD projected process PP methods described 2As technical detail symmetric permutations rows columns required stabilize computations C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPR Fixed Hyperparameters input m desired size active set Initialization I R n j m Create working set J R Compute j j J argmaxj J j Update model include data example I I R R end return I Algorithm General framework greedy subset selection j criterion function evaluated data point j Approximations GPR Fixed Hy perparameters We present approximation schemes GPR subset regressors SR Nystro m method subset datapoints SD pro jected process PP approximation Bayesian committee machine BCM iterative solution linear systems Section provides summary methods comparison performance SARCOS data introduced section Subset Regressors Silverman sec showed mean GP predictor ob tained finite dimensional generalized linear regression model f x n ik x xi prior N K To use mean prediction linear regression model feature space given eq e f x 2n x A 1y A 1p n Setting x k x K 1p K obtain f x n k x n K K nI 1Ky k x K nI 1y agreement eq Note predictive co variance model different GPR A simple approximation model consider subset regres sors fSR x m ik x xi m N K 1mm C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets Again eq obtain f SR x km x KmnKnm nKmm 1Kmny V fSR x 2nkm x KmnKnm nKmm 1km x Thus posterior mean m given m KmnKnm nKmm 1Kmny This method proposed example Wahba chapter Poggio Girosi eq regularization framework The subset regressors SR suggested G Wahba The computa tions equations time O m2n carry necessary matrix computations After prediction mean new test point takes time O m predictive variance takes O m2 Under subset regressors model f N K K isSR marginal likelihood defined eq Thus log marginal likelihood model log pSR y X log K 2nIn y K 2nIn 1y n log Notice covariance function defined SR model form k x x k x K 1mmk x exactly Nystro m approximation covariance function eq In fact covariance function k x x predictive mean variance equations replaced systematically k x x obtain equations shown Appendix If kernel function decays zero x fixed x k x x near zero x distant points set I This case kernel stationary k x x independent x Thus expect approximate kernel poor predictions especially underestimates predictive variance x far points set I An interesting idea suggested Rasmussen Quin onero Candela mitigate problem define SR model m basis func tions extra basis function centered test point x ySR x m ik x xi k x x This model predictions implemented efficiently partitioned matrix inverse equations A A The effect extra basis function centered x maintain predictive variance test point So far said subset I chosen One sim ple method choose randomly X run clustering xi ni obtain centres Alternatively number greedy forward selection algorithms I proposed Luo Wahba choose kernel minimize residual sum squares RSS y Knmm optimizing m Smola Bartlett similar approach choose criterion quadratic form 2n y Knm m mKmm m y K 2nIn 1y C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPR Fixed Hyperparameters right hand follows eq matrix inversion lemma Alternatively Quin onero Candela suggests approxi mate log marginal likelihood log pSR y X eq selection cri terion In fact quadratic term eq terms comprising log pSR y X For suggestions complexity evaluating criterion new example O mn making use partitioned matrix equations Thus likely expensive consider points R iteration likely want consider smaller working set described Algorithm Note SR model obtained selecting subset data points size m random greedy manner The relevance vector machine RVM described section similar flavour automatically comparison RVM selects greedy fashion datapoints use expansion However note important difference RVM uses diagonal prior s SR method m N K 1mm The Nystro m Method Williams Seeger suggested approximating GPR equations replacing matrix K K mean variance prediction equations called Nystro m method approximate GPR Notice proposal covariance function k systematically replaced k occurrences matrix K replaced As SR model time complexity O m2n carry necessary matrix computations O n predictive mean test point O mn predictive variance Experimental evidence Williams et al suggests large m SR Nystro m methods similar performance small m Nystro m method poor Also fact k systematically replaced k means embarrassments occur like approximated predictive variance negative For reasons recommend Nystro m method SR method However Nystro m method effective m m th eigenvalue K smaller 2n Subset Datapoints The subset regressors method described approximated form predictive distribution particularly predictive mean Another simple approximation sample GP predictor GP predictor smaller subset size m data Although clearly wasteful data sense predictions obtained m points sufficiently accurate needs Clearly sense select points taken active set I typically achieved greedy algorithms However C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets wary computation needed especially considers member R iteration Lawrence et al suggest choosing point site clusion active set maximizes differential entropy score j H p fj H pnew fj H p fj entropy Gaus sian site j R function variance site j poste rior Gaussian eq A H pnew fj entropy site observation site j included Let posterior variance fj inclusion vj As p fj yI yj p fj yI N yj fj vnewj v 1j Using fact entropy Gaussian variance v log 2ev obtain j log vj j monotonic function vj maximized choosing site largest variance Lawrence et al method informative vector machine IVM IVM If coded na vely complexity computing variance sites R single iteration O m3 n m m2 need evaluate eq site matrix inversion Kmm 2nI O m3 stored However incrementally growing matrices Kmm Km n m fact cost O mn inclusion leading overall complexity O m2n subset size m For example site chosen inclusion matrix Kmm 2nI grown including extra row column The inverse expanded matrix found eq A better practice numerically use Cholesky decomposition approach described Lawrence et al The scheme evaluates j j R step choose inclusion site This makes sense m small gets larger sense select candidate inclusion sites subset R Lawrence et al randomized greedy selection method ideas choose subset The differential entropy score j criterion site selection For example information gain criterion KL pnew fj p fj Seeger et al The use greedy selection heuristics similar problem active learning e g MacKay 1992c Projected Process Approximation The SR method unattractive feature based degenerate GP finite dimensional model given eq The SD method non degenerate process model makes use m datapoints The projected process PP approximation non degenerate process model use n datapoints We projected process approximation represents m n latent function values computes likelihood involving n datapoints projecting m latent points n dimensions C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPR Fixed Hyperparameters One problem basic GPR algorithm fact likelihood term requires f values n training points However represent m values explicitly denote fm Then remain ing f values R denoted fn m conditional distribution p fn m fm mean given E fn m fm K n m mK 1mmfm Say replace true likelihood term points R byN yn m E fn m fm 2nI Including likelihood contribution points set I q y fm N y KnmK 1mmfm nI written q y fm N y E f fm 2nI The key feature absorbed information n points D m points I The form q y fm eq arbitrary fact shown consider minimizing KL q f y p f y KL divergence approximating distribution q f y true posterior p f y q distributions form q f y p f R fm R positive depends fm form obtain See Seeger Lemma sec C detailed derivations Csato sec To predictions compute posterior distribution q fm y Define shorthand P K 1mmKmn E f fm P fm Then q y fm exp 22n y P fm y P fm Combining prior p fm exp f mK 1mmfm obtain q fm y exp f m K mm 2n PP fm 2n y P fm recognized Gaussian N A A 2n nK mm PP 2n K mm nKmm KmnKnm K mm 2n APy Kmm nKmm KmnKnm 1Kmny Thus predictive mean given Eq f x km x K 1mm km x 2nKmm KmnKnm 1Kmny turns predictive mean SR model given eq However predictive variance different The argument eq yields Vq f x k x x km x K 1mmkm x km x K 1mmcov fm y K mmkm x k x x km x K 1mmkm x 2nkm x 2nKmm KmnKnm 1km x 3There priori reason m points chosen subset n points D disjoint training set However derivations consider subset C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets Notice predictive variance sum predictive variance SR model term eq plus k x x km x K 1mmkm x predictive variance x given fm Thus eq smaller SR predictive variance close k x x x far away points set I As SR model takes time O m2n carry necessary matrix computations After prediction mean new test point takes time O m predictive variance takes O m2 We q y fm N y P fm 2nI p fm N Kmm By integrat ing fm find y N K 2nIn Thus marginal likelihood projected process approximation SR model eq Again question choose points set I arises Csato Opper present method training examples presented sequentially line fashion Given current active set I compute novelty new input point large point added I point added R To precise novelty input x computed k x x km x K 1mmk x recognized predictive variance x given non noisy observations points I If active set gets larger preset maximum size points deleted I specified section Csato Opper Later work Csato et al replaced dependence algorithm described input sequence expectation propagation type algorithm section As alternative method selecting active set Seeger et al suggest greedy subset selection method Algorithm Com putation information gain criterion incorporating new site takes O mn expensive use selection criterion However ap proximation information gain computed cheaply Seeger et al eq Seeger sec C details allows greedy subset algorithm run points R iteration Bayesian Committee Machine Tresp introduced Bayesian committee machine BCM way speeding Gaussian process regression Let f vector function val ues test locations Under GPR obtain predictive Gaussian distri bution p f D For BCM split dataset p parts D1 Dp Di Xi yi approximation p y1 yp f X p p yi f Xi Under approximation q f D1 Dp p f p p yi f Xi c p p f Di pp f c normalization constant Using fact terms numerator denomination Gaussian distributions f easy C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPR Fixed Hyperparameters exercise predictive mean covariance f given Eq f D covq f D p cov f Di 1E f Di covq f D p K p cov f Di K covariance matrix evaluated test points Here E f Di cov f Di mean covariance predictions f given Di given eqs Note eq interesting form predictions dataset weighted inverse predictive covariance We free choose partition dataset D This aspects number partitions assignment data points partitions If wish partition size m p n m Tresp random assignment data points partitions Schwaighofer Tresp recommend clustering data e g p means clustering lead improved performance However note compared greedy schemes clustering use target y values inputs x Although possible predictions number test points n slows method involves inversion n n matrices Schwaighofer Tresp recommend making test predictions blocks size m matrices size In case computational complexity BCM O pm3 O m2n predicting m test points O mn test point The BCM approach transductive Vapnik inductive sense method computes test set dependent model making use test set input locations Note wish prediction test point necessary hallucinate extra test points eq generally better approximation number test points increases Iterative Solution Linear Systems One straightforward method speed GP regression note lin ear system K 2nI v y solved iterative method example conjugate gradients CG See Golub Van Loan sec fur ther details CG method Conjugate gradients gives exact solution ignoring round errors run n iterations approxi mate solution terminated earlier k iterations time complexity O kn2 This method suggested Wahba et al context numerical weather prediction Gibbs MacKay con text general GP regression CG methods context C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets Method m SMSE MSLL mean runtime s SD SR PP BCM Table Test results inverse dynamics problem number different methods Ten repetitions mean loss shown standard deviation Laplace GPC linear systems solved repeatedly obtain MAP solution f sections details One way CG method speeded approximate exact matrix vector multiplication For example recent work Yang et al uses improved fast Gauss transform purpose Comparison Approximate GPR Methods Above presented approximation methods GPR Of retain methods scale linearly n iterative solu tion linear systems discounted Also discount Nystro m ap proximation preference SR method leaving alternatives subset regressors SR subset data SD projected process PP Bayesian committee machine BCM Table shows results methods robot arm inverse dy namics problem described section D input variables training examples test examples As section squared exponential covariance function separate length scale pa rameter input dimensions C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPR Fixed Hyperparameters Method Storage Initialization Mean Variance SD O m2 O m3 O m O m2 SR O mn O m2n O m O m2 PP O mn O m2n O m O m2 BCM O mn O mn O mn Table A comparison space time complexity methods random selection subsets Initialization gives time needed carry preliminary matrix computations test point x known Mean resp variance refers time needed compute predictive mean variance x For SD method subset training data size m selected random hyperparameters set optimizing marginal likeli hood subset As ARD involved optimization D hyperparameters This process repeated times giving rise mean standard deviation recorded Table For SR PP BCM meth ods subsets data hyperparameter vectors obtained SD experiments Note m result available BCM gave memory error These experiments conducted GHz twin processor machine GB RAM The code methods written Matlab A summary time complexities methods given Table Thus test set size n mean variance predictions find SD method time complexity O m3 O m2n SR PP methods O m2n O m2n BCM method O mnn Assuming n m reduce O m2n O m2n O mnn respectively These complexities broad agreement timings Table The results Table plotted Figure As expect general trend m increases SMSE MSLL scores decrease Notice worth runs small m obtain learning curve respect m helps getting feeling useful runs large m Both terms SMSE MSLL surprisingly SD inferior methods similar performance These results obtained random selection active set Some experiments carried active selection SD method IVM SR method lead significant improve ments performance For BCM experimented use p means clustering instead random assignment partitions lead significant improvements performance Overall dataset con 4In BCM case hyperparameters data parti tioned randomly blocks size m 5We thank Anton Schwaighofer making BCM code available C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets S M S E m SD SR PP BCM M S LL m SD PP SR BCM b Figure Panel plot SMSE m Panel b shows MSLL methods The error bars denote standard deviation For clarity panels BCM results slightly displaced horizontally w r t SR results clusion fixed m SR PP methods choice BCM longer running times similar performance However notice com pare runtime SD m competitive SR PP BCM results m time performance In experiments hyperparameters methods set optimizing marginal likelihood SD model size m This means direct comparison different methods hyperparam eters subsets However alternatively optimize approximate marginal likelihood method section compare results Notice hyperparameters optimize approximate marginal like lihood depend method For example Figure b shows maximum marginal likelihood occurs shorter length scales data increases This effect observed V Tresp A Schwaighofer pers comm comparing SD marginal likeli hood eq marginal likelihood computed n datapoints eq Schwaighofer Tresp report experimental comparisons tween BCM method approximation methods number synthetic regression problems In experiments optimized ker nel hyperparameters method separately Their results fixed m BCM performs better methods However results depend factors noise level data generating pro cess report pers comm relatively large noise levels BCM longer displays advantage Based evidence currently available unable provide firm recommendations approximation method research required understand factors affect performance C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximations GPC Fixed Hyperparameters Approximations GPC Fixed Hy perparameters The approximation methods GPC similar GPR need deal non Gaussian likelihood Laplace approximation section expectation propagation EP section In section focus mainly binary classification tasks methods extended multi class case For subset regressors SR method use model fSR x m ik x xi m N K mm The likelihood non Gaussian optimization problem find MAP value m convex obtained Newton iteration Using MAP value m Hessian point obtain predictive mean variance f x fed sigmoid function yield probabilistic predictions As usual question choose subset points arises Lin et al select clustering method Zhu Hastie propose forward selection strategy The subset datapoints SD method GPC proposed Lawrence et al EP style approximation posterior differ ential entropy score section select new sites inclusion Note EP approximation lends naturally sparsification sparse model results site precisions eq zero making cor responding likelihood term vanish A computational gain achieved ignoring likelihood terms site precisions small The projected process PP approximation non Gaussian likelihoods Csato Opper present online method examples processed sequentially Csato et al expectation propagation type algorithm multiple sweeps training data permitted The Bayesian committee machine BCM generalized deal non Gaussian likelihoods Tresp As GPR case dataset broken blocks approximate inference carried Laplace approximation block yield approximate predictive mean Eq f Di approximate predictive covariance covq f Di These predictions combined equations Approximating Marginal Likelihood Derivatives We consider approximations GP regression GP classifica tion For GPR SR PP methods rise approximate marginal likelihood given eq For SD method simple C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets approximation ignoring datapoints active set given log pSD ym Xm log Kmm 2I y m Kmm 2I 1ym m2 log ym subvector y corresponding active set eq simply log marginal likelihood model ym N Kmm 2I For BCM simple approach sum eq evaluated partition dataset This ignores interactions partitions Tresp Schwaighofer pers comm suggested sophisti cated BCM based method approximately takes interactions account For GPC SR approximation simply use Laplace EP approximations finite dimensional model For SD ignore datapoints active set compute approximation log p ym Xm Laplace EP For projected process PP method Seeger p suggests following lower bound log p y X log p y f p f df log q f p y f p f q f df q f log p y f p f q f df q f log q y f df KL q f p f n q fi log p yi fi dfi KL q fm p fm q f shorthand q f y eq follows equation previous line Jensen s inequality The KL divergence term readily evaluated eq A dimensional integrals tackled numerical quadrature We aware work extending BCM approximations marginal likelihood GPC Given approximations marginal likelihood mentioned want compute derivatives order optimize Clearly sense active set fixed optimization note clashes fact methods select active set choose different set covariance function parameters change For classification case derivatives complex fact site parameters MAP values f section change changes We seen example section non sparse Laplace approximation Seeger sec describes ex periments comparing SD PP methods optimization marginal likelihood regression classification problems C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Appendix Equivalence SR GPR Using Nystro m Approximate Kernel Appendix Equivalence SR GPR Us ing Nystro m Approximate Kernel In section derived subset regressors predictors mean variance given equations The aim appendix equivalent predictors obtained replacing k x x systematically k x x GPR prediction equations First mean The GPR predictor E f x k x K 2nI 1y Replacing occurrences k x x k x x obtain E f x k x K 2nI 1y km x K 1mmKmn KnmK mmKmn nI 1y 2n km x K 1mmKmn In KnmQ 1Kmn y 2n km x K 1mm I am KmnKnmQ Kmny 2n km x K 1mm 2nKmmQ Kmny km x Q 1Kmny Q 2nKmm KmnKnm agrees eq Equation follows eq use matrix inversion lemma eq A eq follows eq I am 2nKmm KmnKnm Q For predictive variance V f k x x k x K 2nI 1k x km x K 1mmkm x km x K 1mmKmn KnmK mmKmn nI 1KnmK mmkm x km x K 1mmkm x km x Q 1KmnKnmK mmkm x km x I am Q 1KmnKnm K 1mmkm x km x Q 12nKmmK mmkm x 2nkm x Q 1km x agreement eq The step eqs obtained eqs eq follows eq I am 2nKmm KmnKnm Q Exercises Verify mean covariance BCM predictions equations correct If stuck Tresp details Using eq fact Copt KnmK 1mm E Copt tr K K K KnmK 1mmKmn Now consider adding data point set I Kmm grows K m m Using eq A C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Approximation Methods Large Datasets change E adding extra datapoint computed time O mn If need help Scho lkopf Smola sec details C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Chapter Further Issues Conclusions In previous chapters book concentrated giving solid grounding use GPs regression classification problems includ ing model selection issues approximation methods large datasets con nections related models In chapter provide short descriptions issues relating Gaussian process prediction pointers literature reading So far mainly discussed case output target y single label section describe deal case multiple output targets Similarly regression problem focussed d Gaussian noise section relax condition allow noise process correlations The classification problem characterized non Gaussian likelihood function non Gaussian likelihoods interest described section We observations function values derivatives target function In section discuss use infor mation GPR framework Also happen noise observation input variable x section explain handled In section mention flexible models obtained mixtures Gaussian process models As carrying prediction test inputs wish try find global optimum function compact set Approaches based Gaussian processes problem described section The use Gaussian processes evaluate integrals covered section By scale mixture Gaussians construction obtain mul tivariate Student s t distribution This construction extended Student s t process explained section One key aspect Bayesian framework relates incorporation prior knowledge problem C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Further Issues Conclusions formulation In applications dataset D ad ditional information For example optical character recognition problem know translating input pattern pixel change label pattern Approaches incorporating knowledge discussed section In book concentrated supervised learning problems How GPs components unsupervised learning models de scribed section Finally close conclusions outlook future section Multiple Outputs Throughout book concentrated problem predicting single output variable y input x However happen wish predict multiple output variables channels simultaneously For example robot inverse dynamics problem described section seven torques predicted A simple approach model output variable independent treat separately However lose information suboptimal One way correlation occur correlated noise process Even output channels priori independent noise process correlated induce correlations posterior processes Such situation easily handled GP framework considering joint block diagonal prior function values channel Another way correlation multiple channels occur prior structure For example geostatistical situations correlations abundances different ores e g silver lead This situation requires covariance function models correlation structure channel cross correlations channels Some work topic found geostatistics literature cokriging e g Cressie sec One way inducecokriging correlations number output channels obtain linear combinations number latent channels described Teh et al Micchelli Pontil A related approach taken Boyle Frean introduce correlations processes deriving different convolutions underlying white noise process Noise Models Dependencies The noise models far exclusively assumed Gaussianity independence Non Gaussian likelihoods mentioned section Inside family Gaussian noise models difficult model depen coloured noise dencies This particularly useful models involving time We simply add terms noise covariance function desired structure including C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Non Gaussian Likelihoods hyperparameters In fact approach atmospheric carbon dioxide modelling task section Also Murray Smith Girard autoregressive moving average ARMA noise model ARMA eq B GP regression task Non Gaussian Likelihoods Our main focus regression Gaussian noise classification logistic probit response functions However Gaussian processes priors likelihood functions For example Diggle et al concerned modelling count data measured geographically Poisson likelihood spatially varying rate They achieved placing GP prior log Poisson rate Goldberg et al stayed Gaussian noise model introduced heteroscedasticity e allowing noise variance function x This achieved placing GP prior log variance function Neal robustified GP regression Student s t distributed noise model Gaussian noise Chu Ghahramani described use GPs ordinal regression problem given ranked preference information target data Derivative Observations Since differentiation linear operator derivative Gaussian process Gaussian process Thus use GPs predictions derivatives inference based derivative information In general inference based joint Gaussian distribution function values partial derivatives A covariance function k function values implies following mixed covariance function values partial derivatives partial derivatives cov fi fj xdj k xi xj xdj cov fi xdi fj xej 2k xi xj xdi xej e g Papoulis ch Adler sec With n datapoints D dimensions complete joint distribution f D partial derivatives involves n D quantities typical application access interest subset simply remove rows columns joint matrix needed Observed function values derivatives different noise levels incorporated adding diagonal contribution differing hyperparameters Inference predictions usual This approach context learning dynamical systems Solak et al In Figure posterior process derivative observations compared Noise free derivatives useful way enforce known constraints modelling problem C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Further Issues Conclusions input x ou tp ut y x input x ou tp ut y x b Figure In panel data points dimensional noise free regression problem functions sampled posterior confidence region light grey In panel b observations augmented noise free derivative information indicated small tangent segments data points The covariance function squared exponential unit process variance unit length scale Prediction Uncertain Inputs It happen input values prediction problem uncer tain For example discrete time series perform multi step ahead predictions iterating step ahead predictions However step ahead predictions include uncertainty necessary propagate uncertainty forward proper multi step ahead predictions One sim ple approach use sampling methods Alternatively possible use analytical approaches Girard et al showed possible compute mean variance output analytically SE covariance function Gaussian input noise More generally problem regression uncertain inputs studied statistics literature errors variables regres sion See Dellaportas Stephens Bayesian treatment problem pointers literature Mixtures Gaussian Processes In chapter seen ideas making covariance functions flexible Another route use mixture different Gaussian process models local region input space This kind model generally known mixture experts model Jacobs et al In addition local expert models model manager probabilistically assigns points experts Rasmussen Ghahramani Gaussian process models local experts based manager type stochastic process Dirichlet process Inference model required MCMC methods C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Global Optimization Global Optimization Often faced problem able evaluate continuous function g x wishing find global optimum maximum minimum function compact set A RD There large literature problem global optimization Neumaier useful overview Given dataset D xi g xi n appealing approach fit GP regression model data This mean prediction predictive variance x A Jones examines number criteria suggested function evaluation based predictive mean variance One issue approach need search find optimum criterion multimodal optimization problem However evaluations g expensive time consuming sense work hard new optimization problem For historical references work area Jones Ritter sec VIII Evaluation Integrals Another interesting unusual application Gaussian processes evaluation integrals deterministic function f One evaluates function number locations use Gaussian process posterior functions This posterior functions induces posterior value integral possible function posterior rise particular value integral For covariance functions e g squared exponential compute expectation variance value integral analytically It unusual think value integral random particular deterministic value perfectly line Bayesian thinking treat kinds uncertainty probabilities This idea proposed Bayes Hermite quadrature O Hagan later Bayesian Monte Carlo Rasmussen Ghahramani Another approach related ideas global optimization section combining GPs MCMC9 One use GP model function aid MCMC sampling procedure advantageous function interest computa tionally expensive evaluate Rasmussen combines Hybrid Monte Carlo GP model log integrand uses derivatives function discussed section accurate model integrand evaluations C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Further Issues Conclusions Student s t Process A Student s t process obtained applying scale mixture Gaus scale mixture sians construction Student s t distribution Gaussian process O Hagan O Hagan et al We divide covariances scalar gamma distribution shape mean k x x 1k x x p exp k valid covariance function Now joint prior distribution finite number n function values y p y N y 1Ky p d n n 1Ky y K 1y y n recognized zero mean multivariate Student s t distribution degrees freedom p y T 1Ky We state definition analogous definition page Gaussian process write f T P 1K cf eq The marginal likelihood directly evaluated eq training achieved methods discussed chapter hyperparameters The predictive distribution test cases t distributions derivation left exercise Notice construction clear noise free processes interpretation complicated covariance function k x x contains noise contribution The noise signal entangled com noise entanglement mon factor observations longer written sum independent signal noise contributions Allowing independent noise contributions removes analytic tractability reduce usefulness t process Exercise Using scale mixture representation eq derive poste rior predictive distribution Student s t process Exercise Consider generating process implied eq write pro gram draw functions random Characterize difference Student s t process corresponding Gaussian process obtained limit explain t process exciting hoped Invariances It happen input apparently vector form fact additional structure A good example pixelated image d array C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Invariances pixels arranged vector e g raster scan order Imagine image handwritten digit know image translated pixel remain digit Thus knowledge certain invariances input pattern In section describe number ways invariances exploited Our discussion based Scho lkopf Smola ch Prior knowledge problem tells certain transformations input leave class label invariant include simple geometric transformations translations rotations rescalings ob vious ones line thickness transformations Given data possible learn correct input output mapping sense try use known invariances reduce training data needed There ways prior knowledge described The approach generate synthetic training examples applying synthetic training examplesvalid transformations examples This simple disadvantage creating larger training set As kernel machine training algorithms typically scale super linearly n problematic A second approach predictor invariant small transforma tions training case method developed Simard et al tangent prop neural networks tangent prop For single training image consider manifold images generated transformations applied This manifold complex structure locally approximate tangent space The idea tangent prop output invariant perturbations training example tangent space For neural networks straightforward modify training objective function penalize deviations variance Simard et al details Section Scho lkopf Smola describes ways ideas extended kernel machines The approach dealing invariances develop representation invariant representation input invariant transformations For example binary images handwritten digits skeletonized remove effect line thickness If invariant representation achieved transformations desirable difficult impossible achieve For example given training pattern belong class e g ambiguous handwritten digit clearly possible find new representation invariant transformations leaves classes distinguishable 1The digit recognition problem invariant small rotations avoid turning 2i e changing thickness pen write reasonable bounds change digit write C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Further Issues Conclusions Latent Variable Models Our main focus book supervised learning However GPs components models carrying non linear dimen sionality reduction form unsupervised learning The key idea data apparently high dimensional e g pixelated image lie low dimensional non linear manifold wish model Let z RL latent hidden variable let x RD visible variable We suppose visible data generated picking point z space mapping point data space possibly non linear mapping optionally adding noise Thus p x p x z p z dz If mapping z x linear z Gaussian distribution factor analysis model mean covariance Gaussian x space easily determined However mapping non linear integral computed exactly In generative topographicGTM mapping GTM model Bishop et al 1998b integral approximated grid points z space In original GTM paper non linear mapping taken linear combination non linear basis functions Bishop et al 1998a replaced Gaussian process mapping latent visible spaces More recently Lawrence introduced different model known Gaussian process latent variable model GPLVM Instead having aGPLVM prior posterior distribution latent space consider data point xi derived corresponding latent point zi non linear mapping added noise If Gaussian process non linear mapping easily write joint distribution p X Z visible variables conditional latent variables Optimization routines find locations latent points opti mize p X Z This similarities work regularized principal manifolds Scho lkopf Smola ch GPLVM integrates latent visible mapping optimizing Conclusions Future Directions In section briefly wrap threads developed book discuss possible future directions work Gaussian processes In chapter saw Gaussian process regression natural extension Bayesian linear regression flexible class models For Gaussian noise model treated analytically simple GP model considered replacement traditional linear analogue We seen historically numerous ideas lines Gaussian process models gained sporadic following C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Conclusions Future Directions One speculate GPs currently widely applications We major reasons Firstly application Gaussian processes requires handling inversion large matrices While kinds computations tedious years ago impossible past na ve implementations suffice moderate sized problems anno PC Another possibility historical work GPs fixed covariance functions little guide choose functions The choice degree arbitrary idea able infer structure parameters covariance function discuss chapter known This probably important step turning GPs interesting method practitioners The viewpoint placing Gaussian process priors functions Bayesian Although adoption Bayesian methods machine learning community widespread ideas appreciated widely statistics community Although modern computers allow simple implementations thousand training cases computational constraints significant limitation applications datasets significantly larger In chapter given overview recent work approx imations large datasets Although methods lot work currently undertaken theoretical practical aspects approximations need understood better order useful tool practitioner The computations required Gaussian process classification models developed chapter lot involved regression Although theoretical foundations Gaussian process classification developed clear circumstances expect extra work approximations associated treating probabilistic latent variable model pay The answer depend heavily ability learn meaningful covariance functions The incorporation prior knowledge choice parameter ization covariance function prime target future work GPs In chapter presented families covariance functions widely differing properties chapter presented principled methods choosing adapting covariance functions Particularly machine learning community tendency view Gaussian pro cesses black box exactly goes box important long gives good predictions To mind learn thing statisticians ask models work In fact hierarchical formulation covariance functions hyperparameters testing different hypotheses adaptation hyperparameters gives excellent opportunity understand data We attempted illustrate line thinking carbon dioxide prediction example developed length section Although problem comparatively simple easy intuitive understanding principles trying different components covariance structure C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Further Issues Conclusions adapting parameters universally Indeed use isotropic squared exponential covariance function digit classification examples chapter choice expect provide insight classification problem Although results presented good current methods literature argue use squared exponential covariance function task makes little sense low error rate possibly inherently low difficulty task There need develop sensible covariance functions allow incorporation prior knowledge help gain real insight data Going simple vectorial representation input data account structure input domain theme important Examples include invariances described section arising structure images kernels described section encode structured objects strings trees As brief discussion shows current level development Gaussian process models rich principled framework supervised learning fully developed set tools applications We find Gaus sian process framework appealing confident near future important developments theory methodology prac tice We look forward following developments C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Appendix A Mathematical Background A Joint Marginal Conditional Probability Let n discrete continuous random variables y1 yn joint joint probability probability p y1 yn p y short Technically ought distin guish probabilities discrete variables probability densities continuous variables Throughout book commonly use term prob ability refer Let partition variables y groups yA yB A B disjoint sets union set n p y p yA yB Each group contain variables The marginal probability yA given marginal probability p yA p yA yB dyB A The integral replaced sum variables discrete valued Notice set A contains variable marginal probability joint probability referred depends context If joint distribution equal product marginals independence variables said independent dependent The conditional probability function defined conditional probability p yA yB p yA yB p yB A defined p yB meaningful condition impossible event If yA yB independent marginal p yA condi tional p yA yB equal 1One deal general cases density function exist distribution function C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Mathematical Background Using definitions p yA yB p yB yA obtain Bayes Bayes rule theorem p yA yB p yA p yB yA p yB A Since conditional distributions probabilities use conditioning variables For example supervised learning conditions inputs lead e g version Bayes rule additional conditioning X probabilities eq A eq example A Gaussian Identities The multivariate Gaussian Normal distribution joint probability den Gaussian definition sity given p x m D exp x m x m A m mean vector length D symmetric positive definite covariance matrix size D D As shorthand write x N m Let x y jointly Gaussian random vectors x y N x y A C C B N x y A C C B A marginal distribution x conditional distribution x givenconditioning marginalizing y x N x A x y N x CB y y A CB 1C x y N x A 1C y y A A See e g von Mises sec eqs A A The product Gaussians gives un normalized Gaussianproducts N x A N x b B Z 1N x c C A c C A 1a B 1b C A B Notice resulting Gaussian precision inverse variance equal sum precisions mean equal convex sum means weighted precisions The normalizing constant looks like Gaussian b Z D A B exp b A B b A To prove eq A simply write lengthy expressions introducing eq A eq A eq A expand terms inside exp C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml A Matrix Identities verify equality Hint helpful expand C matrix inversion lemma eq A C A B A A A B 1A B B A B 1B To generate samples x N m K arbitrary mean m covariance generating multivariate Gaussian samplesmatrix K scalar Gaussian generator readily available programming environments proceed follows compute Cholesky decomposition known matrix square root L positive def inite symmetric covariance matrix K LL L lower triangular matrix section A Then generate u N I multiple separate calls scalar Gaussian generator Compute x m Lu desired distribution mean m covariance LE uu L LL K independence elements u In practice necessary add small multiple identity matrix I covariance matrix numerical reasons This eigenvalues matrix K decay rapidly section closely related analytical result stabilization Cholesky decomposition fails The effect generated samples add additional independent noise variance From context usually chosen inconsequential effects samples ensuring numerical stability A Matrix Identities The matrix inversion lemma known Woodbury Sherman Morri matrix inversion lemma son formula e g Press et al p states Z UWV Z Z 1U W V Z 1U 1V Z A assuming relevant inverses exist Here Z n n W ism m U V size n m consequently Z known low rank e m n perturbation Z left hand eq A considerable speedup achieved A similar equation exists determinants determinants Z UWV Z W W V Z 1U A Let invertible n n matrix A inverse A partitioned inversion partitioned matrix A P Q R S A P Q R S A P P n1 n1 matrices S S n2 n2 matrices n n1 n2 The submatrices A given Press et al p P P P 1QMRP Q P 1QM R MRP S M M S RP 1Q A C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Mathematical Background equivalently P N Q NQS R S 1RN S S S 1RNQS N P QS 1R A A Matrix Derivatives Derivatives elements inverse matrix derivative inverse K K K K A K matrix elementwise derivatives For log determinant aderivative log determinant positive definite symmetric matrix log K tr K K A A Matrix Norms The Frobenius norm A F n1 n2 matrix A defined A 2F n1 n2 j aij tr AA A Golub Van Loan p A Cholesky Decomposition The Cholesky decomposition symmetric positive definite matrix A decom poses A product lower triangular matrix L transpose LL A A L called Cholesky factor The Cholesky decomposition useful solving linear systems symmetric positive definite coefficient matrix A To solve Ax b x solve triangular system Ly b forwardsolving linear systems substitution triangular system L x y substitution Using backslash operator write solution x L L b notation A b vector x solves Ax b Both forward backward substitution steps require n2 operations A size n n computational cost The computation Cholesky factor L considered numerically extremely stable takes time n3 method choice applied C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml A Entropy Kullback Leibler Divergence Note determinant positive definite symmetric matrix determinant calculated efficiently A n L2ii log A n logLii A L Cholesky factor A A Entropy Kullback Leibler Divergence The entropy H p x distribution p x non negative measure entropy uncertainty distribution defined H p x p x log p x dx A The integral substituted sum discrete variables Entropy measured bits log base nats case natural log The entropy Gaussian D dimensions measured nats H N log D log 2e A The Kullback Leibler KL divergence relative entropy KL p q tween distributions p x q x defined KL p q p x log p x q x dx A It easy KL p q equality p q For case Bernoulli random variables p q reduces divergence Bernoulli random variables KLBer p q p log p q p log p q A use p q parameter Bernoulli distributions For Gaussian distributions N N divergence Gaussians Kullback sec KL N0 N1 log tr A Consider general distribution p x RD Gaussian distribution q x minimizing KL p q divergence leads moment matching N Then KL p q x x p x dx log D log p x log p x dx A C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Mathematical Background Equation A minimized w r t differentiating w r t parameters setting resulting expressions zero The optimal q matches second moments p The KL divergence viewed extra number nats needed average code data generated source p x distribution q x opposed p x A Limits The limit rational quadratic squared exponential lim x2 exp x2 A A Measure Integration Here sketch definitions concerning measure integration fuller treatments found e g Doob Bartle Let set possible outcomes experiment For example D dimensional real valued variable RD Let F field subsets contains events occurrences interested Then countably additive measure real non negative mutually disjoint sets A1 A2 F Ai Ai A If called finite measure calledfinite measure probability measure The Lebesgue measure defines uniform measure overprobability measure Lebesgue measure subsets Euclidean space Here appropriate algebra Borel algebra BD B algebra generated open subsets R For example line R Lebesgue measure interval b b We restrict RD wish meaning integration function f RD R respect measure f x d x A We assume f measurable e Borel measurable set A R f A BD There cases interest Lebesgue measure ii probability measure For case expression A reduces usual integral notation f x dx 2The restriction field subsets important technically avoid paradoxes Banach Tarski paradox Informally think field restricting consideration reasonable subsets C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml A Fourier Transforms For probability measure x non negative function p x called density measure A BD A A p x dx A If density exists uniquely determined e sets measure zero Not probability measures densities distributions assign zero probability individual points x space densities If p x exists f x d x f x p x dx A If density expression A meaning standard construction Lebesgue integral For RD probability measure related distribution function F RD defined F z x1 z1 xD zD The distribution function general density defined given probability measure A simple example random variable point mass example distribution function density obtained following construction coin tossed probability p comes heads comes heads x chosen U uniform distribution probability p x set This distribution point mass atom x A Lp Spaces Let measure input set X For function f X R p define f Lp X f x p d x p A integral exists For p define f L X ess sup x X f x A ess sup denotes essential supremum e smallest number upper bounds f x The function space Lp X defined p p space functions f Lp X A Fourier Transforms For sufficiently behaved functions RD f x f s e2is x ds f s f x e 2is x dx A 3A measure density absolutely continuous respect Lebesgue measure RD e set Lebesgue measure zero measure zero C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Mathematical Background f s called Fourier transform f x e g Bracewell We refer equation left synthesis equation equation right analysis equation There conventions Fourier transforms particularly involving 2s However tends de stroy symmetry analysis synthesis equations use definitions given Here defined Fourier transforms f x function RD For related transforms periodic functions functions defined integer lattice regular N polygon section B A Convexity Below state definitions properties convex sets functions taken Boyd Vandenberghe A set C convex line segment points C lies C convex sets e x1 x2 C x1 x2 C A A function f X R convex domain X convex set allconvex function x1 x2 X f x1 x2 f x1 f x2 A X possibly improper subset RD f concave f convex A function f convex domain X convex set Hessian positive semidefinite x X C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Appendix B Gaussian Markov Processes Particularly index set stochastic process dimensional real line discretization integer lattice interesting investigate properties Gaussian Markov processes GMPs In Appendix use X t define stochastic process continuous time pa rameter t In discrete time case process denoted X X0 X1 etc We assume process zero mean stated stationary A discrete time autoregressive AR process order p written AR process Xt p k akXt k b0Zt B Zt N Zt s d Notice order p Markov property given history Xt Xt Xt depends previous p X s This relationship conveniently expressed graphical model AR process illustrated Figure B The autoregressive stems fact Xt predicted p previous X s regression equation If stores current X p previous values state vector AR p scalar process written equivalently vector AR process Figure B Graphical model illustrating AR process Moving discrete time continuous time setting question arises generalize Markov notion discrete time AR process define continuoous time AR process It turns correct generalization uses idea having function value p derivatives time t giving rise stochastic differential equation SDE SDE stochastic differential equation C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes apX p t ap 1X p t a0X t b0Z t B X t denotes ith derivative X t Z t white Gaus sian noise process covariance t t This white noise process considered derivative Wiener process To avoid redundancy coefficients assume ap A considerable mathemati cal machinery required rigorous meaning equations e g ksendal As discrete time case write eq B order vector SDE defining state X t p derivatives We begin chapter summary Fourier analysis results section B Fourier analysis important linear time invariant systems equations B B e2ist eigenfunction corre sponding difference resp differential operator We section B discuss continuous time Gaussian Markov processes real line relationship SDE circle In section B describe discrete time Gaussian Markov processes integer lattice lationship difference equation circle In section B explain relationship discrete time GMPs discrete sampling continuous time GMPs Finally section B discuss generalizations Markov concept higher dimensions Much material standard relevant results scattered different sources aim provide unified treatment The relationship tween second order properties SDEs real line circle difference equations integer lattice regular polygon knowledge novel B Fourier Analysis We follow treatment given Kammler We consider Fourier analysis functions real line R periodic functions period l circle Tl functions defined integer lattice Z functions PN regular N polygon discretization Tl For sufficiently behaved functions R f x f s e2isx ds f s f x e 2isx dx B We refer equation left synthesis equation equation right analysis equation For functions Tl obtain Fourier series representations f x k f k e2ikx l f k l l f x e 2ikx l dx B 1The ak coefficients equations B B intended close relation ship An approximate relationship established use finite difference approximations derivatives C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B Fourier Analysis f k denotes coefficient e2ikx l expansion We use square brackets denote argument discrete Xt X t equivalent notations Similarly Z obtain f n l f s e2isn l ds f s l n f n e 2isn l B Note f s periodic period l defined s l avoid aliasing Often transform defined special case l general case emphasizes duality equations B B Finally functions PN discrete Fourier transform f n N k f k e2ikn N f k N N n f n e 2ikn N B Note conventions Fourier transforms particularly involving 2s However tends destroy symmetry analysis synthesis equations use definitions given In case stochastic processes important Fourier relationship covariance function power spectrum known Wiener Khintchine theorem e g Chatfield B Sampling Periodization We obtain relationships functions transforms R Tl Z PN notions sampling periodization Definition B h sampling Given function f R spacing parameter h sampling h construct corresponding discrete function Z n f nh n Z B Similarly discretize function defined Tl PN case h l N N steps size h equal period l Definition B Periodization summation Let f x function R periodization summationrapidly approaches x We sum translates function produce l periodic function g x m f x ml B l Analogously defined Z n rapidly approaches n construct function PN N summation setting n m n mN B C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes Let n obtained h sampling f x corresponding Fourier transforms s f s Then n f nh f s e2isnh ds B n l s e2isn l ds B By breaking domain integration eq B obtain n m m l ml f s e2isnh ds B m l f s ml e2inh s ml ds B change variable s s ml Now set hl use e2inm n m integers obtain n l m f s ml e2isn l ds B implies s m f s ml B l h Alternatively setting l obtains s h m f s m h Similarly f defined Tl n f nlN obtained sampling n m f n mN B Thus sampling x space causes periodization Fourier space Now consider periodization function f x x R l periodic function g x m f x ml Let g k Fourier coefficients g x We obtain g k l l g x e 2ikx l dx l l m f x ml e 2ikx l dx B l f x e 2ikx l dx l f k l B assuming f x sufficiently behaved summation inte gration operations exchanged A similar relationship obtained periodization function defined Z Thus periodization x space gives rise sampling Fourier space C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B Continuous time Gaussian Markov Processes B Continuous time Gaussian Markov Processes We consider continuous time Gaussian Markov processes real line relate covariance function obtained stationary solution SDE circle Our treatment continuous time GMPs R follows Papoulis ch B Continuous time GMPs R We wish find power spectrum covariance function stationary process corresponding SDE given eq B Recall covariance function stationary process k t power spectrum S s form Fourier transform pair The Fourier transform stochastic process X t stochastic process X s given X s X t e 2ist dt X t X s e2ist ds B integrals interpreted mean square limit Let denote complex conjugation denote expectation respect stochastic process Then stationary Gaussian process X s1 X s2 X t X t e 2is1te2is2t dt dt B dt e 2i s1 s2 t dk e 2is1 B S s1 s1 s2 B change variables t t integral representation delta function e 2istdt s This shows X s1 X s2 uncorrelated s1 s2 e Fourier basis eigenfunctions differential operator Also eq B obtain X k t 2is kX s e2ist ds B Now Fourier transform eq B obtain p k ak 2is kX s b0Z s B Z s denotes Fourier transform white noise Taking product equation B complex conjugate taking expectations obtain p k ak 2is1 k p k ak 2is2 k X s1 X s2 b20 Z s1 Z s2 B C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes Let A z p k akz k Then eq B fact power spectrum white noise obtain SR s b20 A 2is B Note denominator polynomial order p s2 The relationship stationary solutions pth order SDEs rational spectral densities traced far Doob Above assumed process stationary However de pends coefficients a0 ap To analyze issue assume solution form Xt et driving term b0 This leads condition stationarity roots polynomial p k ak k lie left half plane Arato p Example AR process In case SDEAR process X t a0X t b0Z t B a0 stationarity This gives rise power spectrum S s b20 2is a0 2is a0 b20 2s a20 B Taking Fourier transform obtain k t b20 2a0 e a0 t B This process known Ornstein Uhlenbeck OU process Uhlenbeck Ornstein introduced mathematical model velocity particle undergoing Brownian motion It shown OU process unique stationary order Gaussian Markov process Example AR p process In general covariance transform correspondingAR p process power spectrum S s p k ak 2is k p k ak 2is k complicated For example Papoulis p gives forms covariance function AR process depending a21 4a0 greater equal However coefficients a0 a1 ap chosen particular way obtain S s 42s2 p B It shown Stein p corresponding covari ance function form p k k t ke t coefficients p For p seen k t e t OU process For p obtain k t e t t These special cases Mate rn class covariance functions described section C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B Continuous time Gaussian Markov Processes Example Wiener process Although derivations focussed stationary Wiener process Gaussian Markov processes important non stationary processes One important Wiener process satisfies SDE X t Z t t initial condition X This process covariance function k t s min t s An interesting variant Wiener process known Brownian bridge tied Wiener process obtained conditioning Wiener process passing X This covariance k t s min t s st s t See e g Grimmett Stirzaker information processes Markov processes derived SDEs order p p times MS differen tiable This easy heuristically eq B given process gets rougher times differentiated eq B tells X p t like white noise process e MS continuous So example OU process Wiener process MS continuous MS differentiable B The Solution Corresponding SDE Cir cle The analogous analysis real line carried Tl X t n X n e2int l X n l l X t e 2int ldt B As X t assumed stationary obtain analogous result eq B e Fourier coefficients independent X m X n S n m n B Similarly covariance function cirle given k t s X t X s n S n e 2in t s l Let l l Then plugging expression X k t n inl kX n einlt SDE eq B equating terms n obtain p k ak inl kX n b0Z n B As real line case form product equation B complex conjugate expectations ST n b20 A inl B Note ST n equal SR n l e sampling SR intervals l SR s power spectrum continuous process real line given equation B Let kT h denote covariance function C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes circle kR h denote covariance function real line SDE Then eq B find kT t m kR t ml B Example 1st order SDE On R OU process kR t b20 2a0 e a0 t 1st order SDE By summing series geometric progressions obtain kT t b20 2a0 e a0l e a0 t e a0 l t b20 2a0 cosh a0 l2 t sinh a0l B l t l Eq B given scaling factors Grenander et al eq obtained limiting argument discrete time GMP Pn section B B Discrete time Gaussian Markov Processes We consider discrete time Gaussian Markov processes Z late covariance function obtained stationary solution difference equation PN Chatfield Diggle provide good coverage discrete time ARMA models Z B Discrete time GMPs Z Assuming process stationary covariance function k denotes XtXt t Z Note stationarity k k We use Fourier approach derive power spectrum covariance function AR p process Defining a0 rewrite eq B p k akXt k b0Zt The Fourier pair X t X t l X s e2ist l ds X s l t X t e 2ist l B Plugging p k akXt k b0Zt obtain X s p k ake ilsk b0Z s B l l As taking product eq B complex conjugate taking expectations obtain SZ s b20 A eils B C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B Discrete time Gaussian Markov Processes Above assumed process stationary However de pends coefficients a0 ap To analyze issue assume solution form Xt zt driving term b0 This leads condition stationarity roots polynomial p k akz p k lie inside unit circle See Hannan Theorem p details As deriving covariance function Fourier transform power spectrum obtained solving set linear equations Our observation Xs independent Zt s t Multiplying equation B Zt taking expectations obtain XtZt b0 Xt iZt By multiplying equation B Xt j j taking expectations obtain Yule Walker equations Yule Walker equations k p aik b B k j p aik j j B The p equations form linear system solve k k p terms b0 a1 ap eq B obtain k j j p recursively Example AR process The simplest example AR process AR AR process process defined Xt a1Xt b0Zt This gives rise Yule Walker equations k a1k b20 k a1k B The linear system k k easily solved k j j X 2X b a21 variance process Notice process stationary require a1 The corresponding power spectrum obtained equation B S s b20 2a1 cos ls a21 B Similarly continuous case covariance function discrete time AR process different forms depending a21 4a2 These described Diggle Example B The Solution Corresponding Difference Equa tion PN We consider variables X X0 X1 XN arranged circle N p By appropriately modifying eq B obtain Xt p k akXmod t k N b0Zt B C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes The Zt s d N Thus Z Z0 Z1 ZN density p Z exp N t Z t Equation B shows X Z related linear transformation p X exp 2b20 N t Xt p k akXmod t k N B This N dimensional multivariate Gaussian For AR p process inverse covariance matrix circulant structure Davis consisting diagonal band 2p entries wide appropriate circulant entries corners Thus p Xt X Xt p Xt Xmod t N Xmod t p N Xmod t N Xmod t p N Geman Geman sided Markov property Notice zeros inverse covariance matrix indicate conditional independence structure section B The properties eq B studied number authors e g Whittle circulant processes Kashyap Chel lappa circular autoregressive models Grenander et al cyclic Markov process As define Fourier transform pair X n N m X m e2inm N X m N N n X n e 2inm N B By similar arguments obtain p k akX m e 2im N k b0Z m B a0 SP m b20 A e2im N B As continuous time case SP m obtained sampling power spectrum corresponding process line SP m SZ ml N Thus eq B kP n m kZ n mN B Example AR process For process Xt a1Xmod t n b0Zt theAR process diagonal entries inverse covariance a21 b non zero diagonal entries a1 b20 By summing covariance function kZ n 2Xa n obtain kP n 2X aN1 n N n n N B C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B The Relationship Between Discrete time Sampled Continuous time GMPs We illustrate result N In case covariance matrix diagonal entries X a31 a31 diagonal entries 2X a31 a1 a21 The inverse covariance matrix structure described Multiplying matrices obtain identity matrix B The Relationship Between Discrete time Sampled Continuous time GMPs We consider relationship continuous time discrete time GMPs In particular ask question regular sampling continuous time AR p process discrete time AR p process It turns answer general negative First define generalization AR processes known autoregressive moving average ARMA processes ARMA processes The AR p process defined special case general ARMA p q process defined Xt p aiXt q j bjZt j B Observe AR p process fact ARMA p process A spectral analysis equation B similar performed section B gives S s B eils A eils B B z q j bjz j In continuous time process rational spectral density form S s B 2is A 2is B known ARMA p q process For define valid covariance function require q p k S s ds Discrete time observation continuous time process Let X t continuous time process having covariance function k t power spectrum S s Let Xh discrete time process obtained sampling X t interval h Xh n X nh n Z Clearly covariance function process given kh n k nh By eq B means Sh s m S s m h B Sh s defined l h C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Gaussian Markov Processes Theorem B Let X continuous time stationary Gaussian process Xh discretization process If X ARMA process Xh ARMA process However X AR process Xh necessarily AR process The proof given Ihara Theorem It easy covariance functions given sections B B discretization continuous time AR process discrete time AR process However Ihara shows general discretization continuous time AR process discrete time AR process B Markov Processes Higher Dimensions We concentrated case t dimensional In higher dimensions interesting ask Markov property general ized Let S infinitely differentiable closed surface separating RD bounded S unbounded S Loosely speaking2 random field X t said quasi Markovian X t t S X u u S independent given X s s S Wong showed isotropic quasi Markov Gaussian field continuous covariance function degen erate case X t X X Gaussian variate However instead conditioning values field takes S conditions somewhat larger set Gaussian random fields non trivial Markov type structure obtained For example random fields inverse power spectrum form k ak1 kDs k1 s kd d k D j kj 2p C s s p k 2p ak1 kDs k1 s kd D C said pseudo Markovian order p For example D dimensional tensor product OU process k t D e ti pseudo Markovian order D For discussion Markov properties random fields Appendix Adler If instead RD wish define Markov random field MRF graph ical structure example lattice ZD things straightforward We follow presentation Jordan Let G X E graph X set nodes correspondence set ran dom variables E set undirected edges graph Let C set maximal cliques G A potential function C xC function possible realizations xC maximal clique XC Potential functions assumed strictly positive real valued functions The probability distribution p x corresponding Markov random field given p x Z C C C xC B Z normalization factor known statistical physics partition function obtained summing integrating C C C xC possible 2For precise formulation definition involving fields Adler p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml B Markov Processes Higher Dimensions signments values nodes X Under definition easy local Markov property holds e variable x conditional distribution x given variables X depends variables neighbours x A useful reference Markov random fields Winkler A simple example Gaussian Markov random field form p x exp x2i j j N xi xj B N denotes set neighbours node xi On Z2 choose connected neighbourhood e nodes north south east west given node C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Appendix C Datasets Code The datasets experiments book implementations algorithms presented available download website book http www GaussianProcess org gpml The programs short stand implementations larger package They meant simple understand modify desired purpose Some programs allow specification covariance functions selection provided link user defined covariance code For plots code provided produces similar plot convenient way conveying details C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Abrahamsen P A Review Gaussian Random Fields Correlation Functions Tech nical Report Norwegian Computing Center Oslo Norway http publications nr Rapport pdf p Abramowitz M Stegun I A Handbook Mathematical Functions Dover New York pp Adams R Sobolev Spaces Academic Press New York p Adler R J The Geometry Random Fields Wiley Chichester pp Amari S Differential Geometrical Methods Statistics Springer Verlag Berlin p Ansley C F Kohn R Estimation Filtering Smoothing State Space Models Incompletely Specified Initial Conditions Annals Statistics p Arato M Linear Stochastic Systems Constant Coefficients Springer Verlag Berlin Lecture Notes Control Information Sciences p Arfken G Mathematical Methods Physicists Academic Press San Diego pp xv Aronszajn N Theory Reproducing Kernels Trans Amer Math Soc pp Bach F R Jordan M I Kernel Independent Component Analysis Journal Machine Learning Research p Baker C T H The Numerical Treatment Integral Equations Clarendon Press Oxford pp Barber D Saad D Does Extra Knowledge Necessarily Improve Generalisation Neural Computation p Bartle R G The Elements Integration Lebesgue Measure Wiley New York p Bartlett P L Jordan M I McAuliffe J D Convexity Classification Risk Bounds Technical Report Department Statistics University California Berkeley Available http www stat berkeley edu tech reports pdf Accepted publication Journal American Statistical Association p Berger J O Statistical Decision Theory Bayesian Analysis Springer New York Second edition pp C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Bishop C M Neural Networks Pattern Recognition Clarendon Press Oxford p Bishop C M Svensen M Williams C K I 1998a Developments Generative Topographic Mapping Neurocomputing p Bishop C M Svensen M Williams C K I 1998b GTM The Generative Topographic Mapping Neural Computation p Blake I F Lindsey W C Level Crossing Problems Random Processes IEEE Trans Information Theory p Blight B J N Ott L A Bayesian approach model inadequacy polynomial regression Biometrika p Boyd S Vandenberghe L Convex Optimization Cambridge University Press Cambridge UK p Boyle P Frean M Dependent Gaussian Processes In Saul L K Weiss Y Bottou L editors Advances Neural Information Processing Systems pages MIT Press p Bracewell R N The Fourier Transform It is Applications McGraw Hill Singapore inter national edition pp Caruana R Multitask Learning Machine Learning p Chatfield C The Analysis Time Series An Introduction Chapman Hall London 4th edition pp Choi T Schervish M J Posterior Consistency Nonparametric Regression Prob lems Under Gaussian Process Priors Technical Report Department Statistics CMU http www stat cmu edu tr tr809 tr809 html p Choudhuri N Ghosal S Roy A Nonparametric Binary Regression Using Gaussian Process Prior Unpublished http www4 stat ncsu edu sghosal papers html p Chu W Ghahramani Z Gaussian Processes Ordinal Regression Journal Machine Learning Research p Collins M Duffy N Convolution Kernels Natural Language In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems MIT Press p Collobert R Bengio S SVMTorch Support Vector Machines Large Scale Re gression Problems Journal Machine Learning Research http www idiap ch bengio projects SVMTorch html pp Cornford D Nabney I T Williams C K I Modelling Frontal Discontinuities Wind Fields Journal Nonparameteric Statsitics p Cox D D Multivariate Smoothing Spline Functions SIAM Journal Numerical Analysis p Cox D D O Sullivan F Asymptotic Analysis Penalized Likelihood Related Esti mators Annals Statistics p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Craven P Wahba G Smoothing Noisy Data Spline Functions Numer Math p Cressie N A C Statistics Spatial Data Wiley New York pp Cristianini N Shawe Taylor J An Introduction Support Vector Machines Cambridge University Press p Cristianini N Shawe Taylor J Elisseeff A Kandola J On Kernel Target Alignment In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems MIT Press p Csato L Gaussian Processes Iterative Sparse Approximations PhD thesis Aston University UK p Csato L Opper M Sparse On Line Gaussian Processes Neural Computation pp Csato L Opper M Winther O TAP Gibbs Free Energy Belief Propagation Spar sity In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems pages MIT Press pp Daley R Atmospheric Data Analysis Cambridge University Press Cambridge UK p David H A Order Statistics Wiley New York pp Davis P J Circulant Matrices Wiley New York p Dawid A P Properties Diagnostic Data Distributions Biometrics p Dellaportas P Stephens D A Bayesian Analysis Errors Variables Regression Models Biometrics p Devroye L Gyo rfi L Lugosi G A Probabilistic Theory Pattern Recognition Springer New York pp Diaconis P Freedman D On Consistency Bayes Estimates Annals Statistics p Diggle P J Time Series A Biostatistical Introduction Clarendon Press Oxford pp Diggle P J Tawn J A Moyeed R A Model based Geostatistics discussion Applied Statistics p Doob J L The Elementary Gaussian Processes Annals Mathematical Statistics p Doob J L Measure Theory Springer Verlag New York p Drineas P Mahoney M W On Nystro m Method Approximating Gram Ma trix Improved Kernel Based Learning Technical Report YALEU DCS TR Yale University http cs www cs yale edu homes mmahoney p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Duchon J Splines Minimizing Rotation Invariant Semi norms Sobolev Spaces In Schempp W Zeller K editors Constructive Theory Functions Several Variables pages Springer Verlag p Duda R O Hart P E Pattern Classification Scene Analysis John Wiley New York p Edgeworth F Y On Observations Relating Several Quantities Hermathena p Faul A C Tipping M E Analysis Sparse Bayesian Learning In Dietterich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems pages Cambridge Massachussetts MIT Press p Feldman J Equivalence Perpendicularity Gaussian Processes Pacific J Math Erratum Pacific J Math p Ferrari Trecate G Williams C K I Opper M Finite dimensional Approximation Gaussian Processes In Kearns M S Solla S A Cohn D A editors Advances Neural Information Processing Systems pages MIT Press p Fine S Scheinberg K Efficient SVM Training Using Low Rank Kernel Representations Journal Machine Learning Research pp Fowlkes C Belongie S Malik J Efficient Spatiotemporal Grouping Using Nystro m Method In Proceedings IEEE Conference Computer Vision Pattern Recognition CVPR p Freedman D On Bernstein Von Mises Theorem Infinite Dimensional Parameters Annals Statistics p Frieze A Kannan R Vempala S Fast Monte Carlo Algorithms Finding Low Rank Approximations In 39th Conference Foundations Computer Science pages p Geisser S Eddy W F A Predictive Approach Model Selection Journal Americal Statistical Association p Geman S Geman D Stochastic Relaxation Gibbs Distributions Bayesian Restora tion Images IEEE Trans Pattern Analysis Machine Intellligence p Gibbs M N Bayesian Gaussian Processes Regression Classification PhD thesis Department Physics University Cambridge p Gibbs M N MacKay D J C Efficient Implementation Gaussian Processes Unpub lished manuscript Cavendish Laboratory Cambridge UK http www inference phy cam ac uk mackay BayesGP html p Gibbs M N MacKay D J C Variational Gaussian Process Classifiers IEEE Transactions Neural Networks p Gihman I I Skorohod A V The Theory Stochastic Processes volume Springer Verlag Berlin p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Girard A Rasmussen C E Quin onero Candela J Murray Smith R Gaussian Process Priors With Uncertain Inputs Application Multiple Step Ahead Time Series Forecasting In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press p Girosi F Models Noise Robust Estimates Technical Report AI Memo MIT AI Laboratory p Girosi F Jones M Poggio T Regularization Theory Neural Networks Architectures Neural Computation p Goldberg P W Williams C K I Bishop C M Regression Input dependent Noise A Gaussian Process Treatment In Jordan M I Kearns M J Solla S A editors Advances Neural Information Processing Systems MIT Press Cambridge MA p Golub G H Van Loan C F Matrix Computations Johns Hopkins University Press Baltimore Second edition pp Gradshteyn I S Ryzhik I M Tables Integrals Series Products Academic Press Corrected enlarged edition prepared A Jeffrey pp Green P J Silverman B W Nonparametric Regression Generalized Linear Models Chapman Hall London p Grenander U Chow Y Keenan D M Hands A Pattern Theoretic Study Biological Shapes Springer Verlag New York pp Grimmett G R Stirzaker D R Probability Random Processes Oxford University Press Oxford England second edition pp Gru nwald P D Langford J Suboptimal Behaviour Bayes MDL Classification Under Misspecification In Proc Seventeenth Annual Conference Computational Learning Theory COLT p Gyo rfi L Kohler M Krzyz ak A Walk H A Distribution Free Theory Nonparametric Regression Springer New York p Hajek J On Property Normal Distributions Any Stochastic Process In Russian Czechoslovak Math J Translated Selected Trans Math Statist Probab Also available Collected Works Jaroslav Hajek eds M Hus kova R Beran V Dupac Wiley p Hand D J Mannila H Smyth P Principles Data Mining MIT Press p Hannan E J Multiple Time Series Wiley New York p Hansen L K Liisberg C Salamon P The Error Reject Tradeoff Open Sys Information Dyn p Hastie T J Tibshirani R J Generalized Additive Models Chapman Hall pp Haussler D Convolution Kernels Discrete Structures Technical Report UCSC CRL Dept Computer Science University California Santa Cruz p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Hawkins D L Some Practical Problems Implementing Certain Sieve Estimator Gaussian Mean Function Communications Statistics Simulation Computation p Hoerl A E Kennard R W Ridge Regression Biased Estimation Nonorthogonal Problems Technometrics p Hornik K Some New Results Neural Network Approximation Neural Networks p Ihara S Information Theory Continuous Systems World Scientific Singapore p Jaakkola T S Diekhans M Haussler D A Discriminative Framework Detecting Remote Protein Homologies Journal Computational Biology pp Jaakkola T S Haussler D Probabilistic Kernel Regression Models In Heckerman D Whittaker J editors Workshop Artificial Intelligence Statistics Morgan Kaufmann p Jacobs R A Jordan M I Nowlan S J Hinton G E Adaptive Mixtures Local Experts Neural Computation p Johnson N L Kotz S Balakrishnan N Continuous Univariate Distributions volume John Wiley Sons New York second edition p Jones D R A Taxonomy Global Optimization Methods Based Response Surfaces J Global Optimization p Jordan M I An Introduction Probabilistic Graphical Models Draft book p Journel A G Huijbregts C J Mining Geostatistics Academic Press p Kailath T RKHS Approach Detection Estimation Problems Part I Deterministic Signals Gaussian Noise IEEE Trans Information Theory p Kammler D W A First Course Fourier Analysis Prentice Hall Upper Saddle River NJ p Kashyap R L Chellappa R Stochastic Models Closed Boundary Analysis Represen tation Reconstruction IEEE Trans Information Theory p Keeling C D Whorf T P Atmospheric CO2 Records Sites SIO Air Sampling Network In Trends A Compendium Data Global Change Carbon Dioxide Information Analysis Center Oak Ridge National Laboratory Oak Ridge Tenn U S A p Kent J T Mardia K V The Link Between Kriging Thin plate Splines In Kelly F P editor Probability Statsitics Optimization pages Wiley p Kimeldorf G Wahba G A Correspondence Bayesian Estimation Stochastic Processes Smoothing Splines Annals Mathematical Statistics p Kimeldorf G Wahba G Some Results Tchebycheffian Spline Functions J Mathematical Analysis Applications p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Kohn R Ansley C F A New Algorithm Spline Smoothing based Smoothing Stochastic Process SIAM J Sci Stat Comput p Kolmogorov A N Interpolation und Extrapolation von stationa ren zufa ligen Folgen Izv Akad Nauk SSSR p Ko nig H Eigenvalue Distribution Compact Operators Birkha user p Kullback S Information Theory Statistics Dover New York pp Kuss M Rasmussen C E Assessing Approximations Gaussian Process Classification In Weiss Y Scho lkopf B Platt J editors Advances Neural Information Processing Systems MIT Press pp Lanckriet G R G Cristianini N Bartlett P L El Ghaoui L Jordan M I Learning Kernel Matrix Semidefinite Programming Journal Machine Learning Research p Lauritzen S L Time Series Analysis A Discussion Contributions Made T N Thiele International Statistical Review p Lawrence N Gaussian Process Latent Variable Models Visualization High Dimensional Data In Thrun S Saul L Scho lkopf B editors Advances Neural Information Processing Systems pages MIT Press p Lawrence N Seeger M Herbrich R Fast Sparse Gaussian Process Methods The Infor mative Vector Machine In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems pages MIT Press pp Leslie C Eskin E Weston J Stafford Noble W Mismatch String Kernels SVM Protein Classification In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press pp Lin X Wahba G Xiang D Gao F Klein R Klein B Smoothing Spline ANOVA Models Large Data Sets With Bernoulli Observations Randomized GACV Annals Statistics p Lindley D V Making Decisions John Wiley Sons London UK second edition p Lodhi H Shawe Taylor J Cristianini N Watkins C J C H Text Classification String Kernels In Leen T K Diettrich T G Tresp V editors Advances Neural Information Processing Systems MIT Press p Luo Z Wahba G Hybrid Adaptive Splines J Amer Statist Assoc p MacKay D J C 1992a A Practical Bayesian Framework Backpropagation Networks Neural Computation pp MacKay D J C 1992b Bayesian Interpolation Neural Computation pp xiii xvi MacKay D J C 1992c Information Based Objective Functions Active Data Selection Neural Computation p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography MacKay D J C 1992d The Evidence Framework Applied Classification Networks Neural Com putation p MacKay D J C Introduction Gaussian Processes In Bishop C M editor Neural Networks Machine Learning Springer Verlag pp MacKay D J C Comparison Approximate Methods Handling Hyperparameters Neural Computation p MacKay D J C Information Theory Inference Learning Algorithms Cambridge Univer sity Press Cambridge UK pp xiv Malzahn D Opper M A Variational Approach Learning Curves In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems MIT Press p Mandelbrot B B The Fractal Geometry Nature W H Freeman San Francisco p Mardia K V Marshall R J Maximum Likelihood Estimation Models Residual Covariance Spatial Regression Biometrika p Mate rn B Spatial Variation Meddelanden fr Statens Skogsforskningsinstitut No Alma nna Fo rlaget Stockholm Second edition Springer Verlag Berlin pp Matheron G The Intrinsic Random Functions Their Applications Advances Applied Probability p Maxwell J C Letter Lewis Campbell reproduced L Campbell W Garrett The Life James Clerk Maxwell Macmillan p v McAllester D PAC Bayesian Stochastic Model Selection Machine Learning p McCullagh P Nelder J Generalized Linear Models Chapman Hall pp Meinguet J Multivariate Interpolation Arbitrary Points Made Simple Journal Applied Mathematics Physics ZAMP p Meir R Zhang T Generalization Error Bounds Bayesian Mixture Algorithms Journal Machine Learning Research p Micchelli C A Pontil M Kernels Multi task Learning In Saul L K Weiss Y Bottou L editors Advances Neural Information Processing Systems pages MIT Press p Micchelli C A Wahba G Design Problems Optimal Surface Interpolation In Ziegler Z editor Approximation Theory Applications pages Academic Press p Minka T P A Family Algorithms Approximate Bayesian Inference PhD thesis Mas sachusetts Institute Technology pp Minka T P A Comparison Numerical Optimizers Logistic Regression http research microsoft com minka papers logreg p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Minka T P Picard R W Learning How Learn Learning With Point Sets http research microsoft com minka papers point sets html p Mitchell T M Machine Learning McGraw Hill New York pp Murray Smith R Girard A Gaussian Process priors ARMA noise models In Irish Signals Systems Conference pages Maynooth http www dcs gla ac uk rod publications MurGir01 pdf p Neal R M Bayesian Learning Neural Networks Springer New York Lecture Notes Statistics pp xiii Neal R M Monte Carlo Implementation Gaussian Process Models Bayesian Regres sion Classification Technical Report Department Statistics University Toronto http www cs toronto edu radford p Neal R M Regression Classification Gaussian Process Priors In Bernardo J M Berger J O Dawid A P Smith A F M editors Bayesian Statistics pages Oxford University Press discussion pp Neal R M Annealed Importance Sampling Statistics Computing p Neumaier A Introduction Global Optimization http www mat univie ac neum glopt intro html p O Hagan A Curve Fitting Optimal Design Prediction Journal Royal Statistical Society B discussion pp O Hagan A Bayes Hermite Quadrature Journal Statistical Planning Inference pp O Hagan A Kennedy M C Oakley J E Uncertainty Analysis Inference Tools Complex Computer Codes In Bernardo J M Berger J O Dawid A P Smith A F M editors Bayesian Statistics pages Oxford University Press discussion p ksendal B Stochastic Differential Equations Springer Verlag Berlin p Opper M Vivarelli F General Bounds Bayes Errors Regression Gaussian Processes In Kearns M S Solla S A Cohn D A editors Advances Neural Information Processing Systems pages MIT Press p Opper M Winther O Gaussian Processes Classification Mean Field Algorithms Neural Computation pp O Sullivan F Yandell B S Raynor W J Automatic Smoothing Regression Functions Generalized Linear Models Journal American Statistical Association pp Paciorek C Schervish M J Nonstationary Covariance Functions Gaussian Process Re gression In Thrun S Saul L Scho lkopf B editors Advances Neural Information Processing Systems MIT Press pp Papoulis A Probability Random Variables Stochastic Processes McGraw Hill New York Third Edition pp C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Plaskota L Noisy Information Computational Complexity Cambridge University Press Cambridge pp Plate T A Accuarcy versus Interpretability Flexible Modeling Implementing Tradeoff Gaussian Process Models Behaviourmetrika p Platt J C Fast Training Support Vector Machines Using Sequential Minimal Optimization In Scho lkopf B Burges C J C Smola A J editors Advances Kernel Methods pages MIT Press p Platt J C Probabilities SV Machines In Smola A Bartlett P Scho lkopf B Schuurmans D editors Advances Large Margin Classifiers pages MIT Press pp Poggio T Girosi F Networks Approximation Learning Proceedings IEEE pp Poggio T Voorhees H Yuille A A Regularized Solution Edge Detection Technical Report AI Memo MIT AI Laboratory p Pontil M Mukherjee S Girosi F On Noise Model Support Vector Machine Regression Technical Report AI Memo MIT AI Laboratory p Press W H Teukolsky S A Vetterling W T Flannery B P Numerical Recipes C Cambridge University Press Second edition pp Quin onero Candela J Learning Uncertainty Gaussian Processes Relevance Vector Machines PhD thesis Informatics Mathematical Modelling Technical Univeristy Denmark p Rasmussen C E Evaluation Gaussian Processes Other Methods Non linear Re gression PhD thesis Dept Computer Science University Toronto http www kyb mpg de publications pss ps2304 ps p Rasmussen C E Gaussian Processes Speed Hybrid Monte Carlo Expensive Bayesian Integrals In Bernardo J M Bayarri M J Berger J O Dawid A P Heckerman D Smith A F M West M editors Bayesian Statistics pages Oxford University Press p Rasmussen C E Ghahramani Z Occam s Razor In Leen T Dietterich T G Tresp V editors Advances Neural Information Processing Systems pages MIT Press p Rasmussen C E Ghahramani Z Infinite Mixtures Gaussian Process Experts In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems MIT Press p Rasmussen C E Ghahramani Z Bayesian Monte Carlo In Suzanna Becker S T Obermayer K editors Advances Neural Information Processing Systems pages MIT Press p Rasmussen C E Quin onero Candela J Healing Relevance Vector Machine Augmentation In Proc 22nd International Conference Machine Learning pp C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Rifkin R Klautau A In Defense One Vs All Classification Journal Machine Learning Research pp Ripley B Spatial Statistics Wiley New York p Ripley B Pattern Recognition Neural Networks Cambridge University Press Cambridge UK p Ritter K Average Case Analysis Numerical Problems Springer Verlag pp Ritter K Wasilkowski G W Woz niakowski H Multivariate Integration Approxima tion Random Fields Satisfying Sacks Ylvisaker Conditions Annals Applied Probability p Rousseeuw P J Least Median Squares Regression Journal American Statistical Association p Sacks J Welch W J Mitchell T J Wynn H P Design Analysis Computer Experiments Statistical Science pp Saitoh S Theory Reproducing Kernels Applications Longman Harlow England p Salton G Buckley C Term weighting Approaches Automatic Text Retrieval Information Processing Management p Sampson P D Guttorp P Nonparametric Estimation Nonstationary Covariance Struc ture Journal American Statistical Association p Santner T J Williams B J Notz W The Design Analysis Computer Experiments Springer New York p Saunders C Gammerman A Vovk V Ridge Regression Learning Algorithm Dual Variables In Shavlik J editor Proceedings Fifteenth International Conference Machine Learning ICML Morgan Kaufmann p Saunders C Shawe Taylor J Vinokourov A String Kernels Fisher Kernels Finite State Automata In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press p Schoenberg I J Metric Spaces Positive Definite Functions Trans American Mathematical Society p Schoenberg I J Spline Functions Problem Graduation Proc Nat Acad Sci USA pp Scho lkopf B Smola A J Learning Kernels MIT Press pp xvi Scho lkopf B Smola A J Mu ller K R Nonlinear Component Analysis Kernel Eigenvalue Problem Neural Computation p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Schwaighofer A Tresp V Transductive Inductive Methods Approximate Gaus sian Process Regression In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press pp Scott D W Multivariate Density Estimation Wiley New York p Seeger M Bayesian Model Selection Support Vector Machines Gaussian Processes Other Kernel Classifiers In Solla S A Leen T K Mu ller K R editors Advances Neural Information Processing Systems MIT Press Cambridge MA pp Seeger M PAC Bayesian Generalisation Error Bounds Gaussian Process Classification Journal Machine Learning Research pp Seeger M Bayesian Gaussian Process Models PAC Bayesian Generalisation Error Bounds Sparse Approximations PhD thesis School Informatics University Edinburgh http www cs berkeley edu mseeger pp Seeger M Expectation Propagation Exponential Families http www cs berkeley edu mseeger papers epexpfam ps gz p Seeger M Jordan M I Sparse Gaussian Process Classification With Multiple Classes Technical Report TR Department Statistics University California Berkeley p Seeger M Williams C K I Lawrence N Fast Forward Selection Speed Up Sparse Gaussian Process Regression In Bishop C Frey B J editors Proceedings Ninth In ternational Workshop Artificial Intelligence Statistics Society Artificial Intelligence Statistics pp Shawe Taylor J Williams C K I The Stability Kernel Principal Components Analysis Relation Process Eigenspectrum In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press p Shepp L A Radon Nikodym Derivatives Gaussian Measures Annals Mathematical Statis tics p Silverman B W Density Ratios Empirical Likelihood Cot Death Applied Statistics p Silverman B W Spline Smoothing The Equivalent Variable Kernel Method Annals Statistics pp Silverman B W Some Aspects Spline Smoothing Approach Non parametric Regression Curve Fitting discussion J Roy Stat Soc B pp Simard P Victorri B Le Cun Y Denker J Tangent Prop A Formalism Specifying Selected Invariances Adaptive Network In Moody J E Hanson S J Lippmann R P editors Advances Neural Information Processing Systems pages Morgan Kaufmann pp Smola A J Bartlett P L Sparse Greedy Gaussian Process Regression In Leen T K Diettrich T G Tresp V editors Advances Neural Information Processing Systems pages MIT Press p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Smola A J Scho lkopf B Sparse Greedy Matrix Approximation Machine Learning In Proceedings Seventeenth International Conference Machine Learning Morgan Kaufmann pp Solak E Murray Smith R Leithead W E Leith D Rasmussen C E Derivative Observations Gaussian Process Models Dynamic Systems In Becker S S T Obermayer K editors Advances Neural Information Processing Systems pages MIT Press p Sollich P Learning Curves Gaussian Processes In Kearns M S Solla S A Cohn D A editors Neural Information Processing Systems Vol MIT Press pp Sollich P Bayesian Methods Support Vector Machines Evidence Predictive Class Probabilities Machine Learning pp Sollich P Williams C K I Using Equivalent Kernel Understand Gaussian Process Regression In Saul L K Weiss Y Bottou L editors Advances Neural Information Processing Systems MIT Press p Stein M L A Kernel Approximation Kriging Predictor Spatial Process Ann Inst Statist Math p Stein M L Interpolation Spatial Data Springer Verlag New York pp Steinwart I Consistency Support Vector Machines Other Regularized Kernel Classifiers IEEE Trans Information Theory p Stitson M O Gammerman A Vapnik V N Vovk V Watkins C J C H Weston J Support Vector Regression ANOVA Decomposition Kernels In Scho lkopf B Burges C J C Smola A J editors Advances Kernel Methods MIT Press p Sundararajan S Keerthi S S Predictive Approaches Choosing Hyperparameters Gaussian Processes Neural Computation p Suykens J A K Vanderwalle J Least Squares Support Vector Machines Neural Processing Letters p Szeliski R Regularization uses Fractal Priors In Proceedings 6th National Conference Artificial Intelligence AAAI pp Teh Y W Seeger M Jordan M I Semiparametric Latent Factor Models In Cowell R G Ghahramani Z editors Proceedings Tenth International Workshop Artificial Inte lligence Statistics pages Society Artificial Intelligence Statistics p Thomas Agnan C Computing Family Reproducing Kernels Statistical Applications Numerical Algorithms p Thompson P D Optimum Smoothing Two Dimensional Fields Tellus p Tikhonov A N Solution Incorrectly Formulated Problems Regularization Method Soviet Math Dokl p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Tikhonov A N Arsenin V Y Solutions I shall posed Problems W H Winston Washington D C p Tipping M E Sparse Bayesian Learning Relevance Vector Machine Journal Machine Learning Research p Tipping M E Faul A C Fast Marginal Likelihood Maximisation Sparse Bayesian Models In Bishop C M Frey B J editors Proceedings Ninth International Workshop Artificial Intelligence Statistics Society Artificial Intelligence Statistics p Tresp V A Bayesian Committee Machine Neural Computation pp Tsuda K Kawanabe M Ra tsch G Sonnenburg S Mu ller K R A New Discriminative Kernel Probabilistic Models Neural Computation p Uhlenbeck G E Ornstein L S On Theory Brownian Motion Phys Rev pp Valiant L G A Theory Learnable Communications ACM p Vapnik V N The Nature Statistical Learning Theory Springer Verlag New York pp Vapnik V N Statistical Learning Theory John Wiley Sons p Vijayakumar S D Souza A Schaal S Incremental Online Learning High Dimensions Accepted publication Neural Computation pp Vijayakumar S D Souza A Shibata T Conradt J Schaal S Statistical Learning Humanoid Robots Autonomous Robot p Vijayakumar S Schaal S LWPR An O n Algorithm Incremental Real Time Learning High Dimensional Space In Proc Seventeenth International Conference Machine Learning ICML pages p Vishwanathan S V N Smola A J Fast Kernels String Tree Matching In Becker S Thrun S Obermayer K editors Advances Neural Information Processing Systems MIT Press p Vivarelli F Williams C K I Discovering Hidden Features Gaussian Processes Regression In Kearns M S Solla S A Cohn D A editors Advances Neural Information Processing Systems MIT Press p von Mises R Mathematical Theory Probability Statistics Academic Press p Wahba G Improper Priors Spline Smoothing Problem Guarding Against Model Errors Regression Journal Royal Statistical Society B p Wahba G A Comparison GCV GML Choosing Smoothing Parameter Generalized Spline Smoothing Problem Annals Statistics p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Wahba G Spline Models Observational Data Society Industrial Applied Mathematics Philadelphia PA CBMS NSF Regional Conference series applied mathematics pp Wahba G Johnson D R Gao F Gong J Adaptive Tuning Numerical Weather Prediction Models Randomized GCV Three Four Dimensional Data Assimilation Monthly Weather Review p Watkins C J C H Dynamic Alignment Kernels Technical Report CSD TR Dept Computer Science Royal Holloway University London p Watkins C J C H Dynamic Alignment Kernels In Smola A J Bartlett P L Scho lkopf B editors Advances Large Margin Classifiers pages MIT Press Cambridge MA p Wegman E J Reproducing Kernel Hilbert Spaces In Kotz S Johnson N L editors Encyclopedia Statistical Sciences volume pages Wiley New York pp Weinert H L editor Reproducing Kernel Hilbert Spaces Hutchinson Ross Stroudsburg Pennsylvania p Wendland H Scattered Data Approximation Cambridge Monographs Applied Compu tational Mathematics Cambridge University Press p Whittle P Prediction Regulation Linear Least square Methods English Universities Press pp Widom H Asymptotic Behavior Eigenvalues Certain Integral Equations Trans American Mathematical Society p Widom H Asymptotic Behavior Eigenvalues Certain Integral Equations II Archive Rational Mechanics Analysis p Wiener N Extrapolation Interpolation Smoothing Stationary Time Series MIT Press Cambridge Mass p Williams C K I Computation Infinite Neural Networks Neural Computation p Williams C K I Barber D Bayesian Classification Gaussian Processes IEEE Transactions Pattern Analysis Machine Intelligence pp Williams C K I Rasmussen C E Gaussian Processes Regression In Touretzky D S Mozer M C Hasselmo M E editors Advances Neural Information Processing Systems pages MIT Press pp Williams C K I Rasmussen C E Schwaighofer A Tresp V Observations Nystro m Method Gaussian Process Prediction Technical report University Edinburgh http www dai ed ac uk homes ckiw online pubs html p Williams C K I Seeger M Using Nystro m Method Speed Up Kernel Machines In Leen T K Diettrich T G Tresp V editors Advances Neural Information Processing Systems pages MIT Press pp C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Bibliography Williams C K I Vivarelli F Upper Lower Bounds Learning Curve Gaussian Proccesses Machine Learning pp Winkler G Image Analysis Random Fields Dynamic Monte Carlo Methods Springer Berlin p Wong E Stochastic Processes Information Dynamical Systems McGraw Hill New York p Wood S Kohn R A Bayesian Approach Robust Binary Nonparametric Regression J American Statistical Association p Yaglom A M Correlation Theory Stationary Related Random Functions Volume I Basic Results Springer Verlag p Yang C Duraiswami R David L Efficient Kernel Machines Using Improved Fast Gauss Transform In Saul L K Weiss Y Bottou L editors Advances Neural Information Processing Systems MIT Press p Ylvisaker D Designs Random Fields In Srivastava J N editor A Survey Statistical Design Linear Models pages North Holland p Yuille A Grzywacz N M A Mathematical Analysis Motion Coherence Theory Inter national Journal Computer Vision p Zhang T Statistical Behaviour Consistency Classification Methods based Convex Risk Minimization discussion Annals Statistics p Zhu H Williams C K I Rohwer R J Morciniec M Gaussian Regression Optimal Finite Dimensional Linear Models In Bishop C M editor Neural Networks Machine Learning Springer Verlag Berlin p Zhu J Hastie T J Kernel Logistic Regression Import Vector Machine In Diettrich T G Becker S Ghahramani Z editors Advances Neural Information Processing Systems pages MIT Press p C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Author Index Abrahamsen P Abramowitz M Adams R Adler R J Amari S Ansley C F Ansley C F Kohn R Arato M Arfken G xv Aronszajn N Arsenin V Y Tikhonov A N Bach F R Baker C T H Balakrishnan N Johnson N L Barber D Barber D Williams C K I Bartle R G Bartlett P L Bartlett P L Lanckriet G R G Bartlett P L Smola A J Belongie S Fowlkes C Bengio S Collobert R Berger J O Bishop C M Bishop C M Goldberg P W Blake I F Blight B J N Boyd S Boyle P Bracewell R N Buckley C Salton G Caruana R Chatfield C Chellappa R Kashyap R L Choi T Choudhuri N Chow Y Grenander U Chu W Collins M Collobert R Conradt J Vijayakumar S Cornford D Cox D D Craven P Cressie N A C Cristianini N Cristianini N Lanckriet G R G Cristianini N Lodhi H Csato L Daley R David H A David L Yang C Davis P J Dawid A P Dellaportas P Denker J Simard P Devroye L Diaconis P Diekhans M Jaakkola T S Diggle P J Doob J L Drineas P D Souza A Vijayakumar S Duchon J Duda R O Duffy N Collins M Duraiswami R Yang C Eddy W F Geisser S C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Author Index Edgeworth F Y El Ghaoui L Lanckriet G R G Elisseeff A Cristianini N Eskin E Leslie C Faul A C Faul A C Tipping M E Feldman J Ferrari Trecate G Fine S Flannery B P Press W H Fowlkes C Frean M Boyle P Freedman D Freedman D Diaconis P Frieze A Gammerman A Saunders C Gammerman A Stitson M O Gao F Lin X Gao F Wahba G Geisser S Geman D Geman S Geman S Ghahramani Z Chu W Ghahramani Z Rasmussen C E Ghosal S Choudhuri N Gibbs M N Gihman I I Girard A Girard A Murray Smith R Girosi F Girosi F Poggio T Girosi F Pontil M Goldberg P W Golub G H Gong J Wahba G Gradshteyn I S Green P J Grenander U Grimmett G R Gru nwald P D Grzywacz N M Yuille A Guttorp P Sampson P D Gyo rfi L Gyo rfi L Devroye L Hajek J Hand D J Hannan E J Hansen L K Hart P E Duda R O Hastie T J Hastie T J Zhu J Haussler D Haussler D Jaakkola T S Hawkins D L Herbrich R Lawrence N Hinton G E Jacobs R A Hoerl A E Hornik K Huijbregts C J Journel A G Ihara S Jaakkola T S Jacobs R A Johnson D R Wahba G Johnson N L Jones D R Jones M Girosi F Jordan M I Jordan M I Bach F R Jordan M I Bartlett P L Jordan M I Jacobs R A Jordan M I Lanckriet G R G Jordan M I Seeger M Jordan M I Teh Y W Journel A G Kailath T Kammler D W Kandola J Cristianini N Kannan R Frieze A Kashyap R L Kawanabe M Tsuda K Keeling C D Keenan D M Grenander U Keerthi S S Sundararajan S Kennard R W Hoerl A E Kennedy M C O Hagan A Kent J T Kimeldorf G Klautau A Rifkin R Klein B Lin X C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Author Index Klein R Lin X Kohler M Gyo rfi L Kohn R Kohn R Ansley C F Kohn R Wood S Kolmogorov A N Ko nig H Kotz S Johnson N L Krzyz ak A Gyo rfi L Kullback S Kuss M Lanckriet G R G Langford J Gru nwald P D Lauritzen S L Lawrence N Lawrence N Seeger M Le Cun Y Simard P Leith D Solak E Leithead W E Solak E Leslie C Liisberg C Hansen L K Lin X Lindley D V Lindsey W C Blake I F Lodhi H Lugosi G Devroye L Luo Z MacKay D J C xiii xiv xvi MacKay D J C Gibbs M N Mahoney M W Drineas P Malik J Fowlkes C Malzahn D Mandelbrot B B Mannila H Hand D J Mardia K V Mardia K V Kent J T Marshall R J Mardia K V Mate rn B Matheron G Maxwell J C v McAllester D McAuliffe J D Bartlett P L McCullagh P Meinguet J Meir R Micchelli C A Minka T P Mitchell T J Sacks J Mitchell T M Morciniec M Zhu H Moyeed R A Diggle P J Mukherjee S Pontil M Mu ller K R Scho lkopf B Mu ller K R Tsuda K Murray Smith R Murray Smith R Girard A Murray Smith R Solak E Nabney I T Cornford D Neal R M xiii Nelder J McCullagh P Neumaier A Notz W Santner T J Nowlan S J Jacobs R A Oakley J E O Hagan A O Hagan A ksendal B Opper M Opper M Csato L Opper M Ferrari Trecate G Opper M Malzahn D Ornstein L S Uhlenbeck G E O Sullivan F Cox D D O Sullivan F Ott L Blight B J N Paciorek C Papoulis A Picard R W Minka T P Plaskota L Plate T A Platt J C Poggio T Poggio T Girosi F Pontil M Pontil M Micchelli C A Press W H Quin onero Candela J Quin onero Candela J Girard A Quin onero Candela J Rasmussen C E C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Author Index Rasmussen C E Rasmussen C E Girard A Rasmussen C E Kuss M Rasmussen C E Solak E Rasmussen C E Williams C K I Ra tsch G Tsuda K Raynor W J O Sullivan F Rifkin R Ripley B Ritter K Rohwer R J Zhu H Rousseeuw P J Roy A Choudhuri N Ryzhik I M Gradshteyn I S Saad D Barber D Sacks J Saitoh S Salamon P Hansen L K Salton G Sampson P D Santner T J Saunders C Schaal S Vijayakumar S Scheinberg K Fine S Schervish M J Choi T Schervish M J Paciorek C Schoenberg I J Scho lkopf B xvi Scho lkopf B Smola A J Schwaighofer A Schwaighofer A Williams C K I Scott D W Seeger M Seeger M Lawrence N Seeger M Teh Y W Seeger M Williams C K I Shawe Taylor J Shawe Taylor J Cristianini N Shawe Taylor J Lodhi H Shawe Taylor J Saunders C Shepp L A Shibata T Vijayakumar S Silverman B W Silverman B W Green P J Simard P Skorohod A V Gihman I I Smola A J Smola A J Scho lkopf B xvi Smola A J Vishwanathan S V N Smyth P Hand D J Solak E Sollich P Sonnenburg S Tsuda K Stafford Noble W Leslie C Stegun I A Abramowitz M Stein M L Steinwart I Stephens D A Dellaportas P Stirzaker D R Grimmett G R Stitson M O Sundararajan S Suykens J A K Svensen M Bishop C M Szeliski R Tawn J A Diggle P J Teh Y W Teukolsky S A Press W H Thomas Agnan C Thompson P D Tibshirani R J Hastie T J Tikhonov A N Tipping M E Tipping M E Faul A C Tresp V Tresp V Schwaighofer A Tresp V Williams C K I Tsuda K Uhlenbeck G E Valiant L G Van Loan C F Golub G H Vandenberghe L Boyd S Vanderwalle J Suykens J A K Vapnik V N Vapnik V N Stitson M O Vempala S Frieze A Vetterling W T Press W H C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Author Index Victorri B Simard P Vijayakumar S Vinokourov A Saunders C Vishwanathan S V N Vivarelli F Vivarelli F Opper M Vivarelli F Williams C K I von Mises R Voorhees H Poggio T Vovk V Saunders C Vovk V Stitson M O Wahba G Wahba G Craven P Wahba G Kimeldorf G Wahba G Lin X Wahba G Luo Z Wahba G Micchelli C A Walk H Gyo rfi L Wasilkowski G W Ritter K Watkins C J C H Watkins C J C H Lodhi H Watkins C J C H Stitson M O Wegman E J Welch W J Sacks J Wendland H Weston J Leslie C Weston J Stitson M O Whittle P Whorf T P Keeling C D Widom H Wiener N Williams B J Santner T J Williams C K I Williams C K I Bishop C M Williams C K I Cornford D Williams C K I Ferrari Trecate G Williams C K I Goldberg P W Williams C K I Seeger M Williams C K I Shawe Taylor J Williams C K I Sollich P Williams C K I Vivarelli F Williams C K I Zhu H Winkler G Winther O Csato L Winther O Opper M Wong E Wood S Woz niakowski H Ritter K Wynn H P Sacks J Xiang D Lin X Yaglom A M Yandell B S O Sullivan F Yang C Ylvisaker D Yuille A Yuille A Poggio T Zhang T Zhang T Meir R Zhu H Zhu J C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Subject Index alignment anisotropy AR process autoregressive process ARD automatic relevance determination ARMA process autoregressive moving average process automatic relevance determination autoregressive moving average process noise model autoregressive process Bayes classifier Bayes theorem Bayesian committee machine BCM Bayesian committee machine bias inductive binary classification bits blitzkrieging fast Gaussian processes Bochner s theorem Brownian bridge Brownian motion Wiener process canonical hyperplane Cholesky decomposition Christ bowels classification binary squares probabilistic multi class probabilistic classifier Gibbs predictive cokriging consistency convex function set covariance predictive covariance function kernel ANOVA compact support dot product exponential exponential Gaussian covariance function squared ex ponential inhomogeneous polynomial Mate rn neural network Ornstein Uhlenbeck OU covariance function Ornstein Uhlenbeck periodic polynomial piecewise radial basis function covariance function squared exponential rational quadratic RBF covariance function squared expo nential SE covariance function squared exponen tial squared exponential covariance matrix Cromwell s dictum cross validation generalized leave C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Subject Index dataset robot inverse dynamics USPS decision region decision surface degenerate kernel degenerate degrees freedom derivative observations Dirichlet process discriminative approach eigenfunction eigenvalue entropy EP expectation propagation insensitive error function equivalent kernel error generalization error function insensitive hinge error reject curve errors variables regression evidence marginal likelihood expectation propagation experimental design optimal factor analysis feature space Fisher information matrix Fisher kernel Fourier transform fractal gamma distribution Gaussian distribution Gaussian Markov process Gaussian process Gaussian process classification Gaussian process latent variable model Gaussian process regression generalization error generative approach generative topographic mapping geostatistics GMP Gaussian Markov process GP Gaussian process GPC Gaussian process classification GPLVM Gaussian process latent variable model GPR Gaussian process regression Gram matrix Green s function GTM generative topographic mapping hidden Markov model hinge error function hyperparameters hyperplane canonical index set informative vector machine integrals evaluation intrinsic random function invariances IRLS iteratively reweighted squares isotropy iteratively reweighted squares IVM informative vector machine jitter kernel covariance function bag characters degenerate equivalent Fisher k spectrum nondegenerate positive definite string tangent posterior odds kernel classifier kernel PCA kernel ridge regression regression ridge kernel kernel smoother kernel trick kriging Kullback Leibler divergence Laplace approximation latent variable model learning curve learning supervised C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Subject Index squares classification leave leave cross validation length scale characteristic likelihood logistic multiple logistic non Gaussian probit linear classifier linear regression link function log odds ratio logistic function logistic regression LOO leave loss negative log probability squared zero loss function loss matrix LSC squares classification MAP maximum posteriori margin functional geometrical marginal likelihood marginalization property Markov chain Monte Carlo Markov random field matrix covariance Fisher information Gram inversion lemma loss partitioned inversion positive definite positive semidefinite maximum posteriori penalized maximum like lihood maximum likelihood penalized MCMC Markov chain Monte Carlo mean function mean square continuity mean square differentiability mean standardized log loss mean field approximation measure Mercer s theorem mixture experts ML II type II maximum likelihood model non parametric parametric semi parametric Moore Aronszajn theorem MS continuity mean square continuity MS differentiability mean square differentiabil ity MSLL mean standardized log loss multi class classification multi task learning multiple outputs Nadaraya Watson estimator nats neural network Newton s method noise model correlated heteroscedastic norm Frobenius null space Nystro m approximation Nystro m method Occam s razor versus rest operator integral optimal experimental design outputs multiple P1NN probabilistic nearest neighbour PAC Bayesian theorem McAllester Seeger penalized maximum likelihood estimate PLSC probabilistic squares classification positive definite matrix positive semidefinite matrix C E Rasmussen C K I Williams Gaussian Processes Machine Learning MIT Press ISBN 026218253X c Massachusetts Institute Technology www GaussianProcess org gpml Subject Index posterior process PP projected process approximation prediction classification averaged MAP probabilistic classification probabilistic squares classification probabilistic nearest neighbour probability conditional joint marginal probit regression projected process approximation pseudo likelihood quadratic form quadratic programming regression errors variables Gaussian process linear polynomial ridge kernel regularization regularization network reject option relative entropy Kullback Leibler divergence relevance vector machine representer theorem reproducing kernel Hilbert space response function ridge regression regression ridge risk RKHS reproducing kernel Hilbert space RVM relevance vector machine scale mixture SD subset datapoints SDE stochastic differential equation SMSE standardized mean squared error softmax splines SR subset regressors standardized mean squared error stationarity stochastic differential equation Student s t process subset datapoints subset regressors supervised learning support vector support vector machine soft margin support vector regression SVM support vector machine SVR support vector regression tangent posterior odds kernel TOP kernel tangent posterior odds kernel transduction type II maximum likelihood uncertain inputs upcrossing rate USPS dataset weight function weight vector Wiener process integrated tied Wiener Khintchine theorem Yule Walker equations Series Foreword Preface Symbols Notation Introduction A Pictorial Introduction Bayesian Modelling Roadmap Regression Weight space View The Standard Linear Model Projections Inputs Feature Space Function space View Varying Hyperparameters Decision Theory Regression An Example Application Smoothing Weight Functions Equivalent Kernels Incorporating Explicit Basis Functions Marginal Likelihood History Related Work Exercises Classification Classification Problems Decision Theory Classification Linear Models Classification Gaussian Process Classification The Laplace Approximation Binary GP Classifier Posterior Predictions Implementation Marginal Likelihood Multi class Laplace Approximation Implementation Expectation Propagation Predictions Marginal Likelihood Implementation Experiments A Toy Problem One dimensional Example Binary Handwritten Digit Classification Example class Handwritten Digit Classification Example Discussion Appendix Moment Derivations Exercises Covariance Functions Preliminaries Mean Square Continuity Differentiability Examples Covariance Functions Stationary Covariance Functions Dot Product Covariance Functions Other Non stationary Covariance Functions Making New Kernels Old Eigenfunction Analysis Kernels An Analytic Example Numerical Approximation Eigenfunctions Kernels Non vectorial Inputs String Kernels Fisher Kernels Exercises Model Selection Adaptation Hyperparameters The Model Selection Problem Bayesian Model Selection Cross validation Model Selection GP Regression Marginal Likelihood Cross validation Examples Discussion Model Selection GP Classification Derivatives Marginal Likelihood Laplace s Approximation Derivatives Marginal Likelihood EP Cross validation Example Exercises Relationships GPs Other Models Reproducing Kernel Hilbert Spaces Regularization Regularization Defined Differential Operators Obtaining Regularized Solution The Relationship Regularization View Gaussian Process Prediction Spline Models A d Gaussian Process Spline Construction Support Vector Machines Support Vector Classification Support Vector Regression Least squares Classification Probabilistic Least squares Classification Relevance Vector Machines Exercises Theoretical Perspectives The Equivalent Kernel Some Specific Examples Equivalent Kernels Asymptotic Analysis Consistency Equivalence Orthogonality Average case Learning Curves PAC Bayesian Analysis The PAC Framework PAC Bayesian Analysis PAC Bayesian Analysis GP Classification Comparison Other Supervised Learning Methods Appendix Learning Curve Ornstein Uhlenbeck Process Exercises Approximation Methods Large Datasets Reduced rank Approximations Gram Matrix Greedy Approximation Approximations GPR Fixed Hyperparameters Subset Regressors The Nystrom Method Subset Datapoints Projected Process Approximation Bayesian Committee Machine Iterative Solution Linear Systems Comparison Approximate GPR Methods Approximations GPC Fixed Hyperparameters Approximating Marginal Likelihood Derivatives Appendix Equivalence SR GPR Using Nystrom Approximate Kernel Exercises Further Issues Conclusions Multiple Outputs Noise Models Dependencies Non Gaussian Likelihoods Derivative Observations Prediction Uncertain Inputs Mixtures Gaussian Processes Global Optimization Evaluation Integrals Student s t Process Invariances Latent Variable Models Conclusions Future Directions Appendix Mathematical Background Joint Marginal Conditional Probability Gaussian Identities Matrix Identities Matrix Derivatives Matrix Norms Cholesky Decomposition Entropy Kullback Leibler Divergence Limits Measure Integration Lp Spaces Fourier Transforms Convexity Appendix Gaussian Markov Processes Fourier Analysis Sampling Periodization Continuous time Gaussian Markov Processes Continuous time GMPs R The Solution Corresponding SDE Circle Discrete time Gaussian Markov Processes Discrete time GMPs Z The Solution Corresponding Difference Equation PN The Relationship Between Discrete time Sampled Continuous time GMPs Markov Processes Higher Dimensions Appendix Datasets Code Bibliography Author Index Subject Index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDrYDQbwKzJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "989b5d52-3146-4ab5-e321-3ea522e51e9b"
      },
      "source": [
        "\n",
        "df['suggestion']=df[0].apply(lambda x: spell_suggestion(x))\n",
        "\n",
        "df.sample(10)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>stop_words_len</th>\n",
              "      <th>no_of_digits</th>\n",
              "      <th>suggestion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Algorithms Reinforcement Learning Draft lectur...</td>\n",
              "      <td>39528</td>\n",
              "      <td>227103</td>\n",
              "      <td>5.745370</td>\n",
              "      <td>12329</td>\n",
              "      <td>381</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Course Machine Learning A Course Machine Lea...</td>\n",
              "      <td>76311</td>\n",
              "      <td>426100</td>\n",
              "      <td>5.583730</td>\n",
              "      <td>29439</td>\n",
              "      <td>1198</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB Data Mining Python sources KB Neural Data M...</td>\n",
              "      <td>34739</td>\n",
              "      <td>213100</td>\n",
              "      <td>6.134316</td>\n",
              "      <td>5810</td>\n",
              "      <td>4555</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introduction Machine Learning Fall Amnon Shash...</td>\n",
              "      <td>36716</td>\n",
              "      <td>191442</td>\n",
              "      <td>5.214130</td>\n",
              "      <td>12083</td>\n",
              "      <td>705</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disruptive Possibilities http oreil ly 1T0KbBh...</td>\n",
              "      <td>26064</td>\n",
              "      <td>161348</td>\n",
              "      <td>6.190454</td>\n",
              "      <td>10598</td>\n",
              "      <td>217</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Artificial Intelligence A Modern Approach Stua...</td>\n",
              "      <td>447365</td>\n",
              "      <td>2663911</td>\n",
              "      <td>5.954670</td>\n",
              "      <td>168319</td>\n",
              "      <td>7361</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gsl_stats March Modeling Data gsl_stats March ...</td>\n",
              "      <td>164032</td>\n",
              "      <td>956177</td>\n",
              "      <td>5.829210</td>\n",
              "      <td>62575</td>\n",
              "      <td>4366</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My title Bayesian Reasoning Machine Learning D...</td>\n",
              "      <td>315097</td>\n",
              "      <td>1699846</td>\n",
              "      <td>5.394675</td>\n",
              "      <td>86801</td>\n",
              "      <td>9762</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gaussian Processes Machine Learning C E Rasmus...</td>\n",
              "      <td>118816</td>\n",
              "      <td>691520</td>\n",
              "      <td>5.820092</td>\n",
              "      <td>35875</td>\n",
              "      <td>3962</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dropbox SzeliskiBookDraft_20210930 pdf Simplif...</td>\n",
              "      <td>7</td>\n",
              "      <td>61</td>\n",
              "      <td>8.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0  ...  suggestion\n",
              "8   Algorithms Reinforcement Learning Draft lectur...  ...          []\n",
              "3   A Course Machine Learning A Course Machine Lea...  ...          []\n",
              "0   KB Data Mining Python sources KB Neural Data M...  ...          []\n",
              "7   Introduction Machine Learning Fall Amnon Shash...  ...          []\n",
              "1   Disruptive Possibilities http oreil ly 1T0KbBh...  ...          []\n",
              "10  Artificial Intelligence A Modern Approach Stua...  ...          []\n",
              "5   gsl_stats March Modeling Data gsl_stats March ...  ...          []\n",
              "4   My title Bayesian Reasoning Machine Learning D...  ...          []\n",
              "9   Gaussian Processes Machine Learning C E Rasmus...  ...          []\n",
              "6   Dropbox SzeliskiBookDraft_20210930 pdf Simplif...  ...          []\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBNzruJPtF0"
      },
      "source": [
        "## **Converting df to text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M6K5xKjPqWO",
        "outputId": "17fb5344-c1a2-4d16-f74b-5b03acb10bf8"
      },
      "source": [
        "text_data = ' '.join(df[0])\n",
        "print(len(text_data))\n",
        "with open(\"/content/training_text_data.txt\", \"w\") as f:\n",
        "  f.write(text_data)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACNDGL_KTBIL"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx6-RFu1XGsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbc347f-85a1-41aa-ee6c-8a1ab3fc2b39"
      },
      "source": [
        "! pip install wordcloud"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTbowFXWXM7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "dfedba84-080d-49bf-9ec1-30826c5629d1"
      },
      "source": [
        "# visualizing data \n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# converting dataframe to string\n",
        "text = ' '.join(df[0])\n",
        "\n",
        "# length of text\n",
        "print('length of text: ', len(text))\n",
        "\n",
        "# wordcloud\n",
        "wc = WordCloud(width=1600, height=1000).generate(text)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of text:  4690794\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lr1BtNNYBgy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}